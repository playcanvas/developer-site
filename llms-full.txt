# PlayCanvas Developer Documentation - Full Content

> This file contains the complete text content of the PlayCanvas documentation.
> It is designed for consumption by Large Language Models (LLMs) and AI assistants.

Base URL: https://developer.playcanvas.com
Total Documents: 552
Generated: 2026-01-14

================================================================================

## 360 lookaround camera

URL: https://developer.playcanvas.com/tutorials/360-lookaround-camera/
Tags: input, camera, tutorial

Sample showing how to move the camera with mouse and touch to look around.

[Interactive Demo]

--------------------------------------------------------------------------------

## Additive Loading Scenes

URL: https://developer.playcanvas.com/tutorials/additive-loading-scenes/
Tags: loading, scenes

Full documentation for Loading Scenes is now in the [User Manual](/user-manual/editor/scenes/loading-scenes/).

[Interactive Demo]

--------------------------------------------------------------------------------

## Advance loading screen

URL: https://developer.playcanvas.com/tutorials/advance-loading-screen/
Tags: loading, tutorial

Project sample showing how to use project image assets in the loading screen.

[Interactive Demo]

--------------------------------------------------------------------------------

## Anim State Graph Blending

URL: https://developer.playcanvas.com/tutorials/anim-blending/
Tags: animation, basics

[Interactive Demo]

:::info

Click on screen to focus, then press the 'p' key to blend to a punch animation.

:::

This tutorial illustrates the basics of animation blending.

Objects in your scene may be animated; machines or characters are good examples of things that you might want to animate. Generally, when 3D content is created, individual animations are authored and these animations are typically referred to as cycles (because they loop). For example, a human character could have an idle cycle, a walk cycle, a run cycle and so on. As a PlayCanvas developer, you'll want a mechanism to play these animations back on your animated object. Additionally, you do not want these animations to 'pop' as one is switched for another. To remedy this, you should use animation blending which implements a smooth transition from one animation to another. This dramatically improves the visual fidelity of your animated object.
Let's examine how this is achieved via PlayCanvas...

## The Animstategraph Asset

In order to add animations to a model, we must first create an animstategraph asset. These assets control multiple animations associated with a single entity, determine when each of those animations should play and how they should be blended together.

When creating an animstategraph asset, you’ll be presented with its editor:

[Image: Initial Editor]

By default, the animstategraph asset creates a single animation state called ‘Initial State’ which has a default transition from the START state to it. This means when the animstategraph first begins playing, it’ll immediately begin playing the Initial State. This can be renamed to Idle which is the name of the initial animation used in this tutorial. We can then right click the canvas and select `Add new state` to create another animation state in the graph. This state can be renamed to Punch:

[Image: New State]

Next we must tell the state graph how and when it should move between the Idle and Punch animation states. This is achieved by creating transitions. Right click the Idle animation and select `Add transition` from the context menu, then select the Punch animation state. This will create a transition moving from the Idle state to the Punch state.

[Image: Add Transition]

Selecting this transition will show the transition inspector in the panel to the right. Here the duration of the transition can be set, which will determine how long the blend between the two animations should last. We’ll set this to 0.2 seconds.

Next, we can set up a parameter which will determine when this transition can activate. First select the `+ parameter` on the parameters panel to the left. Then name this parameter `punch` and set it to the ‘Boolean’ type.

[Image: Add Parameter]

This parameter can now be set as a condition for the transition we just created. Select this transition then select `New Condition` in the transition inspector.

Set this transition to test against the newly created `punch` parameter and set the condition to ‘== true’ as follows:

[Image: Add Condition]

This will ensure that the transition from the Idle state to the Punch state only occurs once the punch parameter has been set to true.

Lastly, we need to create one more transition moving back from Punch to Idle as follows:

[Image: Complete State Graph]

Here we have set it up with the same duration but its condition tests whether the punch parameter is no longer true.

Now with this complete animstategraph, we must connect it to animation assets and the chosen entity. This is where the anim component comes in.

## The Anim Component

First, we must add the anim component to our chosen entity.

[Image: New Anim Component]

Once created it’ll display a slot for the animstategraph asset we just created. Drag this in and it’ll display animation slots for each of the animation states present in it. In this case it’ll have slots for the Idle and Punch states. Fill these slots with the appropriate animation assets and the anim component will become playable:

[Image: Complete Anim Component]

## Keyboard Input

With the animations fully set up, we now need to make it possible for users to interact with our system. This is where scripts come in! A script component is required to enable this behavior which will be written in the script `keyboard_controls.js`. You can see how it’s attached to the entity below:

[Image: Keyboard Input]

Remember that parameter `punch` that we set up before? This script will simply toggle that parameter on and off depending on whether the ‘P’ key is currently pressed and whether the character is currently punching:

From this point, you are able to add more and more animations to the animstategraph asset and start building much more complex animation state graphs!

See the full Scene [here](https://playcanvas.com/editor/scene/1065029)

--------------------------------------------------------------------------------

## Animate entities with curves

URL: https://developer.playcanvas.com/tutorials/animate-entities-with-curves/
Tags: animation, tutorial

Sample showing use of curves to do basic animation.

[Interactive Demo]

--------------------------------------------------------------------------------

## Animated Textures

URL: https://developer.playcanvas.com/tutorials/animated-textures/
Tags: animation, textures

[Interactive Demo]

*See the [full project](https://playcanvas.com/project/405882).*

It can be very useful to animate a material that has been applied to a surface. The example shown here is scrolling a texture to simulate some movement.

## Scrolling a material with map offset

The square plane in the example uses the script `scrolling-texture.js` to constantly move the UV offset every frame. For example, this can be used to simulate flowing water. The update loop is displayed below.

We calculate the required offset into a temporary vector `tmp`. This is simply: `speed * timeStep`. Then we add this offset to the offset property for both the diffuse map and the normal map by modifying the `diffuseMapOffset` and `normalMapOffset` properties. These values are `pc.Vec2`s which shift the UV co-ordinates used to map the texture to the surface. If you are using other maps (e.g. emissive) you will also need to update these offset properties as well. Finally we call `material.update()` to propagate the changes into the shader.

This is a simple straightforward method to modify a material's offset and scroll a texture. It does have one downside which is this code modifies the actual material's properties. So if you have multiple models in a scene with the same material, they will all be affected.

--------------------------------------------------------------------------------

## Animation Blending

URL: https://developer.playcanvas.com/tutorials/animation-blending/
Tags: animation

:::warning

This tutorial uses the deprecated Model and Animation components. Please refer to the [Anim State Graph Blending tutorial](/tutorials/anim-blending/) instead.

:::

[Interactive Demo]

:::info

Click on screen to focus, then press the 'p' key to blend to a punch animation.

:::

This tutorial illustrates the basics of animation blending.

Objects in your scene may be animated; machines or characters are good examples of things that you might want to animate. Generally, when 3D content is created, individual animations are authored and these animations are typically referred to as cycles (because they loop). For example, a human character could have an idle cycle, a walk cycle, a run cycle and so on. As a PlayCanvas developer, you'll want a mechanism to play these animations back on your animated object. Additionally, you do not want these animations to 'pop' as one is switched for another. To remedy this, you should use animation blending which implements a smooth transition from one animation to another. This dramatically improves the visual fidelity of your animated object.

Let's examine how this is achieved via PlayCanvas...

## The Animation Component

In order to apply an animation to a model, you add the animation component to your entity. Below is the configuration of the skinned character as displayed in PlayCanvas Editor.

[Image: Animated Entity]

In the image you can see the animation component in the Inspector. There are 2 animation assets assigned: an 'idle' cycle and a 'punch' cycle. With the animation component configured this way, the behavior is that the first animation (the idle cycle) is played and because the looping option is set, it will continue to animate ad infinitum. However, we would like to achieve something a little more interesting:

* Play a looping idle animation.
* Blend to a looping punch animation on a key press.
* Blend back to idle on key release.

So this kind of functionality goes beyond the abilities of the humble animation component. A script component is required to cook up this additional behavior. You can see the script component in the above screenshot of the skinned character entity in Editor and it refers to a JS file called animation_blending.js. The contents of this file is:

From this point, you are able to add more and more animations to the animation component and start scripting much more complex animation state charts.

See [the full Scene here](https://playcanvas.com/editor/scene/440156)

--------------------------------------------------------------------------------

## Animation without State Graph

URL: https://developer.playcanvas.com/tutorials/animation-without-state-graph/
Tags: animation, tutorial, scripts

Example project on how to add and play animations without using a state graph

[Interactive Demo]

--------------------------------------------------------------------------------

## Audio Effects

URL: https://developer.playcanvas.com/tutorials/audio-effects/
Tags: audio

[Interactive Demo]

*Click on the various buttons to try out different sound effects.*

:::note

This tutorial requires Web Audio API support.

:::

## Using The Web Audio API

PlayCanvas allows you to fully leverage the power of the Web Audio API to add powerful effects to your sounds. This tutorial demonstrates how to add various effects to an audio sample.

## The setup

You can check out this Scene for yourself [here](https://playcanvas.com/editor/scene/440346). The Root Entity in this Scene has a [Sound](/user-manual/editor/scenes/components/sound) Component with one slot that plays a simple looping speech audio sample. If you're not familiar with how Sound Components work make sure to check out this [Basic Tutorial](/tutorials/basic-audio/).

The Root Entity also has a [Script](/user-manual/editor/scenes/components/script) Component with two scripts. One script is responsible for the user interface and the other is the script we're going to focus on: <a href="https://playcanvas.com/editor/asset/4472751" target="_blank">application.js</a>.

This script manages the sound effects of the application.

## Using AudioNodes

The Web Audio API allows you to create various audio nodes which can be connected together to form an audio routing graph. When an audio sample is played it gets processed by each node and eventually reaches the destination usually your speakers. You can find out more details [here](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API).

In this example we are using a [Convolver node](https://developer.mozilla.org/en-US/docs/Web/API/ConvolverNode). In our application script we are creating that node like so:

```javascript
this.convolver = this.app.systems.sound.context.createConvolver();
```

We then use a bunch of audio samples as impulse responses for the convolver as explained [here](https://developer.mozilla.org/en-US/docs/Web/API/ConvolverNode/buffer). Each audio sample is an audio asset and depending on which button you press we use that asset and assign its internal buffer to the convolver buffer like so:

```javascript
var asset = this[assetName];
this.convolver.buffer = asset.resource.buffer;
```

We then connect the convolver to our sound slot like so:

```javascript
this.entity.sound.slot('speech').setExternalNodes(this.convolver);
```

If you click on the button called 'None' we clear all the effects which basically means calling ```clearExternalNodes``` on the slot:

```javascript
this.entity.sound.slot('speech').clearExternalNodes();
```

If you have a bigger graph of Audio Nodes and you want to connect it to a slot you need to supply the first node and the last node of the graph in ```setExternalNodes```. That way PlayCanvas can correctly connect the last node to the speakers.

You can find out more about the Sound Component API [here](https://api.playcanvas.com/engine/classes/Sound.html).

--------------------------------------------------------------------------------

## Basic Audio

URL: https://developer.playcanvas.com/tutorials/basic-audio/
Tags: audio, basics

[Interactive Demo]

*The tank is moving around the robot. You can shoot by clicking anywhere on the game.*

## Sound sources and Listeners

In this tutorial we have placed a robot in the middle of the scene with a tank rotating around the robot. The sound playing from the tank's engine is heard relative to the ears of the robot. As the tank moves around the robot we can hear the sound shifting position from left to back to right to front.

The basic audio system in PlayCanvas centers on two Component types.

The [Sound](/user-manual/editor/scenes/components/sound) Component plays audio assets like Wave, MP3 or Ogg Vorbis files. Imagine Sound Components like loudspeakers in the scene. There can be many Sound Components in the scene each playing multiple sounds.

Each Sound component comprises of a number of slots. Each slot has a name and defines a specific sound to be played. You can have multiple slots on a Sound component each playing independently from each other.

The [AudioListener](/user-manual/editor/scenes/components/audiolistener) Component determines how a Sound is heard. Imagine AudioListener Components like a microphone in the scene. There can only be one active AudioListener at a time.

## Scene Setup

You should look at the [Hierarchy](https://playcanvas.com/editor/scene/440098) for this tutorial yourself in the Editor. These are the main Entities in the Scene:

### Playbot

Our robot. It has an AudioListener component so all sound is going to be heard relative to the position of the robot.

### Rotator

An Entity used to rotate the tank. It has a Script component with a script that rotates the entity around the Y axis at a constant speed.

### Tank

Our tank. It is a child of the Rotator Entity, that way it rotates with it. It has a Sound component with 2 slots:

The "engine" slot is responsible for playing the looping engine sound of the tank. It's set to Auto Play as soon as the scene is loaded.

The other slot is the "shoot" slot. That slot is played by script every time we click on the canvas to make the tank's turret shoot. The Overlap checkbox is true for this slot which means that we can shoot multiple times and a new sound will be created every time instead of using just a single sound that restarts every time.

The tank also has a Script component which handles user input to make its turret shoot and also handles playing the shooting sound and other effects like a simple animation and a particle system.

## Creating Sound sources

To create a Sound source select the Entity you wish to add the Sound to and choose *New Component* from the *Entity* menu. Select *sound* from the list of Components and press the *Add* button.

Sound properties will now appear in the AttributeEditor.

[Image: Sound Component]

Here is the Sound component of the tank Entity. Here's some information about some of the properties:

### Positional

If this is checked it means that the sound will be heard relative to the position of the current AudioListener (our robot) in 3D space. If this is unchecked then the sound will be heard in 2D by both speakers.

### Asset

Each slot has an Asset picker which allows you to play different audio assets from different slots. See [importing assets](/user-manual/editor/assets/importing) for details on how to upload audio assets.

:::note

Not all audio formats are supported across all browsers. At the moment we recommend using MP3 (.mp3) which is supported by most browsers.

:::

Once you have uploaded your assets, use the Asset Picker to select the audio asset for your slot.

### Auto Play

If you want a slot to start playing as soon as the scene is loaded then set this to true. Otherwise set this to false and play the slot using a script.

### Overlap

If this is unchecked then the slot will play its sound from the beginning every time you play it in script. If this doesn't matter to you for example if you are playing some background music or a sound that doesn't need to be repeated leave this unchecked.

If however you want to play a sound repeatedly without stopping every time the sound is replayed, set this to true. Our shooting sound is set to overlap in this tutorial.

### Loop

By default, the audio sample will be played once, and then stop. If you want the sound to loop continuously, e.g. for background music, then you can check the *Loop* box. Our engine is a looping sound in the example but the shooting sound is not.

## Creating Listeners

To create an AudioListener, select the Entity you wish to represent the listener. Often, this will be the in-game camera as this is where the players 'head' is. In our case the AudioListener is attached to the robot.

:::note

You can only have one AudioListener at a time. The last AudioListener Component that is created will be the active one.

:::

--------------------------------------------------------------------------------

## Basic Cameras

URL: https://developer.playcanvas.com/tutorials/basic-cameras/
Tags: camera, basics

## Camera Entities

To view the scene created by your PlayCanvas application a Camera Entity is used to render it to the screen.

In order to run your Scene from the PlayCanvas Editor, you must add at least one active Camera Entity to your Scene.

## Creating a Camera Entity

To create a new Camera Entity, you need to add a Camera [Component](/user-manual/glossary#component) to an Entity.

* Select the root Entity of your Scene in the Entity Explorer
* Create a new Entity by selecting *New Entity* from the *Entity* menu.
* Add a Component by selecting *New Component* from *Component* menu
* Choose *Camera* when you are prompted to choose which type of Component to create

As making a Camera Entity is a common task there is a shortcut: Select *New Entity* -> *New Camera* in the *Entity* menu.
This is equivalent to creating a new Entity and adding a Camera Component to it.

## Camera Properties

Like all Components, the Camera Component has a set of properties which alter its behavior.

### `Enabled`

If the enabled property is true, then the camera will render the scene to its rendering buffer when the Scene loads. Multiple cameras can be enabled at the same time. This is useful in situations where you want to implement a split-screen game or a mini-map, for example. The priority property determines the order in which the enabled cameras are rendered.

### `Clear Color Buffer`

If this property is checked, before rendering the scene, the camera will erase whatever was previously in the camera's frame buffer (namely the previously rendered frame), and fills it with a clear color.

### `Clear Color`

The color to which the frame buffer is cleared, providing that the Clear Color Buffer property has been checked.

### `Clear Depth Buffer`

If this property is checked, before rendering the scene, the camera will erase whatever was previously stored in the camera's depth buffer. Normally, this should be left checked but in certain circumstances, where you don't care about depth order when rendering the scene, it can be an optimization to disable it.

### `Projection`

The projection type determines which type of matrix projection is used to convert the 3D scene in to the 2D view rendered to the page.

The **perspective** projection is the most common type for games. Alternatively, you can use an **orthographic** projection, which renders the scene without perspective so is useful for 2D games.

### `Field of View`

The field of view of a camera determines how much of the scene the camera shows. It is measured in degrees (&deg;) so the default value of 45&deg; means that the top edge of the view to the bottom edge of the view form an arc of 45&deg; from the position of the camera

[Image: Field of view]

You can see in this diagram that because the `fov` value is independent of the width of the display a wide screen view (light blue) shows the same amount vertically but more horizontally than a narrow screen view (dark blue).

### `Near Clip`

The near clipping distance is the distance, in meters, from the camera before which nothing will be drawn.

### `Far Clip`

The far clipping distance is the distance, in meters, from the camera after which nothing will be drawn.

### `Priority`

This value is a number which determines the order in which a camera is rendered, if multiple cameras are enabled. Smaller numbers are higher priority and will be rendered first.

### `Viewport`

The viewport represents a rectangular area on the camera's rendering buffer. There are 4 values in the following format: Bottom Left X, Bottom Left Y, Width, Height. The values are normalized coordinates, where the rendering buffer, regardless of dimensions, is considered to be mapped in X and Y to 0..1. So to limit the camera to rendering in the bottom left quadrant of the screen, set the viewport to 0, 0, 0.5, 0.5.

--------------------------------------------------------------------------------

## Basic Materials

URL: https://developer.playcanvas.com/tutorials/basic-materials/
Tags: materials, basics

Materials are what define the appearance of the surfaces of a 3D Model. At its very simplest a Material sets the color of the polygons, at its most complex a material can simulate the surface of an ocean or the way that light passes through glass.

In this tutorial, we'll walk through how you create a Material, assign it to a primitive shape, and edit the Material's properties. First create a new and empty scene.

## Step 1 - Add a cube to your scene

Select the root Entity of your Scene. At the start this will be called "Root". In the Attribute Editor rename the Entity to something meaningful, like "Material Tutorial".

Select 'New Entity' -> 'Box' menu in the Hierarchy panel. This creates a new Box Entity in the Hierarchy.

[Image: New Box]

## Step 2 - Create a new Material

Create new material directly from the PlayCanvas Editor interface. In the Asset panel use the + button to open the New Asset menu and select Material.

[Image: Add Material]

This creates a new Material Asset and brings up the Material Editor on the right-hand side of the screen. In the Material Editor rename the Material to "Box Material".

## Step 3 - Assign the material to the cube

When you create a new cube in the Editor it will have no material assigned and will be rendered using a plain default material. To assign your new material to a model in the scene simply drag the model from the asset panel into the viewport and drop it on to the model you want to assign it to.

## Step 4 - Change the color of the material

To change the color of the Material we will want to edit the *Diffuse* property of the material.

Select the material either by clicking on it in the asset panel. Or by selecting it from the property inspector when you select the box.

[Image: Diffuse Color]

You'll have to expand the Diffuse settings panel in the material editor in the right panel. Click on the color property to edit the diffuse color of the material.

## Bonus - Add textures to the material

Changing the color of the material is a good start, but you'll quickly want more detail than a flat color. That is where texture maps come in.

Download & save these sample textures:

[Image: Sample Diffuse Map]

[Image: Sample Normal Map]

Then upload them to your project by dragging the files into the Editor.

Once they're uploaded it's time to assign them to texture slots on the Material. Select the Material as before and open up the Diffuse section. Drag the `proto_orange` texture from the asset panel into the empty diffuse slot. Then open the Normals section and drag the `proto_gray_n` texture into the normal map slot.

You'll see something that looks a little like this:

[Image: Cube]

Try modifying the **Tiling** and **Offset** properties to affect the way the texture is wrapped around the cube.

Take a look at the user manual section on [Physical Materials](/user-manual/graphics/physical-rendering/physical-materials/) for more depth on setting up materials.

--------------------------------------------------------------------------------

## Basic touch input

URL: https://developer.playcanvas.com/tutorials/basic-touch-input/
Tags: touch, input, basics, mobile, tutorial

Tutorial demonstrating basic touch input in PlayCanvas. Touch the box and move it around the screen!

[Interactive Demo]

--------------------------------------------------------------------------------

## Billboards

URL: https://developer.playcanvas.com/tutorials/billboards/
Tags: rendering, camera, tutorial

Simple example of planes that always face the camera. i.e. billboards. Click on the box and fly around the scene with WASD

[Interactive Demo]

--------------------------------------------------------------------------------

## Camera following a path

URL: https://developer.playcanvas.com/tutorials/camera-following-a-path/
Tags: camera, tutorial

Sample of a camera following points on a spline path.

[Interactive Demo]

--------------------------------------------------------------------------------

## Camera model masking

URL: https://developer.playcanvas.com/tutorials/camera-model-masking/
Tags: rendering, camera, tutorial

Sample showing how to use render masking on the lights and models to enable lighting to only affect certain models and cameras to render certain models.

[Interactive Demo]

--------------------------------------------------------------------------------

## Capturing a screenshot

URL: https://developer.playcanvas.com/tutorials/capturing-a-screenshot/
Tags: rendering, camera, tutorial

A sample showing how to capture a screenshot from a camera and download as a PNG

[Interactive Demo]

--------------------------------------------------------------------------------

## Changing Scenes

URL: https://developer.playcanvas.com/tutorials/changing-scenes/
Tags: loading, scenes, basics

Full documentation for Loading Scenes is now in the [User Manual](/user-manual/editor/scenes/loading-scenes/).

[Interactive Demo]

--------------------------------------------------------------------------------

## Changing textures at runtime

URL: https://developer.playcanvas.com/tutorials/changing-textures-at-runtime/
Tags: textures, assets, tutorial

A sample showing how to change a material's texture through a script

[Interactive Demo]

--------------------------------------------------------------------------------

## Collision and Triggers

URL: https://developer.playcanvas.com/tutorials/collision-and-triggers/
Tags: collision, physics

[Interactive Demo]

*Rigidbodies collide with each other, a sound is played on a collision and a trigger volume resets the shapes.*

This tutorial introduces the basics of rigid-body physics, collision detection and trigger volumes. Have a look at the [tutorial project](https://playcanvas.com/project/405871).

## The Collision Component

The *collision* component defines a shape which can be used either to trigger events if another Entity enters or exits it -- we call this a Trigger Volume -- or it can be used together with the *rigidbody* component to give an Entity physical properties in your game -- like a bouncing ball or a heavy crate.

The most important property of a *collision* component is its **Type**, this determines the shape of the volume that will be used. There are four options:

- **Box** A simple box
- **Sphere** A simple sphere
- **Capsule** A pill-shaped capsule. Useful for characters, as it can be tall and thin, but has a nice rounded-base with a single contact point.
- **Mesh** Use any arbitrary mesh shape for the volume. **Note** There are some limitations to the mesh collision, in particular, when using it with the *rigidbody* component, they must be **Static**.

### Trigger Volumes

To create a Trigger Volume all we need to do is add a *collision* component to an Entity. In this tutorial we're adding a large box-shaped Trigger Volume underneath the slope to catch the falling bodies and reset their position.

[Image: Collisions & Triggers]

You can see the trigger volume underneath the ramp displayed as a blue outline.

### Rigid Bodies

A rigid body is a physical presence in your game world. You can set it up with real physics properties like Mass and Friction; it will collide with other rigid bodies and respond in a realistic manner.

To create a rigid body in your Scene, pick an Entity and add a *rigidbody* component and a *collision* component. By default you will create a **static box**. The *rigidbody* component has a multitude of options which you can use to tune the properties of your object.

[Image: rigidbody component]

For details on each property take a look at the [*rigidbody* documentation](/user-manual/editor/scenes/components/rigidbody/).

For this demo, the important property is the **Type**. You can pick one of three options:

- **Static** this Entity will never move.
- **Dynamic** this Entity will move under gravity and any other forces that you apply to it.
- **Kinematic** this Entity will not respond to forces, but will move if you directly set its position or velocity.

## Setting up the ground

The first Entity we need in this tutorial is the green block that forms the ground.

<img loading="lazy" src="/img/tutorials/collision/ground_setup.png" width="300" />

You can see in the attribute panel, that it has *render*, *collision* and *rigidbody* components. We've increased the Entity and the *collision* box properties so that it is nice and large. And we've also slightly increased the friction and restitution properties. This means that the surface is slightly rougher and slightly bouncier than the defaults.

## Setting up the trigger

The next Entity we'll need is the trigger.

[Image: Trigger Entity]

With this Entity we have a *collision* component but no *rigidbody* so it acts as a trigger. The trigger Entity also has a *script* component with some code attached. Triggers are only useful if something happens when they are triggered, so we need to add some code to fire and listen for events when the trigger is activated.

There are two significant parts to the code above.

First in the ```initialize``` method we start listening to the **triggerenter** event. This event fires once when a rigid body enters a trigger volume (where a trigger volume is an entity that has a collision component but no rigidbody component). The companion event is **triggerleave** which is fired once the penetrating rigid body leaves the trigger.

```javascript
this.entity.collision.on('triggerenter', this.onTriggerEnter, this);
```

Notice, the third argument, ```this```, which is the 'scope' that will be used in the event listener. Usually, you'll want to add the current Script Object as the third argument so that the value of ```this``` in the event listener is that same Script Object.

The second part of this code is the function which handles the event, ```onTriggerEnter```. When the trigger is entered, this function is called and passed the [```Entity```](https://api.playcanvas.com/engine/classes/Entity.html) object entering the trigger volume.

In this case, when the trigger is fired, we reset the penetrating Entity back up to the position it started in, and reset its velocities.

## The Rigid Bodies

We've set the ground to **Static**, now we'll create the falling objects and make sure they are **Dynamic**.

<img loading="lazy" src="/img/tutorials/collision/box_setup.png" width="300" />

This is the *rigidbody* and *collision* setup for the box component, the sphere and capsule are setup in the same way.

## Contact Events

There are three events available on the *collision* component:

- **contact** - fires for every point of contact when two rigid bodies touch.
- **collisionstart** - fires at the start of a collision when two rigid bodies touch.
- **collisionend** - fires when two rigid bodies separate.

The difference between **contact** and **collisionstart** is subtle but important. Imagine a cube landing at an angle on a flat surface. When the edge of the cube hits the surface the two corners of the cube will strike at the same moment. Three events will fire, two **contact** events for each corner of the cube, and one **collisionstart** event. Then the cube will rotate and continue to fall until it lies flat, all the while remaining in contact with the surface. When it lands flat, two more **contact** events will fire as the edge of the cube hits the surface. As the cube remained in contact with the surface all that time, no more **collisionstart** events are fired.

Both events are useful, but in this demo we'll use the **collisionstart** event to trigger a sound effect that plays when the objects hit the ground. Here's the code:

In the ```initialize``` method we set up the event listener, and then in the event handler we check to see if the other entity has a **rigidbody** component (this is to avoid playing a sound when we enter a trigger volume) and then we play the "hit" sound effect. So now, every time an Entity with the collider script attached collides with another rigid body, it will play the hit sound.

And that's all there is to handling Collisions and Triggers in PlayCanvas.

--------------------------------------------------------------------------------

## Compound Physics Shapes

URL: https://developer.playcanvas.com/tutorials/compound-physics-shapes/
Tags: physics

Full documentation for Compound Physics Shapes is now in the [User Manual](/user-manual/physics/compound-shapes/).

[Interactive Demo]

--------------------------------------------------------------------------------

## Controlling Lights

URL: https://developer.playcanvas.com/tutorials/controlling-lights/
Tags: lighting

[Interactive Demo]

*Press 1, 2 or 3 to enable/disable the spot, point and directional lights respectively.*

In this tutorial we will show you how to enable/disable lights in PlayCanvas and to change light color and intensity. Note that there are many more controllable light features and properties, such as the light range. See the [API listing here](https://api.playcanvas.com/engine/classes/LightComponent.html) for more details.

It is also important to be aware of the different limits for differing light properties, for example red, green and blue values are set between 0 and 1, but intensity reaches from 0 up to 10. Also some lights have properties unique to them, such as the cone angles for the spot light.

## The lighting commands

```javascript
if (app.keyboard.wasPressed(pc.KEY_1)) {
    this.spot.light.enabled = !this.spot.light.enabled;
}
```

This line toggles on and off the light component of the 'spot' entity.

```javascript
this.color1 = new pc.Color(1, 1, 1);
```

A new color array is declared, the first three values affect red, green and blue values respectively.

```javascript
var s = Math.abs(Math.sin(1 + this.timer));
var r = 1-s/2;
var g = s-0.2;
var b = 0.55+s/2;
this.color1.set(r, g, b);
this.spot.light.color = this.color1;
this.spot.light.intensity = 10*s;
```

These lines assign values to r, g and b variables based on a sin wave and then assign these values to the previously declared color array via `color1.set(x, y, z)` and then onto the light property. The intensity is set to vary sinusoidally from the max light intensity value of 10 down to 0.

:::note

Using `entity.light.color.r` to access and change the red value of a light's color will not work. Only changes to the light property `color` are detected, so you must assign a complete `pc.Color` to the property e.g. `entity.light.color = new pc.Color(1, 1, 1);`.

:::

## General setup

We added a spot light (attached to a parent assembly of a basic torch model), an omni light attached to a parent sphere model, in addition to the default directional light that is created for every new Scene. The controlling script was attached to the root entity. The sphere and torch were made children of a blank entity residing in the centre of the scene to enable easy rotation. The [full Editor scene and scripts can be accessed here](https://playcanvas.com/project/405812/overview/tutorial-controlling-lights) in the 'controllingLights' Scene.

The full code used for the above PlayCanvas app is as follows.

--------------------------------------------------------------------------------

## Crash Course - Make a Game

URL: https://developer.playcanvas.com/tutorials/crash-course/
Tags: games, basics, physics, ui

[Interactive Demo]

## Overview

In this tutorial, we will give an introduction to PlayCanvas and take you through making a complete game, end to end with PlayCanvas.

This was recorded for [JS GameDev Summit](https://jsgamedev.com/) and the video is hosted on [GitNation](https://portal.gitnation.org/contents/playcanvas-end-to-end-the-quick-version).

Play below! Try to get as many food items as you can before the timer runs out! Use WASD for movement.

[Interactive Demo]

## Timestamps

| Time      | Description                                        |
|-----------|----------------------------------------------------|
| `0:00:00` | Introduction                                       |
| `0:04:44` | About PlayCanvas                                   |
| `0:12:52` | Let's build something!                             |
|           |                                                    |
| `0:14:50` | **Phase 1 - Project Setup**                        |
| `0:15:08` | Creating and Managing Assets                       |
| `0:28:53` | Creating and Using Templates                       |
| `0:43:53` | Testing in the Launch Tab                          |
|           |                                                    |
| `0:46:04` | **Phase 2 - Interactivity**                        |
| `0:47:24` | Using Physics collision and rigidbody              |
| `1:01:05` | Creating and using Scripts                         |
| `1:18:11` | Creating Animation State Graphs                    |
| `1:30:12` | Attaching, Detaching, and Firing Events            |
| `1:45:10` | Debugging code                                     |
|           |                                                    |
| `1:47:56` | **Phase 3 - Polishing & Publishing** (incomplete)  |
|           |                                                    |
| `2:00:57` | Wrap Up                                            |

:::note

Phase 3 of this tutorial is still incomplete

:::

## Links and resources

* [Presentation slides PDF](pathname:///downloads/playcanvas-crash-course-make-a-game.pdf)
* [Food Run - Complete PlayCanvas project](https://playcanvas.com/project/898163/overview/food-run--full-project)
* [Food Run - Starter Phase 1 PlayCanvas project](https://playcanvas.com/project/910590/overview/food-run-starter-kit)
* [Food Run - Starter Phase 2 PlayCanvas project](https://playcanvas.com/project/910606/overview/food-run--phase-2)
* [Food Run - Starter Phase 3 PlayCanvas project](https://playcanvas.com/project/910630/overview/food-run--phase-3)

--------------------------------------------------------------------------------

## Create QR Code at runtime

URL: https://developer.playcanvas.com/tutorials/create-qr-code-at-runtime/
Tags: ui, tutorial, scripts

Create QR codes at runtime with https://github.com/davidshimjs/qrcodejs

[Interactive Demo]

--------------------------------------------------------------------------------

## Creating Rigid Bodies in Code

URL: https://developer.playcanvas.com/tutorials/creating-rigid-bodies-in-code/
Tags: animation, physics, tutorial, procedural

[Interactive Demo]

--------------------------------------------------------------------------------

## Custom Post Effects

URL: https://developer.playcanvas.com/tutorials/custom-posteffect/
Tags: posteffects

In this tutorial, you'll learn how to create a custom watercolor post effect in PlayCanvas that applies a softening filter and a paper grain texture to your scene. By the end of this guide, you'll have a visually appealing watercolor effect that you can apply to any 3D scene.

[Interactive Demo]

## Step 1: Setting Up Your Shaders

First, we need to create the shaders that will define our watercolor effect. You'll create two shader assets: a vertex shader and a fragment shader.

### Vertex Shader (watercolor.vert)

The vertex shader will pass the UV coordinates from the vertices to the fragment shader. Create a new shader asset in PlayCanvas and name it `watercolor.vert`. Then, copy and paste the following code:

```glsl title="watercolor.vert"
attribute vec2 aPosition;

varying vec2 vUv0;

void main(void)
{
    gl_Position = vec4(aPosition, 0.0, 1.0);
    vUv0 = (aPosition.xy + 1.0) * 0.5;
}
```

### Fragment Shader (watercolor.frag)

The fragment shader will apply the watercolor effect using the color buffer texture and UV coordinates. Create another shader asset named `watercolor.frag` and insert the following code:

```glsl title="watercolor.frag"
precision mediump float;

// The texture containing our rendered scene
uniform sampler2D uColorBuffer;

// The UV coordinates passed from the vertex shader
varying vec2 vUv0;

// Function to create a simple paper grain texture
float paperTexture(vec2 uv) {
    // Create a pseudo-random pattern based on UV coordinates
    float grain = fract(sin(dot(uv, vec2(12.9898, 78.233))) * 43758.5453);
    // Modulate the grain intensity
    grain = smoothstep(0.3, 0.7, grain);
    return grain;
}

void main(void) {
    // Sample the color from the scene texture at this fragment's UV coordinates
    vec4 sceneColor = texture2D(uColorBuffer, vUv0);

    // Apply a softening filter to mimic watercolor fluidity
    // Blend with neighboring pixels (basic blur)
    vec4 blurColor = vec4(0.0);
    float offset = 0.003; // Offset for neighboring pixels; adjust for blur amount
    for (int x = -1; x <= 1; x++) {
        for (int y = -1; y <= 1; y++) {
            blurColor += texture2D(uColorBuffer, vUv0 + vec2(x, y) * offset);
        }
    }
    blurColor /= 9.0;

    // Mix original color with blurred version
    vec4 mixedColor = mix(sceneColor, blurColor, 0.5);

    // Overlay the paper texture
    float grain = paperTexture(vUv0 * 10.0); // Tiling of the grain texture
    mixedColor.rgb += mixedColor.rgb * grain * 0.1; // Modulate to adjust intensity

    // Output the final color
    gl_FragColor = mixedColor;
}
```

## Step 2: Creating the Watercolor Effect Script

Now, you'll create a script to apply the shaders to your scene. Create a new script in PlayCanvas and name it `watercolor.js`. Paste in the code provided:

:::note

Remember to parse the script so that the Editor knows about the script's attributes!

:::

## Step 3: Applying the Effect to a Camera

To see your watercolor effect in action, you need to apply it to a camera in your scene:

* Create a new entity with a [camera component](/user-manual/editor/scenes/components/camera) if you haven't already.
* Add a [script component](/user-manual/editor/scenes/components/script) to the camera entity and assign the watercolor script to it.
* Assign the `watercolor.vert` and `watercolor.frag` shader assets to the corresponding attributes in the watercolor script component.

Now, when you play your scene, you should see the watercolor effect applied, giving your scene a soft, artistic look.

See the Custom Post Effects project [here](https://playcanvas.com/project/406045).

--------------------------------------------------------------------------------

## Custom Shaders

URL: https://developer.playcanvas.com/tutorials/custom-shaders/
Tags: shaders, materials

[Interactive Demo]

:::info

This tutorial uses the [ShaderMaterial](https://api.playcanvas.com/engine/classes/ShaderMaterial.html) API to create a dissolve effect with burning edges that works with both WebGL and WebGPU. The complete project can be found [here](https://playcanvas.com/project/406044/overview/tutorial-custom-shaders).

:::

When you import your 3D models into PlayCanvas by default they will use our [Physical Material](/user-manual/graphics/physical-rendering/physical-materials/). This is a versatile material type that can cover a lot of your rendering needs.

However, you will often want to perform special effects or special cases for your materials. To do this you will need to write a custom shader.

## ShaderMaterial API

PlayCanvas provides the [ShaderMaterial](https://api.playcanvas.com/engine/classes/ShaderMaterial.html) API which simplifies the creation of custom shaders and supports both WebGL (GLSL) and WebGPU (WGSL). This API automatically handles the differences between graphics APIs and provides a cleaner interface for shader development.

## Cross-Platform Shader Support

To ensure your custom shaders work across all devices and browsers, you should provide both [GLSL](/user-manual/graphics/shaders/glsl-specifics/) and [WGSL](/user-manual/graphics/shaders/wgsl-specifics/) versions of your shaders:

- **GLSL** (OpenGL Shading Language): Used by WebGL
- **WGSL** (WebGPU Shading Language): Used by WebGPU

## Vertex Shaders

### GLSL Vertex Shader

```glsl
attribute vec3 aPosition;
attribute vec2 aUv0;

uniform mat4 matrix_model;
uniform mat4 matrix_viewProjection;

varying vec2 vUv0;

void main(void)
{
    vUv0 = aUv0;
    gl_Position = matrix_viewProjection * matrix_model * vec4(aPosition, 1.0);
}
```

### WGSL Vertex Shader

```wgsl
attribute aPosition: vec3f;
attribute aUv0: vec2f;

uniform matrix_viewProjection: mat4x4f;
uniform matrix_model: mat4x4f;

varying vUv0: vec2f;

@vertex
fn vertexMain(input: VertexInput) -> VertexOutput {
    var output: VertexOutput;

    output.vUv0 = aUv0;
    output.position = uniform.matrix_viewProjection * uniform.matrix_model * vec4<f32>(aPosition, 1.0);

    return output;
}
```

## Fragment Shaders

### GLSL Fragment Shader

```glsl
varying vec2 vUv0;

uniform sampler2D uDiffuseMap;
uniform sampler2D uHeightMap;
uniform float uTime;

void main(void)
{
    float height = texture2D(uHeightMap, vUv0).r;
    vec4 color = texture2D(uDiffuseMap, vUv0);

    if (height < uTime) {
        discard;
    }

    // Burning band width
    float edgeWidth = 0.05;

    if (height < (uTime + edgeWidth)) {
        // 0 at inner edge → 1 at outer edge
        float t = (height - uTime) / edgeWidth;

        // Fire gradient: yellow to dark orange
        vec3 burnColor = mix(
            vec3(1.0, 0.7, 0.2),
            vec3(0.6, 0.1, 0.0),
            t
        );

        // Blend the burn color with the original texture
        color = vec4(mix(burnColor, color.rgb, t), 1.0);
    }

    gl_FragColor = color;
}
```

### WGSL Fragment Shader

```wgsl
varying vUv0: vec2f;

uniform uTime: f32;

var uDiffuseMap: texture_2d<f32>;
var uDiffuseMapSampler: sampler;
var uHeightMap: texture_2d<f32>;
var uHeightMapSampler: sampler;

@fragment
fn fragmentMain(input: FragmentInput) -> FragmentOutput {
    var output: FragmentOutput;

    let height = textureSample(uHeightMap, uHeightMapSampler, vUv0).r;
    var color = textureSample(uDiffuseMap, uDiffuseMapSampler, vUv0);

    if (height < uniform.uTime) {
        discard;
    }

    // Burning band width
    let edgeWidth = 0.05;

    if (height < (uniform.uTime + edgeWidth)) {
        // t goes from 0 (just inside edge) to 1 (outer edge)
        let t = (height - uniform.uTime) / edgeWidth;

        // Fire color: bright yellow fading to dark orange
        let burnColor = mix(
            vec3f(1.0, 0.7, 0.2),
            vec3f(0.6, 0.1, 0.0),
            t
        );

        // Blend burn color with original texture (more burn at the outer edge)
        color = vec4f(mix(burnColor, color.rgb, t), 1.0);
    }

    output.color = color;
    return output;
}
```

The shaders above create a dissolve effect with a fire-like burning edge. The vertex shaders transform mesh vertices into screen space, while the fragment shaders create the dissolve effect based on a height map texture. When the `uTime` value is greater than the height map value at a pixel, that pixel is discarded (making the model transparent there). Near the dissolve edge, we blend in a fire-colored gradient for a realistic burning effect.

## Creating the ShaderMaterial

```javascript
// Create a new ShaderMaterial with both GLSL and WGSL versions
this.material = new ShaderMaterial({
    uniqueName: 'Dissolve',
    vertexGLSL: this.vertexGLSL.resource,
    fragmentGLSL: this.fragmentGLSL.resource,
    vertexWGSL: this.vertexWGSL.resource,
    fragmentWGSL: this.fragmentWGSL.resource,
    attributes: {
        aPosition: SEMANTIC_POSITION,
        aUv0: SEMANTIC_TEXCOORD0
    }
});
```

The [ShaderMaterial constructor](https://api.playcanvas.com/engine/classes/ShaderMaterial.html#constructor) takes both GLSL and WGSL shader code. PlayCanvas will automatically choose the appropriate version based on the graphics API being used. The `attributes` object specifies the vertex attributes your shaders expect.

## Setting Shader Parameters

```javascript
// Set the initial time parameter
this.material.setParameter('uTime', 0);

// Set the diffuse texture
const diffuseTexture = this.diffuseMap.resource;
this.material.setParameter('uDiffuseMap', diffuseTexture);

// Set the height map texture
const heightTexture = this.heightMap.resource;
this.material.setParameter('uHeightMap', heightTexture);
```

Uniforms are set using the [`setParameter()`](https://api.playcanvas.com/engine/classes/Material.html#setparameter) method, which works the same way as with regular materials. The ShaderMaterial automatically handles the differences between GLSL and WGSL uniform syntax.

## Script Attributes for Shader Assets

```javascript
/**
 * GLSL vertex shader.
 * 
 * @attribute
 * @title GLSL Vertex Shader
 * @type {pc.Asset}
 */
vertexGLSL;

/**
 * GLSL fragment shader.
 * 
 * @attribute
 * @title GLSL Fragment Shader
 * @type {pc.Asset}
 */
fragmentGLSL;

/**
 * WGSL vertex shader.
 * 
 * @attribute
 * @title WGSL Vertex Shader
 * @type {pc.Asset}
 */
vertexWGSL;

/**
 * WGSL fragment shader.
 * 
 * @attribute
 * @title WGSL Fragment Shader
 * @type {pc.Asset}
 */
fragmentWGSL;

/**
 * Diffuse Map
 * 
 * @attribute
 * @title Diffuse Map
 * @type {pc.Asset}
 */
diffuseMap;

/**
 * Height Map
 * 
 * @attribute
 * @title Height Map
 * @type {pc.Asset}
 */
heightMap;
```

You'll need to create four shader assets (two GLSL and two WGSL) and assign them to these script attributes in the PlayCanvas Editor.

## Updating Uniforms

```javascript
update(dt) {
    this.time += dt;

    // Create a smooth oscillation using sine wave
    const t = (Math.sin(this.time) + 1) / 2;

    // Update the time value in the material
    this.material.setParameter('uTime', t);
}
```

To achieve the dissolving effect, we use the height map value as a threshold that changes over time. In this version, we use a sine wave to create a smooth oscillation between 0 and 1, providing a more natural dissolve animation.

## Complete Script

```javascript

/**
 * Apply a dissolve shader material to an entity's render components.
 */

    scriptName = 'dissolveShader';

    /**
     * GLSL vertex shader.
     * 
     * @attribute
     * @title GLSL Vertex Shader
     * @type {pc.Asset}
     */
    vertexGLSL;

    /**
     * GLSL fragment shader.
     * 
     * @attribute
     * @title GLSL Fragment Shader
     * @type {pc.Asset}
     */
    fragmentGLSL;

    /**
     * WGSL vertex shader.
     * 
     * @attribute
     * @title WGSL Vertex Shader
     * @type {pc.Asset}
     */
    vertexWGSL;

    /**
     * WGSL fragment shader.
     * 
     * @attribute
     * @title WGSL Fragment Shader
     * @type {pc.Asset}
     */
    fragmentWGSL;

    /**
     * Diffuse Map
     * 
     * @attribute
     * @title Diffuse Map
     * @type {pc.Asset}
     */
    diffuseMap;

    /**
     * Height Map
     * 
     * @attribute
     * @title Height Map
     * @type {pc.Asset}
     */
    heightMap;

    time = 0;

    // initialize code called once per entity
    initialize() {
        // Create a new material and set the shader
        this.material = new ShaderMaterial({
            uniqueName: 'Dissolve',
            vertexGLSL: this.vertexGLSL.resource,
            fragmentGLSL: this.fragmentGLSL.resource,
            vertexWGSL: this.vertexWGSL.resource,
            fragmentWGSL: this.fragmentWGSL.resource,
            attributes: {
                aPosition: SEMANTIC_POSITION,
                aUv0: SEMANTIC_TEXCOORD0
            }
        });

        // Set the initial time parameter
        this.material.setParameter('uTime', 0);

        // Set the diffuse texture
        const diffuseTexture = this.diffuseMap.resource;
        this.material.setParameter('uDiffuseMap', diffuseTexture);

        // Set the height map texture
        const heightTexture = this.heightMap.resource;
        this.material.setParameter('uHeightMap', heightTexture);

        // Replace the material on all render components
        const renders = this.entity.findComponents('render');
        for (let i = 0; i < renders.length; ++i) {
            const meshInstances = renders[i].meshInstances;
            for (let j = 0; j < meshInstances.length; j++) {
                meshInstances[j].material = this.material;
            }
        }
    }

    // update code called every frame
    update(dt) {
        this.time += dt;

        // Create a smooth oscillation using sine wave
        const t = (Math.sin(this.time) + 1) / 2;

        // Update the time value in the material
        this.material.setParameter('uTime', t);
    }
}
```

This script demonstrates how to create cross-platform custom shaders using the ShaderMaterial API. The dissolve effect uses a height map to determine which pixels to discard, creating a burning edge effect as the dissolution progresses.

## GLSL vs WGSL Differences

When writing shaders for both APIs, keep these key differences in mind:

- **Syntax**: WGSL uses more explicit typing (`vec3f`, `f32`) while GLSL infers types
- **Attributes/Varyings**: WGSL uses structured input/output while GLSL uses global variables
- **Textures**: WGSL separates textures and samplers, GLSL combines them
- **Entry points**: WGSL uses `@vertex` and `@fragment` decorators, GLSL uses `main()`

The ShaderMaterial API handles these differences automatically, allowing you to focus on the shader logic rather than API-specific details.

--------------------------------------------------------------------------------

## Detecting a double click

URL: https://developer.playcanvas.com/tutorials/detecting-a-double-click/
Tags: input, mouse, tutorial

Double click on the screen to move the camera

[Interactive Demo]

--------------------------------------------------------------------------------

## Detecting a double tap

URL: https://developer.playcanvas.com/tutorials/detecting-a-double-tap/
Tags: touch, input, tutorial

Sample showing how to detect a double tap on a touch screen.

[Interactive Demo]

--------------------------------------------------------------------------------

## Detecting a long press

URL: https://developer.playcanvas.com/tutorials/detecting-a-long-press/
Tags: touch, input, mouse, tutorial

Sample showing how to detect a long press/touch to perform an action.

[Interactive Demo]

--------------------------------------------------------------------------------

## Dynamic UI Scroll View

URL: https://developer.playcanvas.com/tutorials/dynamic-ui-scroll-view/
Tags: ui, tutorial, scripts

An example of adding and removing elements from the scroll view in the UI

[Interactive Demo]

--------------------------------------------------------------------------------

## Entity picking using physics

URL: https://developer.playcanvas.com/tutorials/entity-picking-using-physics/
Tags: input, physics, tutorial, raycast

A sample showing how to use the physics raycast to pick entities

[Interactive Demo]

--------------------------------------------------------------------------------

## Entity picking without physics

URL: https://developer.playcanvas.com/tutorials/entity-picking-without-physics/
Tags: input, tutorial, raycast

Sample showing how to pick at objects without using the physics system (extra 1MB to published project) or the frame buffer.

[Interactive Demo]

--------------------------------------------------------------------------------

## Entity Picking

URL: https://developer.playcanvas.com/tutorials/entity-picking/
Tags: raycast, basics, physics

Collision Picking - click to select a shape

[Interactive Demo]

---

Frame Buffer Picking - click to select a grey shape. The red shapes are set to be not pickable.

[Interactive Demo]

---

Try it from the Editor in the [tutorial project.](https://playcanvas.com/project/405856)

This tutorial describes two methods of selecting an Entity from the 3D scene, for example, on the click of the mouse.

## Collision Picking

Collision based picking uses the collision components to add a shape to each Entity that needs to be picked. Then we use the [raycastFirst()](https://api.playcanvas.com/engine/classes/RigidBodyComponentSystem.html#raycastfirst) method in the rigidbody system to fire a ray from the mouse click position into the screen to see if it hits a collision component. If it does, that Entity is "selected".

## Frame Buffer Picking

Frame buffer based picking uses the [pc.Picker](https://api.playcanvas.com/engine/classes/Picker.html) class to render the scene to a internal buffer. When the mouse is clicked the color of the buffer at the mouse location is used to determine which mesh instance was selected. This has some advantages and disadvantages over the collision based method.

Advantages include:

* Able to use a rectangle to pick many items in a scene at once
* Doesn't require the physics library to be included which reduces preload time.

The main disadvantage is that this uses the `readPixels` method which stalls the graphics pipeline. This can have serious rendering performance implications particularly on mobile and GPU heavy applications.

You are able modify the size of the buffer to be smaller to improve the performance at the cost of accuracy. In the example script below, the attribute `pickAreaScale` is used to do this where the lower the number, the faster and less accurate the picker becomes.

It's also possible to restrict the layers to pick which the script supports via `layerNames` array. We can add the names of the layers that we want to pick from and also improves performance by rendering only what we need to the internal buffer.

--------------------------------------------------------------------------------

## Explosion Particle Effect

URL: https://developer.playcanvas.com/tutorials/explosion-particle-effect/
Tags: particles, camera, tutorial

Sample project showing a multi layered particle effect with screen shake.

[Interactive Demo]

--------------------------------------------------------------------------------

## Facebook API

URL: https://developer.playcanvas.com/tutorials/facebook-api/
Tags: facebook

[Interactive Demo]

:::info

Log in with Facebook to see your photos in a 3D sculpture.

:::

PlayCanvas is a perfect partner for building WebGL Facebook games and applications. Facebook is a great place to find a large audience for your games and PlayCanvas lets you create fast and performant games that are optimized for the web.

In this tutorial we'll show you how to integrate the Facebook API into your PlayCanvas application. Once you have access to the Facebook API there are many possibilities. For example, sharing games with friends, posting stories to Facebook and more. Check out the [tutorial project](https://playcanvas.com/project/405897).

## Facebook Plugin

There is a plugin to help you integrate Facebook available [on github](https://github.com/playcanvas/playcanvas-facebook). This simplifies the work of loading the Facebook javascript SDK. Just attach the plugin script `lib/facebook-setup.js` to an entity in your scene and listen for the `fb:init` event and you'll know the API is ready. More instruction are available on the [github page](https://github.com/playcanvas/playcanvas-facebook).

```javascript
  this.app.on("fb:init", function () {
    // use API
    FB.login();
  }, this);`
```

## Login to Facebook

In this example we've implemented a user interface to let you log in and log out of your Facebook account in the application. This is the code in `fb-ui.js`.

In the initialize step of this code we're listening for the `fb:init` event from the Facebook plugin. Once this has been fired we know that the Facebook SDK has been loaded and is ready to be used. We use three Facebook SDK functions. `FB.getLoginStatus()` reports back whether the user has already logged into Facebook through your application, `FB.login()` pops up a login dialog for the user and `FB.logout()` logs the user out of your application and of Facebook.

:::warning

It is important to notice here is that `FB.login()` must be called in response to a user action like clicking on a button, otherwise the user will see a pop-up warning.

:::

The function `loginChangeFn` is a callback which is used to respond to changes in logged in state and using the four show/hide functions we show the correct dialog box depending on the state.

Note, also we fire our own application events `app:fblogin` and `app:fblogout` to tell other parts of the application that the Facebook status has changed.

### Accessing the Facebook API

The file `face-photo.js` uses the Facebook API to retrieve a list of photos from the user and display them in the 3D world.

Some key parts of this script.

```javascript
this.app.loader.getHandler("texture").crossOrigin = "anonymous";
```

This line is required to ensure that the resource loader can load textures from a different origin (URL) than the location where the application is hosted (i.e. `playcanv.as).

```javascript
this.app.on("fb:login", this.reset, this);
```

This line listens for the login event from our `fb-ui.js` file. When the user logs in, we start the process of loading the photos.

```javascript
// request the most recent photos from user's facebook account
FB.api(path, (lists) => {
    for (var i = 0; i < lists.data.length; i++) {
        let count = lists.data.length;
        var photoId = lists.data[i].id;
        path = pc.string.format("/{0}?fields=images", photoId);

        // request more information including source URL of the photos
        FB.api(path, function (photo) {

            // create a texture asset using the image URL
            var asset = new pc.Asset(photo.id, "texture", {
                url: photo.images[0].source
            });

            app.assets.load(asset);

            asset.ready((asset) => {
                this.createPhoto(asset.resource);
                count--;
                done();
            });
        });
    }
});
```

In this section of code we are using the Facebook API to access their [Graph API](https://developers.facebook.com/docs/graph-api). In this case, we're loading a list of photos from the logged in user, and then querying each photo to get the URL of the image.

Once we have the URL, we create a new `texture` asset and we load the image.

```javascript
createPhoto(texture) {
    // clone the image template entity
    var e = this.template.clone();
    e.enabled = true;
    var mesh = e.model.meshInstances[0];

    // override the emissive map on the mesh instance to display the photo texture
    mesh.setParameter("texture_emissiveMap", texture);

    this.app.root.addChild(e);
    var MIN = -2.5;
    var MAX = 2.5;

    // randomly position the photo and set the aspect ratio to the same as the texture
    e.translate(pc.math.random(MIN, MAX), pc.math.random(MIN, MAX), pc.math.random(MIN, MAX));
    e.rotate(90, 0, 0);

    var aspect = texture.width / texture.height;
    e.setLocalScale(aspect, 1, 1);
};
```

Finally, once we have loaded the texture asset, we create a new Photo entity and we override the emissive texture with our newly loaded photo texture.

### More ideas

This tutorial shows you how you can load the Facebook API and access Facebook data from within your PlayCanvas application. There are loads more things for you to try using the Facebook API. For example, try sharing Facebook Stories when game events occur, like breaking a high score. Or use the user's friend list to get them to challenge their friends to a game.

Our game SWOOOP shows some of these in action. Try it on [Facebook](https://apps.facebook.com/swooop-playcanvas/) now.

--------------------------------------------------------------------------------

## Fading objects in and out

URL: https://developer.playcanvas.com/tutorials/fading-objects-in-and-out/
Tags: materials, tutorial

Sample showing how to fade a model in and out on a per model basis.

[Interactive Demo]

--------------------------------------------------------------------------------

## First Person Movement

URL: https://developer.playcanvas.com/tutorials/first-person-movement/
Tags: input, camera

[Interactive Demo]

This is an application that implements first person character movement.

The scene setup for this controller is important as your character must have a rigidbody and collision component in addition to the script attached. In addition, the script supports adding a camera entity as a child of the Player and manually set in the inspector. If no camera entity is present a new entity is created.

See the full scene setup in the [Tutorial Project](https://playcanvas.com/project/405842).

The script below performs the following functions:

* Listen for mouse and keyboard input
* Update a camera entity from the mouse input
* Apply forces to move the player entity around the scene

Note, the player's velocity is never set directly but it is moved by applying forces via the rigidbody's API function [`applyForce`](https://api.playcanvas.com/engine/classes/RigidBodyComponent.html#applyforce).

On the rigidbody component, we also have the following values set in the Editor Inspector:

* To limit the maximum velocity, we have linear damping applied that stops the player from sliding after the player has released input keys.
* To stop the player from rolling over, we also reduced the angular Factor to 0 on all axes.

[Image: Rigidbody Attributes]

--------------------------------------------------------------------------------

## First Person Shooter Starter Kit

URL: https://developer.playcanvas.com/tutorials/first-person-shooter-starter-kit/
Tags: input, camera, tutorial, scripts

Example project that extends the First Person Camera tutorial to move and jump around a 3D level

[Interactive Demo]

--------------------------------------------------------------------------------

## Flaming fireball

URL: https://developer.playcanvas.com/tutorials/flaming-fireball/
Tags: particles, input, tutorial

Sample with a fireball that moves with the mouse

[Interactive Demo]

--------------------------------------------------------------------------------

## Flappy Bird

URL: https://developer.playcanvas.com/tutorials/flappy-bird/
Tags: games, tutorial

Flappy's Back! Guide Flappy Bird through as many pipes as you can. Made with @playcanvas

[Interactive Demo]

--------------------------------------------------------------------------------

## Frames Per Sec (FPS) counter

URL: https://developer.playcanvas.com/tutorials/frames-per-sec-fps-counter/
Tags: rendering, tutorial, scripts

Sample with self contained FPS counter that can be used in other projects.

[Interactive Demo]

--------------------------------------------------------------------------------

## 3D Gaussian Splat Annotations

URL: https://developer.playcanvas.com/tutorials/gaussian-splat-annotations/
Tags: gaussiansplatting, tutorial

3D Gaussian Splat product visualizer with clickable hotspots and annotations.

[Interactive Demo]

--------------------------------------------------------------------------------

## 3D Gaussian Splat Physics and Relighting

URL: https://developer.playcanvas.com/tutorials/gaussian-splat-physics-and-relighting/
Tags: gaussiansplatting, tutorial

Post office courtyard in the Austrian town of Mödling. This app demonstrates SOG compression, shadow casting, dynamic relighting and physics. The splat was scanned by Christoph Schindelar. Press 'N' to enter Night Mode!

[Interactive Demo]

--------------------------------------------------------------------------------

## 3D Gaussian Splat Statues

URL: https://developer.playcanvas.com/tutorials/gaussian-splat-statues/
Tags: gaussiansplatting, tutorial

A 3D Gaussian Splat statue gallery featuring custom shaders and post effects that create beautiful transitions between each piece.

[Interactive Demo]

--------------------------------------------------------------------------------

## 3D Gaussian Splat Streaming LOD

URL: https://developer.playcanvas.com/tutorials/gaussian-splat-streaming-lod/
Tags: gaussiansplatting, tutorial

An example project showing how to use PlayCanvas' streaming LOD format based on [SOG](/user-manual/gaussian-splatting/formats/sog) (Spatially Ordered Gaussians). The environment being streamed is the Church of Saints Peter and Paul.

[Interactive Demo]

--------------------------------------------------------------------------------

## Google H5 Ads Beta Monetization

URL: https://developer.playcanvas.com/tutorials/google-ads-for-games/
Tags: scripts, monetization, games

Links for this tutorial:

- [Flappy Bird Demo](https://playcanvas.com/project/877568/overview/google-h5-ads-demo)
- [Tutorial template project](https://playcanvas.com/project/889095/overview/google-h5-ad-tutorial-start)
- [Google H5 Games Ads Documentation](https://developers.google.com/ad-placement/)

## Introduction

This tutorial will show you how to integrate Ads from Google H5 Games Ads Beta into your games for the purpose of monetization.

The Ads come in two main forms:

Interstitial Ads that can appear between sessions of play such as going from 'game over' back to the main menu. Think of them as the ad breaks on TV shows.

In our [Flappy Bird example](https://playcanvas.com/project/877568/overview/google-h5-ads-demo), we have the ads show after the player dies and return to the main menu to play again.

The second form are Rewarded Ads that the player can choose to play in order to get a 'reward' such as coins or being able to continue playing. These are considered to be effective as they give players the option to watch an ad rather than being forced to and they also get an immediate in-game reward.

In our example, we use rewarded ads to allow the user to continue after dying which tends to be a strong incentive for users to watch an ad.

For the tutorial we are going to fork the [Google H5 Ad Tutorial (Start)](https://playcanvas.com/project/889095/overview/google-h5-ad-tutorial-start) project and add the ad SDK library and functionality bit by bit.

The end result will look like this where we can call the interstitial and rewarded ads via button clicks and the refresh button will be used to check if the rewarded ads can be shown (more on this later).

[Interactive Demo]

## Setting up

To use Google H5 Games Ads, you will need to create a [Google Adsense account](https://www.google.com/adsense/start/) which will give you a [Publisher ID](https://support.google.com/adsense/answer/105516?hl=en) for the ad integration.

You will also need to review the Google H5 Games Ads [policies](https://support.google.com/publisherpolicies/answer/11975916) and sign up to the [beta here](https://developers.google.com/ad-placement/docs/beta).

Go to the [Google H5 Ad Tutorial (Start)](https://playcanvas.com/project/889095/overview/google-h5-ad-tutorial-start) project dashboard page and click on 'fork' and let's take a look at the project.

This is a simple project where it's ready for us to add the button click callbacks to show the ads in the `ui-controller.js` script:

The first step is to add the Google SDK integration files which we can do by copying the folder 'Google H5 Games Ads' from the [Flappy Bird project](https://playcanvas.com/project/877568/overview/google-h5-ads-demo).

And pasting into our fork of the tutorial project.

Let's take a look inside:

[`afg-integration.js`](https://playcanvas.com/editor/code/877568?tabs=67299908) adds Google's SDK to the page and also sets the time between ads being served to the user which we can change if need be. For the tutorial, we have also enabled test mode to show placeholders instead of real ads.

`ca-pub-XXXXXXXXX` will need to be replaced with your own [Publisher ID](https://support.google.com/adsense/answer/105516?hl=en) from Google Adsense.

```javascript
// https://developers.google.com/ad-placement/docs/example
(function() {
    var script = document.createElement('script');
    script.setAttribute('crossorigin', 'anonymous');

    // Comment out to disable test mode
    script.setAttribute('data-adbreak-test', 'on');

    // Ad frequency is user configuble
    // https://developers.google.com/ad-placement/docs/ad-rate
    script.setAttribute('data-ad-frequency-hint', '30s');

    // Comment in if you would like to use a specific channel for ad reporting
    // https://developers.google.com/ad-placement/docs/advanced-reporting
    // script.setAttribute('data-ad-channel', 'XXXXXX');

    script.src = 'https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXX';
    document.head.appendChild(script);
})();
```

[`afg-setup.js`](https://playcanvas.com/editor/code/877568?tabs=67301236) is our wrapper to make working with ads a bit easier. It will configure Google's SDK and add some helper functions to use for pausing the game before an ad is shown and resuming afterwards.

This includes:

- disabling/enabling input
- muting/unmuting sound
- setting/resetting time scale
- disabling/enabling rendering

```javascript
(function () {
    var app = pc.Application.getApplication();

    window.adsbygoogle = window.adsbygoogle || [];
    var afg = {};

    afg.adBreak = window.adConfig = function (o) { adsbygoogle.push(o); };
    afg.ready = false;

    var onReady = function () {
        afg.ready = true;
    };

    // Config the ad setup
    adConfig({
        // https://developers.google.com/ad-placement/docs/preload-ads
        preloadAdBreaks: 'on',
        // https://developers.google.com/ad-placement/docs/manual-sequence
        onReady: onReady,
    });

    // Create help functions to disable input and sound
    var _mouseTargetElement = null;
    var _keyboardTargetElement = null;
    var _touchTargetElement = null;
    var _beforeAdVolume = null;
    var _beforeAdTimeScale = null;
    var _beforeAutoRender = null;

    var beforeAdCalled = false;

    afg.onBeforeAd = function () {
        // Currently using private API for this. May need to be updated
        // if PlayCanvas engine is updated
        _mouseTargetElement = app.mouse._target;
        _keyboardTargetElement = app.keyboard._element;

        if (app.touch) {
            _touchTargetElement = app.touch._element;
        }

        _beforeAdVolume = app.systems.sound.volume;
        _beforeAdTimeScale = app.timeScale;
        _beforeAutoRender = app.autoRender;

        app.systems.sound.volume = 0;
        app.timeScale = 0;
        app.autoRender = false;

        beforeAdCalled = true;
    };

    afg.onAfterAd = function () {
        // Protect against assigning a null element in case this is called
        // without onBeforeAd being called
        if (beforeAdCalled) {
            app.mouse.attach(_mouseTargetElement);
            app.keyboard.attach(_keyboardTargetElement);

            if (app.touch) {
                app.touch.attach(_touchTargetElement);
            }

            app.systems.sound.volume = _beforeAdVolume;
            app.timeScale = _beforeAdTimeScale;
            app.autoRender = _beforeAutoRender;
        }

        beforeAdCalled = false;
    };

    window.afg = afg;
})();
```

## Adding an Interstitial Ad

Let's start with adding an interstitial ad to the project.

Add the following code to the interstitial ad button click event callback in `ui-controller.js`:

When the refresh button is pressed, we disable the reward button entity so that it isn't shown to the player and we are in a known state.

In the `beforeReward` callback, we keep a reference to the function to show the ad and enable the reward button as we know we can show an ad to the player.

In the `adViewed` callback, we call our helper function `onAfterAd` to resume the game and show the reward to the player.

In the `adDismissed` callback, we just ensure that the reward button entity is disabled as no reward will be given.

`adBreakDone` has to have a callback but in this case, it's empty as we have no use for it in the demo. However, the [`placementInfo` object passed will have information](https://developers.google.com/ad-placement/apis/adbreak#adbreakdone_and_placementinfo) about the ad such as error messages and can be useful for debugging.

Now we need to show the ad when the reward ad button is pressed. In the callback for the button click event we add the following code:

```javascript
// ...
this.rewardedAdButtonEntity.button.on('click', function (e) {
    if (this._showRewardAdFn) {
        afg.onBeforeAd();
        this._showRewardAdFn();
    }
}, this);
// ...
```

First we check that we have the function from the SDK to show the ad first and if we do, use our helper function `onBeforeAd()` to pause the game and call the function to show the ad.

Let's see it in action:

The completed `ui-controller.js` file should look like this:

## Wrapping up

And that's it for the basics of integrating Google's H5 Games Ads for H5 games and apps!

You can find the [completed project here](https://playcanvas.com/project/889020/overview/google-h5-ad-tutorial-finished) and further information about Google's H5 Games Ads SDK and API on [their site](https://developers.google.com/ad-placement/).

:::warning

Important: For the tutorial, we used test mode so we didn't show any actual ads. When you disable test mode to use for production, please ensure that you comply with any privacy/cookies policy for the countries you release in including GDPR. This usually means implementing some form of cookie consent.

:::

--------------------------------------------------------------------------------

## HTML/CSS - Live Updates

URL: https://developer.playcanvas.com/tutorials/htmlcss---live-updates/
Tags: html, ui, assets, tutorial

Example of how to use live HTML and CSS editing.

[Interactive Demo]

--------------------------------------------------------------------------------

## HTML/CSS UI

URL: https://developer.playcanvas.com/tutorials/htmlcss-ui/
Tags: html, ui, tutorial

Example of how to use HTML and CSS to create UI

[Interactive Demo]

--------------------------------------------------------------------------------

## Importing your first Model and Animation

URL: https://developer.playcanvas.com/tutorials/importing-first-model-and-animation/
Tags: animation, basics

[Interactive Demo]

## Overview

In this tutorial, we will be taking you through adding your first animated model to your project as you can see here.

## Getting our example assets

For this, we will be using Kenney's Animated Characters assets pack which can be downloaded from [their site](https://www.kenney.nl/assets/animated-characters-1).

After downloading, unzip the file and let's have a look at the contents.

They've split the model and animation files from each other which is perfect for us.

## Importing the model FBX

We are interested in the model file to begin with.

Let's open the model folder and drag the FBX file into our project assets panel.

This will import the file into our project and the PlayCanvas Editor will create materials, container, render assets and most importantly, a template asset.

If you are importing FBXs into an existing project, please make sure you have the following settings ticked in the Project Settings -> Asset Tasks.

## Adding the model Template to the scene

The template asset (known as prefabs in other game engines) contains the full Entity hierarchy and is used to add the model to the scene.

We can do this multiple ways:

Dragging it into the 3D viewport will add it as a child of the currently selected Entity.

Dragging it onto an Entity in the hierarchy view will add it as a child of that Entity.

Dragging it into the empty area of the hierarchy view will add it as child of the topmost Entity in the scene.

And finally we have the Template context menu options when right clicking on an Entity.

Now that we have it in the scene, we can see it's untextured. In this case, the textures were not embedded into the FBX file and have been supplied separately.

If they were embedded in the FBX, the import process would also create texture assets and assign them to the correct materials.

You can see the texture assets being created here and also the materials with textures assigned to them.

## Adding the texture to the model

Back to our Kenney Character. Let's upload one of the textures from the skins folder and apply that to the material of the model.

We can do this by either dragging the texture asset to the material slot.

Or we can use the 'edit' button on the material slot and find it in the assets panel.

Fantastic! We now have a fully textured model in the scene, ready to be animated.

## Importing the animation FBX

As mentioned earlier, the animations are in a separate FBX so let's import the run animation into the Editor.

This will create a number of assets which includes the animations. As that's all we need, we can delete the other assets that were created.

From here, we can preview the animation by selecting it in the assets panel and in the inspector. We will also need to select a preview asset to apply the animation to. In this case, it will be the template asset created for the model FBX imported earlier.

## Animating the model

To add the animation to the character, we need to use the Anim Component on the Entity and also create an Anim State Graph asset.

The Anim State Graph asset is created by right clicking in the assets panel and selecting 'New Asset' -> 'Anim State Graph'.

The graph asset can have multiple states and each state can be assigned an animation. The flow between the states are controlled by transitions with conditions.

For the purposes of this tutorial, we will just be focusing on a single state and single animation. More documentation and tutorials for animation can be found [here](/user-manual/animation/).

To add the graph to our model, we select the template Entity instance in the scene and add an Anim Component in the inspector.

In the Anim Component, we assign it our Anim State Graph asset and the states in the graph will appear in the component. Here, we can assign the animation asset to the state and launch the project.

And that's it! We've successfully imported our first model and looping animation into PlayCanvas!

--------------------------------------------------------------------------------

## Information hotspots

URL: https://developer.playcanvas.com/tutorials/information-hotspots/
Tags: rendering, camera, raycast, tutorial, input

Sample showing information hotspots on a scene.

[Interactive Demo]

--------------------------------------------------------------------------------

## Making a Simple Game - Part 5

URL: https://developer.playcanvas.com/tutorials/keepyup-part-five/
Tags: games

[Interactive Demo]

*You can find the [full project here](https://playcanvas.com/project/406050). If you haven't see [Part 1](/tutorials/keepyup-part-one/), [Part 2](/tutorials/keepyup-part-two/), [Part 3](/tutorials/keepyup-part-three/) and [Part 4](/tutorials/keepyup-part-four/) read them first.*

## Audio & Particles

### Audio

Audio is a critical part of your game. It will provide valuable feedback to the player that their inputs are having an effect, plus it can create mood and atmosphere.

The Keepy Up game has 3 audio effects: A background music track, a tap sound when you hit the ball and the sad trombone sting when you lose. Each of them is handled slightly differently.

#### Music & Sting

The music and sting are handle in a similar way. The main difference is that the music is set to loop. The `game.js` we have a script attribute which links the game script to the Entity with our sound component and we simply play and stop the correct slot.

```javascript
this.audio.sound.stop(); // stop current sound playing
this.audio.sound.play("gameover") // play the 'gameover' slot
```

#### Ball tap

The ball tap sound is attached directly to the ball Entity. It's a short, non-looping sound. So we play it every time a tap hits the ball.

```javascript
this.entity.sound.play("bounce");
```

### Particles

[Image: Particles]

We have one particle effect in Keepy Up. It's a dust cloud that is triggered whenever the ball is tapped. The dust cloud is a non-looping effect and it needs to be positioned and rotated so that the cloud moves away from the ball when it runs.

```javascript
this.impactEffect.setLocalPosition(tmp);
this.impactEffect.particlesystem.reset();
this.impactEffect.particlesystem.play();
this.impactEffect.lookAt(this.entity.getPosition());
```

In this code we restart the one shot particle effect by calling `reset()` and `play()` and we position and rotate it so that it points towards the center of the ball.

[Image: Curves]

Using the Local Velocity graph in the Particle Effect editor, the particle effect is set up to fire away from the direction it is facing i.e. the particles move along the positive z axis.

Continue on to [Part 6](/tutorials/keepyup-part-six/).

--------------------------------------------------------------------------------

## Making a Simple Game - Part 4

URL: https://developer.playcanvas.com/tutorials/keepyup-part-four/
Tags: games

[Interactive Demo]

*You can find the [full project here](https://playcanvas.com/project/406050). If you haven't see [Part 1](/tutorials/keepyup-part-one/), [Part 2](/tutorials/keepyup-part-two/) and [Part 3](/tutorials/keepyup-part-three/) read them first.*

## The Football

The football is the center of attention in our Keepy Up game. It responds to player input, it responds to the environment (well, gravity), it makes sounds. It's probably the most complicated part of the game. Fortunately, we're going to explain all the bits to you as simply as we can.

## ball.js

### Script Attributes

The first thing you'll notice at the top of the script are a set of script attributes that we've defined. Defining script attributes lets you expose values from your script into the editor. There are three very good reasons to do this.

[Image: Script Attributes]

First, it lets you use the same script for many different Entities with different values. For example, you could have a script attribute which sets a color, and in the editor create a red, blue and green version of a entity just by modifying the script attribute.

Second, you can quickly and easily tune the behavior of scripts. When you modify a script attribute (or indeed any property from the editor) the changes are made instantly to any instance of the game that you have launched from the editor. So for example in the case of the `ballMinimum` property we define here, you can launch the game and test what the value of `ballMinimum` should be to allow the ball to drop off the bottom of the screen without ever having to reload the game. Test the game, modify the value, test the game.

This is known as "iteration speed". The faster you can modify and test your game, the quicker you can get it developed!

For the ball, we define script attributes that let us tweak a number of game play properties like the gravity, the impulse applied when the ball is tapped. These attributes let us very quickly tune the game to our liking.

Third, the script attribute is a great way to link a script to an Entity or an Asset in your scene. For example, the ball script needs to trigger a particle effect when it is tapped. The particle effect is on another Entity in our scene. We define a script attribute called `impactEffect` of type `entity` and in the Editor we link this to the entity with our particle effect. Our script now has a reference to the entity and we are free to modify this entity or change to another entity without breaking our code.

### The Physics Simulation

For those of you with some basic vector maths knowledge this `update()` loop of the ball should be simple, but for everyone else we'll explain a little about simulating a ball in a video game.

A simple way to simulate something in video game is to give that object an acceleration, a velocity and a position. Every time step (or frame) the acceleration (which is the rate of change of velocity) changes the velocity and the velocity (which is the rate of change of position) changes the position. Then you draw your object at the new position.

You can influence the position of your object in one of three ways.

* **Change the acceleration**, this is useful for applying a force over a period of time, like gravity on the ball.
* **Change the velocity**, this is an instantaneous change. Like a ball bouncing off the floor.
* **Change the position**, like teleportation, there isn't a real world equivalent!

In our simulation we have a constant acceleration due to gravity, when you tap the ball we apply an instant change in velocity and when you reset the game we teleport the ball back to it's starting position.

#### Simulating

The update loop does this:

```none
(Change in Velocity) = (Acceleration) * (Time since last frame)
(New Velocity) = (Old Velocity) + (Change in Velocity)
(Change in Position) = (New Velocity) * (Time since last frame)
(New Position) = (Old Position) + (Change in Position)
```

In code this looks like this:

```javascript
var p = this.entity.getLocalPosition();

// integrate the velocity in a temporary variable
tmp.copy(this._acc).scale(dt);
this._vel.add(tmp);

// integrate the position in a temporary variable
tmp.copy(this._vel).scale(dt);
p.add(tmp);

// update position
this.entity.setLocalPosition(p);
```

You will note that we use temporary vector `tmp` to store intermediate values. It's important not to create a new vector every frame for this. Also notice that we have to call `setLocalPosition` to apply the updated position.

Finally, for a nice effect we add rotate the ball by the angular speed value using `entity.rotate()`. This isn't very physically accurate, but it looks nice.

#### Responding to input

You may remember from [Part 2](/tutorials/keepyup-part-two/) that the `input.js` script checked to see if an input has hit the ball and if so it calls the `tap()` method. The `tap()` method defined above applies a direct change to the velocity and the angular speed of the ball. We use a couple of our script attributes `this.speedMult` and `this.angMult` to multiply the new velocity and angular speed to match our expectations of the gameplay.

We also use the tap method to trigger a particle dust cloud at the point of impact and play a sound effect. We'll talk about particle and sounds in [Part 5](/tutorials/keepyup-part-five/).

## Summary

The ball script runs a simple physical simulation to make the ball fall under gravity and respond to taps. It also listens for game events to know when to pause and reset. Finally, it interacts with some other systems to show particle effects and play sounds.

--------------------------------------------------------------------------------

## Making a Simple Game - Part 1

URL: https://developer.playcanvas.com/tutorials/keepyup-part-one/
Tags: games

[Interactive Demo]

*You can find the [full project here](https://playcanvas.com/project/406050)*

In this series of tutorials we're going to show you how a complete game is made using PlayCanvas. We've made a simple "Keepy Up" game where the object is to click or tap on the soccer ball to keep it in the air.

We'll cover these topics:

1. **The Scene and Hierarchy**
1. Material Setup
1. The Game Script
1. Ball Physics and Input
1. Audio & Effects
1. User Interface

This isn't a step-by-step guide, but we will talk about all areas of the scripts and try and explain how each bit works. We recommend you fork the Game project into your own account and follow along as we go through.

## Part 1: The Scene and Hierarchy

In PlayCanvas your scene is described by a hierarchy of Entities. Each Entity is a "thing" in your application, it will always consist of an ID, a name and a transform. A transform is a matrix which defines the position, rotation and scale of the Entity in 3D space. To build your scene you create Entities and arrange them in a tree structure which is displayed on the left panel of the editor. The tree structure allows parent Entities to affect their children, for example, all child Entities inherit their parents position, rotation and scale. Also, if you disable a parent Entity all child Entities will also be disabled.

In our Keepy Up scene we have 7 top level Entities in the hierarchy.

[Image: Hierarchy]

### Camera Entity

A Camera is where your scene is viewed from while the application is running. In this game we only have one camera and it is stationary.

### Directional Light Entity

Lights illuminate 3D models in the scene. The more lights you have active at once, the longer it will take to render a scene and this can effect the frame rate of your game. You should aim to have only a few lights active at once. In this game we have a single stationary Directional Light.

### Football Entity

The football is the main dynamic Entity in the scene. The Football Entity has 3 components attached to it. You can see the components by selecting the Football and viewing the Attribute Panel on the right side of the editor. The 3 components are:

#### Sound Component

The sound component lets you play back sound files. Each Sound component has a number of slots, one for each sound file. You can choose playback settings like, whether the sound will loop, the volume or the pitch. The football has a single slot for the sound made when the ball bounces.

#### Model Component

The model component is used to attach a 3D model asset to an Entity. When you have an enabled model component on an Entity the 3D model will be rendered at the Entity's position in the 3D space. In this case, we have attached the football model.

#### Script Component

The script component lets you attach javascript files to an Entity. Each entity will create an instance of the script inside the javascript file so that you can customize the behavior of the Entity. We'll go into more detail about the script on the football in Part 3.

### Background

The Background Entity has another model component. This time it is the back plane that forms the background to the game. The background is created using a texture of a stadium in a material asset applied to the built in Plane Entity type. We're using the Emissive slot on the material to make sure the background is bright and is not shadowed by the light and the football. This effect is a bit like a matte painting used in an old film.

### Impact Effect Entity

The Impact Effect Entity is a particle effect that plays when the ball is bounced. We'll go into more detail in Part 4.

### Audio

The Audio Entity has more sound components attached to it. This Entity is for playing the music and the game over sound.

### UI (User Interface)

The UI Entity is the parent of several other Entities, one for each screen that is used for the user interface of the game. We'll cover the UI Entity in Part 5.

[Part 2](/tutorials/keepyup-part-two/) covers the main game script.

--------------------------------------------------------------------------------

## Making a Simple Game - Part 6

URL: https://developer.playcanvas.com/tutorials/keepyup-part-six/
Tags: games

[Interactive Demo]

*You can find the [full project here](https://playcanvas.com/project/406050). If you haven't see [Part 1](/tutorials/keepyup-part-one/), [Part 2](/tutorials/keepyup-part-two/), [Part 3](/tutorials/keepyup-part-three/), [Part 4](/tutorials/keepyup-part-four/) and [Part 5](/tutorials/keepyup-part-five/) read them first.*

## User Interface

The user interface of a game is the first thing you see and often it's the last thing people think about when making a game. But a good user interface will not only give your game a great style. It can actively affect the number of people who can play your game.

User Interfaces are built in PlayCanvas using the [Screen Component](/user-manual/editor/scenes/components/screen/) and the [Element Component](/user-manual/editor/scenes/components/element/). There are more details in the [user manual](/user-manual/user-interface/)

### User Interface Entities

[Image: Hierarchy]

We have divided our user interface up into the three game states: Menu, In Game and Game Over. Each state has an Screen Entity which is activated when the state is entered. We also have a Screen especially for the overlay. Because the overlay displays underneath the ball in the menu, we use a 3D Screen component to render the overlay in world space behind the ball.

### Images and Text

[Image: Image Element]

Images and Text are added to the user interface using the Element Component. This Component can display images in the form of a texture asset or text in the form a font asset.

[Image: Image Attriubtes]

### User Interface script

Let's take a look at the script for the main menu.

First we have set up an attribute with a reference to the overlay element. The overlay element is a full screen element which tints the screen green. We also use this to detect input as we only care about the user clicking on the full screen.

When the Entity is enabled we display the full screen overlay by enabling it and then we start listening for the click event. When the Entity is disabled we stop listening for the event and we hide the overlay. When an event is triggered we fire a "ui:start" event which the main game script is listening for and that triggers a change of game state.

We repeat similar behavior in the other two UI scripts where we listen for game events and fire ui events.

## Game Complete

Congratulations on reaching the end of the series! We hope you've learned a lot about how you can structure and build a game using PlayCanvas. If you have any feedback on the tutorial, please get in touch on our [forum](https://forum.playcanvas.com).

--------------------------------------------------------------------------------

## Making a Simple Game - Part 3

URL: https://developer.playcanvas.com/tutorials/keepyup-part-three/
Tags: games

[Interactive Demo]

*You can find the [full project here](https://playcanvas.com/project/406050). If you haven't see [Part 1](/tutorials/keepyup-part-one/) and [Part 2](/tutorials/keepyup-part-two/) read them first.*

## The Game script & Input

These two scripts `game.js` and `input.js` are attached the root entity in the scene, called "Game". Scripts are generally executed in the order they are encountered in the hierarchy so it's easiest to attach any non-Entity specific scripts the first Entity. You can also manage the loading order of Scripts in the Settings panel of the Editor, to load scripts first without attaching them to an Entity.

## game.js

### Game State

The game script manages the overall state of the game, it exposes some methods to alter the game state and fires events to alert other code that the game state has changed.

We've divided the game up into three main states: Menu, In Game and Game Over. The game script provides the methods to transition between each state, `start()`, `gameOver()` and `reset()`. Each one sets the `_state` variable to remember which state we're in; fires an application event to alert other scripts to state changes; switches user interface elements on and off; and manages the state of the music or game over sound effect.

These state change methods will be called from other scripts when the appropriate trigger events occur. For example, the `gameOver()` method is called by `ball.js` when the ball goes off the bottom of the screen.

### Application Events

Let's pause to take a look at the way the game script fires events on the application.

```javascript
this.app.fire("game:start")
```

Events are an extremely useful way to communicate form one script to many other scripts. The way an event works is that an object (in this case `this.app`) chooses to "fire" an event. Any other code that has access to the object can choose to listen to one or more events on this object and the code will be notified when the event is fired.

One of the issues with this is that the code needs access to the object in order to start listening to the event. This is why application events are so useful. Every script in PlayCanvas has access to `this.app`. That makes it useful to act as a central communications hub between any other scripts.

We have chosen to adopt a namespace pattern to make events clearer and avoid clashes. To listen for the `game:start` event from above. You would use this code:

```javascript
this.app.on("game:start", function () {
    console.log("game:start event was fired");
}, this)
```

### Scoring

The game script also manages the current score. It exposes methods that are used to modify the score and also fires events to let other code know that the score has changed.

### Resolution

Finally the game script handles the initial choice of resolution to make sure the main canvas is the correct size on both mobile and desktop. On mobile (defined by a screen less than 640 pixels wide) the game simply fills the entire screen. On desktop we use the predefined resolution set in the project settings.

## input.js

The input script listens for input from the mouse or touchscreen, normalizes the input from the two into a general purpose "tap" and communicates with the rest of the application that a tap has occurred.

First, in initialize we are setting up event listening. We listen for application events to determine if the game is in a paused state (that is in the menu or in the game over state). If the input is paused we don't respond to the taps. Next we listen for touch events (note, you must check if `this.app.touch` is available) and mouse events.

### Touch Events

For touch events we take the first touch and pass through the screen co-ordinates. We also call `preventDefault()` on the browser event to stop the browser also generating a `click` event which it will do otherwise.

### Mouse Events

On "mousedown" events we pass the screen co-ordinates through to the tap code. Note, that PlayCanvas ensures that touch and mouse events have the same coordinate system. This is not the case with normal browser events!

### Taps

`_onTap()` takes a screen co-ordinate (x, y) works out if this has "hit" the ball and if so tells the ball code that it has been tapped.

```javascript
this.camera.camera.screenToWorld(x, y, camPos.z - p.z, worldPos);
```

In detail, this function takes the screen co-ordinates (x, y) and asks the camera to convert them into a position in 3D space under that point on the screen. To do this, we need to supply a depth, as in how far away from the screen do you want the 3D point. In this case we get the 3D point at the same depth as the ball is.

We also pass in a vector `Input.worldPos`. It's important in PlayCanvas applications to avoid creating new objects, like calling `new pc.Vec3()` to create a new vector, in your update loops. The more memory allocations you do (by calling `new`) the more Garbage Collection the browser will have to do to clear up your allocations. Garbage Collection is a (comparatively slow) operation and will cause your game or application to stutter if it happens often.

In most cases, PlayCanvas will provide an option to pass in vector or similar option so that you can pre-allocate and re-use objects.

```javascript
// get the distance of the touch/click to the ball
var dx = (p.x - worldPos.x);
var dy = (p.y - worldPos.y);

// If the click is inside the ball, tap the ball
var lenSqr = dx*dx + dy*dy;
if (lenSqr < this.ballRadius*this.ballRadius) {
    this.ball.script.ball.tap(dx, dy);
}
```

Once we have the the 3D point where we've just tapped, we test to see if it is overlapping with the ball. You'll see here we are testing the radius squared against the distance between the tap and the ball squared. This prevents us doing a slow Square Root operation every time we test.

If the tap has hit the ball, we call the `tap(dx, dy)` function on the ball script we pass in the distance from the ball where the tap occurred. We'll use that in the [Part 4](/tutorials/keepyup-part-four/).

--------------------------------------------------------------------------------

## Making a Simple Game - Part 2

URL: https://developer.playcanvas.com/tutorials/keepyup-part-two/
Tags: games

[Interactive Demo]

*You can find the [full project here](https://playcanvas.com/project/406050). If you haven't seen [Part 1](/tutorials/keepyup-part-one/) read it first.*

## Material Setup

We've kept the graphics very simple for this game so there isn't too much set up but will have to set up some Material assets for the ball, the background and the overlay.

### What is a material?

A Material is a type of asset in PlayCanvas that describes the way that the surface of a 3D model looks when it is rendered to the screen. The material determines the color of the surface, but also the way the surface interacts with light. For example, you can set up a material to be a smooth metal, or a rough rubber. PlayCanvas has a built in material called a Physical Material which should cover 90% of your use cases for a material. When you get more advanced it is possible to create your own materials by using GLSL shader code to write your own.

### Cubemap

Before we jump into the materials, we're going to set up our cubemap asset. A cubemap is an asset that consists of 6 textures on the faces of a cube. You can think of this as the far off environment of your scene. The PlayCanvas Physical materials can use a cubemap to do Image Based Lighting. This uses the colors in the cubemap to light materials in the scene. For example in our scene the cubemap has a blue sky and a green grass surface. So our ball will be lit blue from above and green from below. [Read more](/user-manual/editor/assets/inspectors/cubemap/) in our documentation.

[Image: Cubemap]

To set up a cubemap, first create a Cubemap asset from the New Asset menu, assign 6 textures, one to each face of the cubemap. When your cubemap has all 6 faces assigned, press the "Prefilter" button. **Prefiltering in required for the cubemap to work correctly!**

## Football Material

[Image: Football]

The football is a 3D model that we imported from the [PlayCanvas Asset Library](https://store.playcanvas.com/). The football materials will mostly be set up for you but you may need to make some simple changes. Lets look at each map used by the football

### Diffuse

[Image: Diffuse]

The diffuse map defines the color of the surface. In this case it is the black and white pattern of the football.

### Environment

[Image: Environment]

We're going to jump quickly to the Environment section as this is needed to see the effects of the Specular changes. To set up the Environment drag the Cubemap asset on the the cubemap slot in the material.

### Specular

[Image: Specular]

The specular section determines how the material interacts with lights. There are two "workflows" for specular, the "Metalness workflow" and the "Specular workflow", you can find more information in our [documentation](/user-manual/graphics/physical-rendering/physical-materials/). For the football we are using Metalness, so be sure to check the "Use Metalness" box. Our football is not made of metal, so we slide the metalness slider all the way down to 0. Then we use the glossiness slider to set our rough/smooth the material should be. We've set this to about halfway which gives a nice shine to the ball, but not completely smooth.

### Normal

[Image: Normal]

The final texture map we apply is the normal map. The normal map is used to add details to the model. In this case, the separate patches of the football are not modelled in the geometry (which is smooth sphere) but they are in the normal map. This means the ball will be correctly lit as if the patches were there, but without the overhead of lots of extra polygons.

## Backdrop Material

[Image: Backdrop]

The backdrop material is much simpler, we have only one texture map.

### Emissive

[Image: Emissive]

The emissive map sets the color at the surface as if it was emitting light of that color. This means that lights in the scene don't effect how bright an emissive surface is. Note, an emissive material doesn't affect any other objects in your scene, there is no light actually emitted.

In this case, we don't want the background to be lit, it is just a static scene. So we use the emissive map to do that.

## Overlay Material

[Image: Overlay]

The overlay material is even simpler than the backdrop. In this case all we are doing is setting the color of the emissive property

[Image: Emissive]

Continue on to [Part 3](/tutorials/keepyup-part-three/).

--------------------------------------------------------------------------------

## Basic Keyboard Input

URL: https://developer.playcanvas.com/tutorials/keyboard-input/
Tags: input

[Interactive Demo]

*Click to focus, then press `left arrow`, `right arrow` and `spacebar` to rotate the cube. Press and release the 'a' key to change color.*

Keyboard handling in the PlayCanvas engine is provided by the `pc.Keyboard` object. The Keyboard object provides a simple interface
for common keyboard operations like checking if a key is pressed or held down. It also takes away the various cross-browser problems with
handling keycodes and charcodes.

Take a look at the keyboard input Scene in the [tutorials project](https://playcanvas.com/project/405804/overview/tutorial-basic-keyboard-input). Here is the code for the tutorial:

There are two ways of detecting keyboard input. The first is done in the update method of your scripts. Use `isPressed()` and `wasPressed()` and check whether a key is currently pressed or has just been pressed. The second uses events to respond to a key press or release as it happens.

## `isPressed` vs `wasPressed`

In the demo above you can see the difference in behavior between `isPressed()` and `wasPressed()`.

When you press and hold the left or right arrow keys the cube will rotate by 5&deg;, but it will only rotate once. This is because `wasPressed()` only returns true for the frame immediately after the key was pressed.

If you press and hold the spacebar you will see that the cube rotates continuously by 1&deg; per frame. This is because `isPressed()` returns true for every frame in which the key is pressed.

### `isPressed(key)`

`isPressed(key)` checks to see if `key` is currently pressed and returns true if it is. It will return true for every frame while the key is pressed.

### `wasPressed(key)`

`wasPressed(key)` checks to see if `key` was pressed *since the last frame*. `wasPressed()` will only return true once for a single key press.

## Events

The second method of handling key presses is to listen for events. Two keyboard events are supported on the Keyboard device:

* `pc.EVENT_KEYDOWN`
* `pc.EVENT_KEYUP`

[DOM](/user-manual/glossary#dom) keyboard events are implemented differently on different browsers so the PlayCanvas Engine provides events on the `pc.Keyboard` object so you can use the same code everywhere. When the keyboard events are fired the event handler is passed a `pc.KeyboardEvent` object which contains the key code of the key that was pressed on released.

Notice we are also passing a third argument to on(), which is `this` or the Script Instance itself. The third argument to on() is used as `this` in the event callbacks, so we need to pass it in here, otherwise it won't be set to the correct object.

## Key Codes

Identifying which key is pressed is done using key codes. These are numerical values which match up to a key on the keyboard. For example, pc.KEY_A is the `A` key, pc.KEY_LEFT is the left arrow key.

Note, you should always use the enumeration `pc.KEY_*` rather than using numerical values. As the actual value of these constants may change in the future.

## Try it out

Try it out in full screen [here](https://playcanv.as/p/rFZGQWCi/) or at the top of the page. Compare tapping and holding the arrow keys, and tapping and holding the spacebar.

--------------------------------------------------------------------------------

## Light Cookies

URL: https://developer.playcanvas.com/tutorials/light-cookies/
Tags: lighting

[Interactive Demo]

Find out more by forking the [full project](https://playcanvas.com/project/409793/overview/example-light-cookies).

In theatre it is common to create visual effects on stage by using shape masks to cast shadows across the scene. These masks are called "cucoloris" or "cookies". Creating a light cookie in WebGL using PlayCanvas is achieved using a texture or a cubemap that is applied to the Light Component. Light Cookies are applicable to spot lights and omni lights.

[Image: Grid Cookie]
**A simple grid cookie gives the impression of a window**

## Cookie Texture

A cookie texture is just a regular texture asset which is applied to a spot light component in the Cookie section. When a cookie is applied you can choose which channels of the texture to use. For example, if you are doing a simple mask, you can use just the Alpha channel. Or if you would like a full color effect like a stained glass window you can use the full RGB value.

[Image: Light Settings]

## Cookie Cubemap

For an omni light the cookie needs to be a cubemap so that it can be projected in 6 directions around the point of the light.

## Use Cases

**Shaped Lights** - cookies are useful for masking the shape of a light source. For example, if your scene contains a window or a fluorescent strip light using a spot light will create a light shape that is rounded. You can mask the shape of the light to be long and thin, or square in order to simulate the shape of the light.

[Image: Shaped Light]

**Torches** - Torches and headlights don't light a surface evenly instead they have light and dark "caustic" areas. You can use a light cookie to create this effect.

[Image: Torch Light]

--------------------------------------------------------------------------------

## Light Halos

URL: https://developer.playcanvas.com/tutorials/light-halos/
Tags: lighting

[Interactive Demo]

Find out more by forking the [full project](https://playcanvas.com/project/406040).

This simple effect is great for adding atmosphere to your scene. Add a glow to a light source or an emissive texture to give the effect of a dusty or foggy atmosphere or simulate the effect of looking at a bright light.

It works like this: We create an entity with a plane primitive attached which has a glowing halo material on it. We attach a script to entity which makes the plane always face the camera (billboarding). For added fun, we're fading the halo out if it faces away from the camera to simulate a directional light.

## Assets

### Texture

First you'll need a halo texture. In this example we've just used a very simple blurred blob that was created in an art program like Photoshop.

[Image: blob]

This texture will form the basis of the glow.

### Material

<img loading="lazy" src="/img/tutorials/intermediate/light-halos/material.png" height="600" />

The material for the light halo uses the blob texture in the emissive slot. Use the **tint** property to set the color of your halo. We've also enabled blending in the Opacity slot and it also uses the blob texture with **Color Channel** set to **R**.

The **Blend Type** is set to **Additive Alpha**. The **Additive** part means that the color of the material is added to the color of background underneath it. This means the halo glows against the background. The **Alpha** part means it uses the value of the `opacity` to set how transparent the material is.

## Entities

[Image: entities]

The Entity setup for the glow is simple too. We have a parent Entity for the halo script and a child Entity which has the plane primitive attached to it. The reason we do this is to simplify the code so that we can use `entity.lookAt` to set the orientation of the glow. The Plane primitive faces upwards so we create a child entity and apply a rotation to this child so that the plane is correctly positioned facing the camera.

## Code

The code for this project has two particularly interesting features.

First, we update the halo entity to face the camera every frame

```javascript
// Set the glow to always face the camera
this.entity.lookAt(this.camera.getPosition());
```

Second, if the halo is marked as `unidirectional` (with a script attribute that we've exposed), then we modify the opacity so that the halo is invisible when it is facing away from the camera. In fact we slowly modify the opacity so that it gets more transparent the more it points away from the camera.

```javascript
// If enabled, unidirectional means the glow fades off as it turns away from the camera
if (this.unidirectional) {
    // Get the dot product of the parent direction and the camera direction
    var dot = -1 * tmp.dot(this.camera.forward);
    // If the dot product is less that 0 the glow is facing away from the camera
    if (dot < 0) {
        dot = 0;
    }

    // Override the opacity value on the planes mesh instance to fade to zero as the glow turns away from the camera
    meshes[0].setParameter("material_opacity", dot);
} else {
    // Need to set a default value because of this issue for now: https://github.com/playcanvas/engine/issues/453
    meshes[0].setParameter("material_opacity", 1);
}
```

We're using the `setParameter` method on the [pc.MeshInstance](https://api.playcanvas.com/engine/classes/MeshInstance.html) to set a value for the fragment shader to use. This is currently an undocumented feature in the engine (you won't find it on the link to the developer docs). The reason for this is because it relies on knowing exactly the name of the uniform variable in the shader. These values might change and this API might change in the future. But it's pretty useful, because it lets you override a single value in a shader just for that mesh instance. In this case, it's important because all the glows use the same material, but we will want a different value for the opacity for each instance of the glow.

Here's the complete listing:

That's it. A simple but pretty effect to add to your scene. Take a look at the [project](https://playcanvas.com/project/406040) for more information.

--------------------------------------------------------------------------------

## Load assets with a progress bar

URL: https://developer.playcanvas.com/tutorials/load-assets-with-a-progress-bar/
Tags: assets, tutorial, scripts

Sample showing how to load multiple assets at runtime with a progress bar.

[Interactive Demo]

--------------------------------------------------------------------------------

## Load multiple assets at runtime

URL: https://developer.playcanvas.com/tutorials/load-multiple-assets-at-runtime/
Tags: loading, assets, tutorial

Sample showing how to load multiple assets at runtime.

[Interactive Demo]

--------------------------------------------------------------------------------

## Loading an asset at runtime

URL: https://developer.playcanvas.com/tutorials/loading-an-asset-at-runtime/
Tags: loading, assets, tutorial

Sample showing how to load an asset at runtime so you don't have to preload it at the start if it is not used.

[Interactive Demo]

--------------------------------------------------------------------------------

## Loading Circle UI

URL: https://developer.playcanvas.com/tutorials/loading-circle-ui/
Tags: materials, ui, animation, tutorial

An example of a radial loading circle

[Interactive Demo]

--------------------------------------------------------------------------------

## Loading Draco Compressed GLBs

URL: https://developer.playcanvas.com/tutorials/loading-draco-compressed-glbs/
Tags: rendering, assets, tutorial, scripts

How to load a Draco compressed GLB.

See https://google.github.io/draco/ for more information on Draco 3D Data Compression.

See https://github.com/CesiumGS/gltf-pipeline for the tool to compress glTF models.

[Interactive Demo]

--------------------------------------------------------------------------------

## Loading glTF GLBs

URL: https://developer.playcanvas.com/tutorials/loading-gltf-glbs/
Tags: rendering, assets, tutorial, scripts

How to load a GLB as a binary asset.

[Interactive Demo]

--------------------------------------------------------------------------------

## Loading JSON Data

URL: https://developer.playcanvas.com/tutorials/loading-json/
Tags: loading

[Interactive Demo]

[This project](https://playcanvas.com/project/405827) shows you how to load JSON data in two ways. First, from an asset in the project. Second, over HTTP from a remote server.

## Loading JSON from an asset

You can see in the code above that all you need to do to load JSON data from an asset in your project is to use a Script Attribute of type 'asset' or to retrieve the asset from the asset registry, then access the `resource` property. For an asset of type `json` the data will already be parsed into a standard javascript object when you access the `resource` property.

Once you have the javascript object you can access the data as normal. For example, looping through properties as in `parseCharacterData`.

## Loading JSON from a remote server

In this code we are using the XMLHttpRequest object (which is part of the standard web browser API) to request JSON data from a URL, in this case the Github API.

After receiving the `"load"` event we parse the JSON data using `JSON.parse` (another part of the standard web browser API) and return the data via the `callback` function.

Note, that the call to `loadJsonFromRemote` is **asynchronous**.

Here is the full code listing:

Try [the project](https://playcanvas.com/project/405827) for yourself.

--------------------------------------------------------------------------------

## Locking the mouse

URL: https://developer.playcanvas.com/tutorials/locking-the-mouse/
Tags: input, camera, mouse, tutorial

A sample showing how to lock the mouse upon clicking.

[Interactive Demo]

--------------------------------------------------------------------------------

## Manipulating Entities

URL: https://developer.playcanvas.com/tutorials/manipulating-entities/
Tags: basics

In this tutorial we'll show you how you can change an Entity's position, orientation and scale.

Entities form the basis of most applications built using the PlayCanvas framework. An Entity can represent anything from the player character, a bullet, an enemy or just simply be a point in space.

Entities are a special form of graph node, they inherit a lot of their behavior from `pc.GraphNode`. All the manipulations we apply below can also be applied to graph nodes.

One of the most common operations you will need to perform on Entities is to change its transform matrix. The local transform property of the Entity determines the position, orientation and scale of the Entity and affects all child Entities as well. Learning how to manipulate the transform is critical to making interesting and interactive applications.

### Local and World Co-ordinates

An important part of understanding how to move and manipulate Entities is understanding local and world co-ordinate systems. The world co-ordinate system is shared by all Entities, it has a fixed origin `(0,0,0)` and a fixed orientation - where `(0,1,0)` is up. The local co-ordinate system is relative to the Entity itself. So the local origin is the Entity position, and the orientation follows the orientation of the Entity.

Here is a visual representation of the world-space coordinate system (left) and the local-space coordinate system (right) of an entity:

[Image: World and Local Coordinate Systems]

### Hierarchy

An important part of the Entity system to understand is the Entity Graph or Hierarchy. As Entities are a type of graph node they are collected together in a graph or a hierarchy of parents and children. Each Entity can have a single parent and multiple children. Child Entities inherit transformation information from their parents. An Entity's world transformation matrix is multiplying the local transform by the world transform of the parent Entity. So, for example, if a child Entity has a local translation of `(1,0,0)` and it's parent has a local translation of `(0,1,0)`, the world position of the child will be `(1,1,0)`

## Position

Getting the position of the entity is straightforward

```javascript
// Get the entity's position relative to the coordinate system of the entity's parent
var lp = entity.getLocalPosition();

// Get the entity's position in world space
var wp = entity.getPosition();
```

These methods both return a `pc.Vec3` (a vector quantity in the array form [x,y,z]).

Setting the position of an entity is just as straightforward.

```javascript
// Set the entity's position relative to the coordinate system of the entity's parent
entity.setLocalPosition(x, y, z);

// Set the entity's position in world space
entity.setPosition(x, y, z);
```

### Moving the entity

To move the Entity you can add to the Entity's position or you can use the helper functions translate and translateLocal.

```javascript
// Translate the entity 1 unit down the positive x axis of world space
entity.translate(1, 0, 0);

// Translate the entity 1 unit down the entity's local z axis
entity.translateLocal(0, 0, 1);
```

## Orientation

To set an Entity's orientation you can either set an absolute rotation, or apply an incremental rotation.

Setting absolute rotations can be done using either [Euler angles](https://en.wikipedia.org/wiki/Euler_angles) or [quaternions](https://en.wikipedia.org/wiki/Quaternion). The Wikipedia explanations of these two mathematical representations of rotation are a little hard to follow but the basics are easy to understand. Here are the important facts:

### Euler Angles

* Euler angles are three rotations in degrees about the X, Y and Z axes of a coordinate system *in that order*.
* If looking down a coordinate system axis, a positive Euler angle will result in an anti-clockwise rotation around that axis.
* Euler angles are easy to understand because you can visualize the effect they will have in your head.

### Quaternions

* Quaternions are stored as 4 numbers and represent any orientation in 3D space.
* They are difficult to set directly, but can be set from Euler angles, rotation matrices or an axis-angle representation.
* Although they are hard to visualize, they are useful since they are robust and can be quickly interpolated (when animating rotation).

When scripting entities, it is more likely that you will set an Entity's rotation using Euler angles. For example:

```javascript
// Rotate 30 degrees anticlockwise around the x axis of the parent entity's coordinate
// system and then 45 degrees around its y axis and lastly 60 degrees around its z axis
entity.setLocalEulerAngles(30, 45, 60);

// Rotate 30 degrees anticlockwise around the world space x axis and then 45 degrees
// around the world space y axis and lastly 60 degrees around the world space z axis
entity.setEulerAngles(30, 45, 60);
```

However, if you do want to set an Entity's rotation in quaternion form, you can use the following functions:

```javascript
// Create an identity rotation
var q = new pc.Quat();
// Set the entity to have the same rotation as its parent - equivalent to
// entity.setLocalEulerAngles(0, 0, 0)
entity.setLocalRotation(q);

// Set the entity to have no rotation with respect to the world space coordinate
// system  - equivalent to entity.setEulerAngles(0, 0, 0)
entity.setRotation(q);
```

To rotate an Entity incrementally, you can use rotate to rotate the Entity with respect to world space axes or rotateLocal to rotate with respect to the Entity's current axes.

For example, to rotate an Entity by 180 degrees around the world up axis:

```javascript
entity.rotate(0, 180, 0);
```

Or to rotate the Entity 90 degrees around its local x axis do:

```javascript
entity.rotateLocal(90, 0, 0);
```

## Scale

To scale an Entity you simply need to call the following function:

```javascript
// Scale the entity by a factor of 2 in the local Y axis
entity.setLocalScale(1, 2, 1);
```

And here is a slightly more interesting example:

```javascript
// Scale the entity using a sine function over time
this.timer += deltaTime;
var s = Math.sin(this.timer) + 1;
entity.setLocalScale(s, s, s);
```

Note that you cannot currently set the Entity's scale in world space.

--------------------------------------------------------------------------------

## Mobile UI Safe Areas

URL: https://developer.playcanvas.com/tutorials/mobile-ui-safe-areas/
Tags: ui, tutorial, scripts

Example project to handle safe areas on the UI - https://developer.playcanvas.com/user-manual/user-interface/safe-area/

[Interactive Demo]

--------------------------------------------------------------------------------

## More Cameras

URL: https://developer.playcanvas.com/tutorials/more-cameras/
Tags: basics, camera

[Interactive Demo]

*Click to focus, then press `space` to zoom in and out, press `left arrow` and `right arrow` to switch to the left and right cameras*

The [Basic Cameras](/tutorials/basic-cameras/) tutorial walks you through creating a camera Entity and adding it to your Scene. For a single static camera, no scripting is required. But for a more dynamic and interactive camera or for more advanced usage you might want to attach a script Component and program the camera behavior yourself.

## Altering Attributes

The first way you might want to modify a camera at runtime, is to change the values of attributes on camera Component. You do this the same way that you set attributes on any other Component, by using the `set()` and `get()`
methods on the ComponentSystem.

In this sample pressing the spacebar triggers a change in field of view. With the line `var fov = this.entity.camera.fov` we `get()` the value of `fov` from the camera component of the entity that this script is attached to.

With `this.app.keyboard.wasPressed()` we detect the keypress and toggle between the value of the target fov.

With the final two nested `if(){}` constructs we gradually change the fov values to create the zoom in/ zoom out effect.

With the line `this.entity.camera.fov = fov` we `set()` the fov camera attribute to the new value.

Notice that when you are zoomed out the top and bottom cubes are at the edges of the screen, this matches our expectation from the [PlayCanvas Editor scene](https://playcanvas.com/editor/scene/440116) where the cubes sit next to the
top and bottom sides of the camera [frustum](https://en.wikipedia.org/wiki/Frustum)

## Current Camera

Another way you might want to create interactivity with cameras is by switching between multiple cameras. You can achieve this by adding several camera Entities to your Scene; ensure that only one is activated; and then alter which is the current camera at runtime in your script.

In this sample, pressing the arrow keys sets the current camera to be a left or right camera Entity (from those that are in the currently loaded Scene) and the space key activates the central camera.

We initially create a function to find the camera entity we want by name - with the `findByName()` function applied to the parent entity of this script (given that the cameras are located there, there is no need to use `this.app.root.findByName()` to search through all the entities in the Scene).

We set up an object containing the names of the camera Entities that correspond to the arrow and space keys [(see the Editor scene)](https://playcanvas.com/editor/scene/440116).

Next we loop through the keys and if one was pressed then we find the entity by its name, and we set it to be the current camera using the `setCamera()` function we defined early in the script which disables the current active camera, then finds the new camera to activate.

--------------------------------------------------------------------------------

## Basic Mouse Input

URL: https://developer.playcanvas.com/tutorials/mouse-input/
Tags: mouse, input

[Interactive Demo]

:::info

Move the mouse to move the cube around. Press the mouse buttons to change the color of the cube.

:::

Mouse handling in the PlayCanvas engine is provided by the `pc.Mouse` object. The Mouse object provides a simple interface for detecting when the mouse is moved or when mouse buttons are pressed. It also removes some of the cross-browser inconsistencies with handling mouse co-ordinates.

Take a look at the [tutorial project](https://playcanvas.com/project/405819/overview/tutorial-basic-mouse-input). Here is the code from mouse.js:

### Accessing the mouse

Mouse control is managed by the `pc.Mouse` object. The [framework](/user-manual/glossary#framework) provides an instance of this on the [application app](/user-manual/glossary#application) which is available to all script objects as:

```javascript
this.app.mouse
```

### Disabling the right-click menu

In the constructor for our script object we disable the right-click menu to stop it popping up when we click the right mouse button.

```javascript
this.app.mouse.disableContextMenu();
```

### Binding to events

The `pc.Mouse` object allows you to listen to different events corresponding to mouse actions. In the tutorial, we are binding the method `onMouseMove` to the move event and `onMouseDown` to the button down event.

Notice how we also pass `this` into the on() method for binding to events. This third argument is the object that is used as `this` in the event callback.

```javascript
this.app.mouse.on(pc.EVENT_MOUSEMOVE, this.onMouseMove, this);
this.app.mouse.on(pc.EVENT_MOUSEDOWN, this.onMouseDown, this);
```

Events available on `pc.Mouse` are:

* `pc.EVENT_MOUSEUP` - fires when a mouse button is released
* `pc.EVENT_MOUSEDOWN` - fires when a mouse button is pressed
* `pc.EVENT_MOUSEMOVE` - fires when the mouse is moved
* `pc.EVENT_MOUSEWHEEL` - fires when the mouse wheel is rotated.

Mouse input in browsers is usually implemented by listening to [DOM](/user-manual/glossary#dom) events on elements in your page's DOM. The problem is that different browsers implement the events slightly differently and supply different values. In order to simplify the code you write the PlayCanvas engine allows you to bind your event handlers to the PlayCanvas mouse handler instead of directly the DOM Element. The engine supplies a `pc.MouseEvent` object when the event fires which is consistent across all browsers. If you do need the original DOM event, it is available as the `event` property in `pc.MouseEvent`.

### Moving the mouse

The first event handler is `onMouseMove`. This is fired whenever the mouse moves. For an `EVENT_MOUSEMOVE` event, the `MouseEvent` object will have the current position of the mouse `x` and `y` and also the change in position since the last event in `dx` and `dy`. In our tutorial we're using the current position of the mouse and moving the cube to the cursor position.

### Mouse buttons

The second event handler is `onMouseDown`. This is fired whenever one of the three mouse buttons is clicked. In the `EVENT_MOUSEDOWN` and `EVENT_MOUSEUP` events, the `MouseEvent` object will have a `button` property which contains the button that has been pressed/released. It can be one of the following values:

* `pc.MOUSEBUTTON_NONE`
* `pc.MOUSEBUTTON_LEFT`
* `pc.MOUSEBUTTON_MIDDLE`
* `pc.MOUSEBUTTON_RIGHT`

In our tutorial, we're changing the color of the cube depending on which mouse button was pressed.

### Try it out

Try the tutorial in full screen [here](https://playcanv.as/p/MHIdZgaj/) or at the top of the page. Move the mouse to move the cube and click the left, middle and right mouse button to change the color of the cube.

--------------------------------------------------------------------------------

## Multiple Camera Layers

URL: https://developer.playcanvas.com/tutorials/multiple-camera-layers/
Tags: rendering, camera, ui, tutorial

Demonstration project that shows how to use multiple cameras and layers to render a mixed user interface of UI elements and world-space objects.

[Interactive Demo]

--------------------------------------------------------------------------------

## Multiple Viewport Rendering

URL: https://developer.playcanvas.com/tutorials/multiple-viewport-rendering/
Tags: rendering, camera, tutorial

A sample showing how to render multiple viewports by modifying the viewport properties on the cameras

[Interactive Demo]

--------------------------------------------------------------------------------

## Multitouch input

URL: https://developer.playcanvas.com/tutorials/multitouch-input/
Tags: touch, input, mobile, tutorial

Sample that draws lines between all the current touches on the screen.

[Interactive Demo]

--------------------------------------------------------------------------------

## Creating a Music Visualizer

URL: https://developer.playcanvas.com/tutorials/music-visualizer/
Tags: audio

[Interactive Demo]

*Find out more by forking the [full project](https://playcanvas.com/project/405891).*

This tutorial teaches you how to create a Music Visualizer application in WebGL using PlayCanvas. We're going to take an audio stream extra frequency data and then render that data into a PlayCanvas canvas.

Our music visualizer consists of two scripts. The analyser, plays the audio and extracts the data via an Analyser Node. Which is part of the Web Audio API built into modern browsers. The visualizer, takes the data from the analyser and renders it onto screen as a funky graph.

## The Analyser

Let's take a closer look at the code here.

First we get hold of the `context`. This is the applications instance of an [`AudioContext`](https://developer.mozilla.org/en/docs/Web/API/AudioContext). We use this to create a new [`AnalyserNode`](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode) which is part of the Web Audio API the standard across all modern browsers. The `AnalyserNode` lets us access the raw data of the audio every frame as an array of values. It has a couple of properties `smoothingTimeConstant` determines whether the data sampling is smoothed over time. `0` means no smoothing, `1` means super-smooth. And `fftSize` this determines how many samples the analyser node will generate. It must be a power of two, the higher it is the more detailed and the more expensive for your CPU.

You can access the data from the `AnalyserNode` as integers, in a `Uint8Array` or as floats, in a `Float32Array`. In this demo we use floats, so we allocate two `Float32Arrays` each one needs to be half as big as `fftSize`.

The final part of the setup is to use `setExternalNodes` from the PlayCanvas SoundSlot API to insert this new node into the node chain in the Sound Component.

Then in our update loop we use the `AnalyserNode` methods `getFloatFrequencyData` and `getFloatTimeDomainData` to fill our arrays with data. We'll be using this data in our Visualizer script.

## The Visualizer

The visualizer script takes all the data from the analyser and renders it as line graph using the [`this.app.renderLines`](https://api.playcanvas.com/engine/classes/AppBase.html#renderlines) API.

In our setup we are allocating a load of vectors to use in for the lines. We need 2 for every point of data (for the start and end of the lines). Then we are setting up some scale factors to make sure the lines all appear on the screen. The `AnalyserNode` contains a min and max value of decibels that the data can contain. I've found this isn't particularly accurate (I definitely got values outside of this range) but it forms a good basis for normalizing the data.

The `renderData` function loops through all the data and sets one of our pre-allocated vectors to be the start at the current point and finish at the next point.

In our update loop we render the graphs for both the Frequency Data and the Time Domain Data.

## More ideas?

This is just a taster of how you can visualize your music. Why not try scaling 3D bars, adjusting colors and brightness in time to the music? Hook up the visualizer to SoundCloud and let users pick tracks? There are loads of possibilities.

--------------------------------------------------------------------------------

## Orange Room

URL: https://developer.playcanvas.com/tutorials/orange-room/
Tags: vr, tutorial

Interactive interior visualization with dynamic reflections and HDR lightmaps.

[Interactive Demo]

--------------------------------------------------------------------------------

## Orbit camera

URL: https://developer.playcanvas.com/tutorials/orbit-camera/
Tags: input, camera, tutorial

Sample with an orbit camera around an entity with both mouse and touch. Scroll wheel and 'pinch to zoom' is used to zoom in and out.

[Interactive Demo]

--------------------------------------------------------------------------------

## Pan Camera to Target

URL: https://developer.playcanvas.com/tutorials/pan-camera-to-target/
Tags: input, camera, animation, tutorial

An example showing how to focus a camera on a target location

Credit: https://playcanvas.com/user/lexxik

[Interactive Demo]

--------------------------------------------------------------------------------

## Pause/Play application

URL: https://developer.playcanvas.com/tutorials/pauseplay-application/
Tags: tutorial, time

A sample showing how to pause and resume an app by modifying the timeScale property. Click to pause/play the app

[Interactive Demo]

--------------------------------------------------------------------------------

## Physics raycasting by tag

URL: https://developer.playcanvas.com/tutorials/physics-raycasting-by-tag/
Tags: input, physics, raycast, tutorial

Sample showing how to pick an entity by tag using raycastAll

[Interactive Demo]

--------------------------------------------------------------------------------

## Physics with CCD

URL: https://developer.playcanvas.com/tutorials/physics-with-ccd/
Tags: physics, tutorial

A sample with a script to setup and enable CCD for very fast moving physics objects

[Interactive Demo]

--------------------------------------------------------------------------------

## Place an entity with physics

URL: https://developer.playcanvas.com/tutorials/place-an-entity-with-physics/
Tags: input, physics, tutorial

A sample showing how to place objects in the world by using physics. Click on ground to place a box.

[Interactive Demo]

--------------------------------------------------------------------------------

## Place entity without physics

URL: https://developer.playcanvas.com/tutorials/place-entity-without-physics/
Tags: input, tutorial

A sample showing how to place objects in the world without using physics. Click on the ground to place boxes.

[Interactive Demo]

--------------------------------------------------------------------------------

## Planar Mirror Reflection

URL: https://developer.playcanvas.com/tutorials/planar-mirror-reflection/
Tags: rendering, shaders, tutorial

Credit: https://playcanvas.com/user/lexxik

Example of creating a planar reflection being used for a transparent mirror/water like surface.

[Interactive Demo]

--------------------------------------------------------------------------------

## Planet Earth

URL: https://developer.playcanvas.com/tutorials/planet-earth/
Tags: textures, rendering, materials, shaders, tutorial

[Interactive Demo]

--------------------------------------------------------------------------------

## Point and click movement

URL: https://developer.playcanvas.com/tutorials/point-and-click-movement/
Tags: touch, input, mouse, raycast, tutorial

Sample showing a simple point and click system to move an object where the user has clicked

[Interactive Demo]

--------------------------------------------------------------------------------

## Procedural Gradient Texture

URL: https://developer.playcanvas.com/tutorials/procedural-gradient-texture/
Tags: textures, rendering, tutorial

Example of creating a procedural texture with a gradient effect by LeXXik

[Interactive Demo]

--------------------------------------------------------------------------------

## Procedural Levels

URL: https://developer.playcanvas.com/tutorials/procedural-levels/
Tags: procedural

[Interactive Demo]

This project uses [clone()](https://api.playcanvas.com/engine/classes/Entity.html#clone) function on the Entity to randomly generate a level from Entities that have been created in the Editor.

Try it from the Editor in the [tutorial project.](https://playcanvas.com/project/405864)

This script below is a very simple level generation program. It takes two Entities that have been setup in the Editor: 'Grass' and 'House' and uses them as tiles for a grid based level. The level is created by randomly choosing one of the tiles, cloning the tile to create a new Entity, then placing the new Entity at the correct grid position.

--------------------------------------------------------------------------------

## Programmatically Creating Entities

URL: https://developer.playcanvas.com/tutorials/programmatically-creating/
Tags: procedural, basics

[Interactive Demo]

Usually you will be creating Entities via the PlayCanvas Editor, building up collections of Components and scripts to create the various parts of your game. However, in some cases it is convenient to create Entities in your scripts. This tutorial shows you how.

## Creating an Entity

```javascript
var entity = new pc.Entity(); // Create an Entity

// Add it to the Entity hierarchy
this.app.root.addChild(entity);
```

First you need to create an Entity. This is straightforward, but it is important to add the Entity to the main Entity hierarchy. Only Entities in the hierarchy will have their transforms, Components and scripts updated. In your scripts you can access the root of the Entity hierarchy from the `Application` object which is passed into your script. By convention this is usually named `app` and the hierarchy root is available as `this.app.root`.

## Adding Components

```javascript
// Create a new Entity
var entity = new pc.Entity();

// Add a new Camera Component with default values
entity.addComponent("camera");

// Add a new Model Component and add it to the Entity.
entity.addComponent("render", {
    type: 'box',
});

// Add it to the Entity hierarchy
this.app.root.addChild(entity);
```

An Entity on its own doesn't do much, so you will need to add Components in order to add functionality to your Entity. You can use the `addComponent` method of the Entity to create and add a new Component to the Entity.

Each Component type has different properties that can be passed in on the data object, see the [Component's documentation](/user-manual/editor/scenes/components/) for more detail about which properties are available. The `data` argument can be left out and default values will be used.

## Removing Components

```javascript
var entity = new pc.Entity();

// Attach Camera Component with default values
entity.addComponent("camera");

// Delete the Camera Component
entity.removeComponent("camera");
```

Components can be deleted individually from an Entity by calling the `removeComponent` method on the Entity.

## Deleting Entities

```javascript
// Create a new Entity
var entity = new pc.Entity();

// Create a new Camera Component with default values
entity.addComponent("camera");

// Create a new Model Component and add it to the Entity.
entity.addComponent("render", {
    type: 'box',
});

// Add it to the Entity hierarchy
this.app.root.addChild(entity);

// Delete the Entity and remove it from the hierarchy
entity.destroy();
```

When you are finished with an Entity you call the `destroy` method on the Entity. This will delete all Components and remove the Entity from the hierarchy. It will also delete all child Entities in the same way.

## In Action

This is a complete Entity script which you can see in action at the top of the tutorial. It continually creates and destroys new Entities with a Model Component attached.

See [the full scene here](https://playcanvas.com/editor/scene/440341).

--------------------------------------------------------------------------------

## Rainbow Trail with Mesh API

URL: https://developer.playcanvas.com/tutorials/rainbow-trail-with-mesh-api/
Tags: rendering, shaders, tutorial

A rainbow trail using the pc.Mesh API

[Interactive Demo]

--------------------------------------------------------------------------------

## Raycast with Camera Viewports

URL: https://developer.playcanvas.com/tutorials/raycast-with-camera-viewports/
Tags: input, camera, physics, raycast, tutorial

Raycasting with multiple camera viewports. Click on the shapes in each view

[Interactive Demo]

--------------------------------------------------------------------------------

## Real-time Multiplayer with Colyseus

URL: https://developer.playcanvas.com/tutorials/real-time-multiplayer-colyseus/
Tags: multiplayer, networking

[Interactive Demo]

> *Select create game to open a new game. And click anywhere on the floor to move the object.*

## On this tutorial you will learn: {#on-this-tutorial-you-will-learn}

- Setting up your Colyseus server
- Synchronizing the state between server and client
- Exchanging messages between client and server
- Matchmaking: how to create, join, and list available games

## Materials {#materials}

- [PlayCanvas Project (Client-side)](https://playcanvas.com/project/859259/overview/colyseus-demo)
- [Colyseus TypeScript Project (Server-side)](https://github.com/colyseus/tutorial-playcanvas-server)

## Before you start {#before-you-start}

### Prior Knowledge Expected {#prior-knowledge-expected}

- Basic PlayCanvas knowledge ([See PlayCanvas Developer Resources](https://developer.playcanvas.com/))
- Basic JavaScript/TypeScript understanding ([See TypeScript Handbook](https://www.typescriptlang.org/docs/handbook/intro.html))
- Basic Node.js understanding ([See Introduction to Node.js](https://nodejs.dev/en/learn/))

### Software requirements {#software-requirements}

- [Node.js LTS](https://nodejs.org/en/download/)
- [Visual Studio Code](https://code.visualstudio.com/download)

## Creating the Server {#creating-the-server}

We will be making a basic server, hosted locally on your computer for keeping player states. Changes will be synchronized with clients accordingly.

To create a fresh new Colyseus server, run the following from your command-line:

```sh
npm init colyseus-app ./playcanvas-demo-server
```

Let's make sure you can run the server locally now, by running `npm start`:

```sh
cd playcanvas-demo-server
npm start
```

If successful, the output should look like this in your command-line:

```sh
> my-app@1.0.0 start
> ts-node-dev --respawn --transpile-only src/index.ts

✅ development.env loaded.
✅ Express initialized
🏟 Your Colyseus App
⚔️ Listening on ws://localhost:2567
```

## Including the Colyseus JavaScript SDK {#including-the-colyseus-javascript-sdk}

Now we need to add the Colyseus JavaScript SDK on PlayCanvas.

We can do that through a "external script" on PlayCanvas project settings.

Open **"Menu" → "Settings"**:

[Image: settings]

From the Settings panel, expand on **"External Scripts"**, and increase the number of **"URLs"**.

[Image: CDN]

In the new **"URL"** field, let's include the Colyseus JavaScript SDK, from a CDN:

```none
https://unpkg.com/colyseus.js@^0.16.0/dist/colyseus.js
```

This is going to make the `Colyseus` [JavaScript SDK](https://docs.colyseus.io/getting-started/javascript-client/) available for our PlayCanvas scripts.

## Establishing a Client-Server Connection {#establishing-a-client-server-connection}

Now, from a new PlayCanvas Script, let's instantiate our `Colyseus.Client` instance. ([see "Creating new scripts"](/user-manual/editor/scripting/managing-scripts/))

You can attach this script to a new empty entity called "NetworkManager".

> Note that we're using the local `ws://localhost:2567` endpoint here. You need to [deploy your server](https://docs.colyseus.io/deployment/) to the public internet in order to play with others online. You can also use [Glitch](https://glitch.com/edit/#!/tutorial-playcanvas-server) to host your server publicly.

When you **"Launch"** your PlayCanvas project now, your client is going to establish a connection with the server, and the server is going to create the room `my_room` on demand for you.

Notice that `my_room` is the default room identifier set by the barebones Colyseus server. You can and should change this identifier in the `arena.config.ts` file.

You will be seeing the following message in your server logs, which means a client successfully joined the room!

```none
19U8WkmoK joined!
```

## Room State and Schema {#room-state-and-schema}

In Colyseus, we define shared data through its `Schema` structures.

> `Schema` is a special data type from Colyseus that is capable of encoding its changes/mutations *incrementally*. The encoding and decoding process happens internally by the framework and its SDK.

The state synchronization loop looks like this:

1. State changes (mutations) are synchronized automatically from Server → Clients
2. Clients, by attaching callbacks to their local *read-only* `Schema` structures, can observe for state mutations and react to it.
3. Clients can send arbitrary messages to the server - which decides what to do with it - and may mutate the state (Go back to step **1.**)

---

Let's go back to editing the Server code, and define our Room State in the Server.

We need to handle multiple `Player` instances, and each `Player` will have `x`, `y` and `z` coordinates:

```typescript
// MyRoomState.ts

    @type("number") x: number;
    @type("number") y: number;
    @type("number") z: number;
}

    @type({ map: Player }) players = new MapSchema<Player>();
}
```

> See more about the [Schema structures](https://docs.colyseus.io/state/schema/).

Now, still in the server-side, let's modify our `onJoin()` method to create a `Player` instance whenever a new connection is established with the room.

```typescript
// MyRoom.ts
// ...
    onJoin(client: Client, options: any) {
        console.log(client.sessionId, "joined!");

        // create Player instance
        const player = new Player();

        // place Player at a random position
        const FLOOR_SIZE = 4;
        player.x = -(FLOOR_SIZE/2) + (Math.random() * FLOOR_SIZE);
        player.y = 1.031;
        player.z = -(FLOOR_SIZE/2) + (Math.random() * FLOOR_SIZE);

        // place player in the map of players by its sessionId
        // (client.sessionId is unique per connection!)
        this.state.players.set(client.sessionId, player);
    }
// ...
}
```

Also, when the client disconnects, let's remove the player from the map of players:

```typescript
// MyRoom.ts
// ...
    onLeave(client: Client, consented: boolean) {
        console.log(client.sessionId, "left!");

        this.state.players.delete(client.sessionId);
    }
// ...
```

The state mutations we've done in the server-side **can be observed** in the client-side, and that's what we're going to do in the next section.

## Setting up the Scene for Synchronization {#setting-up-the-scene-for-synchronization}

For this demo, we need to create two objects in our Scene:

- A Plane to represent the floor
- A Capsule to represent the players, which we will clone for each new player joining the room.

### Creating the Plane {#creating-the-plane}

Let's create a Plane with scale `8`.

[Image: Plane]

### Creating the Player {#creating-the-player}

Let's create the Player capsule with scale `1`.

Make sure to uncheck the `"Enabled"` property. We will not have any Player instances enabled until we have active connections with the server.

[Image: Player]

## Listening for State Changes {#listening-for-state-changes}

After a connection with the room has been established, the client-side can start listening for state changes, and create a visual representation of the data in the server.

### Adding new players {#adding-new-players}

As per [Room State and Schema](#room-state-and-schema) section, whenever the server accepts a new connection - the `onJoin()` method is creating a new Player instance within the state.

We're going to listen to this event on the client-side now:

```typescript
//
// In order to attach callbacks to the state, we need to use the `getStateCallbacks()` method
//
const $ = Colyseus.getStateCallbacks(this.room);

// ...
$(this.room.state).players.onAdd((player, sessionId) => {
  //
  // A player has joined!
  //
  console.log("A player has joined! Their unique session id is", sessionId);
});
// ...
```

When playing the Scene, you should see a message in the browser's console whenever a new client joins the room.

For the visual representation, we need to clone the "Player" object, and keep a local reference to the cloned object based on their `sessionId`, so we can operate on them later:

```typescript
// ...

// we will assign each player visual representation here
// by their `sessionId`
this.playerEntities = {};

// listen for new players
$(this.room.state).players.onAdd((player, sessionId) => {
  // find the base Player representation (not enabled)
  const playerEntityToClone = this.app.root.findByName("Player");

  // clone the Player representation, and enabled it!
  const entity = playerEntityToClone.clone();
  entity.enabled = true;

  // set position based on server data
  entity.setPosition(player.x, player.y, player.z);

  // add clone to the Scene
  playerEntityToClone.parent.addChild(entity);

  // assign visual representation by their `sessionId`
  this.playerEntities[sessionId] = entity;
});
// ...
```

### The "Current Player" {#the-current-player}

You can keep a special reference to the current player object by checking the `sessionId` against the connected `room.sessionId`:

```typescript
// ...
$(this.room.state).players.onAdd((player, sessionId) => {
  // ...
  if (this.room.sessionId === sessionId) {
    this.currentPlayerEntity = this.playerEntities[sessionId];
  }
  // ...
});
```

### Removing disconnected players {#removing-disconnected-players}

When a player is removed from the state (upon `onLeave()` in the server-side), we need to remove their visual representation as well.

```javascript
// ...
$(this.room.state).players.onRemove((player, sessionId) => {
  // destroy entity
  this.playerEntities[sessionId].destroy();

  // clear local reference
  delete this.playerEntities[sessionId];
});
// ...
```

## Moving the players {#moving-the-players}

### Sending the new position to the server {#sending-the-new-position-to-the-server}

We are going to allow the "mouse down" event; use [ray cast](/user-manual/physics/ray-casting/) to determine the exact `Vec3` position the player should move towards, and then send it as a message to the server.

```typescript
// ...
this.app.mouse.on(pc.EVENT_MOUSEDOWN, (event) => {
  // Create the "bounding box" for the floor
  const boundingBox = new pc.BoundingBox(new pc.Vec3(0, 0, 0), new pc.Vec3(4, 0.001, 4));;

  // Initialize the ray and work out the direction of the ray
  // from the a screen position
  const ray = new pc.Ray();
  const targetPosition = new pc.Vec3();

  const cameraEntity = this.app.root.findByName("Camera");
  cameraEntity.camera.screenToWorld(event.x, event.y, cameraEntity.camera.farClip, ray.direction);
  ray.origin.copy(cameraEntity.getPosition());
  ray.direction.sub(ray.origin).normalize();

  // Test the ray against the ground
  const result = boundingBox.intersectsRay(ray, targetPosition);

  if (result) {
    // Adjust position height
    targetPosition.y = 1.031;

    //
    // Send new target player position to server.
    //
    this.room.send("updatePosition", {
        x: targetPosition.x,
        y: targetPosition.y,
        z: targetPosition.z,
    });
  }
});
```

### Receiving the message from the server {#receiving-the-message-from-the-server}

Whenever the `"updatePosition"` message is received in the server, we're going to mutate the player that sent the message through its `sessionId`.

```typescript
// MyRoom.ts
// ...
  onCreate(options: any) {
    this.setState(new MyRoomState());

    this.onMessage("updatePosition", (client, data) => {
      const player = this.state.players.get(client.sessionId);
      player.x = data.x;
      player.y = data.y;
      player.z = data.z;
    });
  }
// ...
```

### Updating Player's visual representation {#updating-players-visual-representation}

Having the mutation on the server, we can detect it on the client-side via `$(player).onChange()`, or `$(player).listen()`.

- `$(player).onChange()` is triggered **per schema instance**
- `$(player).listen(prop)` is triggered **per property** change

We are going to use `.onChange()` since we need all the new coordinates at once, no matter if just one has changed individually.

```typescript
// ...
$(this.room.state).players.onAdd((player, sessionId) => {
  // ...
  $(player).onChange(() => {
    this.playerEntities[sessionId].setPosition(player.x, player.y, player.z);
  });

  // Alternative, listening to individual properties:
  // $(player).listen("x", (newX, prevX) => console.log(newX, prevX));
  // $(player).listen("y", (newY, prevY) => console.log(newY, prevY));
  // $(player).listen("z", (newZ, prevZ) => console.log(newZ, prevZ));
});
```

> Read [more about Schema callbacks](https://docs.colyseus.io/state/schema/#client-side)

## Extra: Monitoring Rooms and Connections {#extra-monitoring-rooms-and-connections}

Colyseus comes with an optional monitoring panel that can be helpful during the development of your game.

To view the monitor panel from your local server, go to `http://localhost:2567/colyseus`.

[Image: monitor]

You can see and interact with all spawned rooms and active client connections through this panel.

> See [more information about the monitor panel](https://docs.colyseus.io/tools/monitor/).

## More {#more}

We hope you found this tutorial useful, if you'd like to learn more about Colyseus please have a look at the [Colyseus documentation](https://docs.colyseus.io/), and join the [Colyseus Discord community](https://discord.gg/RY8rRS7).

--------------------------------------------------------------------------------

## Real-time Multiplayer with Photon

URL: https://developer.playcanvas.com/tutorials/real-time-multiplayer-photon/
Tags: multiplayer, networking

[Interactive Demo]

:::info

Click on the floor to move.

:::

The complete project including matchmaking can be found [here](https://playcanvas.com/project/926999/).

Photon (also known as PUN) is used in many games and has a JavaScript SDK available for HTML5 games.

Photon is for free for projects with up to 20 online players (CCU).

## You will learn

- How to add Photon SDK to PlayCanvas
- Multiplayer implementation with Photon

## Setup

### PlayCanvas Project

We start by forking the [tutorial project here](https://playcanvas.com/project/954410/).

[Image: Empty Project]

### Photon account

Account registration is required to use the SDK and view documentation.

Create your Photon account [here](https://www.photonengine.com/) - (Photon Engine).

#### Create a new app

Click **CREATE NEW APP** from the dashboard

[Image: Create New Application]

#### Select Photon Type and Application name

Enter the following

- Photon Type: RealTime
- Name: PlayCanvas-Photon etc.

[Image: Create Real Time Project]

#### Copy of AppID

Please make a note of this AppId, as you will need it in the future.

[Image: App Id]

### Download SDK

Download the SDK from the dashboard.

#### Click SDK from the dashboard

[Image: SDK]

#### Select RealTime JavaScript

[Image: JavaScript SDK]

#### Click Download SDK

[Image: Download SDK]

#### Unzip the SDK

[Image: Unzip SDK]

The SDK will be downloaded in ZIP format, unzip it: `photon-javascript-sdk_vX-X-X-X` → `lib` → **`Photon-Javascript_SDK.min.js`**.

### Importing SDK

Import the SDK you have just downloaded into the PlayCanvas editor.

#### Upload the SDK on the editor

[Image: Upload SDK]

Drag and drop the SDK to the assets in the editor.

#### Change Loading Type "Asset" to "Before Engine"

[Image: Change Loading Type]

## Multiplayer implementation

The multiplayer implementation will do the following:

1. Use Photon class for real-time communication and Load Balancing
2. Connect to Photon master server
3. Create or Join a room
4. Synchronize other players' actions and movement

The [API reference](https://doc-api.photonengine.com/en/javascript/current/Photon.LoadBalancing.LoadBalancingClient.html) and [glossary](https://doc.photonengine.com/ja-jp/quantum/v1/reference/glossary) are available on Photon's site.

### Using Photon with PlayCanvas

#### Instantiate classes from PlayCanvas to use Photon

Create a script asset named **photon-loadbalancing-playcanvas.js** to the project to initialize Photon.

- **Photon.LoadBalancing.LoadBalancingClient**  This class contains many of the features of the Photon SDK for real-time communication.

#### Set Script for Root entity

Create a new script asset **photon-loadbalancing-playcanvas.js** and attach it to the Root entity in the Editor.

[Image: Root Entity - Inspector]

#### Paste AppId into the script attribute

Enter AppId as a script attribute.

[Image: Script Attributes]

```javascript
this.loadBalancingClient = new Photon.LoadBalancing.LoadBalancingClient( this.wss ? 1 : 0, this.appId, this.appVersion );
```

- **wss** Secure connection via WebSocket.
- **appId** The application identifier value.
- **appVersion** Used for versioning. Different versions cannot be connected to each other.

### Connect to the Photon master server

#### Connect to the master server using `connectToRegionMaster`

- **connectToRegionMaster** Connects to the master server in the specified region.
- **this.region** Used to configure the region.

If you successfully connect to the lobby by running connectToRegionMaster, JoinedLobby will be displayed in the log.

[Image: Console Log]

### Create or Join a room

**onRoomList** function is called when a connection is made to the lobby.

**JoinRandomOrCreateRoom** to join a room if it exists, or randomly join a room if it does not exist.

- **onRoomList(rooms)** List of rooms in the lobby.
- **joinRandomOrCreateRoom(options, createRoomName, createOptions)** Join to a random room. If the room does not exist, a new room will be created.
- **onJoinRoom** When you join a room, this is called.

### Join and Leave

When a player joins a room, it is synchronized with other players.
Use **onActorJoin** and **onActorLeave**.

```javascript
PhotonLoadBalancingPlayCanvas.prototype.initialize = function () {

    // Photon Settings
    this.loadBalancingClient = new Photon.LoadBalancing.LoadBalancingClient(this.wss ? 1 : 0, this.appId, this.appVersion);

    // pc.Application
    this.loadBalancingClient.app = this.app;

    // Connect to the master server
    if (!this.loadBalancingClient.isInLobby()) {
        this.loadBalancingClient.connectToRegionMaster(this.region);
    }

    this.loadBalancingClient.onRoomList = this.onRoomList;
    this.loadBalancingClient.onJoinRoom = this.onJoinRoom;

    // Added
    this.loadBalancingClient.onActorJoin = this.onActorJoin;
    this.loadBalancingClient.onActorLeave = this.onActorLeave;
};

PhotonLoadBalancingPlayCanvas.prototype.onRoomList = function () {
    this.joinRandomOrCreateRoom();
};

PhotonLoadBalancingPlayCanvas.prototype.onJoinRoom = function (createdByMe) {
    console.log("Joined the room.");
};

PhotonLoadBalancingPlayCanvas.prototype.onActorJoin = function (actor) {
    const { actorNr } = actor;
    if (actor.isLocal) return;
    const otherPlayer = new pc.Entity();
    otherPlayer.addComponent("render", { type: "capsule" });
    otherPlayer.setLocalPosition(0, 1, 0);
    otherPlayer.name = actorNr;
    this.app.root.children[0].addChild(otherPlayer);
};

PhotonLoadBalancingPlayCanvas.prototype.onActorLeave = function (actor) {
    const { actorNr } = actor;
    const otherPlayer = this.app.root.children[0].findByName(actorNr);
    if (actor.isLocal || !otherPlayer) return;
    otherPlayer.destroy();
};
```

[Image: Actor]

- **actor** contains `name`, `actorNr`, `isLocal`, and `userId`.
- **onActorJoin** when a new user connects, you can get the joined actor.
- **onActorLeave** when a user is disconnected, you can get the disconnected actor.

If successful, the entity is added when the player joins.

[Image: Console log - Actors ]

#### Player Movement

Create a new **player.js** for character movement.

- **this.app.keyboard.isPressed:** check if the keyboard is pressed

### Synchronize other players

Use **raiseEvent** and **onEvent** to synchronize the player's location.

#### Position synchronization using **raiseEvent**

- **raiseEvent(eventCode,data, options)** send `eventCode` and `data`.
- **onEvent(code, content, actorNr)** receive data. Includes `actorNr` and `eventCode`.

#### Changed to fire events when player moves

- **this.app.fire** [communication](/user-manual/scripting/events/) between scripts.

### Done

You can now play multiplayer in Photon!

[Image: Project]

You can create a room using Photon and synchronize the positions of players with each other.

Although this project was only a simple real-time communication between players, you can also create a project that includes matchmaking. For the full project, including room creation and room listings, please [click here](https://playcanvas.com/project/926999/) .

--------------------------------------------------------------------------------

## Real Time Multiplayer

URL: https://developer.playcanvas.com/tutorials/real-time-multiplayer/
Tags: multiplayer, networking

:::note

This tutorial covers how to start creating your own multiplayer from scratch. If you prefer to use a hosted multiplayer service, we have tutorials for [Colyseus](/tutorials/real-time-multiplayer-colyseus) and [Photon](/tutorials/real-time-multiplayer-photon).

:::

[Interactive Demo]

*Use WASD to move the player around. If you only see one capsule, try opening this page in another tab or on another computer.*

In this tutorial we’ll cover how to setup a basic multiplayer project using Node.js and Socket.io. We’ll focus on implementing it in PlayCanvas. By the end you should have a project similar to the one above. You can find the [tutorial project here](https://playcanvas.com/project/406048/overview/tutorial-realtime-multiplayer).

## Setting up the Server

We'll be implementing a client-server model (as opposed to peer-to-peer). This will be a basic server that will receive data from all clients (which are our PlayCanvas instances) and broadcast it back.

[Glitch](https://glitch.com/) provides a really convenient way to write and deploy backend apps for free completely in your browser! You can use it without an account but creating one will let you easily find your work. [Create a new Node app](https://glitch.com/edit/#!/new-project) and replace the contents of `server.js` with this:

```javascript
const http = require('http');
const { Server } = require('socket.io');

const server = http.createServer();

// Configure Socket.IO with CORS
const io = new Server(server, {
    cors: {
        // If you only want to allow PlayCanvas launch domain:
        // origin: "https://launch.playcanvas.com",

        // Or allow all origins (less secure, but quick for testing)
        origin: "*",
        methods: ["GET", "POST"]
    }
});

io.on('connection', (socket) => {
    console.log(`New client connected: ${socket.id}`);
});

const PORT = process.env.PORT || 3000;
server.listen(PORT, () => {
    console.log(`Server started on port ${PORT}`);
});
```

Glitch will automatically re-run the server every time you finish typing. Once you’ve copied this, you should get an error. Click on the `Logs` button at the bottom of the window to open up the server console. Here you can see any server output, as well as the errors. You should see `Error: Cannot find module 'socket.io'`.

[Image: Opening the log]

To include a package, go to `package.json` and click on the `Add Package` button on the top. Search for `socket.io`.

[Image: Adding a package]

Once `socket.io` has finished installing, the server will automatically restart and you should see `Server started on port 3000` in the log. Congratulations! You've successfully deployed a server!

You can find the domain your server is deployed at by clicking `Settings` in the left sidebar. This is where you can also rename the project.

This server will simply log a message every time someone connects. This should be enough to start working on our client and confirm that it connects to the server.

## Setting up the Project

Create a new project on PlayCanvas. We first need to include the Socket.io client JS library, as an external script. To do this. go to your project settings:

[Image: Project settings]

Find and open 'External Scripts'.

[Image: External scripts settings]

Change the value from 0 to 1 and add the CDN URL for the socket library from their [framework server](https://cdnjs.com/libraries/socket.io). In this case, we will be using version `4.8.1` as that is the latest at time of writing:

[Image: Project settings]

```none
https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.8.1/socket.io.min.js
```

Now we need to create a new script to handle the network logic. Create a new script called `Network.js`. We first need to create a connection to the server. We can do this by adding this line in the initialize method:

```javascript
this.socket = io.connect('https://playcanvas-multiplayer-server.glitch.me');
```

Replace `https://playcanvas-multiplayer-server.glitch.me` with the address of your own server.

To confirm that this works, attach this network script to the `Root` entity, and then launch the game. Keep your eye on the server log at Glitch. If everything worked, the server should log `New client connected:` along with the client ID. The project is now setup to send and receive messages to and from the server.

## Server and Client Communication

The way you can send data between the client and server is with the socket connection we made earlier. To send data from the client (in Network.js on PlayCanvas), we use the `emit` function. Here’s an example:

```javascript
this.socket.emit('playerJoined', 'John');
```

This emits a message called `playerJoined`, with the data `John`. For the server to receive the message, we need to write in the server file (in `server.js` on Glitch):

```javascript
socket.on('playerJoined', function (name) {
    console.log (name);
});
```

This will log whatever data is sent to the server when `playerJoined` is emitted.

For this demo, we’re aiming to have players move around with others in real time, so we'll need to create an environment. Start by create an entity to use as a ground, and add a collision box and static rigidbody. Here is what the settings on the ground entity should look like:

<img loading="lazy" src="/img/tutorials/multiplayer/ground_entity.png" width="360" />

Next we’ll need a player to control. Create a new capsule and call it `Player`. add a dynamic rigidbody and collision box, and change the rigid body settings to match the picture below.

<img loading="lazy" src="/img/tutorials/multiplayer/player_entity.png" width="360" />

Duplicate the player entity and rename it as 'Other'. Uncheck the `Enabled` box on this new entity so that it's disabled to begin with.  This is the entity we'll be using to simulate other players in the game.

Add a script component to your player, and attach a new script called `Movement.js`:

When you launch the game you should be able to use WASD to move your player around. If not, you’ve missed a step or did not set the correct settings for the entity. Try changing the speed attribute on the movement script.

For the game to work in real time multiplayer, we need to keep track of all players in the game. Replace the current server code with this:

```javascript
const http = require('http');
const { Server } = require('socket.io');

/** 
 * Class to track each connected player (id + position)
 */
class Player {
    constructor(id) {
        this.id = id;
        this.x = 0;
        this.y = 0;
        this.z = 0;
    }
}

const server = http.createServer();

// Configure Socket.IO with CORS
const io = new Server(server, {
    cors: {
        // If you only want to allow PlayCanvas launch domain:
        // origin: "https://launch.playcanvas.com",

        // Or allow all origins (less secure, but quick for testing)
        origin: "*",
        methods: ["GET", "POST"]
    }
});

const players = {};

/**
 * Handle new socket connections
 */
io.on('connection', (socket) => {
    console.log(`New client connected: ${socket.id}`);

    // Fired when the client is ready to initialize their Player object
    socket.on('initialize', () => {
        const newPlayer = new Player(socket.id);
        players[socket.id] = newPlayer;

        // Send to this client its own ID and the current list of players
        socket.emit('playerData', { id: socket.id, players });

        // Tell everyone else about this new player
        socket.broadcast.emit('playerJoined', newPlayer);
    });
});

const PORT = process.env.PORT || 3000;
server.listen(PORT, () => {
    console.log(`Server started on port ${PORT}`);
});
```

In the code above, when a player sends the message `initialize`, we send him his unique ID and data about other players in the game. It also tells others that a new player has connected. Let’s add that logic into our Network script.

Add this code in the `initialize`:

```javascript
// Your io.connect function call should be up here

this.socket.emit('initialize');
var socket = this.socket;

this.player = this.app.root.findByName('Player');
this.other = this.app.root.findByName('Other');

var self = this;
socket.on('playerData', function (data) {
    self.initializePlayers (data);
});

socket.on('playerJoined', function (data) {
    self.addPlayer(data);
});
```

And then declare these new functions inside Network.js:

Now when we join the game, the client tells the server we've connected, and the server sends us a list of players with their positions. The game then creates a new entity for each player connected, and moves them to their current position. The only problem is, the server doesn't know the positions of all players. We need to send the server our current position every frame.

Add this code into the `initialize` of your Network.js script:

```javascript
socket.on('playerMoved', function (data) {
    self.movePlayer(data);
});
```

Replace your `update` with this:

```javascript
Network.prototype.update = function (dt) {
    this.updatePosition();
};
```

And then declare these new functions inside Network.js:

Back on the server, we need to account for what happens when the player sends us their position. On the server, we need to add a new event:

```javascript
// Update player position
socket.on('positionUpdate', (data) => {
    if (!players[socket.id]) return;
    players[socket.id].x = data.x;
    players[socket.id].y = data.y;
    players[socket.id].z = data.z;

    // Broadcast updated position to all other players
    socket.broadcast.emit('playerMoved', {
        id: socket.id,
        x: data.x,
        y: data.y,
        z: data.z
    });
});
```

Finally, we need to handle player disconnects. We can do this by listening for the `disconnect` event on the socket.

```javascript
// Handle disconnections
socket.on('disconnect', () => {
    console.log(`Client disconnected: ${socket.id}`);
    if (!players[socket.id]) return;
    delete players[socket.id];
    // Notify other players to remove this player
    socket.broadcast.emit('killPlayer', socket.id);
});
```

Let's review the full and final server code:

```javascript
const http = require('http');
const { Server } = require('socket.io');

/** 
 * Class to track each connected player (id + position)
 */
class Player {
    constructor(id) {
        this.id = id;
        this.x = 0;
        this.y = 0;
        this.z = 0;
    }
}

const server = http.createServer();

// Configure Socket.IO with CORS
const io = new Server(server, {
    cors: {
        // If you only want to allow PlayCanvas launch domain:
        // origin: "https://launch.playcanvas.com",

        // Or allow all origins (less secure, but quick for testing)
        origin: "*",
        methods: ["GET", "POST"]
    }
});

const players = {};

/**
 * Handle new socket connections
 */
io.on('connection', (socket) => {
    console.log(`New client connected: ${socket.id}`);

    // Fired when the client is ready to initialize their Player object
    socket.on('initialize', () => {
        const newPlayer = new Player(socket.id);
        players[socket.id] = newPlayer;

        // Send to this client its own ID and the current list of players
        socket.emit('playerData', { id: socket.id, players });

        // Tell everyone else about this new player
        socket.broadcast.emit('playerJoined', newPlayer);
    });

    // Update player position
    socket.on('positionUpdate', (data) => {
        if (!players[socket.id]) return;
        players[socket.id].x = data.x;
        players[socket.id].y = data.y;
        players[socket.id].z = data.z;

        // Broadcast updated position to all other players
        socket.broadcast.emit('playerMoved', {
            id: socket.id,
            x: data.x,
            y: data.y,
            z: data.z
        });
    });

    // Handle disconnections
    socket.on('disconnect', () => {
        console.log(`Client disconnected: ${socket.id}`);
        if (!players[socket.id]) return;
        delete players[socket.id];
        // Notify other players to remove this player
        socket.broadcast.emit('killPlayer', socket.id);
    });
});

const PORT = process.env.PORT || 3000;
server.listen(PORT, () => {
    console.log(`Server started on port ${PORT}`);
});
```

## Conclusion

That's about it! If you'd like, try adding some of these ideas on your own:

* Players are removed when they close the game.
* Adding respawning functionality for when players fall off the edge.

Keep in mind this is only a very basic implementation of multiplayer. Realistically, when creating larger multiplayer games you'll want to consider using an authoritative server, instead of handling all the game logic on the client. You can read a more in depth tutorial about [how Socket.io works and how to develop multiplayer in JavaScript here](https://code.tutsplus.com/create-a-multiplayer-pirate-shooter-game-in-your-browser--cms-23311t).

You can find the [full server code on Glitch here](https://glitch.com/edit/#!/playcanvas-multiplayer-server), where you can also fork it and extend it.

--------------------------------------------------------------------------------

## Render 3D World to UI

URL: https://developer.playcanvas.com/tutorials/render-3d-world-to-ui/
Tags: rendering, camera, ui, tutorial, scripts

Render 3D objects as part of the UI

[Interactive Demo]

--------------------------------------------------------------------------------

## Resolution Scaling

URL: https://developer.playcanvas.com/tutorials/resolution-scaling/
Tags: mobile, rendering, resolution, tutorial

Example project on rendering the world at different resolutions without the UI being affected

[Interactive Demo]

--------------------------------------------------------------------------------

## Right to left language support

URL: https://developer.playcanvas.com/tutorials/right-to-left-language-support/
Tags: ui, tutorial, scripts

Scripts to use for RTL language support such as Arabic

[Interactive Demo]

--------------------------------------------------------------------------------

## Rotating Objects with Mouse

URL: https://developer.playcanvas.com/tutorials/rotating-objects-with-mouse/
Tags: touch, input, mouse, tutorial

Sample showing how to rotate an object using the mouse in screen space

[Interactive Demo]

--------------------------------------------------------------------------------

## Shockwave

URL: https://developer.playcanvas.com/tutorials/shockwave/
Tags: tutorial, posteffects

Shockwave ripple post effect

[Interactive Demo]

--------------------------------------------------------------------------------

## Simple shape raycasting

URL: https://developer.playcanvas.com/tutorials/simple-shape-raycasting/
Tags: input, physics, raycast, tutorial

Sample showing how to pick at an entity

[Interactive Demo]

--------------------------------------------------------------------------------

## Simple water surface

URL: https://developer.playcanvas.com/tutorials/simple-water-surface/
Tags: rendering, materials, tutorial

[Interactive Demo]

--------------------------------------------------------------------------------

## Smooth camera movement

URL: https://developer.playcanvas.com/tutorials/smooth-camera-movement/
Tags: camera, tutorial

Sample with a smooth transition of a camera from one position and rotation to another using the Lerp and Slerp functions.

[Interactive Demo]

--------------------------------------------------------------------------------

## Sound volume control using curve

URL: https://developer.playcanvas.com/tutorials/sound-volume-control-using-curve/
Tags: audio, tutorial

Sample showing how to control the volume of a sound slot using curve data. Click on the scene to start the audio.

[Interactive Demo]

--------------------------------------------------------------------------------

## Space Rocks!

URL: https://developer.playcanvas.com/tutorials/space-rocks/
Tags: games, tutorial

Get started making your own space shooter game by forking this template Asteroids shooter! Aim with your mouse and fire with your left button. Survive as long as you can!

[Interactive Demo]

--------------------------------------------------------------------------------

## Static Batching

URL: https://developer.playcanvas.com/tutorials/static-batching/
Tags: rendering, tutorial

[Interactive Demo]

--------------------------------------------------------------------------------

## Stencil Buffer - 3D Magic Card

URL: https://developer.playcanvas.com/tutorials/stencil-buffer---3d-magic-card/
Tags: rendering, culling, materials, tutorial, scripts

Example project that uses the stencil buffer to create a magic window like effect on a card.

Credit: @ alexanderrdx for the original project

[Interactive Demo]

--------------------------------------------------------------------------------

## Switch Full Scene

URL: https://developer.playcanvas.com/tutorials/switch-full-scene/
Tags: loading, scenes

Full documentation for Loading Scenes is now in the [User Manual](/user-manual/editor/scenes/loading-scenes/).

[Interactive Demo]

--------------------------------------------------------------------------------

## Switching materials at runtime

URL: https://developer.playcanvas.com/tutorials/switching-materials-at-runtime/
Tags: materials, assets, tutorial

Sample switching materials on a model at runtime.

[Interactive Demo]

--------------------------------------------------------------------------------

## Terrain Generation from Heightmap

URL: https://developer.playcanvas.com/tutorials/terrain-generation/
Tags: procedural

[Interactive Demo]

This project uses the [`pc.Mesh`](https://api.playcanvas.com/engine/classes/Mesh.html) API to procedurally generate and texture a rolling hillside from a heightmap texture.

Try it from the Editor in the [tutorial project.](https://playcanvas.com/project/406046)

The script below performs the terrain generation.

--------------------------------------------------------------------------------

## Third Person Controller

URL: https://developer.playcanvas.com/tutorials/third-person-controller/
Tags: input, camera, physics, tutorial

A simple third person controller.

[Interactive Demo]

--------------------------------------------------------------------------------

## Tic Tac Toe

URL: https://developer.playcanvas.com/tutorials/tic-tac-toe/
Tags: games, tutorial

The classic game Tic Tac Toe

[Interactive Demo]

--------------------------------------------------------------------------------

## Timers

URL: https://developer.playcanvas.com/tutorials/timers/
Tags: tutorial, time

Example of extending the PlayCanvas engine to create one shot timers. Press P to pause time.

[Interactive Demo]

--------------------------------------------------------------------------------

## Touchscreen Joypad Controls

URL: https://developer.playcanvas.com/tutorials/touch-joypad/
Tags: input, ui

[Interactive Demo]

[Click here to see the project](https://playcanvas.com/project/1007506/overview/touchscreen-joypad-controls).

## Overview

This tutorial shares a Touchscreen Joypad library that can be installed and customized for any project that needs touch screen controls.

We recommend reading about the [PlayCanvas UI system](/user-manual/user-interface/) if you have not used it before as it is used to render and position the controls for this library.

## How to install

Open the [example project](https://playcanvas.com/project/1007506/overview/touchscreen-joypad-controls), right click on the folder 'touch-joypad' and click on 'Copy'.

Open your project, right click in the assets panel and click on 'Paste'

## Adding your joystick

In the folder we've just pasted, open the 'templates' folder and there will be 4 preconfigured templates for the joysticks to cover the most common use cases.

- **Left/Right Fixed Touch Joystick** - A fixed joystick that is anchored to the bottom left/right of the screen corners. Recommended for quick reaction games where an instant response is expected such as a retro platforming game.
- **Left/Right Half Touch Joystick** - A joystick where the input area the left/right half of the screen and the joystick will move to where you touch in that area and them requires dragging for movement. Recommended for when 'pushing' or 'pulling' an object in the application.

For this example, let's use the 'Left Half Touch Joystick' template.

Create a 2D screen Entity and [add the template](/user-manual/editor/templates/#adding-templates-in-your-scene) as a child of the screen Entity.

The joystick is made of three Entities:

- Input area (outlined in red)
- Base (outlined in blue)
- Nub (outlined in blue)

The input area will listen for touch and mouse (for debugging purposes) events and also stops the propagation of the event.

This means that any UI Elements under this and any [pc.Mouse](https://api.playcanvas.com/engine/classes/Mouse.html) or [pc.Touch](https://api.playcanvas.com/engine/classes/Touch.html) events will not be fired if this area is interacted with first.

As the input area is an UI Element, it can be positioned, sized and anchored specifically for your needs via the [UI system layout](/user-manual/user-interface/elements/).

It also has the 'touchJoystick' which has all the logic and attributes for the joystick. Each attribute has tooltips describing what they are for with some more details below.

The base and nub Entities are controlled by the script and are UI Elements. If you would like the joystick to be in a different place than where it is in the template, please position and anchor the base Entity to where you need them to be and not the input area.

### Behavior Types

There are 3 behavior types for the joystick that are commonly found in touch screen games and applications.

'Fixed in place' where the base of the joystick does not move from its position:

'Move to first touch and fixed' where the base of the joystick moves to where the user first touches in the input area and then stays fixed:

'Move to first touch and drags' where the base of the joystick moves to where the user first touches in the input area and then is dragged when the user drags pass the joystick range:

This can be changed at runtime so it's possible to add this as a user option in the application as part of the in-application settings.

### Reading joystick values

The red circle is dead zone and if the nub position is within that circle, the value from the joystick will return 0 for both axes. The blue circle is the range and the nub cannot go outside that circle.

The values are always normalized between -1 and 1 on both axes based on the where the nub is between the dead zone (red circle) and the range (blue circle).

The size of the circles can be set in the Editor via the script attributes.

Joystick values can accessed in code from the global Javascript object `window.touchJoypad.sticks` with the identifier. By default, the identifier is 'joystick0' but can be changed in the Editor on the script to be more specific.

Example code:

```javascript
// Get the joystick by the identifier from the global object
var joystick = window.touchJoypad.sticks['joystick0'];

// Get the normalized values of both joystick axes and print to console
console.log('X: ' + joystick.x + ', Y: ' + joystick.y);
```

[In the demo](https://playcanvas.com/project/1007506/overview/touchscreen-joypad-controls), the camera is controlled by the right joystick and you can see how it gets and uses the values in the [script here](https://playcanvas.com/editor/code/1007506?tabs=111433673).

## Adding your buttons

Buttons are fixed position UI Elements on the screen. There is a template for button in the 'templates' folder and [should be added](/user-manual/editor/templates/#adding-templates-in-your-scene) as a child of the screen Entity.

As they are UI Elements, they can be positioned, sized and anchored specifically for your needs via the [UI system layout](/user-manual/user-interface/elements/).

Like the joysticks, they have an identifier so they can be accessed in code from the global Javascript object `window.touchJoypad.buttons` with the following API.

| Function name | Description |
|---------------|-------------|
| isPressed     | Takes the button identifier and returns true if the button is currently being pressed. |
| wasPressed    | Takes the button identifier and returns true if the button was pressed since the last frame. |
| wasReleased   | Takes the button identifier and returns true if the button was released since the last frame. |
| wasTapped     | Takes the button identifier and returns true if the button was pressed and released within 200ms. i.e. A quick tap. |

The joysticks are also buttons which gives extra flexibility in how they can be used. For example, using the `wasTapped` API with a joystick identifier can act like a L3/R3 input on a PlayStation controller.

Example code:

```javascript
// Get the button global object
var buttons = window.touchJoypad.buttons

// Check if the button was pressed since the last frame
console.log('Was pressed: ' + buttons.wasPressed('button0'));
```

[In the demo](https://playcanvas.com/project/1007506/overview/touchscreen-joypad-controls), the character is controlled by the left joystick and buttons. You can see how it gets and uses the buttons in the [script here](https://playcanvas.com/editor/code/1007506?tabs=111432679) to play attack animations.

--------------------------------------------------------------------------------

## Tutorial: Layout Groups

URL: https://developer.playcanvas.com/tutorials/tutorial-layout-groups/
Tags: ui, tutorial

Use the Layout Group component to build a user interface.

[Interactive Demo]

--------------------------------------------------------------------------------

## Tutorial: Normal Mapped Text

URL: https://developer.playcanvas.com/tutorials/tutorial-normal-mapped-text/
Tags: textures, rendering, materials, tutorial

Dynamically generate normal maps and parallax maps for text

[Interactive Demo]

--------------------------------------------------------------------------------

## Tutorial: Plasma Shader Chunk

URL: https://developer.playcanvas.com/tutorials/tutorial-plasma-shader-chunk/
Tags: shaders, tutorial

A demonstration of a GLSL effect from Shadertoy being used in a shader chunk on a PlayCanvas material

[Interactive Demo]

--------------------------------------------------------------------------------

## Tutorial: Shop User Interface

URL: https://developer.playcanvas.com/tutorials/tutorial-shop-user-interface/
Tags: ui, tutorial

Build a Shop User Interface screen using UI Elements, Layout Groups, Button components, Sprites and Texture Atlases

[Interactive Demo]

--------------------------------------------------------------------------------

## Using the Tween library

URL: https://developer.playcanvas.com/tutorials/tweening/
Tags: animation, scripts

Often we want to animate an Entity or some arbitrary value between two points. This is called tweening. We have created a tweening library for that exact purpose. You can find the library at [https://github.com/playcanvas/playcanvas-tween](https://github.com/playcanvas/playcanvas-tween).

To use the library just upload the `tween.js` file to your project. This will allow you to tween Entity properties like position, rotation, scale etc like so:

```javascript
entity.tween(entity.getLocalPosition()).to({x: 10, y: 0, z: 0}, 1, pc.SineOut);
```

Here is an example on how to tween the local position of an Entity:

[Interactive Demo]

Here are links to the [Project](https://playcanvas.com/project/452634/overview/using-the-tween-library) and the [Editor](https://playcanvas.com/editor/scene/491504) for this example.

To get the above we are doing:

```javascript
this.entity
    .tween(this.entity.getLocalPosition())
    .to(new pc.Vec3(4, 0, 0), 1.0, pc.SineOut)
    .loop(true)
    .yoyo(true)
    .start();
```

Here is an example on how to tween the local rotation of an Entity:

[Interactive Demo]

Here are links to the [Project](https://playcanvas.com/project/452634/overview/using-the-tween-library) and the [Editor](https://playcanvas.com/editor/scene/491558) for this example.

To get the above we can do:

```javascript
this.entity
    .tween(this.entity.getLocalEulerAngles())
    .rotate(new pc.Vec3(180, 0, 180), 1.0, pc.Linear)
    .loop(true)
    .yoyo(true)
    .start();
```

Here's how to tween the local scale of an Entity:

[Interactive Demo]

Here are links to the [Project](https://playcanvas.com/project/452634/overview/using-the-tween-library) and the [Editor](https://playcanvas.com/editor/scene/491585) for this example.

To get the above we can do:

```javascript
this.entity
    .tween(this.entity.getLocalScale())
    .to(new pc.Vec3(3, 3, 3), 1.0, pc.SineOut)
    .loop(true)
    .yoyo(true)
    .start();
```

And finally here's a way to tween colors:

[Interactive Demo]

Here are links to the [Project](https://playcanvas.com/project/452634/overview/using-the-tween-library) and the [Editor](https://playcanvas.com/editor/scene/491559) for this example.

To get the above we can do:

```javascript
var color = new pc.Color(0, 0, 0);
var material = this.entity.render.material;
this.app
    .tween(color)
    .to(new pc.Color(1, 1, 1), 1.0, pc.Linear)
    .loop(true)
    .yoyo(true)
    .onUpdate(function () {
        material.diffuse = color;
        material.update();
    })
    .start();
```

Again you can find the library at [https://github.com/playcanvas/playcanvas-tween](https://github.com/playcanvas/playcanvas-tween).

--------------------------------------------------------------------------------

## User Interface - Buttons

URL: https://developer.playcanvas.com/tutorials/ui-elements-buttons/
Tags: ui

[Interactive Demo]

*Simple buttons using Element and Button components. See the [full scene](https://playcanvas.com/editor/scene/547900).*

When building a UI for your application you will almost certainly need to create buttons. This tutorial demonstrates how you can achieve that using the built-in [Elements](/user-manual/user-interface/elements/).

In this [scene](https://playcanvas.com/editor/scene/547900) we have created a 2D [Screen](/user-manual/user-interface/screens/) that looks like this in the hierarchy:

[Image: Hierarchy]

## Screen setup

Our screen is set up like so:

[Image: Screen]

Since it's a 2D screen we have ticked Screen Space. Our Reference Resolution is the resolution that we are targeting - in this case it's 1080 x 1920. We choose Blend for Scale Mode so that our Screen adapts to resolution changes and we set Scale Blend to 1 so that the Screen will adapt only to height changes.

Our screen has various children like an Image Element for the logo that is displayed on the top, a Text Element for showing the 'SELECT QUALITY' text and 3 buttons.

## Adding a button to the UI

There are two ways to add a button to the scene.

### Via the Hierarchy Panel

This is the easiest way to add a button to the scene as it creates the necessary entities, components and preconfigures the properties.

<img loading="lazy" src="/img/tutorials/ui/buttons/adding-button-via-hierarchy.gif" />

### With an existing Element

If there is an existing Element that we would like to turn into a button, we can add Button component to it in the Inspector panel and configure it ourselves.

<img loading="lazy" src="/img/tutorials/ui/buttons/adding-button-via-inspector.gif" width="300" />

Remember to enable Use Input on the Element component so the user can interact with it:

<img loading="lazy" src="/img/tutorials/ui/buttons/use-input-element.png" width="300" />

And set the Image Entity property on the Button component to be same Entity that the Element component is on.

<img loading="lazy" src="/img/tutorials/ui/buttons/set-image-entity-button.png" width="300" />

## Button setup

Let's take a closer look at the first button in the example project:

<img loading="lazy" src="/img/tutorials/ui/buttons/button.png" width="300" />

The button has 3 components:

- **Element component** - Positions and renders the button UI relative to its parent Screen.
- **Button component** - Handles how the button looks when the user is interacting with it.
- **Script component** - Listens for events on the Button component and handles the logic on what to do when clicked.

The button Entity also has a Text Element as a child for showing text (this is optional depending on the style of your button).

<img loading="lazy" src="/img/tutorials/ui/buttons/text-element.png" width="300" />

The Element's type is Image and it's anchored to the bottom of the screen.

<img loading="lazy" src="/img/tutorials/ui/buttons/bottom-anchor-pivot.png" width="300" />

After anchoring the button we give it an offset from the bottom by simply moving it up.

<img loading="lazy" src="/img/tutorials/ui/buttons/offset-position.png" width="300" />

We also have Use Input enabled in order to interact with the button.

<img loading="lazy" src="/img/tutorials/ui/buttons/use-input-element.png" width="300" />

### Changing how the button looks on interaction

We can change how the button looks when the user interacts with the button for the following states:

- **Hover** - When the mouse cursor is over the button.
- **Pressed** - When the user presses on the button.
- **Inactive** - When the button is inactive.

This can be done via two Transition Modes:

#### Tinting the color

Tinting the button color in each state is the easiest method to add some user feedback when they interact with it. In the project, the High Quality button has the following setup:

<img loading="lazy" src="/img/tutorials/ui/buttons/high-quality-button-setup.png" width="300" />

With the following effect:

<img loading="lazy" src="/img/tutorials/ui/buttons/high-quality-button-effect.gif" />

#### Changing the Sprite

We can also change the sprite image of the button in the different states for cases where you may want the button to change shape or want to give a look of the button being 'pressed' into the screen. The Low Quality button has the following setup:

<img loading="lazy" src="/img/tutorials/ui/buttons/low-quality-button-setup.png" width="300" />

With the following effect:

<img loading="lazy" src="/img/tutorials/ui/buttons/low-quality-button-effect.gif" />

### Button events

We have a script in the project that listens for when the user clicks on the button and updates the text in the UI to the quality setting that they've selected.

This script is attached to the button entities in the scene.

The Button component has a [`click` event](https://api.playcanvas.com/engine/classes/ButtonComponent.html#event:click) against which a callback function can be registered that works for both mouse and touch input.

```javascript
this.entity.button.on('click', function(event) {
    this.textEntity.element.text = this.description;
}, this);
```

There are other events that can be listened to such as `mouseenter` and `mouseleave`. A full list can be found in the [API documentation](https://api.playcanvas.com/engine/classes/ButtonComponent.html#event:click).

These events will only fire if Use Input is enabled on the Element component so make sure that has been ticked in the inspector.

<img loading="lazy" src="/img/tutorials/ui/buttons/use-input-element.png" width="300" />

--------------------------------------------------------------------------------

## User Interface - Leaderboard

URL: https://developer.playcanvas.com/tutorials/ui-elements-leaderboard/
Tags: ui

[Interactive Demo]

*A leaderboard using Element components. See the [full scene](https://playcanvas.com/editor/scene/547907).*

This tutorial demonstrates how to create a simple leaderboard using the built-in [Elements](/user-manual/user-interface/elements/). The leaderboard is filled programmatically with data that come from a JSON asset.

## Hierarchy

This is what our UI looks like in the hierarchy:

[Image: Hierarchy]

As you can see we have a 2D [Screen](/user-manual/user-interface/screens/), two Elements to show the title and sub title and two Image Elements which are going to be used as the backgrounds and panels for our leaderboard data. Under `Your Score` we are going to show the player's position in the leaderboard and under `Leaderboard` we will show the rest.

You will also notice a disabled Entity called `Entry Template`. This is a template that we will use for each row of the template. We will clone that template for each leaderboard entry that exists in our JSON asset and add each clone under the respective panel.

## Screen setup

Our [screen](/user-manual/user-interface/screens/) is set up like so:

[Image: Screen]

Since it's a 2D screen we have ticked Screen Space. Our Reference Resolution is the resolution that we are targeting - in this case it's 1080 x 1920. We choose Blend for Scale Mode so that our Screen adapts to resolution changes and we set Scale Blend to 1 so that the Screen will adapt only to height changes.

The screen entity also has a script component that contains the `leaderboard` script that we will see below.

## Panel setup

For each panel we have an Image element that shows its background. Under the panel we are going to programmatically add clones of the Entry Template. Our panels are anchored to the center of the screen.

## Entry Template setup

This is what our template for each leaderboard row looks like in the hierarchy:

[Image: Entry Template]

It has four child Text Elements for displaying the Position in the leaderboard, the name of the player, the player's score and a label that says 'PTS'.

The `Entry Template` itself is a Group Element:

[Image: Entry Template Attributes]

Notice how the Group Element has split horizontal anchors:

[Image: Split Anchors]

The horizontal anchors are not equal (they are 0 and 1) which means that the Element will expand automatically to fill the entire horizontal area if the Screen is resized. We also have a horizontal margin of 50 pixels to allow a small gap from the edges - the margin can only be set when anchors are split.

Now let's look at the rest of the Group's children.

### Position

Position is anchored to the left:

[Image: Position]

### Name

Name is anchored to the left and moved a bit to the right:

[Image: Name]

### Score

Score is anchored to the right

[Image: Score]

### Points

Points are anchored to the right

[Image: Pts]

## Script

This is the `leaderboard` script that reads our JSON asset and fills the leaderboard:

--------------------------------------------------------------------------------

## User Interface - Progress Bar

URL: https://developer.playcanvas.com/tutorials/ui-elements-progress/
Tags: ui

[Interactive Demo]

*A progress bar using Element components. See the [full scene](https://playcanvas.com/editor/scene/547906).*

We can easily create progress bars using the built-in [Elements](/user-manual/user-interface/elements/). In this tutorial we have a progress bar that loops from empty to full every few seconds.

## Hierarchy

This is what our UI looks like in the Hierarchy:

[Image: Hierarchy]

## Screen setup

Our [screen](/user-manual/user-interface/screens/) is set up like so:

[Image: Screen]

Since it's a 2D screen we have ticked Screen Space. Our Reference Resolution is the resolution that we are targeting - in this case it's 1080 x 1920. We choose Blend for Scale Mode so that our Screen adapts to resolution changes and we set Scale Blend to 1 so that the Screen will adapt only to height changes.

The screen has a child Text Element to show the POWER text and an Entity called `Progress Bar` which show our progress bar.

## Progress Bar setup

The progress bar is made of 2 Elements. The background image and the fill image.

The background image is the `Progress Bar` entity in our example. It has an Image Element and it shows the background image of the progress bar:

[Image: Background Image]

The fill image is the `Fill Image` entity in our example. It is a child of the `Progress Bar` entity and it has an Image Element to show the fill of the progress bar. This image is anchored to the left of its parent background image. That allows us to change the width of the element in order to make the progress bar grow.

[Image: Fill Image]

## Script

The `Progress Bar` entity has a script to control how the progress bar is resized:

The script has 2 attributes - the Entity that shows the fill image and the max width of that image. It has a `setProgress` function which sets the progress to a value between 0 and 1.

The `update` method essentially loops progress between 0 and 1. The important thing to note in this script is how we need to change the `width` and the `rect` of the fill image in order to properly resize our progress bar.

Changing the `width` makes the fill image larger and changing the `rect` makes sure that we only show the portion of the texture that is visible, so that we avoid stretching the visible texture. [Here](https://api.playcanvas.com/engine/classes/ElementComponent.html#rect) is the API reference for `rect`.

--------------------------------------------------------------------------------

## User Interface - Stats Counter

URL: https://developer.playcanvas.com/tutorials/ui-elements-stats-counter/
Tags: ui

[Interactive Demo]

*How to use buttons, progress bars and interact with elements. See the [full scene](https://playcanvas.com/editor/scene/547905).*

In this tutorial we are going to use the built-in [Elements](/user-manual/user-interface/elements/) to create a few simple widgets that allow you to increase a stat by clicking on plus / minus buttons.

For each stat we need a minus button, a plus button, a progress bar and some text to show the current stat value.

## Hierarchy

This is what our UI looks like in the Hierarchy:

[Image: Hierarchy]

## Screen setup

First we start by adding a new 2D [Screen](/user-manual/user-interface/screens/). This is what our Screen looks like:

[Image: Screen]

Since it's a 2D screen we have ticked Screen Space. Our Reference Resolution is the resolution that we are targeting - in this case it's 1080 x 1920. We choose Blend for Scale Mode so that our Screen adapts to resolution changes and we set Scale Blend to 1 so that the Screen will adapt only to height changes.

## Stats setup

For each stat we will create a different Group Element. This allows us to treat sub-elements of the Group to be anchored to the Group edges and allows us to treat each stat as a separate widget.

We will only examine the Boost stat - the others are exactly the same. This is what the `stats-boost` Entity looks like in the viewport:

[Image: Boost]

And these are its attributes:

[Image: Boost Attributes]

As you can see it has a Group Element component with the appropriate size to contain all our elements and it's anchored to the bottom of the screen. It also has a Script Component with the script `uiStats` assigned to it. This script will allow us to handle interactions with the Elements of the group.

This is what our group looks like in the Hierarchy:

[Image: Boost Hierarchy]

Our group has the following child Elements:

- `text-title`: A Text Element for the title of the group - anchored to the top of the group.
- `btn-minus`: An Image Element that has a child Text Element. This is our minus button and it's anchored to the bottom left of the group.
- `btn-plus`: An Image Element that has a child Text Element. This is our plus button and it's anchored to the bottom right of the group.
- `progress-bar`: Our progress bar anchored to the bottom of the Group. This is an Image Element for the background of the progress bar and it has 2 child Elements:
  - `image-progress`: The resizable Image Element that actually displays progress. Anchored to the left of the `progress-bar` Element.
  - `text`: The Text Element that displays our stats. Anchored to the center of the `progress-bar` Element.

## Scripts

We have a script on each button to allow us to change their texture based on hover states. This is similar to the script found in [this tutorial](/tutorials/ui-elements-buttons/). We also have a script to handle our progress bar. There is more info on progress bars in [this tutorial](/tutorials/ui-elements-progress/).

The main script that handles the interactions for each stat is `uiStats`:

In this script we find our child elements and when the plus or minus buttons are clicked we increase / decrease the stat and update the progress bar and its text.

--------------------------------------------------------------------------------

## User Interface - Text Input

URL: https://developer.playcanvas.com/tutorials/ui-text-input/
Tags: ui

[Interactive Demo]

[Click here to see the project](https://playcanvas.com/project/1005906/overview/ui-text-input).

## Overview

Text input can be done many ways in PlayCanvas and this tutorial shares a library that aims to be flexible enough to cover the most common cases.

It uses a HTML input element that is overlaid on top of the PlayCanvas rendering canvas and is positioned depending on whether mouse or touch is used to interact with the input element.

Using a HTML input element allows all the OS level operations that a user would expect on a webpage including copy and paste and password managers.

If touch is used, there is an assumption that a virtual keyboard would be shown and the HTML input element is positioned accordingly.

Here are examples of it being used on desktop:

And on mobile:

<video autoPlay muted loop controls src='/video/tutorial-text-input.mp4' style={{width: '100%', height: 'auto'}} />

:::note

This doesn't support 3D elements.

:::

## How to install

Open the [example project](https://playcanvas.com/project/1005906/overview/ui-text-input), right click on the folder 'ui-input' and click on 'Copy'.

Open your project, right click in the assets panel and click on 'Paste'

## Adding your first text input

Create an UI Image/Group Element Entity as a child of a UI 2D Screen and size it accordingly. Make sure to enable 'Use Input' on the Element component. This defines the input area for the user to click on and start inputting text.

Add a Script Component to the Element Entity and add the Script Type 'uiInputField'.

Create a UI Text Element Entity as a child of the Element Entity that we just created. The text in the element will updated by the Script Type 'uiInputField' based on the user input and the script attributes data.

Go back to the Script Type 'uiInputField' that added earlier, reference the Text Element Entity and change the script attributes data to your use case. Hover over each attribute to see the tooltip and description.

Finally, launch the scene to test.

## Advanced: How to style

The library uses a neutral color style for the HTML input element. If you want to change the style to better suit your application, you can modify the CSS in 'ui-input-library.js' in function `createInputDom`.

--------------------------------------------------------------------------------

## Using the Asset Registry

URL: https://developer.playcanvas.com/tutorials/using-assets/
Tags: loading, assets, basics

[Interactive Demo]

:::info

Click to focus, hold and release SPACEBAR to switch between two A and B models. Press 'L' to load the C model. Hold 'C' to display the C model.

:::

For simple games and products you will set up all your assets in the Editor, they will be preloaded before your application starts, and your app will just work.

For more advanced products you may wish to access your assets in code, change references, modify properties and also stream data in so that your application can load more quickly. Only loading the assets as they are needed. To do this you'll use the [`AssetRegistry`](https://api.playcanvas.com/engine/classes/AssetRegistry.html).

In this tutorial, we'll build a small scene which lets you swap the model on a render component by pressing a key. We'll also dynamically load a third model that is not preloaded. You can see the completed [project here](https://playcanvas.com/project/406036).

## Setup

The project is set up as follows:

* Three model assets are uploaded: **A** is a model of the letter A, **B** is a model of the letter B and **C** is a model of the letter C.
* The **C** render asset is set up *not* to be preloaded.
* A render Entity is added to the scene and the model **A** is assigned to the render component.
* A script component is added to the render Entity and a new script is created called `update_asset.js`.

Download the [A model](pathname:///downloads/tutorials/A.dae), [B model](pathname:///downloads/tutorials/B.dae) and [C model](pathname:///downloads/tutorials/C.dae) and upload them to your project. Ensure that the files are named A.dae, B.dae and C.dae as this will influence the asset names.

## The AssetRegistry

The [`pc.AssetRegistry`](https://api.playcanvas.com/engine/classes/AssetRegistry.html) is available in all scripts as `this.app.assets`. The registry is populated with the details of all the runtime assets added to your project whether they are loaded or not. Use the Asset Registry to find the assets you need in your application.

In this case we've declared three script attributes `a`, `b` and `c` which are assigned to assets in the Editor. Then they are automatically available in our script.

## Using preloaded assets

```javascript
    if (app.keyboard.isPressed(pc.KEY_SPACE)) {
        if (this.entity.render.asset !== this.b.id) {
            // update the render component to the new Render Asset
            console.log('Changed to B Render Asset');
            this.entity.render.asset = this.b;
        }
    } else {
        // ...
            if (this.entity.render.asset !== this.a.id) {
                // restore original Render Asset
                console.log('Changed to A Render Asset');
                this.entity.render.asset = this.a;
            }
        // ...
    }
```

The **A** and **B** assets are marked as **preload** in this project. This means that during the loading screen, these assets are downloaded. They will be ready to use as soon as your application starts. When an asset is loaded, the loaded resource is available as `asset.resource` and we can assign the asset to the [render component asset property](https://api.playcanvas.com/engine/classes/RenderComponent.html#asset). If `asset.loaded` is `false`, then the asset isn't loaded.

<img loading="lazy" src="/img/tutorials/using_assets/using-assets-a-preload.png" width="360" />

So, the `A` and `B` models are preloaded, which means we know they will be ready when we are running the application. This code checks if the space bar is pressed, and if so we change the render asset on the render component to be the resource property of the asset. In this case `asset.resource` will be a `pc.Render` object. For each different asset type (audio, texture, etc), the `asset.resource` property will be the relevant type.

## Loading assets at runtime

```javascript
if (app.keyboard.isPressed(pc.KEY_C)) {
    if (this.c.loaded) {
        if (this.entity.render.asset !== this.c.id) {
            console.log('Changed to C Render Asset');
            this.entity.render.asset = this.c;
        }
    }
} else {
    if (this.entity.render.asset !== this.a.id) {
        // restore original Render Asset
        console.log('Changed to A Render Asset');
        this.entity.render.asset = this.a;
    }
}
```

The **C** render asset is not marked as *preload*, so in the code above, you can see that we check if the resource is loaded before we use it. if `asset.loaded` is false, then the resource isn't loaded and we can't change the render component. If the **C** render asset is loaded then `this.c.resource` will be the `pc.Render` property and `asset.loaded` will be true, we'll be then able to assign it.

<img loading="lazy" src="/img/tutorials/using_assets/using-assets-c-preload.png" width="360" />

```javascript
if (this.app.keyboard.isPressed(pc.KEY_L)) {
    this.app.assets.load(this.c);
}
```

When you press the `L` key we load the **C** model. To do this we pass the unloaded asset into `this.app.assets.load()`. If the asset is already loaded, this method will do nothing.

Once the asset is loaded `asset.resource` will be a `pc.Render` instance and we can assign the asset to the render component by pressing the `C` key.

## The complete script

## AssetRegistry events

One thing we don't demonstrate in this example is how to know when an asset is loaded. To do this we use `pc.AssetRegistry` events like the `"load"` event. Here's some sample code:

```javascript
// find the asset by name in the registry
var asset = this.app.assets.find("A");
// set up a one-off event listener for the load event
this.app.assets.once("load", function (asset) {
    // asset.resource is now ready
}, this);
```

The `"load"` event is quite broad. It is fired for every asset that is loaded, so if assets are loaded elsewhere, you won't know that this is your asset. Instead you can narrow your event down by using the `"load:id"` event.

```javascript
// find the asset in the registry
var asset = this.app.assets.find("A");
// set up a one-off event listener for the load event
this.app.assets.once("load:" + asset.id, function (asset) {
    // asset.resource is now ready
}, this);
```

The above event will only be fired for that specific asset. Much more useful.

Finally, there is one specific coding pattern, that often occurs. So often, in fact, that we've provided a convenient method to do it for you.

```javascript
var asset = this.app.assets.find("A");
if (!asset.loaded) {
    this.app.assets.once("load:" + asset.id, function (asset) {
        // do something with asset.resource
    });
    this.app.assets.load(asset);
} else {
    // do something with asset.resource
}
```

This code loads an asset when it is needed, but it's a bit long winded. So, instead, you can use the `asset.ready()` method. This code performs the same function as above

```javascript
var asset = this.app.assets.find("A");
asset.ready(function (asset) {
    // do something with asset.resource
});
this.app.assets.load(asset);
```

The `asset.ready()` method will call it's callback as soon as the asset is loaded, if the asset is already loaded, it will call it straight away. `app.assets.load()` does nothing if the asset is already loaded.

--------------------------------------------------------------------------------

## Using events with scripts

URL: https://developer.playcanvas.com/tutorials/using-events-with-scripts/
Tags: events, tutorial, scripts

Sample showing how to communicate between scripts using events.

[Interactive Demo]

--------------------------------------------------------------------------------

## Forces and Impulses

URL: https://developer.playcanvas.com/tutorials/Using-forces-on-rigid-bodies/
Tags: physics, collision

[Interactive Demo]

*Use the cursor keys to apply impulses, the WASD keys to apply torques and rotate the cube. Press and hold F to apply a constant upward force to cancel gravity effects.*
*Press R to reset the cube.*

*Try to get the cube to balance and spin on one of its corners!*
*The full code used is shown at the bottom of this page.*

In this tutorial we will show you how to use forces to control a dynamic rigidbody and produce the demo shown above. We will briefly show the use of forces, impulses, torques and the use of rigidbody component UI to customize behavior.

## Scripting Forces

### Applying a Constant Force

```javascript
if (app.keyboard.isPressed(pc.KEY_F) ) {
    this.entity.rigidbody.applyForce(0, 9.8, 0);
}
```

Here a force along the global y-axis is applied to the accessed entity when the user presses the F key via [`applyForce(x, y, z)`](https://api.playcanvas.com/engine/classes/RigidBodyComponent.html#applyforce). The point of application of the force vector can also be set.

### Impulses

```javascript
if (app.keyboard.isPressed(pc.KEY_LEFT) ) {
    this.entity.rigidbody.applyImpulse(-1, 0, 0);
}
```

The cube is given an x-axis impulse to impart an instant change of velocity via [`applyImpulse(x, y, z)`](https://api.playcanvas.com/engine/classes/RigidBodyComponent.html#applyimpulse).

### Torques

```javascript
if (app.keyboard.isPressed(pc.KEY_W) ) {
    this.entity.rigidbody.applyTorque(-this.torque, 0, 0);
}
```

[Torques](https://en.wikipedia.org/wiki/Torque) (rotational forces) are applied to the entity via [`applyTorque(x, y, z)`](https://api.playcanvas.com/engine/classes/RigidBodyComponent.html#applytorque).

### TorqueImpulses

```javascript
this.entity.rigidbody.applyTorqueImpulse(x, y, z)
```

Instantaneous changes in angular velocity are applied via [`applyTorqueImpulse(x, y, z)`](https://api.playcanvas.com/engine/classes/RigidBodyComponent.html#applytorqueimpulse). This was not used in the code for the above demo.

## Moving dynamic rigidbodies

In order to move rigidbodies, you apply linear forces and rotational forces (torque) using the methods above. Usually you should try to avoid directly modifying the position or velocity of a rigidbody as this will override the simulation and it can lead to odd effects, especially when objects collide.

However, if you need to, you can override the velocity by assigning a new '[pc.Vec3](https://api.playcanvas.com/engine/classes/Vec3.html)' set of values directly to `entity.rigidbody.linearVelocity` or `entity.rigidbody.angularVelocity`.

For more information on rigidbody types, see [the collision API page](https://api.playcanvas.com/engine/classes/CollisionComponent.html), [the pc namespace page](https://api.playcanvas.com/engine/), [the fps-controller tutorial](/tutorials/first-person-movement/) and [the collision tutorial](/tutorials/collision-and-triggers/).

## General setup

We set up a basic scene with a spotlight, a cube (entity with model, rigidbody, collision and script components) and a floor (with model, rigidbody and collision components). The cube's rigidbody was set to dynamic while the floor's rigidbody was set to static. We created some materials for each box and changed the diffuse colors just to make it easier on the eye. We have also activated the 'cast shadows' option on both the SpotLight and DynamicBody entities. The full 'usingForces' Scene and code for [this PlayCanvas app can be found here](https://playcanvas.com/project/405828/overview/tutorial-forces--impulses).

## Limiting and control

Some Editor settings were set to prevent the constant application of unbalanced forces (and so prevent a body from continuously accelerating and moving out of control). We enabled angular damping on the cube's attribute editor as well as friction on both the cube and floor. Linear damping is not used here, however it can be used to simulate air resistance, and of course decelerations can be applied as required via code.

<img loading="lazy" src="/img/tutorials/forces/rigidbody_settings.jpg" alt="rigidbody_settings" />

## Teleporting a Body

To instantly teleport a body to a new position, you can't use the setPosition function from the pc.Entity API. This is because the physics engine would still think the body is in the old location. Instead, you have to use the rigidbody component's teleport function:

```javascript
//code within the update function
this.playerPos = this.entity.getLocalPosition();

// Keeping the cube on screen - cube moves off of one screen edge then appears from the opposite edge.
if (this.playerPos.x < -9.0) {
    this.entity.rigidbody.teleport(8.8, this.playerPos.y, this.playerPos.z);
}
if (this.playerPos.x > 9.0) {
    this.entity.rigidbody.teleport(-8.8, this.playerPos.y, this.playerPos.z);
}
```

If the cube moves beyond the viewable area in the x-direction, the teleport function is called and the cube entity is teleported across the screen. The entity is teleported to a less extreme left/right position so as not to continuously activate the `if()` statement.

## Reset cube code

```javascript
if (app.keyboard.wasPressed(pc.KEY_R)) {
    this.reset();
}
```

```javascript
reset: function () {
    this.entity.rigidbody.teleport(0, 2, 0);
    this.entity.rigidbody.linearVelocity = pc.Vec3.ZERO;
    this.entity.rigidbody.angularVelocity = pc.Vec3.ZERO;
}
```

We include a reset function that brings the cube to its original position and, as mentioned above, synchronizes the rigidbody's location to that of the teleported entity. The final two lines in the reset function reset the body's linear and angular velocities to zero. The object's orientation could also be reset, but is not carried out in this code.

## Full code listing

--------------------------------------------------------------------------------

## Vehicle Physics

URL: https://developer.playcanvas.com/tutorials/vehicle-physics/
Tags: collision, vr, physics, tutorial, raycast

An implementation of vehicle physics in PlayCanvas, using the RaycastVehicle API in ammo.js. Supports desktop, mobile and VR.

[Interactive Demo]

--------------------------------------------------------------------------------

## VHS/CRT Post Effect

URL: https://developer.playcanvas.com/tutorials/vhscrt-post-effect/
Tags: tutorial, posteffects

Basic fullscreen effect simulating a bad video screen like an old CRT.

[Interactive Demo]

--------------------------------------------------------------------------------

## Video Textures

URL: https://developer.playcanvas.com/tutorials/video-textures/
Tags: video, textures

[Interactive Demo]

Try it from the Editor in the [tutorial project.](https://playcanvas.com/project/405850)

This project creates a texture and runtime, downloads and plays a video file and renders the video into the texture. This texture is then applied to a model and used in the scene.

This script performs the following functions:

* Create new Texture
* Create an HTML Video element and play the video
* Apply the new texture to the material on the TV model
* Update the texture with video data every frame

--------------------------------------------------------------------------------

## Vignette Aberration

URL: https://developer.playcanvas.com/tutorials/vignette-abberation/
Tags: tutorial, posteffects

[Interactive Demo]

--------------------------------------------------------------------------------

## Warp a Sprite with GLSL

URL: https://developer.playcanvas.com/tutorials/warp-a-sprite-with-glsl/
Tags: rendering, shaders, tutorial

[Interactive Demo]

--------------------------------------------------------------------------------

## WebXR 360 Image

URL: https://developer.playcanvas.com/tutorials/webxr-360-image/
Tags: vr, tutorial

360 Image with WebXR support | Image Credit: Bob Dass from https://www.flickr.com/photos/54144402@N03/

[Interactive Demo]

--------------------------------------------------------------------------------

## WebXR 360 Video

URL: https://developer.playcanvas.com/tutorials/webxr-360-video/
Tags: vr, video, tutorial

360 Video with WebXR support

[Interactive Demo]

--------------------------------------------------------------------------------

## WebXR AR: DOM Overlay

URL: https://developer.playcanvas.com/tutorials/webxr-ar-dom-overlay/
Tags: camera, ar, tutorial

Example of how to use DOM (HTML + CSS) with WebXR AR session.

[Interactive Demo]

--------------------------------------------------------------------------------

## WebXR AR: Hit Test

URL: https://developer.playcanvas.com/tutorials/webxr-ar-hit-test/
Tags: camera, ar, tutorial, raycast

Example of how to use the WebXR Hit Test API. This allows you to hit test real-world geometry.

[Interactive Demo]

--------------------------------------------------------------------------------

## WebXR: AR Image Tracking

URL: https://developer.playcanvas.com/tutorials/webxr-ar-image-tracking/
Tags: camera, ar, tutorial

Example of how to use the WebXR Augmented Reality Image Tracking API. This allows you to *actively* track real-world images based on a provided sample.

[Interactive Demo]

--------------------------------------------------------------------------------

## WebXR AR Raycasting Shapes

URL: https://developer.playcanvas.com/tutorials/webxr-ar-raycasting-shapes/
Tags: input, ar, tutorial

Example of how to raycast in the PlayCanvas scene when using WebXR AR.

Tap the shapes to change their color!

[Interactive Demo]

--------------------------------------------------------------------------------

## WebXR Controller/Hand Models

URL: https://developer.playcanvas.com/tutorials/webxr-controllerhand-models/
Tags: input, vr, tutorial

Sample application with WebXR controller/hand models loaded based on input source profile. Models sourced from: https://github.com/immersive-web/webxr-input-profiles

[Interactive Demo]

--------------------------------------------------------------------------------

## WebXR Hands

URL: https://developer.playcanvas.com/tutorials/webxr-hands/
Tags: input, vr, tutorial

Sample application with WebXR Hands Tracking.

[Interactive Demo]

--------------------------------------------------------------------------------

## WebXR Hello World

URL: https://developer.playcanvas.com/tutorials/webxr-hello-world/
Tags: vr, tutorial

Basic scene with support for VR camera

[Interactive Demo]

--------------------------------------------------------------------------------

## WebXR: Plane Detection

URL: https://developer.playcanvas.com/tutorials/webxr-plane-detection/
Tags: camera, ar, tutorial

Example of how to use the WebXR Augmented Reality Plane Detection API. This allows you to *actively* track real-world surface estimations.

[Interactive Demo]

--------------------------------------------------------------------------------

## WebXR UI Interaction

URL: https://developer.playcanvas.com/tutorials/webxr-ray-input/
Tags: vr, ar, input, ui

[Interactive Demo]

*Click the VR/AR button if you have a VR/AR compatible device/headset.*

This is a WebXR experience that provides interaction between UI and XR input source, such as: laser pointer; gaze; touch screen. Supports desktop, mobile, Oculus Browser, Google Cardboard™, Google Daydream™, Samsung Gear VR™ and other VR/AR headsets.

Let's have a look at the source of the [tutorial project](https://playcanvas.com/project/460449/overview/webvr-ray-input).

## Entering VR/AR

Every WebXR experience on PlayCanvas will always have these two elements in some form:

* Adding a user interaction for the user to enter VR/AR
* Enabling VR/AR on the camera

```javascript
button.element.on('click', function() {
    // check support for VR
    if (app.xr.isAvailable(pc.XRTYPE_VR)) {
        // start VR session
        cameraEntity.camera.startXr(pc.XRTYPE_VR, pc.XRSPACE_LOCALFLOOR);
    }
});
```

In this project, we have `xr.js` which is added to the Root entity. It manages VR and AR UI buttons, reacts to XR availability changes and XR session state changes.

To read more about the direct PlayCanvas API for WebXR, please refer to the [User Manual](/user-manual/xr/using-webxr/).

## XR Input Types

The level of fidelity for input devices can be broken into the following groups (DOF == Degrees of Freedom):

* **Gaze** - The default type which has no position and orientation of its own, and is based on the orientation of the head mounted display. Simply put - it is always facing forwards in the direction the user is facing. These include mobile-based VR devices such as Google Cardboard™ and Samsung Gear VR™.
* **Screen** - Touch based input source, which is possible in AR. For example, on mobile devices with touch screens.
* **Tracked Pointer** - Input source which has a tracked rotation and an optionally tracked position in space. This is usually a grippable device, and is associated with hands, either as hand controllers or tracked hands itself. This can be: Google Daydream™, Gear VR™ Controller, Oculus Touch™, Vive™ controllers and many others.

Every input source has a ray with an origin where it starts and a direction in which it is pointing. WebXR input source implementation in PlayCanvas supports all input source types without any extra work from a developer. If an input source is grippable, then we can render its model based on the provided position and rotation.

### Input Sources

The system for the tracked input sources consists of two files:

#### `controllers.js`

This tracks added input sources using [XrInput](https://api.playcanvas.com/engine/classes/XrInput.html) and makes instances of controller entities for them. For example:

```javascript
app.xr.input.on('add', function (inputSource) {
    // new input source is added
});
```

#### `controller.js`

This is attached to each entity that represents an input source and has the original [XrInputSource](https://api.playcanvas.com/engine/classes/XrInputSource.html) associated with it. When an input source can be gripped, it will enable the child entity for the visual model for a controller.

On each update, it will position and rotate the entity based on the input source position and rotation:

```javascript
if (inputSource.grip) {
    this.visualEntity.enabled = true;
    this.entity.setPosition(this.inputSource.getPosition());
    this.entity.setRotation(this.inputSource.getRotation());
}
```

## UI

3D UI is created using [Button](https://api.playcanvas.com/engine/classes/ButtonComponent.html) and [Element](https://api.playcanvas.com/engine/classes/ElementComponent.html) components. Using a combination of both, we can create interactive buttons in 3D space.

Creating a 3D UI for an XR environment is exactly the same as creating a 3D UI for mouse/touch interaction in a non-XR environment. Read more on creating [User Interfaces](/user-manual/user-interface/).

By default, each XrInputSource has an `elementInput` property enabled. This means it will interact with Button components just like mouse or touch input, but using its associated 3D ray. Each input source has a ray that has an [origin](https://api.playcanvas.com/engine/classes/XrInputSource.html#getorigin) and a [direction](https://api.playcanvas.com/engine/classes/XrInputSource.html#getdirection). In this tutorial, we visualize an input source's ray:

```javascript
// set starting point of ray
vecA.copy(inputSource.getOrigin());
// set end point of ray
vecB.copy(inputSource.getDirection());
vecB.scale(1000).add(vecA);
// render line between those two points
app.renderLine(vecA, vecB, color);
```

## UI Interaction

In this tutorial, we have two types of buttons: Rotate (button-rotate.js) and Color (button-color.js) buttons. When rotate button is [clicked](https://api.playcanvas.com/engine/classes/ButtonComponent.html#event:click), it will set the rotation speed of a cube:

```javascript
entity.button.on('click', function() {
    targetEntity.script.cube.rotateSpeed = rotateSpeed;
});
```

When the color button is clicked, we change the diffuse color of each mesh instance of a cube model.

This UI interaction is agnostic to input source: either it originates from VR handheld devices; gaze input of mobile VR; on-screen touch in an AR environment; as well as classic mouse and touch. So creating truly multi-platform applications and testing is easy.

--------------------------------------------------------------------------------

## WebXR Realistic Hands

URL: https://developer.playcanvas.com/tutorials/webxr-realistic-hands/
Tags: input, vr, tutorial

Realistic hands in VR!

[Interactive Demo]

--------------------------------------------------------------------------------

## WebXR Tracked Controllers

URL: https://developer.playcanvas.com/tutorials/webxr-tracked-controllers/
Tags: input, vr, tutorial

A sample application with boilerplate code for WebXR support with tracked controllers using PlayCanvas.

[Interactive Demo]

--------------------------------------------------------------------------------

## WebXR VR Lab

URL: https://developer.playcanvas.com/tutorials/webxr-vr-lab/
Tags: input, vr, tutorial

A living project built by the PlayCanvas team to help developers learn about creating scalable and responsive WebXR VR applications for all devices.

[Interactive Demo]

--------------------------------------------------------------------------------

## World space UI rendering on top

URL: https://developer.playcanvas.com/tutorials/world-space-ui-rendering-on-top/
Tags: rendering, ui, tutorial

Learn how to render world space UI over the top of the world.

[Interactive Demo]

--------------------------------------------------------------------------------

## World to UI Screen space

URL: https://developer.playcanvas.com/tutorials/world-to-ui-screen-space/
Tags: camera, ui, tutorial

Sample in which a UI Element is positioned over a world space point.

[Interactive Demo]

--------------------------------------------------------------------------------

## Welcome

URL: https://developer.playcanvas.com/user-manual/

Welcome to the PlayCanvas User Manual. Your exciting journey learning PlayCanvas begins now!

[Image: PlayCanvas Demos]

Let's [get started](getting-started)!

--------------------------------------------------------------------------------

## 2D

URL: https://developer.playcanvas.com/user-manual/2D/

The PlayCanvas Engine is designed to make creating 3D games and applications fast and simple. However, we also support a number of great features for creating 2D games. With PlayCanvas' 2D features you get all the benefits of a powerful 3D engine but for 2D games.

## Basic Features

### Sprites

[Image: Sprite]

2D graphics are often known as **Sprites**. In PlayCanvas you can create [Sprite Assets](/user-manual/editor/assets/inspectors/sprite) and [Sprite Components](/user-manual/editor/scenes/components/sprite). The Sprite Component is attached to Entities in order to display 2D graphics in your scene. Sprite Assets in PlayCanvas store multiple image frames from a Texture Atlas in sequence. So you can use a Sprite Asset to create flip-book style animated graphics in your games.

### Texture Atlases

[Image: Texture Atlas]

A [Texture Atlas](/user-manual/editor/assets/inspectors/texture-atlas) is an enhanced version of the standard [Texture](/user-manual/editor/assets/inspectors/texture) asset. In addition to the regular texture features, a Texture Atlas includes the definitions of a set of "Frames". Each frame is a region of the texture which can be referenced in a Sprite Asset.

### Sprite Editor

[Image: Sprite Editor]

The [Sprite Editor](/user-manual/2D/sprite-editor) is the tool used to generate Texture Atlas frames and Sprite Assets. You can open the Sprite Editor by double-clicking on any Texture Atlas or Sprite Asset. [Read More](/user-manual/2D/sprite-editor).

*Artwork created by [PixelBoy](https://twitter.com/2pblog1)*

--------------------------------------------------------------------------------

## 9-slicing

URL: https://developer.playcanvas.com/user-manual/2D/9-slicing/

9-slicing (sometimes called 9-patch) is a technique for 2D graphics that splits a single image into 9 areas which are scaled individually in order to prevent stretching when the image is displayed at different sizes and with different aspect ratios.

[Image: 9 Sliced Button]

In the image above you can see the 9 areas that a defined using the Texture Atlas editing features of the [Sprite Editor](/user-manual/2D/sprite-editor). When added to a scene using either a [Sprite Component](/user-manual/editor/scenes/components/sprite) or an [Image Element Component](/user-manual/editor/scenes/components/element), the image can be resized using the width and height properties of the component. Each area is scaled using the following rules:

* **Center** - stretch or tile both horizontally and vertically
* **Top, Bottom** - stretch or tile horizontally only
* **Left, Right** - stretch or tile vertically only
* **TopLeft, TopRight, BottomLeft, BottomRight** - Do not stretch or tile

[Image: Button Resize Animation]

## Setting up 9-slicing

[Image: Setup 9-slicing]

To setup a 9-sliced sprite. Create a frame around the area that you wish to use 9-slicing on in the Sprite Editor. Then use the blue handles or the Border property in the Frame Inspector to set the borders to outline the center portion of the image that you wish to be the stretch part of your sprite.

Finally click the **New Sliced Sprite From Selection** to create a new Sprite with the render mode set to *Sliced*.

## Render Modes

Sprite Assets can have one of three Render Modes.

### Simple Sprites

[Image: Simple Render Mode]

*Simple* Render Mode has no 9-slicing. Use this mode for regular sprites.

### Sliced Sprites

[Image: Sliced Render Mode]

*Sliced* Render Mode stretches portions of the image. The center stretches horizontally and vertically; the left and right sections stretch vertically; the top and bottom sections stretch horizontally and the corners do not stretch at all.

### Tiled Sprites

[Image: Tiled Render Mode]

*Tiled* Render Mode is similar to *Sliced* mode except instead of stretching the sections repeat in a tiled manner. The center tiles horizontally and vertically; the left and right tile vertically; the top and bottom tile horizontally and the corners do not tile at all.

--------------------------------------------------------------------------------

## Sprite Editor

URL: https://developer.playcanvas.com/user-manual/2D/sprite-editor/

The Sprite Editor is used to edit both Texture Atlas assets and Sprite assets. It is also used to create Sprite Assets.

To open the Sprite Editor, double-click on any Texture Atlas or Sprite asset in the Editor asset panel.

[Image: Sprite Editor]

## Viewport

[Image: Viewport]

The viewport in the Sprite Editor shows the currently selected Texture Atlas. Frames are shown as grey boxes on the texture atlas. The currently selected asset is shown with two sets of handles. green and blue. Green handles modify the frame's width and height, blue handles modify the frame's border property.

### Editing Texture Atlases

Within the viewport use the mouse to drag the outline of a Frame, then use the green and blue handles to modify the frames extents and borders.

#### Frames

Frames are a defined region on a Texture Atlas. A frame has a name, a position and size and a border. The border is used to define the edges of the 9-slicing region. See the [9-slicing documentation](/user-manual/2D/slicing/) for more information.

## Inspector

The inspector panel shows the properties of the currently selected item. The inspector changes depending on whether the selected item is a Texture Atlas, Frame or Sprite Asset.

### Texture Atlas Inspector

[Image: Texture Atlas Inspector]

The Texture Atlas Inspector allows you to automatically slice a texture atlas into a fixed grid.

First choose what to do with existing frames. You can choose to *Delete Existing* frames or do not delete and *Only Append* frames.

Then select the method of specifying the grid cells

- Grid By Frame Count - Specify the number of grid cells horizontally and vertically to divide the texture into
- Grid By Frame Size - Specify the size in pixels that each grid cell should be

Then set the values for Offset and Spacing. Offset is the number of pixels to move in from the top left of the texture before starting the grid. Spacing is the number of pixels between each grid cell.

Finally set the pivot for all newly generated frames, the pivot determines the position and rotation point of the sprite. It is specified as a proportion of the width and height.

Once all your settings are chosen, click the **Generate Frames** button to create your frames.

### Frame Inspector

[Image: Frame Inspector]

The Frame Inspector lets you set the properties for an individual frame or group of frames. It is visible when you select one or more Frames. The Frame Inspector is also where you create a new Sprite from.

The Inspector lets you modify the Position, Size and Border properties of the frame.

#### Frame Inspector Actions

- **New Sprite From Selection** - Create a new Sprite Asset and assign the frames that you have selected to the Sprite, in the order that they were selected.
- **New Sliced Sprite From Selection** - Create a new Sprite Asset and set its type to *Sliced*. Assign the frames that you have selected to the Sprite, in the order that they were selected.
- **Focus on Selection** - Focus the viewport on the selected Frames. [Keyboard Shortcut: F]
- **Trim Selected Frames** - Reduce the size of the selected frames to remove any excess empty space around an image. [Keyboard Shortcut: T]
- **Delete Selected Frames** - Delete the selected Frames [Keyboard Shortcut: Delete]

### Sprite Inspector

[Image: Sprite Inspector]

The Sprite Inspector is visible when a Sprite Asset is selected. This inspector is used to edit the properties of one or more Sprites. See the [Sprite Asset](/user-manual/editor/assets/inspectors/sprite/) for more details on the properties.

Click the **Add Frames to Sprite Asset** button to enter the Sprite Edit Mode. In this mode, you can select multiple frames from the Frame Panel or Viewport and add them to the Sprite. Click the **Add Selected Frames** to add the frames to your Sprite.

Using the Sprite Inspector you can also re-order the Frames in your Sprite, or delete individual Frames.

## Frames Panel

The Frames Panel lets you select and delete Frames from the Texture Atlas

## Asset Panel

The Asset Panel shows you all the Sprite Assets that have been created from the current Texture Atlas. You can select Sprite Assets from this panel.

*Artwork created by [PixelBoy](https://twitter.com/2pblog1)*

--------------------------------------------------------------------------------

## Using Texture Packers

URL: https://developer.playcanvas.com/user-manual/2D/texture-packing/

## What is texture packing?

It's common to find sprites or UI images bundled as separate images. Texture packing is combining those separate images into a single [texture atlas](/user-manual/editor/assets/inspectors/texture-atlas/).

This has several advantages including:

- Faster loading times as it's a single network request instead of many.
- As it's a single texture, the sprites can be batched into a single draw call.

## Tools

Here are some texture packing tools that are compatible with PlayCanvas.

### TexturePacker Online (free)

([Website](https://www.codeandweb.com/tp-online))

A free browser tool that is able to do the basics of texture packing.

1. Clear the texture atlas.
2. Add your sprites/UI images.
3. Data format should be kept as JSON hash.
4. Download .png for the texture atlas.
5. Download .json for the frame data.

### Texture Packer tool

([Website](https://www.codeandweb.com/texturepacker))

TexturePacker is a paid tool for download that has many more features and options for creating texture atlases. These include more control over the layout of the sprites/UI images and being able to set up [9 slicing](/user-manual/2D/slicing/).

Basic steps:

1. Add your sprites/UI images.
2. Set Output files -> Framework as PlayCanvas.
3. Click on Publish sprite sheet to create the texture atlas and JSON frame data.

## Creating frames in Sprite Editor

Once you have uploaded the texture atlas file into the Editor and created a [texture atlas asset](/user-manual/editor/assets/inspectors/texture-atlas/), open the Sprite Editor.

Click on 'Upload Texture Packer JSON' and select the JSON frame data file to upload.

This will create frames in the texture atlas that you can create sprite assets from.

When updating the texture atlas later in development, as long as the sprite filenames and frame names remain the same, the following will happen when uploading the new frame data:

- Sprites that have been removed on the atlas will have their frames removed.
- Sprites that have been added on the atlas will have new frames added to the end of the list.
- Existing sprites will have their frames updated.

--------------------------------------------------------------------------------

## Account Management

URL: https://developer.playcanvas.com/user-manual/account-management/

Welcome to Account Management! This section covers everything you need to know about managing your PlayCanvas account, from initial setup to advanced Organization features.

## User Accounts

New to PlayCanvas? Start here to create your account and get familiar with the basics:

- [User Accounts Overview](user-accounts/) - Learn about User Accounts
- [Creating Your Account](user-accounts/account-creation.md) - Sign up, log in, and initial setup
- [User Home](user-accounts/user-home.md) - Your user home page
- [Account Settings](user-accounts/settings.md) - Privacy and account preferences

## Organizations

For teams and businesses working together:

- [Organizations Overview](organizations/) - What are organizations and their benefits
- [Creating Organizations](organizations/creating.md) - Set up team workspaces
- [Managing Organizations](organizations/managing.md) - Administration and permissions

## Billing

Understanding subscriptions and payments:

- [Billing & Subscriptions](billing.md) - Plans, invoices, and payment management

---

Whether you're an individual developer or part of a large team, this section will help you make the most of your PlayCanvas account.

--------------------------------------------------------------------------------

## Billing

URL: https://developer.playcanvas.com/user-manual/account-management/billing/

Welcome to the Billing FAQ that lists the common questions relating to your PlayCanvas subscription.

### Can businesses sign up to the Personal plan?

Yes, providing the annual revenue of that business is below $100,000. Otherwise, the Organization plan should be selected.

:::note

We don't verify company revenues, so there's an element of trust involved. We rely on our users to support the ongoing development of PlayCanvas by choosing the plan that best matches their circumstances. Thanks for helping us keep PlayCanvas sustainable!

:::

### Does my subscription auto-renew?

Yes. Once you subscribe to the Personal or Organization plan, payment will automatically be taken on a monthly basis, on the day of the month when you first subscribed.

### Can I pay in advance for my subscription?

Yes, but only for Organization plan subscriptions with a minimum term of 1 year. If you'd like to pay in advance for an Organization plan, please contact us at [billing@playcanvas.com](mailto:billing@playcanvas.com) with the following information:

* Number of Organization seats required
* Term of the upgrade
* Business name
* Business address
* Contact name
* Contact email
* VAT number (if applicable)

We will generate an online invoice that can be settled by debit or credit card. Once payment is received, we'll provide you with an account upgrade code that you can apply on the [upgrade page](https://playcanvas.com/upgrade).

Personal plan subscriptions are only available as monthly auto-renewing subscriptions.

### How do I cancel my subscription?

You can cancel your plan subscription at any time.

1. Visit your [Account page](https://playcanvas.com/account).
2. Scroll down to the 'Current Plan' section.
3. If you have **Personal** showing here, hit 'Cancel' to cancel it.
4. Hit 'OK' in the **CANCEL PLAN** dialog.
5. Also in the 'Current Plan' section, you will see a list of your Organizations.
6. For each Organization listed, click its icon to visit its Account page.
7. Scroll down to the 'Current Plan' section.
8. If you have **Organization** showing here, hit 'Cancel' to cancel it.
9. Hit 'OK' in the **CANCEL PLAN** dialog.

Once your subscription is cancelled, you will not be billed again unless you resubscribe.

:::note

When you cancel your subscription, it will not revert to the free tier immediately. Instead, this will happen at your next billing date. If between the time you cancel and your next billing date you decide you would like to continue your subscription, just return to your Account page and hit 'Resume'.

:::

### What happens to my published apps when I cancel my subscription?

Your published apps will be completely unaffected if you cancel your subscription. This is true regardless of whether your apps are hosted by us or by you.

### What happens to my private projects when I cancel my subscription?

Your private projects will become locked (inaccessible) when you cancel your subscription. To unlock them, you have two options:

1. Resubscribe to a plan.
2. Make the projects public.

### How do I receive invoices?

Invoices for monthly subscriptions are emailed to the owner of the subscribed account on the monthly billing date. The subject line for these emails begins with "Your receipt from PlayCanvas Ltd".

:::tip

If invoices are not reaching your inbox, please check your spam folder.

:::

### Where can I find my billing history?

Your historical invoices are available to download on `playcanvas.com`. Visit your account page and scroll to the `Invoices` section. You should see something similar to this:

[Image: Billing History]

### How do I update my details on my invoices?

You may want to edit certain details that appear on your invoices such as:

* Billing Name
* Billing Address
* Tax ID

To do this, visit the account page for your subscribed account (remembering that Organizations have their own account page). Navigate to the Billing Info section and hit the Edit link. Fill out the payment details form and hit 'PAY NOW'.

:::note

While the button says 'PAY NOW', editing the details of an existing subscription will not trigger a payment straight away. You will be billed as normal on the next billing date and the invoice will show the updated details.

:::

### How is billing for Organization accounts calculated?

The day you subscribe to an Organization account is your monthly billing date. So if you subscribe on 10th July, you will be billed on that date with subsequent billing dates as 10th August, 10th September and so on.

The Organization plan allows you to add and remove seats at any time. When seats are added or removed, it is important to understand that this will increase or decrease the cost of your plan. However, this change in cost is not accounted for immediately when the number of seats is changed. Instead, it is accounted for at the next billing date.

#### Example

Imagine the following sequence of events:

* A user signs up for a 1-seat Organization plan on 10th May.
* The user then adds 1 seat on 20th June.
* The user removes 1 seat on 20th August.

The billing events from May to October will be as follows:

* 10th May: **$50**
* 10th June: **$50**. The plan has 1 seat and no changes were made to the plan in the previous month.
* 10th July: $100 + $50 x 20/30 = **$133.33**. The user has two seats on this billing date so is charged 2 x $50 for the upcoming month. One of these seats was added on 20th June and the use of that seat between 20th June and 10th July must now be accounted for. Since this period is 20 days of the 30 days between the two billing dates, $50 x 20/30 is added to the bill. In other words, seats that are added or removed are accounted for ‘pro-rata’ on the next billing date.
* 10th August: **$100**. The plan has 2 seats and no changes were made to the plan in the previous month.
* 10th September: $50 - $50 x 21/31 = **$16.13**. The users has one seat on this billing date so is charged $50 for the upcoming month. A seat was removed on 20th August so it was only used for 10 of the 31 days between billing dates. Since the user paid $50 for that seat on the previous billing date, they are due a refund for the unused 21 days. So $50 x 21/31 is subtracted from the bill.
* 10th October: **$50**. The plan has 1 seat and no changes were made to the plan in the previous month.

### I have a Personal plan. Can I add a free user to my private project?

No. You can only add users with a Personal plan to your private projects.

### How can I contact PlayCanvas about billing?

If your question about billing is not answered on this page, please email [billing@playcanvas.com](mailto:billing@playcanvas.com).

--------------------------------------------------------------------------------

## Organizations

URL: https://developer.playcanvas.com/user-manual/account-management/organizations/

Organizations offer a way for businesses and large projects to manage many users. They provide enhanced collaboration features, permissions management, and team coordination tools.

## What are Organizations?

Organizations are team workspaces that allow multiple users to collaborate on projects with structured permissions and management controls.

## Free Organization Features

Every free Organization includes:

- **Unlimited public projects** - No limit on public project creation
- **Unlimited public team members** - Add as many collaborators as needed
- **Project-based permissions** - Fine-grained control over who can access what
- **Offline Archive & Restore** - Backup and restore project data
- **Branding removal** - Ability to remove PlayCanvas branding from published apps
- **REST API access** - Programmatic access to your projects and data

## Organization Plans

You can also purchase an [Organization Plan](https://playcanvas.com/plans) which includes additional features:

- **Unlimited private projects** - Keep your work confidential during development
- **Additional storage space** - More room for assets and project data
- **Advanced team management** - Enhanced user administration tools
- **Priority support** - Faster response times for technical issues

## When to Use Organizations

Organizations are ideal for:

### Business Teams

- Companies developing commercial games or applications
- Teams that need private project development
- Organizations requiring formal permissions structure

### Large Projects

- Complex projects with multiple specialists
- Long-term development efforts
- Projects requiring role-based access control

## Getting Started

To start using Organizations:

1. [Create an Organization](creating.md) - Set up your team workspace
2. [Add team members](managing.md#seats) - Invite collaborators
3. [Configure permissions](managing.md#permissions) - Set up access controls
4. [Create team projects](managing.md#projects) - Start collaborative development

## Organization vs Personal Accounts

| Feature | Personal Account | Organization |
|---------|------------------|--------------|
| Private Projects | Limited | Unlimited (with plan) |
| Team Members | Limited collaboration | Unlimited |
| Permissions | Basic | Advanced role-based |
| Branding | PlayCanvas branding | Removable |
| API Access | Limited | Full REST API |
| Support | Standard | Priority (with plan) |

--------------------------------------------------------------------------------

## Creating Organizations

URL: https://developer.playcanvas.com/user-manual/account-management/organizations/creating/

There are various ways to create an Organization. Any organizations you are part of will appear next to your name on your profile like so:

[Image: Profile Organizations]

### Create from the dropdown menu

The first way is to click on NEW ORGANIZATION in the top-right dropdown menu:

[Image: Dropdown]

This will bring up the following popup:

[Image: New Organization]

Enter the name for the Organization and an Organization ID which is a string with only alphanumeric characters and dashes allowed. The default e-mail address is your own but you can change it to a different one.

Click CREATE and that will take you to the profile page of the Organization.

If you have existing projects (such as private projects on a Personal plan), you can transfer them to the Organization account by the following steps:

* On your user account, [transfer all projects ownership](/user-manual/editor/projects/ownership-transfers#initiating-ownership-transfers) to the Organization account.
* On the Organization account, accept the project transfer requirements.
* Cancel the Personal Plan (if you are on one) to downgrade to the Free plan. Please note, this will happen immediately, regardless of when you last paid.
* [Add yourself to the projects as Admin](/user-manual/account-management/organizations/managing/#projects) on the Organization account.

All the projects will now be under the Organization account with your user account as Admin for the projects.

### Convert a user account into an Organization

Another way to create an Organization is to convert your user account. You can do this by clicking CONVERT in your [account](/user-manual/account-management/user-accounts/settings/#convert-account-to-organization) page.

[Image: Convert Organization]

This will bring up the following popup:

[Image: Convert Popup]

Converting your user account into an Organization will mean that you will no longer be able to log in with this user account. For that reason you need to specify an owner for the new Organization.

:::warning

Make sure you can log in with the new owner's account.

:::

If you have subscribed for a paid plan, that plan will be cancelled unless you choose to subscribe to an Organization plan which is offered in the popup. The number of seats to purchase will be calculated for you automatically based on the number of users in your existing private projects.

After you convert your account, you will be logged out. Then log back in with the new owner's account and you will be able to access the converted account and all its projects.

--------------------------------------------------------------------------------

## Managing Organizations

URL: https://developer.playcanvas.com/user-manual/account-management/organizations/managing/

### Permissions {#permissions}

You can manage permissions of your Organization from its account page.

[Image: permissions]

Here you can see who is the Owner and the Administrators of the Organization. You can transfer ownership to a different user and add or remove Administrators.

Administrators can do everything the owner can do e.g. create and delete projects, except delete the Organization.

### Seats {#seats}

To give a user a seat, add them to any private project that is owned by the organization. Public projects do not require seats.

[Image: seats]

You can see which users have access to your private projects and occupy your available seats. From here you can remove users which will remove them from all your projects, freeing up seats.

The Owner of the organization does not need to occupy a seat if they are not actively working on any projects. It is recommended to have at least one user who will occupy a seat, to be an Admin of the Organization so that they can create new projects and manage existing projects.

[Image: upgrade]

Here you can increase or decrease the number of seats for your Organization. This view is available if you have subscribed for an Organization plan.

[Image: delete]

The Owner of the Organization can delete the Organization from here. This will completely erase the account and all its projects. This action cannot be reversed.

### Projects {#projects}

Administrators of the organization can add themselves to any project owned by the organization. On the account page, click on the drop down arrow on the right and then click on 'Add me as admin'.

[Image: add to project]

From here, you can add other users to the project as usual.

--------------------------------------------------------------------------------

## User Accounts

URL: https://developer.playcanvas.com/user-manual/account-management/user-accounts/

Your user account is your personal space within PlayCanvas. This section covers creating, managing and configuring your account.

## Why Create a PlayCanvas User Account?

Creating a free PlayCanvas account opens up a world of possibilities for building 3D web applications! Once you are signed up, you can:

- 🛠️ **Access the [PlayCanvas Editor](../../editor/)** - Build 3D apps using a powerful, visual interface.

But not everyone opts to use the PlayCanvas Editor - perhaps you prefer to use the [PlayCanvas Engine](../../engine/) directly, or [PlayCanvas Web Components](../../web-components/) or [PlayCanvas React](../../react/) instead. Does it still make sense to create a PlayCanvas account? Absolutely! And here's why:

- 🔍 **Explore and learn** - Browse scripts and assets in thousands of public Editor projects to learn and find inspiration
- ⚙️ **Generate Assets** - Use the Editor's asset processing capabilities to generate assets for use in any PlayCanvas-based project
- ✨ **Publish Gaussian Splats** - Upload and share your splats using [SuperSplat](https://playcanvas.com/products/supersplat)
- 👥 **Community participation** - Star, follow and comment on the projects of other PlayCanvas community members
- 🔔 **Stay Informed** - Creating an account allows us to notify you of exciting updates related to the entire PlayCanvas platform

So no matter how you use PlayCanvas, we strongly recommend you create an account on playcanvas.com!

--------------------------------------------------------------------------------

## Creating Your Account

URL: https://developer.playcanvas.com/user-manual/account-management/user-accounts/account-creation/

To join the PlayCanvas community, we strongly recommend that you create a new account. Click **SIGN UP** at the top right of [https://playcanvas.com](https://playcanvas.com).

[Image: Sign Up]

At the sign up page, you have 3 options for account creation:

1. **Username and Password** - enter your email address and password.
2. **Sign in with Google** - authenticate using your Google account.
3. **Sign in with GitHub** - authenticate using your GitHub account.

[Image: Sign Up Form]

If you select the Username and Password option, you will be asked to confirm your email address:

[Image: Confirm Email]

Head to your email inbox and click the verification link. You will be taken to this page:

[Image: Email Verified]

On hitting continue, you'll be prompted to add some details about yourself:

[Image: Details Form]

- **Full Name** - A non-unique display name. We recommend setting this to your actual name.
- **Username** - Your handle that is used to uniquely identify you on the platform. This is used to determine the URL of your PlayCanvas profile page.
- **Skills** - Pick the skills you have related to building interactive graphics apps (options are 'Coder', 'Artist', 'Designer' and 'Musician'). These skills will be displayed on your public profile.
- **Email Notifications** - Enable this to receive tips and news about PlayCanvas.

Once you have completed the form, hit **Create Account**!

You begin by being dropped straight into your first project in the PlayCanvas Editor.

[Image: Roll Ball]

It is a clone of a simple 'Roll a Ball' game. This is your playground for getting familiar with the Editor. A set of clickable hot-spots will guide you through some of the key elements of the Editor.

Jump to the [Editor section](../../editor/index.md) to learn more about the PlayCanvas Editor.

--------------------------------------------------------------------------------

## Account Settings

URL: https://developer.playcanvas.com/user-manual/account-management/user-accounts/settings/

Your account settings page provides access to all your personal account configuration options, from basic profile information to billing and security settings. To view your user account settings, navigate to [https://playcanvas.com/account](https://playcanvas.com/account). You should see something like this:

[Image: Account Settings]

## Profile Picture

Upload and manage your profile picture that appears across PlayCanvas. Click on your current profile picture to upload a new image.

## Account Info

### Username

Your unique PlayCanvas username that appears in URLs and identifies you across the platform. You can change your username, providing that username has not already been taken. If it has, you will see the message **Already used**.

### Your Full Name

The display name shown on your profile and in collaborations. You can edit this whenever you choose.

### Email Address

Your primary email address for account notifications and login. Click "Edit" to change your email address. If you sign in with username+password, you'll need to verify the new email address before the change takes effect.

## Password

Update your account password for security:

- **Current Password** - Enter your existing password
- **New Password** - Choose a new password of at least 8 characters
- **Confirm Password** - Re-enter your new password to confirm

:::tip

Use a strong password with a mix of uppercase, lowercase, numbers, and special characters for better security. Consider using a password manager to make this process easier.

:::

## Skills

Tag yourself with the skills that best match your experience:

- **Coder** - Programming and scripting
- **Artist** - 3D modeling, texturing, animation
- **Designer** - UI/UX and game design
- **Musician** - Audio and music composition

These skills are displayed in your public profile header.

## Current Plan

View your current subscription plan.

For free accounts, this shows "Free $0 / month" with basic plan limitations.

For accounts on the Personal plan, this shows "Personal $15 / month".

:::tip

To cancel your Personal plan, click the **Cancel** link and confirm. Your account will drop back to the free tier and the end of your current billing cycle.

:::

## Billing Info

Users on the Free plan will see "No credit card stored in your account" until a paid plan is purchased.

Users on the Personal plan will see something of the form "Card ending in - 0000". Click **Edit** to update billing info:

- Card details
- Billing address
- VAT number (if supported in your country)

## Invoices

Access your billing history and download invoices. By default, your 3 latest invoices are displayed. Keep clicking **Load More...** to load the next 10 invoices.

## Seats

View and manage team members across your projects:

- Lists all users who are team members on projects you own
- Shows which specific projects each user has access to

:::tip

Click the `x` icon to the right of a listed user to remove them from all of your projects that they are on the team of.

:::

## Usage

This section tracks your account resource consumption. The usage meter shows current consumption against your plan limits.

### Storage Breakdown

- **Public Projects** - Number created vs. limit
- **Private Projects** - Number created vs. limit (plan-dependent)
- **Assets** - Storage used for 3D models, textures, audio
- **Apps** - Published application data
- **Code** - Script and code storage
- **Checkpoints** - Version history storage
- **Splats** - Gaussian splat data (uploaded from SuperSplat)

## API Tokens

Manage programmatic access to your PlayCanvas account:

- Generate new API tokens
- View existing token permissions
- Revoke access for unused tokens

See the [API Documentation](/user-manual/api/) for details on using API tokens.

## Email Preferences

Configure which notifications you receive:

- **My Projects** - Updates about your own projects
- **Watched Projects** - Notifications from projects you follow
- **New Comments** - Comments on your projects or posts
- **New Stars** - When users star your projects
- **My Organizations** - Organization-related updates
- **Exciting PlayCanvas Stuff** - Platform news and updates

Uncheck any categories you don't want to receive emails about.

## Organizations

### Convert Account to Organization {#convert-account-to-organization}

Transform your personal account into an [organization](../organizations/index.md) account:

- Allows team collaboration
- Enables advanced permission management
- Requires designating a new owner

:::warning

Converting to an organization means you can no longer log in with this account. Make sure you have another account ready to become the organization owner.

:::

:::tip

You cannot convert a user account into an organization if it currently manages other organizations. Transfer those organizations to be owned by another user account first.

:::

## Delete Account

Permanently remove your PlayCanvas account:

- All projects and data will be deleted
- This action cannot be undone
- Consider downloading important projects first

:::danger

Account deletion is permanent and irreversible. Make sure to backup any important work before proceeding.

:::

:::tip

You can't delete a user account if it currently manages any organizations. Please delete the organizations first or transfer their ownership to another user account.

:::

---

For billing-specific questions and detailed subscription management, see [Billing](../billing.md).

--------------------------------------------------------------------------------

## User Home

URL: https://developer.playcanvas.com/user-manual/account-management/user-accounts/user-home/

Once you have created your user account, you can visit your User Home page. The URL for this page will be of the form:

```none
https://playcanvas.com/user/<USERNAME>
```

:::tip

Assuming you are logged in, simply visiting [https://playcanvas.com/](https://playcanvas.com/) will redirect you to your User Home page.

:::

Here's what a new User Home page should look like:

[Image: User Home Page]

It defaults to showing your list of [Editor Projects](../../editor/projects/index.md).

--------------------------------------------------------------------------------

## Animation

URL: https://developer.playcanvas.com/user-manual/animation/

PlayCanvas provides a powerful state-based animation system which can be used to animate character models and other arbitrary scene object models. Users can work with any of their .FBX animation assets. These can be organized using animation state machines to easily control the animated behavior of scene models at runtime.

## System Overview

The animation system touches on three main areas of the PlayCanvas platform. This section will walk through how these areas can be used together to create complex animation behavior for your models. The following sections of the animation user manual then will explore each area in more detail.

### Animating in PlayCanvas

In order to begin animating a PlayCanvas entity, you must have a set of animation assets available and imported into your PlayCanvas project. These animation assets will drive the animation of a given model you wish to animate. For example a humanoid character may have a set of animations; Idle, Walk, Jump.

[Image: Animations]

These three animations can be organized into a single animation system to create a simple locomotion system for that character. The way this is achieved in PlayCanvas is through the use of an animstategraph asset. These assets can be thought of as state machines for an entity’s animation behavior. With each state in this asset relating to an animation, the state machine can be set up to define the complex animation behavior of an entity’s model. This includes defining when the system should stop one animation and start another and how the transition between these animations should be blended.

The anim component is then used to assign an animstategraph asset to a particular entity in your scene. Once an entity has been assigned an animstategraph asset, each state in the graph can have an actual animation asset assigned to it. Once all states have been assigned animations, the anim component will become playable. At this point the animation system is complete and the defined animation behavior will be viewable in the PlayCanvas launcher.

--------------------------------------------------------------------------------

## Animation Assets

URL: https://developer.playcanvas.com/user-manual/animation/anim-animation-assets/

[Image: Animation Assets]

Animation assets are the animation keyframe data that’s used to drive animations of a model in PlayCanvas. They are linked to an animstategraph asset via an entity's anim component.

The anim component currently supports animation assets that have been imported into a PlayCanvas project from .FBX files using the `Convert to GLB` asset tasks settings option.

[Image: Asset Tasks]

--------------------------------------------------------------------------------

## Anim Component

URL: https://developer.playcanvas.com/user-manual/animation/anim-component/

Click here to learn how to use the [Anim Component](/user-manual/editor/scenes/components/anim/).

[Image: New Anim Component]

--------------------------------------------------------------------------------

## Anim Events

URL: https://developer.playcanvas.com/user-manual/animation/anim-events/

Anim events can be used to trigger event listeners during the playback of an animation. Each event is associated with a specified frame of the animation asset it is attached to. When the playback of the animation reaches that frame, the event will fire and the associated event listener is called.

### Creating Events

To create a new event, select the animation asset in the asset panel which you'd like to create an event for. You should then see the `+ EVENT` button in the asset inspector as shown below:

[Image: Animation Asset With Event]

Each event has the following modifiable properties:

| Variable | Description |
|----------|-------------|
| time     | Defines the specific time during the playback of the animation when the event should trigger. Given in seconds. |
| name     | The name of the event is used to identify the event when attaching an event listener to the anim component. |
| number   | An additional property which can be set to any number. Used to pass additional details to the event listener. |
| string   | An additional property which can be set to any string. Used to pass additional details to the event listener. |

### Event Listeners

After creating an event for an animation asset, the event will be fired whenever that asset is played back by an anim component. You can therefore attach listeners to the anim component to handle the event. The following example shows how to attach event listeners to the anim component:

Any number of animation events can be attached to a single animation asset and used by any number of anim components. Making use of the additional `number` and `string` properties of an event allows you to differentiate between events that are passed to the same event listener.

--------------------------------------------------------------------------------

## Anim Layer Masks

URL: https://developer.playcanvas.com/user-manual/animation/anim-layer-masking/

When creating complex animation behavior for your game objects, it is often necessary to isolate the playback of certain animations to specific bones in each object's model. This is particularly useful when animating characters that need to carry out multiple actions at the same time. This can be achieved in PlayCanvas by creating a mask for a given [animation layer](/user-manual/animation/anim-state-graph-assets/#layers) in your anim component.

### Creating a mask

After creating an Anim State Graph asset and attaching it to an anim component, you'll be presented with a list of layers contained in your graph. You can create a mask for any of these layers by clicking the **Create Mask** button under each layer panel:

[Image: Anim Component Create Mask]

 This will open up the mask inspector for that layer which is shown below:

[Image: Mask Inspector]

The mask inspector displays the full hierarchy which the anim component is driving, starting at the `root bone` specified in the anim component. Each bone in the hierarchy can be selected to be included in the mask. You can also right-click specific bones to include or exclude whole sections of the hierarchy. Any bones which are not selected in this mask will not be driven by any of the animations which play in this mask's layer.

After creating masks, you can use [layer blending](/user-manual/animation/anim-state-graph-assets/#layer-blending) to smoothly blend the masked animations of multiple layers together.

--------------------------------------------------------------------------------

## Animstategraph Assets

URL: https://developer.playcanvas.com/user-manual/animation/anim-state-graph-assets/

Animstategraph assets are used to organize a set of different animation states, which are all the various ways in which a model might animate. It can be used to define each of these animation states, determine when each state should play and how states transition and therefore blend between one another. Animstategraph assets do not store or link to any real animation assets themselves, but rather act as a template for how animation assets should be organized. Actual animation assets are linked to the animstategraphs animation states through the [Anim Component](/user-manual/editor/scenes/components/anim/).

The system was designed so that a single animstategraph can be used on many different entities, each with their own set of animation assets. An example being an animstategraph asset which manages the animations of humanoid character locomotion. This single asset could be used on a human entity, an elf entity and a dwarf entity. Each of these entities would be able to link their own character animation assets, all the while maintaining the same animation behavior as each other.

These assets are therefore state machines for a model's animation behavior and they control the flow of animation sequences over the lifecycle of an entity. A simple animstategraph asset used to define the behavior of a wheel may define only two animation states; static and spinning. This asset can be defined to control when the wheel starts and stops spinning, for how long it will spin, the speed of the wheel spin and how sharply it starts / stops spinning. More advanced assets can be used to combine a multitude of animation states to create complex humanoid character animation behavior.

When selecting an animstategraph asset in the editors asset panel, you’ll open up the anim state graph editor view:

[Image: Initial Editor]

Within this view you can edit your animation state graph. The following sections will highlight how different elements of the animstategraph asset can be used to define specific animation behavior.

## States {#states}

In essence, states are used to specify which animations should play at a given point in time. An anim state graph can only be in one of these states at a given time.

There are four types of states present in state graphs. Animation states, along with the START state, END state and ANY state. Only animation states can be created and deleted by the user and only these will be linked to animation assets. The other states are used to control the flow through the state machine.

### Animation States {#animation-states}

[Image: State]

Animation states define a playable animation such as ‘Idle’, ‘Jump’ or ‘Walk’. New animation states can be created by right clicking on the blank canvas behind the state graph and selecting ‘Add new state’ from the menu. The editor will target your newly created state and show its inspector panel on the right hand side. Within this inspector the following state variables can be modified:

| Variable | Description |
|----------|-------------|
| Name     | The name that this state should be called by. This is used to find and edit and play states via script. Names must be unique per state graph layer. |
| Speed    | The playback speed for animations that are linked to this state. |
| Loop     | Whether animations linked to this state should loop during playback. If set to false the animation will pause on its last keyframe until this state is exited. |

### START state {#start-state}

[Image: Start State]

The START state is the entry point of every state graph. When an anim component begins playing its assigned anim state graph, it will first enter this state and transition directly to the animation state it’s connected to. This animation state is called the default state and it can be selected via the layers panel here:

[Image: Layers]

It is not possible to create any other transitions to or from the START state. It can only be entered again by transitioning to the END state.

### END state {#end-state}

[Image: End State]

The end state marks an exit out of the current state graph. If your animation state is set up to transition to the END state, the system will move directly to the default animation state which is connected to the START state. This is useful to create cyclical flows through the graph while still laying out your graph in a linear fashion. It is not possible to create transitions from the END state to any other state. It will always transition directly to the START state.

### ANY state {#any-state}

[Image: Any State]

This state is used to create transitions which can be activated while the system is currently in any of the other animation states. Any transitions that trigger from this state will blend as if they had been connected directly from the currently active animation state. You can create transitions from the ANY state but not to it.

This is useful to set up transitions which you want to activate, no matter which state you’re currently in. For example you could have a jump state which should be reachable from both an idle and walk state. Instead of setting up transitions from both the idle and walk states to the jump state, a transition can be set up between the ANY state and the jump state.

### Transitions {#transitions}

Transitions define how the anim state graph can move from one animation state to another. They can be created by right clicking an animation state and selecting `Add transition` from the context menu.

By setting the variables of a given transition you can also control how the animations of the transitioning states will blend together.

The available transition variables are:

| Variable            | Description |
|---------------------|-------------|
| Duration            | The duration of the transition in seconds. |
| Exit Time           | The time at which to exit the source state and enter the destination state. Given in normalized time based on the source state's duration. Providing no value allows the source state to exit with this transition at any time. A value of less than 1 will make the transition available for exit at that time during every loop of the source state. |
| Offset              | If provided, the destination state will begin playing its animation at this time. Given in normalized time based on the destination state's duration. Must be between 0 and 1. |
| Interruption Source | Defines whether another transition can interrupt this one and which of the current or previous states' transitions can do so. |

It is possible to create multiple transitions between two animation states, which have different values and conditions set. The priority of these transitions can be reordered in the transition inspector after selecting a transition's arrow in the graph. The priority order determines which transition will be used by the state graph if multiple transitions have their conditions met.

### Parameters {#parameters}

The parameters of an anim state graph are variables which are used to control the flow of animations during runtime. These variables can be accessed via scripts and set to new values at any time. They are then the way in which users can control the behavior of an entity's animation during its lifecycle.

New parameters can be added to a state graph via the parameters panel on the left inspector:

[Image: Parameters]

Each parameter has three variables which can be set:

| Variable      | Description |
|---------------|-------------|
| Name          | The name that this parameter should be called by. This is used to find and set the parameter via script. Names must be unique per state graph. |
| Type          | The type of variable that the parameter contains. One of: Boolean, Float, Integer or Trigger. The Trigger type acts as a Boolean but with the special property that its value is set back to false after it has been used to successfully activate a transition. |
| Default Value | The value of the parameters variable when the state graph launches. |

The way in which they control the state graph is through the use of transition conditions. Each transition in the graph can have a list of conditions which define when a transition is usable by the system. A state will not be able to pass to another state through a given transition unless all of its conditions are met.

Each condition consists of a conditional statement which compares the current value of a parameters variable to the given value in the condition using the designated operator. For example, the following condition:

[Image: Condition]

Can be used in the transition between the Idle and Jump animation states to ensure that a character only jumps when the ‘Jump’ parameter has been set to true via a script.

### Layers {#layers}

So far, animstategraph assets have been discussed in the context of editing a single animation state graph. It may sometimes be necessary however to have the animations of a single model driven by multiple separate state graphs, each with their own defined behavior. An example could be animating a main character's movement and locomotion on a single layer, while animating its facial expressions on a separate layer that’s driven by its own state graph and parameters.

When an animstategraph is created, it comes with a single base layer. This layer is not deletable and for many scenarios will be the only one necessary. However if you wish to create another layer you can do so by selecting the new layer button on the layers panel to the left of the state graph view:

[Image: Layers]

It is then possible to switch to editing this layer by selecting it from the layer select dropdown which is present at the top right of the graph view:

[Image: Select Layer]

### Layer Blending {#layer-blending}

By default, layers animate a model in the order that they’re created in the layers panel. Any animation values they set on a model's bones will be overwritten by subsequent layers. If instead you wish to blend the animation values of the layers together, you can set the `blend type` of your layers to `Additive` rather than the default `Override`:

[Image: Layer Blend]

The blend weight value of each layer is used when blending multiple layers to determine how much each layer should contribute to the final animation. These blend weights can be adjusted at runtime in your game scripts to update the blends of your layers in real time:

If you [mask your layers](anim-layer-masking.md), you can set the `blend type` of your layers to `Additive` to blend in an animation which only controls part of your model's bones. Updating the `blend weight` in real time as described above can allow you to create smooth blends between animations on different layers. This is particularly useful when animating characters that need to carry out different actions using their upper and lower body. For example, you could have a `shooting` animation that is blended in and out on a characters upper body, while freeing up the lower body for various locomotion animations such as `walking` and `running`.

Any layers that are set to `Overwrite` will completely replace the animation values of the model's bones that are animated in that layer. In these instances, previous layers will not be taken into consideration when producing the final animation.

--------------------------------------------------------------------------------

## REST API

URL: https://developer.playcanvas.com/user-manual/api/

:::warning

The REST API is currently in beta. This means we may change certain endpoints and API responses.

:::

## Authorization {#authorization}

You can only access the REST API via https. In order to access the REST API you need to use an Access Token. You can generate an Access Token by going to your Organization's Account page.

[Image: Account Tab]

In the API Tokens section click on Generate Token.

[Image: Generate Token]

Give your token a name and click the button to create your new token. A new window will appear showing you your new access token.

Make sure you note that down because you will not be able to see the token once you close this window. This token is meant to be kept secret so do not share it with anyone other than your team (for example do not post this on forums).

[Image: New Token]

From your Account page you can also Revoke all the tokens you have generated or a specific one. You can also edit the name of a token.

[Image: Remove Token]

When you make calls to the API you must set the 'Authorization' header in your HTTP request to this value:

```none
Bearer [access_token]
```

Replace `[access_token]` with an Access Token you generated in your Account page.

For example:

```none
curl -H "Authorization: Bearer nesgdxhiqe7hylfilr6ss1rds0gq1uj8" https://playcanvas.com/api/...
```

## Parameters {#parameters}

Various routes accept a number of parameters. For GET requests if the parameter is not part of the URL, you can pass it as an HTTP query string parameter. For POST, PUT and DELETE requests parameters not included in the URL should be encoded as JSON with a Content-Type of 'application/json'.

There are several common parameters that are used in each endpoint:

### project_id {#project_id}

This can be found in the URL on the project overview page.

[Image: Project ID]

### scenes {#scenes}

When opening a scene in the Editor, the scene id is in the URL.

[Image: Scene ID]

### branch_id {#branch_id}

This is found in the [version control](/user-manual/editor/version-control/) panel and can be selected and copied.

[Image: Branch ID]

## Response Format {#response-format}

Our REST API is following some generic guidelines when it comes to the response format of each API call.

### GET resource {#get-resource}

If you are trying to GET a single resource the response will be a JSON object with the resource you requested.

### GET multiple resources {#get-multiple-resources}

If you are trying to GET multiple resources like for example listing the Apps of a Project you will get a JSON object with this format:

```none
{
    "result": [
        resource_1,
        resource_2,
        ...,
        resource_N
    ],
    "pagination": {
        "limit": number,
        "skip": number,
        "total": number
    }
}
```

As you can notice the response in this case also contains pagination data. To control the pagination of the response you can pass the following URL parameters:

| Name    | Description                                                                                                                      |
| ------- | -------------------------------------------------------------------------------------------------------------------------------- |
| `limit` | The maximum number of items to include in the response.                                                                          |
| `skip`  | The number of items to skip from the original result set.                                                                        |
| `sort`  | The name of the field to use to sort the result set. See the documentation of each request to see which values are allowed here. |
| `order` | If you want results in ascending order pass 1 otherwise pass -1 for descending order.                                            |

So for example to get 32 items after the first 16 items you would send this request:

```none
https://playcanvas.com/api/items?limit=32&amp;skip=16
```

### Errors {#errors}

When an error is raised you will get a JSON object with this format:

```json
{
    "error": "This is the error message"
}
```

Also the status code of the response will be the appropriate HTTP error code.

## Rate Limiting {#rate-limiting}

Calls to the REST API have a rate limit. Check your actual limits by querying [this endpoint](https://playcanvas.com/api/ratelimits). There are different rate limits depending on the request:

| Rate Limit Type | Description               | Limit for free accounts | Limit for personal/org accounts |
| --------------- | ------------------------- | ----------------------- | ------------------------------- |
| Normal          | The normal rate limit     | 120 requests/minute     | 240 requests/minute             |
| Strict          | The strict rate limit     | 5 requests/minute       | 10 requests/minute              |
| Assets          | The assets rate limit     | 60 requests/minute      | 120 requests/minute             |

The response will contain the following headers to help you regulate how often you call the API:

| Name                    | Description                                                                                                             |
| ----------------------- | ----------------------------------------------------------------------------------------------------------------------- |
| `X-RateLimit-Limit`     | The number of requests allowed in a minute.                                                                             |
| `X-RateLimit-Remaining` | The remaining number of requests that you are allowed to make this minute.                                              |
| `X-RateLimit-Reset`     | The time at which the current rate limit window resets in [UTC epoch seconds](https://en.wikipedia.org/wiki/Unix_time). |

If you exceed the rate limit you will get a `429 Too Many Requests` status code. You will have to wait for the current window to reset in order to continue making requests.

--------------------------------------------------------------------------------

## Apps - Download app

URL: https://developer.playcanvas.com/user-manual/api/app-download/

## Route URL

```none
POST https://playcanvas.com/api/apps/download
```

## Description

This will allow you to download an app which you can self host on your own server. The request will start an export job and the job details will be returned in the response. You can [poll the job by id](/user-manual/api/job-get) until its status is either 'complete' or 'error'. When the job is done, its data will contain a URL to download the exported app.

## Example

```none
curl -H "Authorization: Bearer {accessToken}" -H "Content-Type: application/json" -X POST -d '{"project_id": 9999999, "scenes": [9999999], "name": "My App"}' "https://playcanvas.com/api/apps/download"
```

## Parameters

| Name                    | Type       | Required | Description                                                                                                                                                           |
| ----------------------- | ---------- | :------: | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `project_id`            | `number`   | ✔️      | The id of the project.                                                                                                                                                |
| `name`                  | `string`   | ✔️      | The name of the app. Must be less than 1000 characters.                                                                                                               |
| `scenes`                | `number[]` | ✔️      | A list of scene ids to be included in the app. When you specify scenes then the first scene in the list will be used as the initial scene of the application.         |
| `branch_id`             | `string`   |          | The id of the branch. If no id is specified the main branch will be used.                                                                                             |
| `description`           | `string`   |          | The description of the app. Must be less than 10,000 characters.                                                                                                      |
| `version`               | `string`   |          | The version of the app. Can be a string up to 20 characters.                                                                                                          |
| `release_notes`         | `string`   |          | Release notes for the app. Can be a string up to 10,000 characters.                                                                                                   |
| `scripts_concatenate`   | `boolean`  |          | Set it to true if you want scripts to be concatenated.                                                                                                                |
| `scripts_minify`        | `boolean`  |          | Set it to true if you want scripts to be minified. Defaults to true.                                                                                                  |
| `scripts_sourcemaps`    | `boolean`  |          | Set it to true if you want script sourcemaps to be generated. Defaults to false.                                                                                      |
| `optimize_scene_format` | `boolean`  |          | Set it to true if you want scenes to be in an optimized format (see [Optimize Scene Format](/user-manual/optimization/optimizing-scene-format) for more information). |
| `engine_version`        | `string`   |          | Set it to a Engine version string ([full list of releases](https://github.com/playcanvas/engine/releases)) if a specific version is needed for the app. Defaults to the latest Editor supported major version depending on your project.              |

## Response Schema

```none
Status: 201 Created
```

```json
{
    "status": "running" | "complete" | "error",
    "messages": list of strings,
    "created_at": date,
    "modified_at": date,
    "data": {
        "concatenate": boolean,
        "branch_id": string,
        "optimize_scene_format": boolean,
        "minify": boolean,
        "name": string,
        "sourcemaps": boolean,
        "scenes": list of int scene ids,
        "engineVersion": string,
        "preload_bundle": boolean,
        "project_id": int,
        "owner_id": int
    },
    "id": int
}
```

## Errors

| Code | Description       |
| ---- | ----------------- |
| 401  | Unauthorized      |
| 403  | Forbidden         |
| 404  | Project not found |
| 404  | Owner not found   |
| 404  | Scene not found   |
| 429  | Too many requests |

## Rate Limiting

This route uses a [strict](/user-manual/api#rate-limiting) rate limit.

--------------------------------------------------------------------------------

## Apps - Get primary app

URL: https://developer.playcanvas.com/user-manual/api/app-get-primary/

## Route URL

```none
GET https://playcanvas.com/api/projects/:projectId/app
```

## Description

Gets the Primary App of a Project.

## Example

```none
curl -H "Authorization: Bearer {accessToken}" https://playcanvas.com/api/projects/{projectId}/app
```

## Parameters

| Name        | Type     | Description            |
| ----------- | -------- | ---------------------- |
| `projectId` | `number` | The id of the project. |

## Response Schema

```none
Status: 200
```

```json
{
    "id": int,
    "project_id": int,
    "owner_id": int,
    "name": string,
    "description": string,
    "version": string,
    "release_notes": string,
    "thumbnails": {
        "s": string,
        "m": string,
        "l": string,
        "xl": string
    },
    "size": int,
    "views": int,
    "completed_at": date,
    "created_at": date,
    "modified_at": date
}
```

## Errors

| Code | Description                         |
| ---- | ----------------------------------- |
| 401  | Unauthorized                        |
| 403  | Forbidden                           |
| 404  | Project not found                   |
| 404  | Project does not have a primary app |
| 404  | App not found                       |
| 429  | Too many requests                   |

## Rate Limiting

This route uses a [normal](/user-manual/api#rate-limiting) rate limit.

--------------------------------------------------------------------------------

## Apps - Get project apps

URL: https://developer.playcanvas.com/user-manual/api/app-get-project/

## Route URL

```none
GET https://playcanvas.com/api/projects/:projectId/apps
```

## Description

Lists all the published Apps of a Project.

## Example

```none
curl -H "Authorization: Bearer {accessToken}" https://playcanvas.com/api/projects/{projectId}/apps
```

## Parameters

| Name        | Type     | Description            |
| ----------- | -------- | ---------------------- |
| `projectId` | `number` | The id of the project. |

## Response Schema

```none
Status: 200
```

```json
{
  "result": [{
    "id": int,
    "project_id": int,
    "owner_id": int,
    "name": string,
    "description": string,
    "version": string,
    "release_notes": string,
    "thumbnails": {
        "s": string,
        "m": string,
        "l": string,
        "xl": string
    },
    "size": int,
    "views": int,
    "completed_at": date,
    "created_at": date,
    "modified_at": date
  }, ... ],
  "pagination": {
     ...
  }
}
```

## Errors

| Code | Description       |
| ---- | ----------------- |
| 401  | Unauthorized      |
| 403  | Forbidden         |
| 404  | Project not found |
| 429  | Too many requests |

## Rate Limiting

This route uses a [normal](/user-manual/api#rate-limiting) rate limit.

--------------------------------------------------------------------------------

## Apps - Get app

URL: https://developer.playcanvas.com/user-manual/api/app-get/

## Route URL

```none
GET https://playcanvas.com/api/apps/:id
```

## Description

Gets a published App by id.

## Example

```none
curl -H "Authorization: Bearer {accessToken}" https://playcanvas.com/api/apps/{id}
```

## Parameters

| Name | Type     | Description        |
| ---- | -------- | ------------------ |
| `id` | `number` | The id of the app. |

## Response Schema

```none
Status: 200
```

```json
{
    "id": int,
    "project_id": int,
    "owner_id": int,
    "name": string,
    "description": string,
    "version": string,
    "release_notes": string,
    "thumbnails": {
        "s": string,
        "m": string,
        "l": string,
        "xl": string
    },
    "size": int,
    "views": int,
    "completed_at": date,
    "created_at": date,
    "modified_at": date
}
```

## Errors

| Code | Description       |
| ---- | ----------------- |
| 401  | Unauthorized      |
| 403  | Forbidden         |
| 404  | App not found     |
| 404  | Project not found |
| 429  | Too many requests |

## Rate Limiting

This route uses a [normal](/user-manual/api#rate-limiting) rate limit.

--------------------------------------------------------------------------------

## Assets - Create asset

URL: https://developer.playcanvas.com/user-manual/api/asset-create/

## Route URL

```none
POST https://playcanvas.com/api/assets
```

## Description

Create a new asset.

:::note

This endpoint currently only supports creating `script`, `html`, `css`, `text`, `shader` and `json` type assets.

:::

**Unlike other REST API endpoints. The Create Asset endpoint expects data to be sent in `multipart/form-data`**

## Example

```none
curl -H "Authorization: Bearer {accessToken}" -X POST -F 'name={name}' -F 'projectId={projectId}' -F 'parent={parent}' -F 'preload={preload}' -F 'pow2={pow2}' -F 'file=@./script.js' "https://playcanvas.com/api/assets"
```

HTTP Request

```text
POST https://playcanvas.com/api/assets
Authorization: Bearer {accessToken}
Content-Type: multipart/form-data; boundary=----WebKitFormBoundaryTdsfsfT

------WebKitFormBoundaryTdsfsfT
Content-Disposition: form-data; name="projectId"

{projectId}
------WebKitFormBoundaryTdsfsfT
Content-Disposition: form-data; name="branchId"

{branchId}
------WebKitFormBoundaryTdsfsfT
Content-Disposition: form-data; name="parent"

{parent}
------WebKitFormBoundaryTdsfsfT
Content-Disposition: form-data; name="preload"

{preload}
------WebKitFormBoundaryTdsfsfT
Content-Disposition: form-data; name=""; filename="script.js"
Content-Type: application/javascript

{fileContent}
------WebKitFormBoundaryTdsfsfT--
```

## Parameters

| Name        | Type      | Required | Description                                                                                                 |
| ----------- | --------- | :------: | ----------------------------------------------------------------------------------------------------------- |
| `name`      | `string`  | ✔️      | The name of the asset.                                                                                      |
| `projectId` | `number`  | ✔️      | The id of the project.                                                                                      |
| `branchId`  | `string`  | ✔️      | The id of the branch.                                                                                       |
| `parent`    | `number`  |          | Parent asset's id.                                                                                          |
| `preload`   | `boolean` |          | Preload the asset (true / false).                                                                           |
| `file`      | `file`    |          | Data to store as the asset file.                                                                            |
| `pow2`      | `boolean` |          | Only used for textures and defaults to false. Resize the texture to power of two dimensions (true / false). |

## Response Schema

```none
Status: 201
```

```json
{
    "id": int,
    "modifiedAt": date,
    "createdAt": date,
    "state": "ready" | "processing" | "error",
    "name": string,
    "type": string,
    "scope":{
        "type": string,
        "id": int
    },
    "source": bool,
    "sourceId": bool,
    "tags": list of strings,
    "preload": bool,
    "data": {
        ... asset data
    },
    "file": {
        "hash": string,
        "filename": string,
        "size": int,
        "url": string
    },
    "parent": int
}
```

## Errors

| Code | Description       |
| ---- | ----------------- |
| 401  | Unauthorized      |
| 403  | Forbidden         |
| 404  | Project not found |
| 429  | Too many requests |

## Rate Limiting

This route uses an [assets](/user-manual/api#rate-limiting) rate limit.

--------------------------------------------------------------------------------

## Assets - Delete asset

URL: https://developer.playcanvas.com/user-manual/api/asset-delete/

## Route URL

```none
GET https://playcanvas.com/api/assets/:assetId?branchId=:branchId
```

## Description

Permanently delete an asset from a branch of your project.

:::warning

Deleting an asset is permanent and unrecoverable unless you have taken a checkpoint of it.

:::

## Example

```none
curl -H "Authorization: Bearer {accessToken}" -X DELETE "https://playcanvas.com/api/assets/{assetId}?branchId={branchId}"
```

HTTP Request

```text
DELETE https://playcanvas.com/api/assets/{assetId}?branchId={branchId}
Authorization: Bearer {accessToken}
```

## Parameters

| Name       | Type     | Required | Description                                    |
| ---------- | -------- | :------: | ---------------------------------------------- |
| `assetId`  | `number` | ✔️      | The id of the asset to delete.                 |
| `branchId` | `string` | ✔️      | The id of the branch to delete the asset from. |

## Response Schema

```none
Status: 200
```

## Errors

| Code | Description                |
| ---- | -------------------------- |
| 401  | Unauthorized               |
| 403  | Forbidden                  |
| 404  | Project or Asset not found |
| 429  | Too many requests          |

## Rate Limiting

This route uses a [normal](/user-manual/api#rate-limiting) rate limit.

--------------------------------------------------------------------------------

## Assets - Get Asset File

URL: https://developer.playcanvas.com/user-manual/api/asset-file/

## Route URL

```none
GET https://playcanvas.com/api/assets/:assetId/file/:filename?branchId=:branchId
```

## Description

Get the details of a single asset

## Example

```none
curl -H "Authorization: Bearer {accessToken}" "https://playcanvas.com/api/assets/{assetId}/file/{filename}?branchId={branchId}"
```

HTTP Request

```text
GET https://playcanvas.com/api/assets/{assetId}/file/{filename}?branchId={branchId}
Authorization: Bearer {accessToken}
```

## Parameters

| Name       | Type     | Required | Description                |
| ---------- | -------- | :------: | -------------------------- |
| `assetId`  | `number` | ✔️      | The id of the asset.       |
| `branchId` | `string` | ✔️      | The id of the branch.      |
| `filename` | `string` | ✔️      | The filename of the asset. |

## Response Schema

```none
Status: 200
```

```none
{fileContents}
```

## Errors

| Code | Description                |
| ---- | -------------------------- |
| 401  | Unauthorized               |
| 403  | Forbidden                  |
| 404  | Project or Asset not found |
| 429  | Too many requests          |

## Rate Limiting

This route uses a [normal](/user-manual/api#rate-limiting) rate limit.

--------------------------------------------------------------------------------

## Assets - Get Asset

URL: https://developer.playcanvas.com/user-manual/api/asset-get/

## Route URL

```none
GET https://playcanvas.com/api/assets/:assetId?branchId=:branchId
```

## Description

Get the details of a single asset

## Example

```none
curl -H "Authorization: Bearer {accessToken}" https://playcanvas.com/api/assets/{assetId}?branchId={branchId}
```

HTTP Request

```text
GET https://playcanvas.com/api/assets/{assetId}?branchId={branchId}
Authorization: Bearer {accessToken}
```

## Parameters

| Name       | Type     | Required | Description           |
| ---------- | -------- | :------: | --------------------- |
| `assetId`  | `number` | ✔️      | The id of the asset.  |
| `branchId` | `string` | ✔️      | The id of the branch. |

## Response Schema

```none
Status: 200
```

```json
{
    "id": int,
    "modifiedAt": date,
    "createdAt": date,
    "state": "ready" | "processing" | "error",
    "name": string,
    "type": string,
    "scope":{
        "type": string,
        "id": int
    },
    "source": bool,
    "sourceId": bool,
    "tags": list of strings,
    "preload": bool,
    "file": {
        "hash": string,
        "filename": string,
        "size": int,
        "url": string
    },
    "parent": int
}
```

## Errors

| Code | Description                |
| ---- | -------------------------- |
| 401  | Unauthorized               |
| 403  | Forbidden                  |
| 404  | Project or Asset not found |
| 429  | Too many requests          |

## Rate Limiting

This route uses a [normal](/user-manual/api#rate-limiting) rate limit.

--------------------------------------------------------------------------------

## Assets - List assets

URL: https://developer.playcanvas.com/user-manual/api/asset-list/

## Route URL

```none
GET https://playcanvas.com/api/projects/:projectId/assets?branchId=:branchId&skip=:skip&limit=:limit
```

## Description

Get the details of all assets in a project for a specific branch

## Example

```none
curl -H "Authorization: Bearer {accessToken}" "https://playcanvas.com/api/projects/{projectId}/assets?branchId={branchId}&skip={number}&limit={number}"
```

HTTP Request

```text
GET https://playcanvas.com/api/projects/{projectId}/assets?branchId={branchId}&skip={number}&limit={number}
Authorization: Bearer {accessToken}
```

## Parameters

| Name        | Type       | Required | Description                                                                  |
| ----------- | ---------- | :------: | ---------------------------------------------------------------------------- |
| `projectId` | `number`   | ✔️      | The id of the project.                                                       |
| `branchId`  | `string`   | ✔️      | The id of the branch.                                                        |
| `skip`      | `number`   |          | Number of assets to skip before listing. Used for pagination. Defaults to 0. |
| `limit`     | `number`   |          | Maximum number of assets to list. Defaults to 16. Maximum 100000.            |

## Response Schema

```none
Status: 200
```

```json
{
    "result": [{
        "id": int,
        "modifiedAt": date,
        "createdAt": date,
        "state": "ready" | "processing" | "error",
        "name": string,
        "type": string,
        "scope":{
            "type": string,
            "id": int
        },
        "source": bool,
        "sourceId": bool,
        "tags": list of strings,
        "preload": bool,
        "file": {
            "hash": string,
            "filename": string,
            "size": int,
            "url": string
        },
        "parent": int
    }, ...],
    "pagination": {
        "skip": int,
        "limit": int,
        "total": int,
    }
}
```

## Errors

| Code | Description       |
| ---- | ----------------- |
| 401  | Unauthorized      |
| 403  | Forbidden         |
| 404  | Project not found |
| 429  | Too many requests |

## Rate Limiting

This route uses a [normal](/user-manual/api#rate-limiting) rate limit.

--------------------------------------------------------------------------------

## Assets - Update asset

URL: https://developer.playcanvas.com/user-manual/api/asset-update/

## Route URL

```none
PUT https://playcanvas.com/api/assets/:assetId
```

## Description

Update an existing asset's file.

:::note

This endpoint currently only supports updating `script`, `html`, `css`, `text`, `shader` and `json` type assets.

:::

**Unlike other REST API endpoints. The Update Asset endpoint expects data to be sent in `multipart/form-data`**

## Example

```none
curl -H "Authorization: Bearer {accessToken}" -X PUT -F 'pow2={pow2}' -F 'file=@./script.js' "https://playcanvas.com/api/assets/{assetId}"
```

## Parameters

| Name       | Type      | Required | Description                                                                                                 |
| ---------- | --------- | :------: | ----------------------------------------------------------------------------------------------------------- |
| `assetId`  | `number`  | ✔️      | The id of the asset.                                                                                        |
| `file`     | `file`    | ✔️      | Data to update asset file with.                                                                             |
| `pow2`     | `boolean` |          | Only used for textures and defaults to false. Resize the texture to power of two dimensions (true / false). |

## Response Schema

```none
Status: 200
```

```json
{
    "id": int,
    "modifiedAt": date,
    "createdAt": date,
    "state": "ready" | "processing" | "error",
    "name": string,
    "type": string,
    "scope":{
        "type": string,
        "id": int
    },
    "source": bool,
    "sourceId": bool,
    "tags": list of strings,
    "preload": bool,
    "data": {
        ... asset data
    },
    "file": {
        "hash": string,
        "filename": string,
        "size": int,
        "url": string
    },
    "parent": int
}
```

## Errors

| Code | Description                |
| ---- | -------------------------- |
| 401  | Unauthorized               |
| 403  | Forbidden                  |
| 404  | Project or Asset not found |
| 429  | Too many requests          |

## Rate Limiting

This route uses an [assets](/user-manual/api#rate-limiting) rate limit.

--------------------------------------------------------------------------------

## Branches - List branches

URL: https://developer.playcanvas.com/user-manual/api/branch-list/

## Route URL

```none
GET https://playcanvas.com/api/projects/:projectId/branches
```

## Description

Get a list of all open branches for a project

## Example

```none
curl -H "Authorization: Bearer {accessToken}" "https://playcanvas.com/api/projects/{projectId}/branches"
```

HTTP Request

```text
GET https://playcanvas.com/api/projects/{projectId}/branches
Authorization: Bearer {accessToken}
```

## Parameters

| Name        | Type     | Description            |
| ----------- | -------- | ---------------------- |
| `projectId` | `number` | The id of the project. |

## Response Schema

```none
Status: 200
```

```json
{
    "result": [    {
      "id": string,
      "projectId": int,
      "name": "Branch Name",
      "createdAt": date,
      "closed": bool,
      "latestCheckpointId": string,
      "user": {
        "id": int,
        "fullName": string,
        "username": string
      }
    },, ...],
    "pagination": {
        "hasMore": bool
    }
}
```

This endpoint uses a slightly different pagination method. If a response contains the value `hasMore: true` then additional results are available. Use `?skip=branchId` query parameter with the last received branch id to receive more branches in alphabetical order.

## Errors

| Code | Description       |
| ---- | ----------------- |
| 401  | Unauthorized      |
| 403  | Forbidden         |
| 404  | Project not found |
| 429  | Too many requests |

## Rate Limiting

This route uses a [normal](/user-manual/api#rate-limiting) rate limit.

--------------------------------------------------------------------------------

## Jobs - Get job

URL: https://developer.playcanvas.com/user-manual/api/job-get/

## Route URL

```none
GET https://playcanvas.com/api/jobs/:id
```

## Description

Gets a Job by id.

## Example

```none
curl -H "Authorization: Bearer {accessToken}" "https://playcanvas.com/api/jobs/{id}"
```

## Parameters

| Name | Type     | Description        |
| ---- | -------- | ------------------ |
| `id` | `number` | The id of the job. |

## Response Schema

```none
Status: 200
```

```json
{
    "id": int,
    "created_at": date,
    "modified_at": date,
    "status": "running" | "complete" | "error",
    "messages": list of strings,
    "data": object - contents depend on the job
}
```

## Errors

| Code | Description       |
| ---- | ----------------- |
| 401  | Unauthorized      |
| 403  | Forbidden         |
| 404  | Job not found     |
| 429  | Too many requests |

## Rate Limiting

This route uses a [normal](/user-manual/api#rate-limiting) rate limit.

--------------------------------------------------------------------------------

## Projects - Archive project

URL: https://developer.playcanvas.com/user-manual/api/project-archive/

## Route URL

```none
POST https://playcanvas.com/api/projects/:id/export
```

## Description

This will allow you to download a zip archive of your entire project. You can import that archive from your Projects Dashboard to create a new Project from that archive. More about importing projects [here](/user-manual/editor/projects/backup-archiving#restoring-projects).

The request will start an archive job and the job details will be returned in the response. You can [poll the job by id](/user-manual/api/job-get) until its status is either 'complete' or 'error'. When the job is done, its data will contain a URL to download the project archive.

## Example

```none
curl -H "Authorization: Bearer {accessToken}" -H "Content-Type: application/json" -X POST -d '{"branch_id": "99999999-9999-9999-9999-999999999999"}' "https://playcanvas.com/api/projects/{projectId}/export"
```

## Parameters

| Name        | Type     | Required | Description                                                                |
| ----------- | -------- | :------: | -------------------------------------------------------------------------- |
| `projectId` | `number` | ✔️      | The id of the project.                                                     |
| `branch_id` | `string` |          | The id of the branch. If no id is specified, the main branch will be used. |

## Response Schema

```none
Status: 201 Created
```

```json
{
    "id": int,
    "created_at": date,
    "modified_at": date,
    "status": "running" | "complete" | "error",
    "messages": [ list of strings ],
    "data": {
      "project": {
         'id': int
      },
      "url": string
    }
}
```

## Errors

| Code | Description       |
| ---- | ----------------- |
| 401  | Unauthorized      |
| 403  | Forbidden         |
| 404  | Project not found |
| 404  | Owner not found   |
| 429  | Too many requests |

## Rate Limiting

This route uses a [strict](/user-manual/api#rate-limiting) rate limit.

--------------------------------------------------------------------------------

## Scenes - List scenes

URL: https://developer.playcanvas.com/user-manual/api/scene-list/

## Route URL

```none
GET https://playcanvas.com/api/projects/:projectId/scenes?branchId=:branchId
```

## Description

Get a list of all scenes for a project

## Example

```none
curl -H "Authorization: Bearer {accessToken}" "https://playcanvas.com/api/projects/{projectId}/scenes?branchId={branchId}"
```

HTTP Request

```text
GET https://playcanvas.com/api/projects/{projectId}/scenes?branchId={branchId}
Authorization: Bearer {accessToken}
```

## Parameters

| Name        | Type     | Required | Description                                                                |
| ----------- | -------- | :------: | -------------------------------------------------------------------------- |
| `projectId` | `number` | ✔️      | The id of the project.                                                     |
| `branchId`  | `string` |          | The id of the branch. If no id is specified, the main branch will be used. |

## Response Schema

```none
Status: 200
```

```json
{
    "result": [{
      "id": int,
      "projectId": int,
      "name": string,
      "created": date,
      "modified": date
    }, ...]
}
```

## Errors

| Code | Description       |
| ---- | ----------------- |
| 401  | Unauthorized      |
| 403  | Forbidden         |
| 404  | Project not found |
| 429  | Too many requests |

## Rate Limiting

This route uses a [normal](/user-manual/api#rate-limiting) rate limit.

--------------------------------------------------------------------------------

## Splat Publishing

URL: https://developer.playcanvas.com/user-manual/api/splat-publish/

#### Overview

Publish a splat (e.g., .ply or .sog) to the SuperSplat platform using a direct AWS S3 upload using signed URL + publish flow.

The flow consists of three main steps:

1. **Request a signed upload URL** from the backend.
2. **Upload the file** directly to S3 using that signed URL.
3. **Start the processing job** by calling the backend API.

All API requests must include a valid Bearer token in the `Authorization` header. Check [this document](https://developer.playcanvas.com/user-manual/api/#:~:text=You%20can%20generate%20an%20Access,you%20your%20new%20access%20token) to read about allocating a token.

### Routes

#### Get Signed URL for AWS S3 upload

```none
POST https://playcanvas.com/api/upload/signed-url

Body

{ "fileName": "scene.ply" }

Response

{ "signedUrl": "string", "s3Key": "string" }
```

Example:

```javascript
const response = await fetch(`https://playcanvas.com/api/upload/signed-url`, {
    method: 'POST',
    body: JSON.stringify({
        filename: filename
    }),
    headers: {
        'Authorization': `Bearer ${token}`,
        'Content-Type': 'application/json'
    }
});
```

#### Upload to AWS S3 using signed url

```none
PUT "signedUrl"

Response

{ "signedUrl": "string", "s3Key": "string" }
```

Example:

```javascript
const uploadResponse = await fetch(signedUrl, {
    method: 'PUT',
    body: fileData,
    headers: {
        'Content-Type': 'binary/octet-stream'
    }
});
```

#### Start processing

```none
POST https://playcanvas.com/api/splats/publish

Body

{
  "s3Key": "string",
  "title": "string",
  "description": "string",
  "listed": boolean,
  "settings": { /* Settings */ },
  "format": "compressed.ply" // or "sog"
}

Settings: 

const settings = {
    camera: {
        fov: 65, // field of view
        position: [1,1,-1], target: [-0.1,0.6,-0.2],
        startAnim: 'none',
        animTrack:null 
    },
    background: {
        color: [0.4,0.4,0.4]
    },
    animTracks:[]
};

Response (Splat data)

{
  "id": "string", 
  "hash": "string",
  "title": "string",
  "description": "string",
  "format": "compressed.ply | sog"
  "version": "string",
  "release_notes": "string",
  "thumbnails": number,
  "size": number
  "views": number, 
  "comments": number, 
  "starred": number, 
  "listed": boolean,
  "completedAt": DateTime, 
  "createdAt": DateTime, 
  "modifiedAt": DateTime
}
```

Example:

```javascript
const response = await fetch(`https://playcanvas.com/api/splats/publish`, {
    method: 'POST',
    body: JSON.stringify({
      s3Key: s3Key,
      title: 'Some Title',
      description: 'Some Description',
      listed: true,
      settings: settings,
      format: format
    }),
    headers: {
      'Authorization': `Bearer ${token}`,
      'Content-Type': 'application/json'
    }
});
```

Sample response

```json
{
  comments = 0
  completedAt = null
  createdAt = '2025-10-21T12:42:13.331Z'
  currentVersion = 1
  description = 'Some Description'
  featuredWeight = 0
  format = ''
  hash = '982a2820'
  hasThumbnails = false
  id = 50
  listed = true
  modifiedAt = '2025-10-21T12:42:13.331Z'
  releaseNotes = null
  size = 0
  starred = 0
  tags = (0) []
  task = {status: 'running', message: ''}
  title = 'Some Title'
  url = 'https://superspl.at/view?id=982a2820'
  userId = 7
  version = 0
  views = 0
}
```

Status - 201 Success

#### Check the status of uploaded splat

```none
GET https://playcanvas.com/api/splats/{ID}

Response (Splat)

{
  "id": "string",
  "hash": "string",
  "title": "string",
  "description": "string",
  "format": "compressed.ply | sog"
}
```

Example:

```javascript
const response = await fetch(`https://playcanvas.com/api/splats/1000`)
```

Errors

| Code | Description |
|------|-------------|
| 400  | Bad request / invalid payload / over storage allowance |
| 401  | Unauthorized (missing/invalid token) |
| 403  | Forbidden |
| 404  | Resource not found (e.g., sceneId) |
| 5xx  | Server/S3 error during upload or finalize |

--------------------------------------------------------------------------------

## Assets

URL: https://developer.playcanvas.com/user-manual/assets/

Assets are the building blocks of your PlayCanvas application. They represent all the external resources your application needs, such as 3D models, textures, audio files, and scripts.

## Assets vs Resources

In PlayCanvas, there's an important distinction between **Assets** and **Resources**:

- **Asset** - A record in the asset registry that contains metadata about a resource, including its name, type, tags, and a reference to the underlying resource data. Assets are managed by the [`AssetRegistry`](asset-registry).

- **Resource** - The actual runtime data that gets loaded into memory and used by the engine. For example, a texture asset's resource is the actual image data that can be applied to materials.

When you load an asset, PlayCanvas downloads and parses the underlying file to create the resource. The asset object then holds a reference to this resource via its `resource` property.

```javascript
const asset = this.app.assets.find('my-texture');
asset.ready((asset) => {
    const texture = asset.resource; // The actual Texture object
});
this.app.assets.load(asset);
```

## Asset Lifecycle

Assets go through several stages during the lifetime of your application:

1. **Registry** - Assets are registered in the [`AssetRegistry`](asset-registry), making them discoverable by ID, name, or tags
2. **Loading** - Asset data is downloaded from the server
3. **Ready** - The resource is parsed and available for use
4. **Unloading** - Resources can be unloaded to free memory

For details on controlling when assets load, see [Preloading](preloading) and [Loading and Unloading](loading-unloading).

## Supported Formats

PlayCanvas supports a wide variety of file formats for different asset types. See [Supported Formats](supported-formats) for a complete list.

## Working with Assets

### In the Editor

If you're using the PlayCanvas Editor, see the [Editor Assets Guide](/user-manual/editor/assets/) for information on:

- Importing and organizing assets
- Configuring asset properties
- Using the Asset Store

### Programmatically

For working with assets in code:

- **[Asset Registry](asset-registry)** - Find and manage assets at runtime
- **[Preloading](preloading)** - Control which assets load before your app starts
- **[Loading and Unloading](loading-unloading)** - Dynamically load assets during runtime

## Finding Assets

Looking for 3D models, textures, or audio for your project? See [Finding Assets](finding) for a list of asset marketplaces and resources.

--------------------------------------------------------------------------------

## Asset Registry

URL: https://developer.playcanvas.com/user-manual/assets/asset-registry/

The [`AssetRegistry`](https://api.playcanvas.com/engine/classes/AssetRegistry.html) is the central system for managing assets in PlayCanvas. It maintains a collection of all assets available to your application and provides methods to find, load, and manage them.

## Accessing the Registry

The asset registry is available via the application object:

```javascript
const assets = this.app.assets;
```

## Finding Assets

### By ID

Every asset has a unique numeric ID. This is the most reliable way to reference an asset:

```javascript
const asset = this.app.assets.get(123456);
```

### By Name

Find an asset by its name. Returns the first matching asset:

```javascript
const asset = this.app.assets.find('My Texture');
```

Find all assets with a given name:

```javascript
const assets = this.app.assets.findAll('Enemy');
```

### By Tag

Assets can be tagged for easy grouping. Find all assets with a specific tag:

```javascript
const levelAssets = this.app.assets.findByTag('level-1');
```

Find assets matching multiple tags (AND logic):

```javascript
// Assets tagged with BOTH 'level-1' AND 'enemy'
const enemies = this.app.assets.findByTag('level-1', 'enemy');
```

Find assets matching any of several tags (OR logic):

```javascript
// Assets tagged with 'level-1' OR 'level-2'
const assets = this.app.assets.findByTag(['level-1', 'level-2']);
```

## Asset Events

The registry emits events when assets are added, removed, or loaded:

### Registry Events

```javascript
// Asset added to registry
this.app.assets.on('add', (asset) => {
    console.log('Asset added:', asset.name);
});

// Asset removed from registry
this.app.assets.on('remove', (asset) => {
    console.log('Asset removed:', asset.name);
});

// Asset loaded
this.app.assets.on('load', (asset) => {
    console.log('Asset loaded:', asset.name);
});

// Asset failed to load
this.app.assets.on('error', (err, asset) => {
    console.error('Failed to load:', asset.name, err);
});
```

### Individual Asset Events

You can also listen for events on specific assets:

```javascript
const asset = this.app.assets.find('My Texture');

// Called when the asset's resource is ready
asset.on('load', (asset) => {
    console.log('Texture loaded:', asset.resource);
});

// Called if loading fails
asset.on('error', (err, asset) => {
    console.error('Failed:', err);
});

// Called when the asset is removed from the registry
asset.on('remove', (asset) => {
    console.log('Asset removed');
});

// Called when any property changes
asset.on('change', (asset, property, newValue, oldValue) => {
    console.log(`${property} changed from ${oldValue} to ${newValue}`);
});
```

### Using ready()

The `ready()` method is a convenient way to execute code when an asset is loaded. If the asset is already loaded, the callback fires immediately:

```javascript
const asset = this.app.assets.find('My Texture');

asset.ready((asset) => {
    // Asset is guaranteed to be loaded here
    const texture = asset.resource;
    material.diffuseMap = texture;
});

// Make sure to trigger loading if not already loaded
this.app.assets.load(asset);
```

## Adding Assets at Runtime

You can create and add new assets to the registry at runtime:

```javascript
const asset = new pc.Asset('New Texture', 'texture', {
    url: 'path/to/texture.png'
});

this.app.assets.add(asset);
this.app.assets.load(asset);
```

## See Also

- [Preloading](preloading.md) - Control which assets load before your app starts
- [Loading and Unloading](loading-unloading.md) - Dynamically manage asset loading
- [`AssetRegistry` API Reference](https://api.playcanvas.com/engine/classes/AssetRegistry.html)

--------------------------------------------------------------------------------

## Third-party Asset Sites

URL: https://developer.playcanvas.com/user-manual/assets/finding/

## Where can I get 3D models/Sound FX/Music for PlayCanvas?

Sometimes it's important to make your own unique assets, but sometimes a completely bespoke asset is too expensive or time-consuming to make. So we've gathered together a list of links and resources for places that you can get free or paid-for assets of all types.

If you've got other suggestions for this page. Then let us know on the [forum](https://forum.playcanvas.com/).

## Asset Marketplaces

Asset marketplaces are online libraries of content that you can download, either for free or for a fee, and add to your PlayCanvas game.

| Provider                                                                 | 2D Art   | 3D Art   | Audio    |
|--------------------------------------------------------------------------|----------|----------|----------|
| [3D Models Textures](https://www.3dmodels-textures.com/)                 |          | &#x2713; | &#x2713; |
| [BlendSwap](https://www.blendswap.com/)                                  |          | &#x2713; |          |
| [CGTrader](https://www.cgtrader.com/)                                    |          | &#x2713; |          |
| [Game Dev Market](https://www.gamedevmarket.net?ally=O0I9alFp)           | &#x2713; | &#x2713; | &#x2713; |
| [GameSounds.xyz](https://gamesounds.xyz/)                                |          |          | &#x2713; |
| [Kenney](https://kenney.nl/)                                             | &#x2713; | &#x2713; | &#x2713; |
| [Mixamo](https://www.mixamo.com/)                                        |          | &#x2713; |          |
| [PlayOnLoop](https://www.playonloop.com/music-loops-category/videogame/) |          |          | &#x2713; |
| [Open Game Art](https://opengameart.org/)                                | &#x2713; | &#x2713; | &#x2713; |
| [Sound Bible](https://soundbible.com/)                                   |          |          | &#x2713; |
| [Turbosquid](https://www.turbosquid.com/)                                | &#x2713; | &#x2713; | &#x2713; |

## Procedural Generation Tools

There are tools available that can generate game assets procedurally. Here are some good examples:

* [Sound FX Generator](https://www.bfxr.net/)
* [Spacescape Skybox Generator](http://alexcpeterson.com/spacescape)
* [Sloyd 3D Model Generator](https://sloyd.ai)

--------------------------------------------------------------------------------

## Loading and Unloading

URL: https://developer.playcanvas.com/user-manual/assets/loading-unloading/

PlayCanvas provides APIs to dynamically load and unload assets at runtime. This gives you fine-grained control over memory usage and allows you to stream content as needed.

## Loading Assets

### Loading Registered Assets

For assets that are already in the asset registry (e.g., assets added in the Editor), use `app.assets.load()`:

```javascript
const asset = this.app.assets.find('My Texture');

asset.ready((asset) => {
    // Asset is loaded and ready to use
    const texture = asset.resource;
});

this.app.assets.load(asset);
```

### Loading from URL

To load an asset from a URL at runtime, use `app.assets.loadFromUrl()`:

```javascript
this.app.assets.loadFromUrl('path/to/texture.png', 'texture', (err, asset) => {
    if (err) {
        console.error('Failed to load texture:', err);
        return;
    }
    
    // Asset is loaded and added to the registry
    const texture = asset.resource;
});
```

The second parameter specifies the asset type. Common types include:

- `texture` - Images (PNG, JPG, WebP, etc.)
- `model` - 3D models (GLB)
- `audio` - Sound files (MP3, OGG, WAV)
- `json` - JSON data
- `binary` - Binary data
- `css` - Stylesheets
- `html` - HTML documents
- `script` - JavaScript files
- `shader` - Shader code

### Loading with Options

You can pass additional options when loading from URL:

```javascript
this.app.assets.loadFromUrlAndFilename(
    'path/to/model.glb',
    'model.glb',
    'container',
    (err, asset) => {
        if (err) {
            console.error('Failed to load model:', err);
            return;
        }
        
        // Create an entity from the loaded model
        const entity = asset.resource.instantiateRenderEntity();
        this.app.root.addChild(entity);
    }
);
```

## Handling Load Events

### Using ready()

The `ready()` method executes a callback when an asset is loaded. If the asset is already loaded, the callback fires immediately:

```javascript
const asset = this.app.assets.find('My Model');

asset.ready((asset) => {
    // Safe to use asset.resource here
});

// Trigger loading if not already loaded
if (!asset.loaded) {
    this.app.assets.load(asset);
}
```

### Using Events

You can listen for load events on individual assets or the registry:

```javascript
// Listen on a specific asset
asset.on('load', (asset) => {
    console.log('Asset loaded:', asset.name);
});

asset.on('error', (err, asset) => {
    console.error('Load failed:', asset.name, err);
});

// Listen on the registry for any asset
this.app.assets.on('load', (asset) => {
    console.log('Some asset loaded:', asset.name);
});
```

## Unloading Assets

To free memory, you can unload assets that are no longer needed:

```javascript
const asset = this.app.assets.find('Large Texture');

// Unload the resource but keep the asset in the registry
asset.unload();

// The asset can be loaded again later
this.app.assets.load(asset);
```

### Removing Assets

To completely remove an asset from the registry:

```javascript
const asset = this.app.assets.find('Temporary Asset');

// Remove from registry (also unloads the resource)
this.app.assets.remove(asset);
```

## Loading Multiple Assets

To load multiple assets and wait for all of them:

```javascript
const assetNames = ['texture1', 'texture2', 'model1'];
const assets = assetNames.map(name => this.app.assets.find(name));

let loaded = 0;
const total = assets.length;

const onAssetLoad = () => {
    loaded++;
    if (loaded === total) {
        // All assets loaded
        this.onAllAssetsReady();
    }
};

for (const asset of assets) {
    asset.ready(onAssetLoad);
    if (!asset.loaded) {
        this.app.assets.load(asset);
    }
}
```

## Best Practices

- **Unload unused assets** - Free memory by unloading assets when changing levels or scenes
- **Use ready()** - It handles both loaded and not-yet-loaded cases
- **Handle errors** - Always provide error handling for dynamic loads
- **Batch loads** - Load related assets together to avoid visual inconsistencies

## See Also

- [Asset Registry](asset-registry.md) - Finding and managing assets
- [Preloading](preloading.md) - Loading assets before your app starts

--------------------------------------------------------------------------------

## Importing 3D Models

URL: https://developer.playcanvas.com/user-manual/assets/models/

3D models and animations are imported into PlayCanvas by uploading scene files from a [3D modeling application](/user-manual/assets/models/building) such as [Blender](https://www.blender.org/), 3D Studio Max or Maya.

PlayCanvas supports a wide variety of formats, such as glTF binary (GLB), FBX, COLLADA and obj. We recommend using the GLB format for best results.

Uploading one of these files will create a [Source Asset](/user-manual/glossary#source-asset) of type 'Model' and will produce several [Target Assets](/user-manual/glossary#target-asset) including a '[Template](/user-manual/editor/assets/inspectors/template/)' with the model hierarchy and 'Render' assets. You can add an instance of the 'Template' in your game.

Learn more about:

* [Building Models](/user-manual/assets/models/building)
* [Exporting Models](/user-manual/assets/models/exporting)
* [Importing Models](/user-manual/editor/assets/import-pipeline/import-hierarchy/)
* [Using Templates](/user-manual/editor/templates/)

--------------------------------------------------------------------------------

## Building Models

URL: https://developer.playcanvas.com/user-manual/assets/models/building/

Building art and animations for PlayCanvas can be done using almost any of the many 3D modeling programs available. For example: Blender, SketchUp, Autodesk 3D Studio Max or Autodesk Maya.

PlayCanvas is designed to import content most faithfully via the open source glTF binary (GLB) format so, in general, if your modeling application supports GLB, PlayCanvas supports it too.

The import of GLBs supports the following glTF features for incredibly fast scene creation:

- Node hierarchy
- Standard material
- Ratified material extensions
- Animated skeletons and skinning data
- Morph targets
- Cameras
- Draco compressed models
- Punctual light support

As such, to target PlayCanvas, you do not have to use any special art tool plugins and there are no PlayCanvas-specific workflows you must follow.

[Image: GLB Import Example]  
*Model by Loïc Norgeot and mosquito scan by Geoffrey Marchal for [Sketchfab](https://sketchfab.com/3d-models/real-time-refraction-demo-mosquito-in-amber-37233d6ed84844fea1ebe88069ea58d1) licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)*

PlayCanvas also supports the importing of FBX files which is another common interchange format. However, FBX materials are not imported as faithfully as glTF materials and might require some editing after import.

We recommend using GLBs whenever possible for the best experience and highest compatibility.

Some modeling and animation programs to consider:

- Free options: Blender\*, Wings3D, Voidworld, SketchUp, Sculptris, Daz Studio\*.
- Lower-cost paid options: 3D-Coat, NVIL, Hexagon.
- Higher-cost paid options: ZBrush, Autodesk 3D Studio Max\*, Autodesk Maya\*, Luxology Modo\*.

\* *this program can create animations too.*

For generating textures, consider these programs:

- Free: Blender, Pixexix Free, xNormal, MaPZone, GIMP, Krita desktop, MyPaint.
- Paid: Substance Designer (as well as Substance Painter and Bitmap2Material), Photoshop, Quixel NDO and DDO, CrazyBump, Pixexix Pro, Clip Studio Paint/Manga Studio, SERIF DrawPlus/PhotoPlus.

Note many of the programs listed above for modeling also allow texture painting. If you would like more options, this [external wiki](http://wiki.polycount.com/wiki/Tools) page can help. Please note that some of the information may be out-of-date.

## General Considerations for Mesh Construction

Regardless which modeling application you are using, there are a number of things to be aware of when building 3D scenes intended for PlayCanvas.

- Meshes can be skinned to up to 256 bones.
- NURBS meshes will be converted to triangle meshes on conversion to the PlayCanvas native format.

---

## Blender

[Image: Blender Logo]

[Blender](https://www.blender.org/) is a fantastic free tool for 3D modeling and it is perfect for generating 3D assets for PlayCanvas.

Blender supports exporting to GLB, FBX and COLLADA but the built-in exporters do have some limitations.

### **Map Types**

As of Blender 2.71, both diffuse and normal maps of your material will be exported to the FBX file. If other map types are lost on export you have to set-up these maps in the Material Editor within the Editor.

### **Embedding Textures**

Embedded Textures make importing much easier.

The Blender COLLADA exporter does not have the ability to embed textures into the exported model file.

The 2.71 release of Blender features a revamped FBX export module that enables multiple embedded textures directly from Blender. First make sure the object is rendered correctly within Blender. When exporting to FBX, set the 'Path Mode' to Copy and check the 'Embed Textures' box.

Alternatively, use the [Autodesk FBX Converter](https://www.autodesk.com/developer-network/platform-technologies/fbx-converter-archives) to convert an export from Blender into one with embedded media. Just open the file in the FBX Converter and re-save with the *Embedded Media* checkbox set.

:::warning

There seems to be an issue with Blender 2.71's FBX export generating emissivity despite no emissive properties being set in Blender - this is not an issue with the PlayCanvas engine. To avoid this from within Blender, you can change the material's Diffuse color setting to 0 (under the 'Material' tab in the 'Properties Editor'). Or simply reduce emissivity from within the PlayCanvas Editor.

:::

### **Animations**

As of Blender 2.71:

Animations included within the blend file are exported with the default fbx exporter settings and are compatible with the PlayCanvas asset import pipeline. Note that for multiple animations for the same model you will need to upload multiple fbx files - one for each animation. To save memory you can delete models and textures from the blend file before exporting, choose to export only armatures in the fbx exporter settings, or simply delete the duplicate 'model' target assets from the assets page on your project dashboard (select 'target' from the drop-down menu on the assets page to view target assets).

### **Learning Blender**

There are plenty of resources for learning how to use Blender on the web. A couple that we recommend are:

- [Blender Cookie](https://cgcookie.com/learn-blender)
- [Blender Guru](https://www.blenderguru.com/)

---

## Autodesk 3D Studio Max

### Max Materials

You should use the Standard material type in Max, or the Multi/Sub-Object material type providing the materials it references are Standard materials. The highlighted areas in the screenshot below show the settings that are respected when you export to PlayCanvas:

[Image: Max material editor]

### Supported Map Slots

PlayCanvas interprets a subset of the 3DS Max material map types on export. Note that maps can be in any format that 3DS Max supports, but if the maps are not in a web format (namely JPEG, GIF or PNG) then they will be converted to these formats on export.

#### Diffuse Color

Assigning a Bitmap map to this slot enables diffuse mapping on the PlayCanvas material. This essentially overrides whatever diffuse color has been set on the material (via the color picker). If the diffuse map has an alpha channel, it will be used as the per pixel alpha value in the PlayCanvas runtime.

Additionally, it is possible to assign an RGB Multiply map to the Diffuse Color slot. This has the effect of enabling lightmapping, where Map 1 is the diffuse map and Map 2 is the lightmap.

#### Specular Color

Assigning a Bitmap map to this slot enables specular mapping in the PlayCanvas material. This essentially overrides whatever specular color has been set on the material (via the color picker). This allows you to mask out areas of specularity on a surface, or tinge specular highlights different colors in different areas. If the specular map has an alpha channel, it will be used to set per-pixel shininess.

#### Specular Level

Assigning a Bitmap map to this slot activates per-pixel attenuation of the material's specular color. This essentially overrides whatever specular level has been set on the material.

#### Glossiness

Assigning a Bitmap map to this slot activates per-pixel glossiness on the material. This essentially overrides whatever glossiness has been set on the material.

#### Self-Illumination

Assigning a Bitmap map to this slot enables emissive mapping in the PlayCanvas material. An emissive map can be full RGB (you are not limited to greyscale) and will essentially be added to the result of lighting a surface. So a black pixel in an emissive map will add nothing to a pixel's value.  Anything else will increase a pixel's luminosity.

#### Opacity

Assigning a Bitmap map to this slot enables opacity mapping on the PlayCanvas material. A black pixel in the opacity map will be fully transparent, a white pixel will be fully opaque and any grey pixel will correspond to some equivalent intermediate alpha value.

If the material has an opacity map, it will override any alpha channel that may have been set in the diffuse map. It is less efficient to use the opacity map slot over the alpha channel of the diffuse map since two textures are generated in the PlayCanvas runtime instead of one and the default shader must do a little more work. However, if performance is not a concern and it is convenient to use an opacity map, the functionality is supported.

#### Bump

Assigning a Normal Bump map to this slot enables normal mapping. Note that, by default, the 'Amount' value for the Bump slot is set to 30. PlayCanvas effectively ignores this value and treats it as if it were set to 100, so it is recommended that you set this to 100 in Max also in order to make a render match the real-time rendering more closely.

There are different ways to author normal maps, the main two differing in the format of the green component. For example, by default, 3DS Max would expect a normal map for a brick wall to appear as follows:

[Image: Normal map]

Notice lighter areas at the bottom edge of raised areas. PlayCanvas expects the green component (or the Y component of each normal in the normal map) to be flipped. Loading the file into Photoshop, selecting the green channel and pressing Ctrl+I to invert the channel gives:

[Image: Normal map with Y component flipped]

Now, lighting in PlayCanvas will be consistent.

If the assigned normal map has an alpha channel, it will be treated as a parallax map with the alpha channel interpreted as a height map. Lighter areas of the height map are treated as 'higher' than the darker areas.

#### Reflection

Assigning a Bitmap map to this slot enables sphere mapping in the PlayCanvas material. The bitmap would look something like this:

[Image: Sphere map]

Sphere mapping is one of the cheapest, least 'convincing' forms of reflection mapping, but in many scenarios it is sufficient to provide a pleasing glossy sheen to surfaces.

---

## Autodesk Maya

### Maya Materials

You should use the standard material types in Maya: lambert, blinn and phong. The highlighted areas in the screenshot below show the material settings that are respected when you export to PlayCanvas:

[Image: Maya material editor]

--------------------------------------------------------------------------------

## Exporting Assets

URL: https://developer.playcanvas.com/user-manual/assets/models/exporting/

PlayCanvas can import 3D content in the following formats: glTF binary (GLB), FBX, OBJ, 3DS and COLLADA (DAE). We strongly recommend you use GLB as it's an open source industry standard and very well supported in the Editor.

[Image: GLB Import Example]  
*Model by Loïc Norgeot and mosquito scan by Geoffrey Marchal for [Sketchfab](https://sketchfab.com/3d-models/real-time-refraction-demo-mosquito-in-amber-37233d6ed84844fea1ebe88069ea58d1) licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)*

If GLB is not available, then please use FBX as it is a robust and well tested interchange industry format.

## General export tips

* Only export what you need from your scene. For example, if your scene contains a red car and a blue car but you only want to import the red car, export only the red car. Your modeling application should have an 'Export Selected' or similar option.

## Tips for Exporting to GLB

* Set the format to glTF Binary (GLB).
* Ensure that materials and images are set to be exported and embedded in the GLB. Otherwise, you must upload all scene textures separately and relink all textures to materials and re-configure the materials parameters.

## Tips for Exporting to FBX

* Ensure you have the latest available FBX exporter for your modelling application installed.
* Select the 'Embed Media' option in the FBX exporter panel. This packages the scene's textures in the exported FBX. This preserves material to textures links during import into PlayCanvas. Otherwise, you must upload all scene textures separately and relink all textures to materials.
* Select the 'Binary' file format option in the FBX exporter panel. This reduces FBX file size considerably.

--------------------------------------------------------------------------------

## Units

URL: https://developer.playcanvas.com/user-manual/assets/models/units/

PlayCanvas scenes generally treat 1 unit as 1 meter.

When authoring artwork for PlayCanvas, it is important to ensure you do so to the desired scale. To do this, you should check the working units for your scene in your modelling application of choice. You can choose whatever working units you like, but just ensure you respect them. For example, to make a cube that is 1 meter in dimension, you could set your working units to meters and create a 1x1x1 cube. Alternatively, you could set working units to centimeters and create a 100x100x100 cube. Exporting either scene to FBX and importing it to PlayCanvas will result in a cube which is 1x1x1.

### Blender

To ensure units are exported correctly from Blender, check that Scene Properties unit system is set to metric and that scale is set to 1.0:

[Image: Blender units]

In addition, when exporting to FBX format, check that "Apply Scaling" is set to "FBX All":

[Image: Blender FBX Export]

### Autodesk 3D Studio Max

To check or set working units in 3D Studio Max, open the Units Setup dialog and click the System Unit Setup button:

[Image: 3DS Max units]

### Autodesk Maya

To check or set working units in Maya, open the Preferences dialog:

[Image: Maya units]

--------------------------------------------------------------------------------

## Preloading

URL: https://developer.playcanvas.com/user-manual/assets/preloading/

On the web, it's critical to get users into your application as soon as possible. The PlayCanvas asset system provides preloading to ensure essential assets are ready before your application starts.

## The Preload Flag

Every asset has a `preload` property. When set to `true`, the asset will be downloaded and its resource created before the application's `initialize` phase begins.

You should use preloading for assets that are needed immediately when your application starts. This prevents assets from "popping in" after the application is already running.

:::tip

In the PlayCanvas Editor, you can set the preload flag in the asset's properties panel. By default, new assets have preload enabled.

:::

## When Are Assets Loaded?

Assets are loaded according to these rules:

1. **Preloaded assets** (`preload = true`) are loaded before the application starts
2. **Referenced assets** are loaded when an enabled component references them. For entities enabled in the scene, this happens immediately after preloading completes
3. **Dependent assets** are loaded when their parent asset loads. For example, when a model loads, its referenced materials load, which in turn load their referenced textures

## Streaming vs Preloading

If an asset is not preloaded, it will be streamed in when needed. Components handle this gracefully and begin operating once their assets are ready. However, you may see visual "popup" as models appear before their textures finish loading.

## Loading Asset Groups with Tags

To avoid popup, you can load groups of assets before displaying them. Use asset tags to organize assets into logical groups:

```javascript
// Find all assets tagged for level 1
const assets = this.app.assets.findByTag('level-1');
let loadedCount = 0;

// Load each asset
for (const asset of assets) {
    asset.once('load', () => {
        loadedCount++;
        if (loadedCount === assets.length) {
            // All level-1 assets are loaded
            this.startLevel();
        }
    });
    this.app.assets.load(asset);
}
```

You can also use more complex tag queries:

```javascript
// Assets tagged with BOTH 'level-1' AND 'enemy'
const enemies = this.app.assets.findByTag('level-1', 'enemy');

// Assets tagged with 'level-1' OR 'level-2'
const assets = this.app.assets.findByTag(['level-1', 'level-2']);
```

## Best Practices

- **Preload essential assets** - UI elements, player models, and anything needed immediately
- **Stream large assets** - Background music, distant scenery, optional content
- **Use tags for levels** - Group assets by level or area to load them together
- **Show loading progress** - For streamed content, display a loading indicator

## See Also

- [Asset Registry](asset-registry.md) - Finding and managing assets
- [Loading and Unloading](loading-unloading.md) - Dynamic asset management

--------------------------------------------------------------------------------

## Supported Formats

URL: https://developer.playcanvas.com/user-manual/assets/supported-formats/

PlayCanvas supports a wide variety of file formats for different asset types. This page lists the formats that can be used in your projects.

## 3D Models

| Format | Extension | Notes |
|--------|-----------|-------|
| glTF Binary | `.glb` | Recommended format. Efficient, widely supported |
| glTF | `.gltf` | JSON-based with external files |
| FBX | `.fbx` | Converted to GLB on import in Editor |
| COLLADA | `.dae` | Converted to GLB on import in Editor |
| OBJ | `.obj` | Basic mesh format, no animations |

:::tip

GLB is the recommended format for 3D models. It's compact, loads quickly, and supports all PlayCanvas features including animations, materials, and morph targets.

:::

## Textures

| Format | Extension | Notes |
|--------|-----------|-------|
| PNG | `.png` | Lossless, supports transparency |
| JPEG | `.jpg`, `.jpeg` | Lossy compression, no transparency |
| WebP | `.webp` | Modern format, good compression |
| AVIF | `.avif` | Next-gen format, excellent compression |
| GIF | `.gif` | Converted to PNG/JPG on import |
| TGA | `.tga` | Converted to PNG/JPG on import |
| BMP | `.bmp` | Converted to PNG/JPG on import |
| TIFF | `.tif`, `.tiff` | Converted to PNG/JPG on import |
| HDR | `.hdr` | High dynamic range, for environment maps |
| EXR | `.exr` | High dynamic range, converted to RGBM PNG |

### Texture Compression

For optimized delivery, textures can be compressed to GPU-native formats:

| Format | Platform | Notes |
|--------|----------|-------|
| Basis | All | Universal compressed format |
| DXT/BC | Desktop | Windows/Mac/Linux |
| PVRTC | iOS | Apple devices |
| ETC | Android | Most Android devices |
| ASTC | Modern mobile | iOS 8+, Android with ASTC support |

## Audio

| Format | Extension | Notes |
|--------|-----------|-------|
| MP3 | `.mp3` | Widely supported, good compression |
| OGG Vorbis | `.ogg` | Open format, good quality |
| WAV | `.wav` | Uncompressed, large files |
| M4A | `.m4a` | AAC audio |

:::note

For best browser compatibility, provide both MP3 and OGG versions of audio files. PlayCanvas will use the format supported by the user's browser.

:::

## Fonts

| Format | Extension | Notes |
|--------|-----------|-------|
| TrueType | `.ttf` | Converted to bitmap font on import |
| WOFF | `.woff` | Web font format |

## Scripts

| Format | Extension | Notes |
|--------|-----------|-------|
| JavaScript | `.js` | Classic scripts |
| ES Module | `.mjs` | ESM scripts (recommended) |

## Data Files

| Format | Extension | Notes |
|--------|-----------|-------|
| JSON | `.json` | Structured data |
| Text | `.txt` | Plain text |
| CSV | `.csv` | Tabular data (as text) |
| XML | `.xml` | Markup data (as text) |
| HTML | `.html` | HTML documents |
| CSS | `.css` | Stylesheets |

## Shaders

| Format | Extension | Notes |
|--------|-----------|-------|
| GLSL | `.glsl` | OpenGL shading language |
| Vertex Shader | `.vert` | Vertex shader source |
| Fragment Shader | `.frag` | Fragment shader source |

## Other

| Format | Extension | Notes |
|--------|-----------|-------|
| WebAssembly | `.wasm` | Compiled binary modules |
| Binary | `.bin` | Raw binary data |
| PLY | `.ply` | 3D Gaussian Splat data |

## See Also

- [Models](models/index.md) - Preparing 3D models for PlayCanvas
- [Asset Inspectors](/user-manual/editor/assets/inspectors/) - Configure asset properties in the Editor

--------------------------------------------------------------------------------

## Entity Component System (ECS)

URL: https://developer.playcanvas.com/user-manual/ecs/

PlayCanvas uses an **Entity Component System (ECS)** to organize and manage the objects in your application.  
In this design pattern:

- **[Entities](https://api.playcanvas.com/engine/classes/Entity.html)** are containers — they hold components but have no behavior of their own.
- **[Components](https://api.playcanvas.com/engine/classes/Component.html)** add functionality or data to an Entity.
- **[Systems](https://api.playcanvas.com/engine/classes/ComponentSystem.html)** manage all instances of a given Component type.

This approach provides:

- **Flexibility** — you can mix and match components to build complex behaviors.
- **Modularity** — logic is encapsulated within components.
- **Performance** — systems process components in efficient batches.

--------------------------------------------------------------------------------

## Components

URL: https://developer.playcanvas.com/user-manual/ecs/components/

A **[`Component`](https://api.playcanvas.com/engine/classes/Component.html)** adds data and behavior to an Entity.

## Examples

- [`CameraComponent`](https://api.playcanvas.com/engine/classes/CameraComponent.html)
- [`LightComponent`](https://api.playcanvas.com/engine/classes/LightComponent.html)
- [`RenderComponent`](https://api.playcanvas.com/engine/classes/RenderComponent.html)
- [`RigidBodyComponent`](https://api.playcanvas.com/engine/classes/RigidBodyComponent.html) & [`CollisionComponent`](https://api.playcanvas.com/engine/classes/CollisionComponent.html)
- [`ScriptComponent`](https://api.playcanvas.com/engine/classes/ScriptComponent.html)

:::tip
Only add components you actually need, and remove unused ones to keep Entities lean.
:::

## Adding a Component in code

```javascript
entity.addComponent('camera', {
    nearClip: 1,
    farClip: 100,
    fov: 55
});
```

See [`addComponent`](https://api.playcanvas.com/engine/classes/Entity.html#addcomponent).

## Accessing a Component

```javascript
const camera = entity.camera;
```

## Removing a Component

```javascript
entity.removeComponent('camera');
```

See [`removeComponent`](https://api.playcanvas.com/engine/classes/Entity.html#removecomponent).

## Enabling / Disabling Components

```javascript
entity.model.enabled = false;
```

See [`enabled`](https://api.playcanvas.com/engine/classes/Component.html#enabled).

:::tip
If a component is temporarily not needed, consider disabling it instead of removing it.
:::

--------------------------------------------------------------------------------

## Entities

URL: https://developer.playcanvas.com/user-manual/ecs/entities/

An **Entity** is the basic building block of your PlayCanvas scene ([`Entity`](https://api.playcanvas.com/engine/classes/Entity.html)).

## Key characteristics

- An Entity can have **zero or more components**.
- Entities can be **parented** to form a hierarchy ([`addChild`](https://api.playcanvas.com/engine/classes/GraphNode.html#addchild), [`removeChild`](https://api.playcanvas.com/engine/classes/GraphNode.html#removechild)).
- Entities can be **enabled** or **disabled** ([`enabled`](https://api.playcanvas.com/engine/classes/GraphNode.html#enabled)).

:::tip
Keep Entities lightweight — avoid adding unnecessary components.
:::

## Creating an Entity in code

```javascript
const entity = new pc.Entity("MyEntity");
app.root.addChild(entity);
```

## Enabling / Disabling Entities

```javascript
entity.enabled = false; // Disables the Entity and all its components
```

:::tip
Disable Entities when not in use to reduce processing and improve performance.
:::

## Lifecycle

- **Creation** — [`Entity constructor`](https://api.playcanvas.com/engine/classes/Entity.html#constructor).
- **Parenting** — [`addChild`](https://api.playcanvas.com/engine/classes/GraphNode.html#addchild) / [`removeChild`](https://api.playcanvas.com/engine/classes/GraphNode.html#removechild).
- **Destruction** — [`destroy`](https://api.playcanvas.com/engine/classes/GraphNode.html#destroy).

:::tip
When an Entity is no longer needed, call `destroy` to free resources and detach it from the hierarchy.
:::

--------------------------------------------------------------------------------

## Hierarchy and Transformations

URL: https://developer.playcanvas.com/user-manual/ecs/hierarchy-and-transformations/

Entities can be arranged in a **parent-child hierarchy**. The `Entity` class inherits its transform capabilities from the [`GraphNode`](https://api.playcanvas.com/engine/classes/GraphNode.html) superclass.

## Key points

- **Transforms are relative** to the parent.
- **World transforms** are calculated by combining local transforms through the hierarchy.
- Moving a parent affects all its children.

:::tip
Minimize deep hierarchies. Shallower hierarchies are easier to manage and can perform better.
:::

## Example

```javascript
childEntity.setLocalPosition(1, 0, 0); // relative to parent
console.log(childEntity.getWorldPosition()); // global position
```

See [`setLocalPosition`](https://api.playcanvas.com/engine/classes/GraphNode.html#setlocalposition) and [`getWorldPosition`](https://api.playcanvas.com/engine/classes/GraphNode.html#getworldposition).

## Re-parenting

```javascript
newParent.addChild(childEntity);
```

## Scaling considerations

- Non-uniform scaling can cause visual or physics issues.
- Avoid scaling physics-enabled entities unless necessary.

--------------------------------------------------------------------------------

## Searching the Hierarchy

URL: https://developer.playcanvas.com/user-manual/ecs/searching-the-hierarchy/

## By Name

```javascript
const found = app.root.findByName("Player");
```

See [`findByName`](https://api.playcanvas.com/engine/classes/GraphNode.html#findbyname).

## By Tag

Tags are string labels you can assign to Entities ([`Tags`](https://api.playcanvas.com/engine/classes/Tags.html)).

```javascript
entity.tags.add("enemy");
const enemies = app.root.findByTag("enemy");
```

See [`tags`](https://api.playcanvas.com/engine/classes/Entity.html#tags) and [`findByTag`](https://api.playcanvas.com/engine/classes/Entity.html#findbytag).

:::tip
Use tags for grouping related Entities. Tag-based searches are typically faster and more flexible than deep name-based searches.
:::

## By Component

```javascript
const lights = app.root.findComponents("light");
```

See [`findComponents`](https://api.playcanvas.com/engine/classes/Entity.html#findcomponents).

## Recursion and Scope

- Searches can be started from any Entity, not just `app.root`.
- Searching from a smaller subtree is faster than searching the whole scene.

--------------------------------------------------------------------------------

## PlayCanvas Editor

URL: https://developer.playcanvas.com/user-manual/editor/

The PlayCanvas Editor is a powerful, browser-based development environment for creating stunning 3D applications. No downloads, no installations—just open your browser and start building.

[Image: PlayCanvas Editor Interface]

## Why Choose the PlayCanvas Editor?

### 🌐 Accessible Anywhere

The entire Editor runs in your web browser. Whether you're at your desk, on a laptop, or borrowing a friend's computer, your projects are just a login away. Your work is automatically saved to the cloud, so you never lose progress.

### 👁️ What You See Is What You Get

The Editor uses the same [PlayCanvas Engine](../engine) that powers your published applications. Every material, every light, every shadow you see in the Editor is exactly what your users will experience. No surprises at runtime.

### ⚡ Instant Iteration

Make a change, see it instantly. With [live editing](scripting/hot-reloading), you can tweak materials, adjust positions, and even modify scripts while your application runs. Hit the [Launch](interface/launch-page) button and your project opens in a new tab, ready to test. The feedback loop between idea and implementation has never been shorter.

## Built for Teams

### Real-time Collaboration

[Image: Collaboration in the Viewport]

Work together with your team in real-time. See where others are looking, what they're selecting, and chat directly within the Editor. Multiple developers, artists, and designers can build the same scene simultaneously—no merge conflicts, no stepping on each other's toes. [Learn more about collaboration →](realtime-collaboration)

### Professional Version Control

[Image: Version Control Graph]

PlayCanvas includes a complete [version control system](version-control) designed specifically for 3D projects:

- **Checkpoints** — Snapshot your project at any point and roll back if needed
- **Branches** — Develop features in isolation without affecting the main project
- **Merging** — Combine work from multiple branches with visual conflict resolution
- **History** — Track every change to every asset and entity over time

## Complete Development Toolkit

### Visual Scene Building

Construct your 3D worlds using intuitive [transform gizmos](interface/viewport), a hierarchical [entity tree](interface/hierarchy), and a comprehensive [property inspector](interface/inspector). Drag and drop assets directly into your scene, parent entities with a click, and fine-tune every detail.

### Integrated Code Editor

Write scripts without leaving your browser. The built-in [code editor](scripting/code-editor) features syntax highlighting, intelligent autocomplete, and real-time error checking. For larger projects, use the [VS Code extension](scripting/vscode-extension) and develop locally with full IDE power.

### Powerful Asset Pipeline

Import models, textures, audio, and more with a streamlined [asset pipeline](assets/import-pipeline). The Editor automatically optimizes your assets and handles format conversions. Organize everything in the [Assets Panel](assets/asset-panel) with folders, search, and filtering.

### Component-Based Architecture

Build complex behaviors by combining [components](scenes/components)—camera, light, physics, audio, scripts, and more. This modular approach keeps your project organized and your code reusable.

## Publish Everywhere

When you're ready to share your creation, PlayCanvas has you covered:

| Platform | Description |
|----------|-------------|
| [Web](publishing/web) | Deploy to PlayCanvas hosting or self-host anywhere |
| [Mobile](publishing/mobile) | Package as native iOS and Android apps |
| [Desktop](publishing/desktop) | Build standalone applications for Windows, macOS, and Linux |
| [Playable Ads](publishing/playable-ads) | Export optimized builds for Facebook, Snapchat, and more |

## Getting Started

Ready to dive in? Here's your path forward:

1. **[Create Your First App](getting-started/your-first-app)** — Build a simple 3D scene in minutes
2. **[Explore the Interface](interface)** — Master the Editor's tools and panels
3. **[Learn Scripting](scripting)** — Add interactivity with JavaScript
4. **[Publish Your Project](publishing)** — Share your creation with the world

## Extend and Customize

For advanced workflows, the [Editor API](editor-api) lets you build custom tools, automate repetitive tasks, and integrate with external systems. If you can imagine it, you can build it.

---

:::info[Engine Compatibility]

The Editor evolves alongside the PlayCanvas Engine. If you're upgrading an older project or experiencing issues, check the [engine compatibility](engine-compatibility) guide or visit [troubleshooting](troubleshooting) for solutions.

:::

--------------------------------------------------------------------------------

## Assets

URL: https://developer.playcanvas.com/user-manual/editor/assets/

The PlayCanvas Editor provides a complete asset management system for your project. This section covers how to work with assets in the Editor, including importing, organizing, and configuring them.

## What You'll Learn

- **[Assets Panel](asset-panel)** - Navigate and manage your project's assets using the Assets Panel
- **[Importing Assets](importing)** - Upload and import assets into your project
- **[Import Pipeline](import-pipeline/)** - Configure how assets are processed during import
- **[Asset Inspectors](inspectors/)** - Configure asset properties using the Inspector
- **[Asset Store](asset-store/)** - Browse and import assets from the PlayCanvas Asset Store
- **[Asset Viewers](viewers)** - Inspect models and textures in dedicated viewer tools

## Overview

Assets in the Editor are managed through the [Assets Panel](asset-panel), which provides a visual interface for:

- Organizing assets into folders
- Uploading new assets via drag-and-drop
- Searching and filtering assets
- Inspecting and editing asset properties
- Copying assets between projects

When you upload a file, the Editor runs it through the [import pipeline](import-pipeline/) to convert and optimize it for use in your application. The resulting asset can then be configured using the appropriate [asset inspector](inspectors/).

For conceptual information about assets and how to work with them programmatically, see the [Assets](/user-manual/assets/) section in Common Topics.

--------------------------------------------------------------------------------

## Assets Panel

URL: https://developer.playcanvas.com/user-manual/editor/assets/asset-panel/

The Assets Panel manages all of the Assets that are available in your project. From here, you can create, upload, delete, inspect and edit any Asset.

[Image: Assets Panel]

## Folder Hierarchy {#folder-hierarchy}

The folder panel allows you to organize your assets into a tree of folders.

To create a new Folder, select the Add Asset (+) button and select 'Folder'. Alternatively, right click the folder where you would like a new folder to be created and select 'New Asset' > 'Folder'.

To rename a folder, double click it in the hierarchy panel and edit the Name field in the Inspector.

To delete a folder, double click it in the hierarchy and hit delete. Alternatively, right click the folder you want to delete and select 'Delete' from the context menu.

Folders can be drag and dropped into each other if you want to reorganize your folder structure.

:::note

Folders are essentially meta assets, exclusively designed to facilitate the organization of assets within the Editor. Folder assets are excluded from builds and are not used when building the folder structure of a published build.

:::

## Creating and Uploading Assets {#creating-and-uploading-assets}

You can create new assets by dragging a file from you computer's file system into the Assets Panel. The Editor will upload and import the asset for you.

You can also create certain assets types using the Add Asset (+) icon.

You can delete assets by selecting them and clicking the Delete Asset icon.

## Editing Assets {#editing-assets}

Certain text-based assets can be opened and edited in the PlayCanvas Script Editor: text, json, shader, html, css and script assets. To do this, simply double click on the asset's thumbnail.

## Inspecting Assets {#inspecting-assets}

To inspect the details of any given asset, select its thumbnail in the Assets Panel. The asset's details will be displayed in the Inspector.

## Filtering {#filtering}

Filter which assets are visible using the filter drop down to select the type of asset to view.

## Searching {#searching}

You can perform a global search for assets in your project using the Search box. Simply start typing into the box and the Editor will show matching results dynamically in the Assets Panel.

**ID** - Specific asset can be found by its unique ID, by simply typing ID in search field it will recognize exact match and only show one asset with that ID.

**RegExp** - It is possible to search using regular expressions. Add `*` at the beginning of search field and type regexp query after. To search for all assets use `*.` (any character) regexp query.

**Tags** - To search by tags and their combinations type tags in square brackets `[ ]`. Simple query operators: AND, OR are allowed by expressing query as array of strings or other arrays with strings. Logic of query is same as for [`findByTag`](https://api.playcanvas.com/engine/classes/AssetRegistry.html#findbytag) from `AssetsRegistry`.
Here are some examples:

- `[ level-1 ]` - returns all assets that are tagged by `level-1`.
- `[ level-1, level-2 ]` - returns all assets that are tagged by `level-1 OR level-2`.
- `[ [ level-1, monster ] ]` - returns all assets that are tagged by `level-1 AND monster`. Notice extra brackets.
- `[ [ level-1, monster ], [ level-2, monster ] ]` - returns all assets that are tagged by `(level-1 AND monster) OR (level-2 AND monster)`.

## Drag and Drop {#drag-and-drop}

Assets can be moved to different folders by dragging them from the main panel to a folder in the folder hierarchy. Note that assets support multi-selection. Pressing CTRL+A will select all assets in the currently selected folder.

You can also drag Assets from the Asset Panel to highlighted slots in the [Inspector](/user-manual/editor/interface/inspector). Slots in the Inspector will either be asset attributes of components or asset-type script attributes.

You can also drag model, material, and cubemap assets directly into the [Viewport](/user-manual/editor/interface/viewport)

- If you drag a model asset into the Viewport, a new entity will be created with a model component with the model asset assigned. The viewport camera will automatically zoom to the newly created entity.
- If you drag a material over a particular mesh instance in the Viewport, its material will be switched (as a preview) to the material being dragged. To make the material change stick, simply drop the material.
- If you drag a cubemap over the background of a scene in the Viewport, the cubemap will be assigned as the skybox cubemap of the scene. This property can also be set in the [Project Settings](/user-manual/editor/interface/settings/rendering/).

## Copy and Paste between Projects {#copy-and-paste-between-projects}

To copy an asset or a selection of assets between projects, select the asset(s) and right-click to bring up the context menu to select 'Copy'. You can also use the hotkey Ctrl/Cmd + C instead if the context menu is not available due to being a read-only project.

<img loading="lazy" src="/img/user-manual/editor/assets-panel/right-click-copy.png" alt="Right click copy menu" width="500" />

In the project that you want to copy the asset(s) to, right click in the assets panel and select 'Paste'. Ctrl/Cmd + V hotkey can be used instead.

<img loading="lazy" src="/img/user-manual/editor/assets-panel/right-click-paste.png" alt="Right click paste menu" width="500" />

Copy and pasting an asset will also copy its asset dependencies too. For example, here we have a model which references two materials and they reference a set of textures.

<img loading="lazy" src="/img/user-manual/editor/assets-panel/copy-and-paste-model-with-dependencies.png" alt="Model example" width="100%" />

If you copy and paste just the model asset into a different project, those asset dependencies are copied too.

<img loading="lazy" src="/img/user-manual/editor/assets-panel/pasted-reference-assets.png" alt="Pasted referenced assets" width="100%" />

By default, it is pasted as a flat hierarchy. If you want keep the folder structure, hold Shift when the context menu is opened and an option will appear called 'Paste (keep folders)'. This will attempt to keep the folder structure using the folder you are pasting into as the root folder.

<img loading="lazy" src="/img/user-manual/editor/assets-panel/right-click-paste-keep-folders.png" alt="Right click paste (keep folders) menu" width="500" />

Will result in the following where the folder structure is preserved:

<img loading="lazy" src="/img/user-manual/editor/assets-panel/pasted-assets-keep-folders.png" alt="Pasted referenced assets with folders" width="100%" />

We generally recommend that if you will be using this feature for reusable libraries and assets, to keep it contained to a root level folder that can be easily copied and pasted to other projects. This will keep the folder structure of projects simpler and cleaner.

:::note

Note that copy and pasting assets does not overwrite existing assets with the same name and will create a new asset.

:::

## Checking References {#checking-references}

Sometimes it's useful to know where assets are being used (or referenced) within a particular scene. If the Editor cannot detect any references for an asset, a small dot will be displayed on its thumbnail:

[Image: Unreferenced Asset]

:::warning

Note that the Editor cannot detect references to assets that are made in code. So think carefully before you delete an asset based on this indicator!

:::

If an asset does have references, you can check them via the References content menu item:

[Image: Asset References]

Selecting a reference will load it into the Inspector panel.

--------------------------------------------------------------------------------

## Asset Store

URL: https://developer.playcanvas.com/user-manual/editor/assets/asset-store/

The PlayCanvas Asset Store is a library of free assets that you can use in your projects. It contains 3D models, fonts, scripts and more.

## Accessing the Asset Store

You can access the Asset Store from within the Editor by clicking the ASSET STORE button in the top-right of the [Assets Panel](../../interface/assets.md).

[Image: Asset Store Button]

The Asset Store will then open:

[Image: Asset Store]

## Selecting a Store

There are three top-level Stores to choose from:

| Store | Description |
| ----- | ----------- |
| PLAYCANVAS | Assets curated and supplied by PlayCanvas and selected partner creators. This store contains: 3D models, fonts, scripts, sky boxes, templates and textures. |
| SKETCHFAB  | Assets curated and supplied by [Sketchfab](https://sketchfab.com/). This store contains 3D models only. [Learn more](sketchfab) about the Sketchfab Store. |
| MY ASSETS  | Assets imported by you and stored in your user account. This store is currently in a closed beta. |

## Searching the Store

To search the Store, simply enter a search term in the Search bar at the top. You can then filter and sort the search results using the Search Options:

[Image: Asset Store Search]

## Importing Assets into your Project

Select any store item in the search results to open its details page:

[Image: Asset Store Details]

On the details page you can:

* Read a description of the store item.
* Inspect the files included in the store item.
* View statistics related to the store item (file size, last updated date, etc).
* Preview the item in the Model Viewer (only applies to 3D model assets).
* View author and license information.

:::note

Please respect the license terms for the store items you use in your projects. For example, the [Oldsmobile Cutlass Supreme Sedan '71](https://sketchfab.com/3d-models/oldsmobile-cutlass-supreme-sedan-71-78f76d386a4341b0b71745bdc50fd5ab) in the screenshot above was authored by [Barbo](https://sketchfab.com/barbo-autos) and has CC BY 4.0 license. This means that you must give the author appropriate credit for using their works.

:::

--------------------------------------------------------------------------------

## Sketchfab

URL: https://developer.playcanvas.com/user-manual/editor/assets/asset-store/sketchfab/

The Asset Store is integrated with [Sketchfab](https://sketchfab.com/), the popular online platform for publishing, sharing, and discovering 3D, VR, and AR content. This integration gives you access to thousands of free, high-quality 3D models without having to leave the Editor.

[Image: Sketchfab Store]

## Logging in to Sketchfab

The first time you attempt to import a model from Sketchfab, you will be asked to either sign up or log in to Sketchfab:

[Image: Sketchfab Authentication]

This will take you to `sketchfab.com` to authenticate. Once you are signed in to Sketchfab, you need to accept Sketchfab's authorization request:

[Image: Sketchfab Authorization]

You should then receive a message that you have successfully connected your Sketchfab account to PlayCanvas! 🎉

--------------------------------------------------------------------------------

## Asset Import Pipeline

URL: https://developer.playcanvas.com/user-manual/editor/assets/import-pipeline/

Some assets are uploaded in source format and need to be converted into a "target" format before they can be used in a game at runtime. This process is called *Importing*. For example, a 3D model can be uploaded as an FBX file, but must be converted into a PlayCanvas compatible model file before it can be loaded in the game.

Some assets don't need to be imported before they can be used. For example a PNG image can be used as a texture immediately.

## Asset Tasks {#asset-tasks}

When a source asset is uploaded, PlayCanvas starts an Asset Task to perform this import process on our server.

There are a variety of options available to tune the behavior of the import pipeline to suit your needs.

<img loading="lazy" src="/img/user-manual/assets/import-pipeline/asset-tasks.png" width="480" />

### Search related assets {#search-related-assets}

When you update an source asset by uploading a new version of the file. There are two possible behaviors for how we update the target assets that are created by the import pipeline.

* If **Search related assets** is enabled, the pipeline will update target assets no matter what folder they are located in.
* If **Search related assets** is not enabled, the pipeline will only look for the target assets in the same folder as the source asset.

So, if you leave this enabled, you are able to organize your source and target assets into folders and be sure that when you update a source assets it will update all related assets.

### Assets default to preload {#assets-default-to-preload}

Newly created assets will automatically be set to [preload](/user-manual/assets/preloading/) or not depending on whether this option is enabled or not. The exception to this are JavaScript script files which will always be set to preloaded when created.

## Texture Import Settings {#texture-import-settings}

These options only affect the importing of images and textures.

### Texture POT (Power of Two) {#texture-pot-power-of-two}

When this option is enabled textures that are not a power of two will be converted to the nearest power of two resolution when they are imported.

### Create Atlases {#create-atlases}

Images that are uploaded will be imported as a texture atlas instead of a normal texture asset. This is a useful time saver when uploading many spritesheets or UI assets.

## Model Import Settings {#model-import-settings}

These options only affect the importing of model or scene files (e.g. FBX, Collada, obj, etc)

### Preserve material mappings {#preserve-material-mappings}

When a model file is updated or reimported, the Editor will try to preserve the material mappings that were set on it.

### Overwrite Models {#overwrite-models}

When a model file is updated or reimported this option determines whether or not the target model file is overwritten. The default behavior is to overwrite with the new model.

### Overwrite Animations {#overwrite-animations}

When a model file is updated or reimported this option determines whether or not a animations created from the model are overwritten. The default behavior is to overwrite with the new animations.

### Overwrite Materials {#overwrite-materials}

When a model file is updated or reimported this option determines whether or not materials created from the model are overwritten. The default behavior is to leave existing materials.

### Overwrite Textures {#overwrite-textures}

When a model file is updated or reimported this option determines whether or not textures created from the model are overwritten. The default behavior is to overwrite with the new textures.

### Convert to GLB {#convert-to-glb}

Enabled by default on new projects, imported models and animations will create GLB model and animation assets instead of the older, deprecated JSON format.

### Import Hierarchy {#import-hierarchy}

Only available if using [Convert to GLB](#convert-to-glb) option. When a model file is imported, a template asset is created that contains the full hierarchy of the model as entities allowing to you to manipulate them directly in the Editor. See more information about this feature [here](/user-manual/editor/assets/import-pipeline/import-hierarchy/).

### Mesh Compression {#mesh-compression}

Only available if using [Convert to GLB](#convert-to-glb) option. Setting this to a compression format will automatically compress mesh data when importing or re-importing model files. This can drastically reduce the size of GLB files at the cost of some runtime decompression cost.

If using Draco compression, remember to import the Draco WASM module into the project otherwise the models will not load.

<img loading="lazy" src="/img/user-manual/assets/import-pipeline/draco-import-button.png" width="480" />

### Create FBX Folder {#create-fbx-folder}

When importing a model file (e.g a GLB or FBX), the Editor will create a folder for the assets created by the import such as render, template and material assets.

If there is already a Model (Source) file in the current folder or a folder with the same name as the file being imported, it will overwrite the existing assets instead of creating a new folder.

## Animation Import Settings {#animation-import-settings}

Please refer to the [Animation section](/user-manual/editor/assets/inspectors/animation/) for more details.

--------------------------------------------------------------------------------

## Import Hierarchy

URL: https://developer.playcanvas.com/user-manual/editor/assets/import-pipeline/import-hierarchy/

PlayCanvas supports importing models with their meshes as a hierarchy of entities in the scene. This allows you to edit the different meshes directly in the Editor. Components can be added, attach other entities, move/rotate/scale entities in the hierarchy etc.

[Image: Edit model hierarchy in the Editor]

## How to enable

:::note

This is now enabled by default for new projects.

:::

Open the 'Project Settings'

<img loading="lazy" src="/img/user-manual/assets/import-pipeline/import-hierarchy/project-settings.png" width="480" />

Scroll down to 'Asset Tasks' and enable 'Import Hierarchy':

<img loading="lazy" src="/img/user-manual/assets/import-pipeline/import-hierarchy/asset-tasks.png" width="360" />

## Importing models

A full tutorial on importing your first model and animation can be found [here](/tutorials/importing-first-model-and-animation/).

As an overview, when you drag the model file into the 'Assets Panel':

[Image: Drag Model into Assets Panel]

The following assets will be created when imported:

[Image: Created Assets]

| Asset Type | Description |
|------------|-------------|
| **[Materials](/user-manual/editor/assets/inspectors/material/)** | Materials used by the imported model, mapped to the mesh instance. |
| **[Textures](/user-manual/editor/assets/inspectors/texture/)** | Embedded textures in the model file (if any). These will automatically be mapped to the associated materials. |
| **[Template](/user-manual/editor/templates/)** | The template stores the scene hierarchy of the model. Create an instance of the template to bring the model into the scene. |
| **Container** | The GLB that stores all the meshes of the model. |
| **Render** | Render assets reference a mesh in the container asset and are used by the [Render Component](https://api.playcanvas.com/engine/classes/RenderComponent.html) to render the mesh in the scene. They can also be used with the [Collision Component](https://api.playcanvas.com/engine/classes/CollisionComponent.html) as a mesh for physics. |

## Updating models

As the hierarchy is created as part of a template, when a model is updated it may affect the template instances in the scenes.

Added components to entities in the template will be kept during the update unless the mesh instance that the entity represents no longer exists in the updated model.

The exception to this is if a script component is added to an entity in the template and the mesh instance it represents is no longer part of the updated model, it will be moved under the root entity in the template so there is no data loss. Any entities that were under it before the update will be kept as well.

How the Editor decides what is a new or removed mesh instance is done by the following:

- If in the update, a mesh instance's name and its parent mesh's instance name matches an existing mesh instance and its parent, they are assumed to be the same mesh instance in the hierarchy and is updated.
- If in the update, a mesh instance's name does not exist, it's assumed to be a new mesh instance and a new entity is added to the template.
- If in the update, an existing entity of the template does not have a matching mesh instance given the rules above, it's assumed that this mesh instance has been removed and the entity will be removed from the template. The exception being mentioned above, if there was a script component added to it on the template, those entities are preserved on the root. If there were no script components added, those are deleted from the template.

--------------------------------------------------------------------------------

## Uploading and Importing

URL: https://developer.playcanvas.com/user-manual/editor/assets/importing/

## Creating New Assets

To upload an asset to PlayCanvas, follow these steps:

### In the Editor

* Drag and drop your asset file into the Asset panel in the Editor window.
* A progress bar will appear along the bottom.
* When the progress bar disappears, your new Asset will appear in the Asset panel.

Now, when you attempt to choose an asset via the asset picker attribute control (for the Model component or the Animation component for example), your asset will be available for assignment.

:::note

There is a file size limit of 340MB.

:::

## Updating Existing Assets

To update an Asset that you have already uploaded. Upload the asset again, ensuring that you have the same filename as the existing asset. PlayCanvas will update the existing asset with the new upload.

## Migrating from JSON to glTF GLB

As of Wed 14 Oct 2020, PlayCanvas will be creating GLBs for model and animation imports instead of JSON. This gives a large reduction in parsing times which means lower load times and more responsive applications.

For projects created prior to this date, they will still create JSON assets by default for consistency.

If you would like to migrate assets to the GLB format, please go to 'Project Settings'.

<img loading="lazy" src="/img/user-manual/assets/importing/project-settings.png" alt="Project settings" width="300" />

Open 'Asset Tasks' and tick 'Convert to GLB'.

<img loading="lazy" src="/img/user-manual/assets/importing/asset-tasks.png" alt="Asset tasks settings" width="400" />

And finally reimport the model and/or animation file (via drag and drop or the 'Upload' menu option) to create the GLB asset.

<img loading="lazy" src="/img/user-manual/assets/importing/drag-and-drop.gif" alt="Drag and drop file" />

Once created, this can be referenced to Entities in place of the existing JSON asset.

--------------------------------------------------------------------------------

## Asset Inspectors

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/

When you select an asset in the [Assets Panel](/user-manual/editor/assets/asset-panel), its properties are displayed in the Inspector. Each asset type has its own set of configurable properties.

## Common Properties

All asset inspectors display a common set of properties in the header section:

[Image: Common Asset Inspector Properties]

| Property | Description |
|----------|-------------|
| ID | The unique identifier for the asset. Useful for referencing the asset in scripts. |
| Name | The display name of the asset. This can be edited to rename the asset. |
| Tags | Tags assigned to the asset for organization and filtering, both in the [Assets Panel](/user-manual/editor/assets/asset-panel#searching) and at runtime via the [Engine API](/user-manual/assets/asset-registry#by-tag). |
| Type | The [asset type](#asset-types) (read-only). |
| Exclude | When enabled, the asset is excluded from the published build. Useful for development-only assets such as test scripts or READMEs. |
| Preload | When enabled, the asset is loaded at application startup. When disabled, referenced assets load asynchronously after the app starts, while unreferenced assets must be loaded manually via script. |
| Size | The file size of the asset (read-only). |
| Source | A reference to the source asset from which this asset was derived, if applicable (read-only). |
| Created | The date and time the asset was created (read-only). |

### Script Assets

Script assets display additional properties:

[Image: Script Asset Inspector Properties]

| Property | Description |
|----------|-------------|
| Loading Order | Opens the [script loading order manager](/user-manual/editor/scripting/loading-order) to control the order scripts are loaded. |
| Loading Type | Controls when the script is loaded: <ul><li>**Asset** - loaded as a regular asset</li><li>**Before Engine** - loaded before the PlayCanvas engine</li><li>**After Engine** - loaded after the engine but before application start</li></ul> |

### Asset Store Assets

Assets imported from the [Asset Store](/user-manual/editor/assets/asset-store) display additional attribution properties:

[Image: Asset Store Inspector Properties]

| Property | Description |
|----------|-------------|
| License | The license under which the asset is provided, with a link to license details. |
| Author | The original author of the asset, with a link to their profile. |

## Asset Types

| Type                             | Imported From                    | Resource Extensions              | Description                        |
| -------------------------------- | -------------------------------- | -------------------------------- | ---------------------------------- |
| [`animation`](animation)         | `.glb`, `.fbx`                   | `.glb`                           | Animation keyframe data            |
| [`audio`](audio)                 | `.mp3`, `.wav`, `.ogg`           | `.mp3`, `.wav`, `.ogg`           | Sound data                         |
| `binary`                         | `.bin`                           | `.bin`                           | Binary data                        |
| `bundle`                         | Created in the Editor            | `.tar`                           | Bundled assets                     |
| [`css`](css)                     | `.css`                           | `.css`                           | Stylesheets for HTML               |
| [`cubemap`](cubemap)             | `.png`, `.jpg`, `.webp`, `.avif` | `.png`, `.jpg`, `.webp`, `.avif` | Environment lighting data          |
| [`font`](font)                   | `.ttf`, `.woff`                  | `.json`, `.png`                  | Font data for rendering text       |
| [`gsplat`](gsplat)               | `.ply`                           | `.ply`                           | 3D Gaussian Splat data             |
| [`html`](html)                   | `.html`                          | `.html`                          | HTML documents                     |
| [`json`](json)                   | `.json`                          | `.json`                          | JSON documents                     |
| [`material`](material)           | `.glb`, `.fbx`                   | None                             | Material definitions for 3D models |
| [`render`](render)               | `.glb`, `.fbx`                   | `.glb`                           | 3D mesh data                       |
| [`script`](../../scripting/index.md) | `.js`, `.mjs`                | `.js`, `.mjs`                    | Scripts                            |
| [`shader`](shader)               | `.glsl`, `.vert`, `.frag`        | `.glsl`, `.vert`, `.frag`        | Custom shaders for rendering       |
| [`sprite`](sprite)               | Created in the Editor            | None                             | 2D images for UIs or textures      |
| [`template`](template)           | `.glb`                           | None                             | Templates for entity hierarchy     |
| [`text`](text)                   | `.txt`                           | `.txt`                           | Text documents                     |
| [`texture-atlas`](texture-atlas) | `.png`, `.jpg`, `.webp`, `.avif` | `.png`, `.jpg`, `.webp`, `.avif` | Sprite sheet image data            |
| [`texture`](texture)             | `.png`, `.jpg`, `.webp`, `.avif` | `.png`, `.jpg`, `.webp`, `.avif` | Image data for 3D models or UIs    |
| [`wasm`](wasm)                   | `.wasm`                          | `.wasm`                          | WebAssembly modules                |

--------------------------------------------------------------------------------

## Animation

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/animation/

An Animation asset is used to play a single animation on a 3D model. Animations are imported by uploading 3D scenes (such as FBX files) which contain animation data. The animation data is extracted from the uploaded file by the [asset pipeline](/user-manual/glossary#asset-pipeline) and a [Target Asset](/user-manual/glossary#target-asset) is created to use in game.

## Animation Preview

The inspector has a viewer that can be used to preview the animation with any Template or Model asset. This is useful if you have a single animation that can be applied to different character rigs.

[Image: Asset Inspector Preview]

There is also a viewer in the [Anim State Graph Editor](/user-manual/animation/anim-state-graph-assets/) when you select a state in the graph. When selected, all Entities in the scene that are using the graph will show in a list and can be selected for preview.

[Image: Anim State Graph Preview]

## Animation Import Settings

:::warning

This is an experimental feature. Please let us know your feedback in the [forums](https://forum.playcanvas.com/).

:::

When importing animations, there are settings that can be tweaked to adjust the animation quality against the file size.

They can be found in the Project Settings under Asset Tasks.

[Image: Animation Import Settings]

### Naming Strategy

Only available for GLB export format. When importing an animation, the generated asset name can either be set from the 'Take Name' in the animation file, or use the animation filename instead.

This is useful with assets that are brought/taken from a store such as [Mixamo](https://www.mixamo.com/) where all the take names are 'mixamo.com' and using the filename as the asset name is clearer.

### Sample rate

Available for both JSON and GLB export formats. The higher the rate, the higher detail and fidelity the animation at the cost of size. If you would like to keep the keyframes that have been set and defined in the original animation, select Disabled.

### Curve tolerance

Available for both JSON and GLB export formats. Curve tolerance controls a lossy compression setting of the animation import with the idea that a saving in file size can be made with little or no noticeable difference.

This is a value between 0 and 100 where the higher number, the smaller the file size but at cost of losing information in the animation. 0 would be no compression and 100 would lose all information.

1 or 2 is considered to be good starting point.

### Cubic curves

Only available for GLB export formats. Enable this option if you wish to keep the easing in the animation curves from the original animation. However, this will mean that the file will have extra information per keyframe and increase the size.

If enabling this option, it is recommended that Sample Rate is disabled and Curve Tolerance is set to 0.

--------------------------------------------------------------------------------

## Audio

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/audio/

Audio assets are sound files which can be played back using the [Sound Component](/user-manual/editor/scenes/components/sound/).

Any audio format that is supported by the web browser is supported by PlayCanvas. No processing is done to the audio file on import.

Common formats like MP3, AAC, Ogg Vorbis, and WAV are supported across all modern browsers.

:::tip
MP3 is recommended as it offers good compression and [universal compatibility](https://caniuse.com/mp3).
:::

## Inspector

[Image: Audio Asset Inspector]

The Audio asset inspector displays audio information and provides playback controls to preview the sound.

## Properties

| Property | Description |
|----------|-------------|
| Duration | The length of the audio file in seconds (read-only). |

The inspector also includes playback controls (play/pause button and timeline) to preview the audio.

:::tip
To use this asset in scripts, see [Asset Attributes](/user-manual/scripting/script-attributes/esm/#asset-attribute).
:::

--------------------------------------------------------------------------------

## CSS

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/css/

A CSS asset contains CSS code. You can create a new CSS asset in the Editor or by uploading a file with a `.css` extension.

To edit a CSS asset, right click on it in the Editor and select Edit.

## Inspector

[Image: CSS Asset Inspector]

The CSS asset inspector displays a preview of the CSS code contained in the asset.

## Properties

This asset type has no configurable properties in the Inspector. It displays a read-only preview of the CSS code.

:::tip
To use this asset in scripts, see [Asset Attributes](/user-manual/scripting/script-attributes/esm/#asset-attribute).
:::

--------------------------------------------------------------------------------

## Cubemap

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/cubemap/

Cubemaps are a special type of texture asset. They are formed from 6 texture assets where each texture represents the face of a cube. They typically have two uses:

1. A cubemap can define your scene's sky box. A sky box contains imagery of the distant visuals of your scene such as hills, mountains, the sky and so on.
2. A cubemap can add reflections to any material. Imagine a shiny, chrome ball bearing in your scene. The ball reflects the surrounding scene. For open environments, you would normally set the scene's sky box cubemap as the cubemap on a reflective object's materials.

[Interactive Demo]

## Importing Cubemap Textures

A cubemap is an asset that requires six texture assets as input. Therefore, in order to fully configure a new cubemap asset, you must first import 6 images into your project. To do this, simply drag 6 images from your file system into the Assets panel (or select the Asset panel's Upload option). Once uploaded and processed, the images will appear in the Assets panel where they are now ready to be assigned to a cubemap asset.

## Creating Cubemaps

You can create new cubemap assets directly from the PlayCanvas Editor interface. Use the Create Asset menu in the Asset panel.

[Image: Cubemap Creation]

This creates a new cubemap Asset and opens up the Cubemap Editor on the right-hand side of the screen.

## Selecting Cubemaps

To select a cubemap in order to edit it, select it in the Asset Panel. The easiest way to do this is to select the cubemap filter to narrow down the options for selection. Cubemaps are identified by cross-shaped thumbnails:

[Image: Cubemap Thumbnails]

When a cubemap is selected, it will be loaded into the Inspector panel on the right of the Editor.

## Cubemap Properties

Once you have a cubemap selected, you can edit its properties.

[Image: Cubemap Properties]

### Filtering

This setting determines how the pixels of the cubemaps are interpolated as they are magnified. Magnification is when the texel to screen pixel ratio is less than one. Linear gives the best results visually, followed by Nearest.

### Anisotropy

Anisotropy is a value between 1 and 16 that gives control over the quality of texture sampling as the camera's view vector becomes more closely aligned with the plane of a textured surface.

## Assigning Textures to Cubemaps

[Image: Cubemap Preview]

The cubemap Preview panel displays the six faces of a cubemap flattened into the shape of a cross. Imagine a cardboard box that has been unfolded to lay flat. To construct a cubemap, simply drag texture assets from the Assets panel to the face slots in the Preview panel. You can also select a cubemap face slot and then select a texture asset from the Assets panel.

Cubemap faces must be:

- Square (the same resolution in width and height)
- Power of two in dimension (1x1, 2x2, 4x4, 8x8, 16x16, 32x32 and so on)
- All faces must be the same resolution

To assist you, the Editor attempts to figure out how to auto-assign textures to faces intelligently. It does this when you drag the first face to a slot by trying to match commonly used naming conventions for cubemap faces, such as:

- negx, posx, negy, posy, negz, posz
- left, right, top|up, bottom|down, front|forward, back|backward
- 0-5|1-6

An example of a texture set that would match is:

- face_posx.jpg
- face_negx.jpg
- face_posy.jpg
- face_negy.jpg
- face_posz.jpg
- face_negz.jpg

## Image Based Lighting

This technique allows you to use Environment Map such as CubeMap in order to simulate physically based ambient light and reflection on materials. [Read more](/user-manual/graphics/physical-rendering/image-based-lighting/) on how it works and how to author CubeMaps for IBL.

## Assigning Cubemaps to Materials

The default Phong and Physical material types both have reflection properties. If you expand the Environment property section, you see the following:

[Image: Cubemap Material]

You can click the Empty slot to select a cubemap or drag and drop a cubemap asset from the asset panel into the cubemap slot.

:::note

A Physical material will use the scene's skybox as a default environment map if it is assigned and prefiltered.

:::

## Converting Equirectangular or Octahedral HDRIs to Cubemaps

Environment textures often are in a equirectangular or Octahedral format ([Poly Haven](https://polyhaven.com/hdris) for example) and will need to be converted to cubemaps before they can be used in PlayCanvas.

This can be done via [PlayCanvas Texture Tool](https://playcanvas.com/texture-tool), available in the browser.

1. Download the HDR version of environment texture and press 'Add Files' button in PlayCanvas Texture Tool to load the file.
2. Select the loaded texture on the left.
3. Under 'Reproject' section, change the 'source' to the format of texture.
4. Change 'target' to 'cube'.
5. Change 'encoding' to the desired format:
    - 'rgbe' for exporting to 'HDR'
    - 'rgbm' for exporting to 'PNG'
6. Set the width to the desired size per face texture. 512 is a good balance between quality and file size.
7. Press 'Reproject' button to do the conversion to a cubemap.
8. Press 'Export to PNG' or 'Export to HDR' to download the 6 individual cubemap face textures that are ready to be uploaded to PlayCanvas.

[Image: PlayCanvas Texture Tool]

Other tools that can also do this conversion include:

- Mateusz Wisniowski's [HDRI-to-CubeMap tool](https://matheowis.github.io/HDRI-to-CubeMap/) (browser)
- [cmftStudio](https://github.com/dariomanesku/cmftStudio) (desktop) with [guide](https://jamie-white.com/webgl/equirectangular-hdr-image-to-face-list/)

--------------------------------------------------------------------------------

## Font

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/font/

A Font asset contains an image with all the characters of the font that the user chose to include, and data related to how each character should be displayed. Font assets are used to render text using an [Element](/user-manual/editor/scenes/components/element/) component of type Text. To render text, add an Element Component to an Entity set its type to Text and drag and drop the Font asset to the Font slot of the Element Component.

You can create a Font asset by uploading a font file ending in `.ttf`, `.ttc`, `.otf` or `.dfont`. We convert the uploaded font to a multi-channel signed distance field. This makes a font keep its details at various sizes so you only need to upload one font and use the same one for every size you want to display. The technique might work better for some fonts than others.

Here are the properties that you can edit for a Font asset in the Editor:

[Image: Font Asset]

## Properties

### Intensity

Intensity is used to boost the value read from the signed distance field, 0 is no boost, 1 is max boost. This can be useful if the font does not render with clean smooth edges with the default intensity or if you are rendering the font at small font sizes.

## Character Presets

Click on a preset to include its characters to the Font asset. Clicking on a preset will add the characters to the existing selection.

## Custom Character Range

If you want to include a specific range of characters to the Font asset, enter the range in Hex and click the Plus icon to add the range to the character selection.

## Font

### Characters

The characters that should be included in the Font asset. If you only need numbers for example, there is no need to include all the other characters of the Font. This is useful to keep Font assets small. Please note that some characters may not exist in the uploaded font.

## Process Font

Click on the Process Font button after you make changes to the Font asset in order to generate a new version.

--------------------------------------------------------------------------------

## GSplat

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/gsplat/

A GSplat asset contains 3D Gaussian Splat data. PlayCanvas supports the import of GSplat data from [PLY](/user-manual/gaussian-splatting/formats/ply) files (including compressed PLY) and [SOG](/user-manual/gaussian-splatting/formats/sog) files.

:::tip
The [SOG format](/user-manual/gaussian-splatting/formats/sog) is recommended for its efficient compression, resulting in smaller file sizes and faster load times.
:::

## Inspector

You can select a GSplat asset in the [Assets Panel](/user-manual/editor/interface/assets) and view it in the [Inspector](/user-manual/editor/interface/inspector).

[Image: GSplat Asset Inspector]

## Properties

The META section lists the key properties of the GSplat data.

| Property | Description |
|----------|-------------|
| Format | The file format: PLY (`binary_little_endian 1.0`), Compressed PLY, or SOG (read-only). |
| Splats | The total number of Gaussians stored in the PLY file (read-only). |
| SH Bands | The number of spherical harmonics bands used for view-dependent color (read-only). |
| Bound Min | The minimum bounds of the Gaussian splat data in 3D space (read-only). |
| Bound Max | The maximum bounds of the Gaussian splat data in 3D space (read-only). |

:::tip
To use this asset in scripts, see [Asset Attributes](/user-manual/scripting/script-attributes/esm/#asset-attribute).
:::

--------------------------------------------------------------------------------

## HTML

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/html/

An HTML asset contains HTML code. The code can either be a full HTML page or just partial HTML. You can create a new HTML asset in the Editor or by uploading a file with an `.html` extension.

To edit an HTML asset, right click on it in the Editor and select Edit.

## Inspector

[Image: HTML Asset Inspector]

The HTML asset inspector displays a preview of the HTML code contained in the asset.

## Properties

This asset type has no configurable properties in the Inspector. It displays a read-only preview of the HTML code.

:::tip
To use this asset in scripts, see [Asset Attributes](/user-manual/scripting/script-attributes/esm/#asset-attribute).
:::

--------------------------------------------------------------------------------

## JSON

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/json/

[JSON](https://en.wikipedia.org/wiki/JSON) (JavaScript Object Notation) is a lightweight data-interchange format that is easy for humans to read and write and easy for machines to parse and generate.

In PlayCanvas, JSON assets are used to store various types of structured data. Some example use cases are:

- Configuration files
- Data for procedural generation
- Storing game settings
- Level design data

## Inspector

[Image: JSON Asset Inspector]

The JSON asset inspector displays a formatted preview of the JSON data contained in the asset.

## Properties

This asset type has no configurable properties in the Inspector. It displays a read-only preview of the JSON data with syntax formatting.

:::tip
To use this asset in scripts, see [Asset Attributes](/user-manual/scripting/script-attributes/esm/#asset-attribute).
:::

--------------------------------------------------------------------------------

## Material

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/material/

Every surface on a 3D model is rendered using a material. The material defines the properties of that surface, such as its color, shininess, bumpiness.

In PlayCanvas, a material is an Asset type which collects all these properties together. By default, it represents a Physical material. This exposes the fundamental properties that can be used to create many different types of visual effects, from smooth plastic, to rough wood, or scratched metal.

We also support our old Phong Material type.

## Importing Materials {#importing-materials}

Materials are imported automatically when you upload a 3D model (e.g. FBX or COLLADA) file into PlayCanvas. Materials will be generated with the same properties as they exist in your 3D modelling tool. If you upload using embedded media (FBX only) all the relevant texture maps will be automatically set up for you.

## Creating New Materials {#creating-new-materials}

You can create new materials directly from the PlayCanvas Editor interface.

[Image: Create Material]

This creates a new material Asset and opens up the material inspector on the right-hand side of the screen.

## Selecting a Material {#selecting-a-material}

[Image: Model Inspector]

In order to edit a material, first you must select it. This will bring up the material inspector.

You can select a material in the asset panel. You can also select materials from the model inspector or from the Model Component.

Generally, clicking on a material preview icon will take you to the material inspector.

## Assigning Materials {#assigning-materials}

[Image: Model Component]

You can modify which materials are assigned to where on a model asset or you can customize the materials of a particular Entity that has a Model Component.

When you select an Entity with a Model Component you will see two buttons - Asset Materials and Entity Materials.

[Image: Model Inspector Free Slot]

Clicking on Asset Materials will select the model asset. You can also select the model asset from the asset panel. The model inspector will show the meshes of model and which material is assigned to each. You can clear a material using the X button, and click the empty slot to assign a new material.

You can also drag and drop material Assets from the asset panel onto the material slot.

Clicking on Entity Materials will first ask you to select the mesh instance for which you want to customize the material:

[Image: Select Mesh Instance]

After selecting the mesh instance a new material picker will appear in the Model Component:

[Image: Selected Mesh Instance]

Then you can select a different material for this particular Entity:

[Image: Select Different Material]

## Editing a Material {#editing-a-material}

[Image: Material Inspector]

Once you have a material selected you can edit its properties.

## Material Maps {#material-maps}

[Image: Material Map Slot]

Much of editing a material involves creating and assigning textures maps to the various slots detailed on the pages above.

There are a few options that are available on most texture map slots for a material.

### Texture Asset {#texture-asset}

First is the texture asset. Upload an image to PlayCanvas and we'll create a texture asset for you. You can assign this to a slot on a material.

### Color or Tint Color {#color-or-tint-color}

Some map slots can be a flat color instead of a texture map. Some slots also support a tint color if a texture is also assigned. If enabled the tint color is multiplied by the color in the texture map slot.

### Channel {#channel}

Some maps only require a single grayscale value e.g. 0.0 -> 1.0. In this case it is possible to select which channel of the texture to use. **RGB** means that all three channels are used. **R**, **G** or **B** means that only the red, green or blue channel will be used.

### Offset & Tiling

<img loading="lazy" src="/img/user-manual/assets/types/material/offset-tiling.jpg" width="300" />

| Property          | Description |
|-------------------|-------------|
| Apply to all Maps | Uncheck this to apply offset and tiling values to individual maps. |
| Offset            | The offset in U and V to apply to the first UV channel referenced by maps in this material. |
| Tiling            | The scale in U and V to apply to the first UV channel referenced by maps in this material. |

### Ambient

Ambient properties determine how the material appears in ambient light.

<img loading="lazy" src="/img/user-manual/assets/types/material/ambient.jpg" width="300" />

| Property   | Description |
|------------|-------------|
| Tint       | Check this to multiply the scene's global ambient color with a material specific color. |
| Color      | The tint color to multiply the scene's global ambient color. |
| AO Texture | An ambient occlusion map containing pre-baked ambient occlusion. |

### Diffuse

Diffuse properties define how a material reflects diffuse light emitted by dynamic light sources in the scene.

<img loading="lazy" src="/img/user-manual/assets/types/material/diffuse.jpg" width="300" />

| Property   | Description |
|------------|-------------|
| Texture    | The diffuse map that specifies the per-pixel diffuse material color. If no diffuse map is set, the diffuse color is used instead. |
| Tint       | Check this to modulate the material's diffuse map with a material specific diffuse color. |
| Color      | If no diffuse map is set, this is the diffuse color of the material. If a diffuse map is set and tint is enabled, this color modulates the material's diffuse map. |

### Specular

Specular properties define the color of the specular highlights, i.e. the shininess.

<img loading="lazy" src="/img/user-manual/assets/types/material/specular.jpg" width="300" />

| Property      | Description |
|---------------|-------------|
| Use Metalness | Toggle between specular and metalness workflow. |
| Specular Map  | The specular map that specifies the per-pixel specular color. If no specular map is set, the specular color is used instead. |
| Tint          | Check this to modulate the material's specular map with a material specific specular color. |
| Color         | If no specular map is set, this is the specular color of the material. If a specular map is set and tint is enabled, this color modulates the material's specular map. |
| Metalness Map | [Only when using metalness] This map specifies per-pixel metalness values. A value of 1 is metal and a value of 0 is non-metal. |
| Gloss Map     | The gloss map that specifies a per-pixel shininess value. The gloss map is modulated by the shininess property. |
| Glossiness    | A value determining the smoothness of a surface. For smaller shininess values, a surface is rougher and specular highlights will be broader. For larger shininess values, a surface is smoother and will exhibit more concentrated specular highlights (as the surface is polished and shiny). |

### Emissive

Emissive properties control how the material emits light (as opposed to reflecting light).

<img loading="lazy" src="/img/user-manual/assets/types/material/emissive.jpg" width="300" />

| Property   | Description |
|------------|-------------|
| Texture    | The emissive map that specifies the per-pixel emissive color. If no emissive map is set, the emissive color is used instead. |
| Tint       | Check this to modulate the material's emissive map with a material specific emissive color. |
| Color      | If no emissive map is set, this is the emissive color of the material. If an emissive map is set and tint is enabled, this color modulates the material's emissive map. |
| Intensity  | A multiplier for emissive color that can achieve overbright effects for exceptionally bright emissive materials. |

### Opacity

Opacity sets the transparency level.

<img loading="lazy" src="/img/user-manual/assets/types/material/opacity.jpg" width="300" />

| Property   | Description |
|------------|-------------|
| Texture    | The opacity map that specifies the per-pixel opacity. The opacity map is modulated by the 'Amount' property. |
| Intensity  | The opacity of the material. This is a value between 0 (completely transparent) and 1 (completely opaque. It defaults to 1. |

### Normals

Use this to specify normal maps (these determine bumpiness - note you have to use normal maps in PlayCanvas, not height maps).

<img loading="lazy" src="/img/user-manual/assets/types/material/normals.jpg" width="300" />

| Property   | Description |
|------------|-------------|
| Bumpiness  | The strength of the applied normal map. This is a value between 0 (the normal map has no effect) and 2 (the effect of the normal map is exaggerated). It defaults to 1. |
| Texture    | The normal map that specifies the per-pixel surface normals. The normal map is modulated by the 'Bumpiness' property. |

### Parallax

A parallax map gives further realism to a normal map by giving the illusion of depth to a surface. Note that parallax options are only enabled if you have set a normal map on the material.

<img loading="lazy" src="/img/user-manual/assets/types/material/parallax.jpg" width="300" />

| Property    | Description |
|-------------|-------------|
| Height Map  | The height map that specifies the per-pixel strength of the parallax effect. White is full height and black is zero height. |
| Strength    | The strength of a parallax effect (a value between 0 and 2, defaulting to 1). |

### Environment

Environment properties determine how a material reflects the environment.

<img loading="lazy" src="/img/user-manual/assets/types/material/environment.jpg" width="300" />

| Property            | Description |
|---------------------|-------------|
| Sphere Map          | A sphere map texture asset that approximates environment reflection. If a sphere map is set, the Cube Map property will be hidden (since these properties are mutually exclusive). |
| Cube Map            | A cube map texture asset that approximates environment reflection (with greater accuracy than is possible with a sphere map). If a cube map is set, the Sphere Map property will be hidden (since these properties are mutually exclusive). |
| Reflectivity        | A factor to determine what portion of light is reflected from the material. This value defaults to 1 (full reflectivity). |
| Refraction          | A factor to determine what portion of light passes through the material. |
| Index of Refraction | Determines the amount of distortion of light passing through the material. |

### Light Map

Light maps contain pre-baked diffuse lighting. Using light maps is considered an optimization in that runtime dynamic lighting calculations can be pre-calculated.

<img loading="lazy" src="/img/user-manual/assets/types/material/lightmap.jpg" width="300" />

| Property   | Description |
|------------|-------------|
| Texture    | The lightmap texture that contains pre-baked diffuse lighting. The lightmap requires the material to be applied to a mesh that has two UV sets. The lightmap uses the second set of UVs. |

### Other Render States

Other Render States gives additional controls over how a mesh is rendered with the specified material.

<img loading="lazy" src="/img/user-manual/assets/types/material/other.jpg" width="300" />

| Property        | Description |
|-----------------|-------------|
| Depth Test      | If checked, when a mesh with the material is rendered, a per pixel check is performed to determine if the pixel passes the engine's depth test. By default, the test is that the pixel must have a z depth less than or equal to whatever is already in the depth buffer. In other words, the mesh is only visible if nothing is in front of it. If unchecked, the mesh is rendered regardless of what is already in the depth buffer. Defaults to on. |
| Depth Write     | If checked, when a mesh with the material is rendered, its depth information is written to the depth buffer. This ensures that when subsequent meshes are rendered, they can be successfully depth tested against meshes rendered with this material. Defaults to on. |
| Cull            | Options are: <ul><li>None: Both front faces and back faces are rendered.</li><li>Front Faces: front faces are rendered and back faces are not.</li><li>Back Faces: back faces are rendered and front faces are not. This is the default.</li></ul> PlayCanvas dictates that a counter-clockwise vertex winding specifies a front face triangle. Note that backface culling is often good for performance because backface pixels are often overwritten (for convex meshes) which can result in redundant filling of pixels. |
| Blend Type      | Options are: <ul><li>None: The mesh is opaque. This is the default.</li><li>Normal: The mesh is transparent, like stained glass.</li><li>Additive: The mesh color is added to whatever has already been rendered to the frame buffer.</li><li>Pre-multiply: Like 'Normal' blending except it is assumed that the color of the mesh being rendered with this material has already been modulated by its alpha value.</li><li>Multiply: When rendered, the mesh color is multiplied by whatever has already been rendered to the frame buffer.</li></ul> |
| Shadow Sampling | Options are: <ul><li>Hard</li><li>PCF 3x3</li></ul> |

--------------------------------------------------------------------------------

## Render

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/render/

A Render asset contains 3D mesh data extracted from imported 3D model files (such as GLB or FBX). It defines the shape and structure of 3D models and is used by the [Render Component](/user-manual/editor/scenes/components/render/) to display geometry in the scene.

Render assets are primarily used for:

- Defining the shape and structure of 3D models
- Applying materials to mesh surfaces

## Inspector

The Render asset inspector displays metadata about the mesh and its relationship to the source container asset.

## Properties

### Meta

[Image: Render Asset Inspector - Meta]

| Property | Description |
|----------|-------------|
| Vertices | The total number of vertices in the mesh (read-only). |
| Triangles | The total number of triangles in the mesh (read-only). |
| Meshes | The number of mesh instances contained in the asset (read-only). |
| Skinned | Whether the mesh contains skinning data for skeletal animation (read-only). |
| Attributes | The vertex attributes present in the mesh data, such as position, normal, UV coordinates (read-only). |
| Mesh Compression | The compression format used, if any (e.g., Draco) (read-only). |

### Render

[Image: Render Asset Inspector - Render]

| Property | Description |
|----------|-------------|
| Index | The index of this render asset within its source container (read-only). |
| Container | A reference to the source container asset (GLB) from which this render was extracted (read-only). |

:::tip
To use this asset in scripts, see [Asset Attributes](/user-manual/scripting/script-attributes/esm/#asset-attribute).
:::

--------------------------------------------------------------------------------

## Shader

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/shader/

A Shader asset contains GLSL code for custom rendering effects. You can create a new Shader asset by clicking New Shader in the Asset Panel or by uploading a file with an extension of `.vert`, `.frag`, or `.glsl`.

To edit a Shader asset, right click on it in the Editor and select Edit.

## Inspector

[Image: Shader Asset Inspector]

The Shader asset inspector displays a preview of the GLSL shader code contained in the asset.

## Properties

This asset type has no configurable properties in the Inspector. It displays a read-only preview of the shader code.

:::tip
To use this asset in scripts, see [Asset Attributes](/user-manual/scripting/script-attributes/esm/#asset-attribute).
:::

--------------------------------------------------------------------------------

## Sprite

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/sprite/

A Sprite is a 2D graphic that can be rendered into a scene. A Sprite asset is a reference to a [Texture Atlas](/user-manual/editor/assets/inspectors/texture-atlas) and a sequence of frames from that atlas. In this way a sprite can either represent a single image (taken out of the atlas) or a flip-book style animation (multiple frames from the atlas).

## Inspector

[Image: Sprite Asset Inspector]

## Properties

| Property | Description |
|----------|-------------|
| Pixels Per Unit | The number of pixels in the sprite image that maps to 1 Unit in the PlayCanvas scene. For example, if `pixelsPerUnit` is 1 and the sprite is 32x32, it will be 32 units across and high when rendered in the scene. By default, a sprite with *Simple* render mode has `pixelsPerUnit` set to 100, meaning a 100x100 sprite will be 1 unit wide/high. *Sliced* sprites default to 1 because they are typically used for UI where 1 sprite pixel should map to 1 screen pixel. |
| Render Mode | Controls how the sprite is rendered: **Simple** - the sprite does not use border values; **Sliced** - uses border values to perform [9-sliced](/user-manual/2D/slicing) rendering by stretching; **Tiled** - uses border values to perform [9-sliced](/user-manual/2D/slicing) rendering by tiling. |
| Texture Atlas | The Texture Atlas asset that the sprite references. |

:::tip
To use this asset in scripts, see [Asset Attributes](/user-manual/scripting/script-attributes/esm/#asset-attribute).
:::

--------------------------------------------------------------------------------

## Template

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/template/

A Template (also called a Prefab) is an asset that contains a piece of an Entity hierarchy. It has a root Entity and can have any number of children. A Template is a reusable Entity that you can instantiate dynamically at runtime or place multiple instances of it in your Scene. When you change the Template asset, all instances of the Template will also change.

For more information about working with Templates, see [Templates](/user-manual/editor/templates/).

## Inspector

{/*[Image: Template Asset Inspector]*/}

The Template asset inspector displays an interactive 3D preview of the Template's entity hierarchy. You can rotate the preview by clicking and dragging.

## Properties

This asset type has no configurable properties in the Inspector. It provides a visual preview only.

:::tip
To use this asset in scripts, see [Asset Attributes](/user-manual/scripting/script-attributes/esm/#asset-attribute).
:::

--------------------------------------------------------------------------------

## Text

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/text/

A Text asset is used for storing plain text data. It's versatile and can be used for various purposes such as storing dialogue, configuration data, or any other textual information.

You can create a new Text asset in the Editor or by uploading a file with a `.txt` extension.

## Inspector

[Image: Text Asset Inspector]

The Text asset inspector displays a preview of the text content contained in the asset.

## Properties

This asset type has no configurable properties in the Inspector. It displays a read-only preview of the text content.

:::tip
To use this asset in scripts, see [Asset Attributes](/user-manual/scripting/script-attributes/esm/#asset-attribute).
:::

--------------------------------------------------------------------------------

## Texture Atlas

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/texture-atlas/

A Texture Atlas asset is a texture asset with additional data which describes 'Frames' inside the texture. Frames are regions defined in the texture as a rectangle with a 'pivot' point which sets the positional and rotational center of the frame.

Texture Atlases are created by right-clicking on a regular Texture asset and selecting **Create Atlas**, or by changing the default asset settings to enable [Create Atlases](/user-manual/editor/interface/settings/asset-import/).

The Texture Atlas is used in combination with the [Sprite asset](/user-manual/editor/assets/inspectors/sprite) to render 2D graphics.

## Inspector

[Image: Texture Atlas]

## Properties

### Frames

The Frames of a texture atlas are keyed by a unique value, usually an integer string. The format of a frame is:

```javascript
{
    rect: [0, 0, 0, 0],  // u,v,width,height - width and height in pixels
    pivot: [0, 0],       // x,y - as a proportion 0-1
    border: [0, 0, 0, 0] // left, bottom, right, top - in pixels
}
```

### Texture Properties

In addition to frame data, the Texture Atlas contains the same properties as a [Texture asset](/user-manual/editor/assets/inspectors/texture).

:::tip
To use this asset in scripts, see [Asset Attributes](/user-manual/scripting/script-attributes/esm/#asset-attribute).
:::

--------------------------------------------------------------------------------

## Texture

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/texture/

A texture is an image that can be assigned to a [material](/user-manual/editor/assets/inspectors/material) and then applied to a graphical primitive.

## Importing Textures

There are 3 ways you can import texture assets into PlayCanvas:

1. Drag and drop images into the Assets panel.
2. Select 'Upload' from the context menu in the Assets panel and select an image using the file browser.
3. Import an FBX file that embeds textures.

Supported image formats are:

* JPG
* PNG
* AVIF
* WEBP
* GIF
* TGA
* BMP
* TIF
* HDR
* EXR

Imported JPG, PNG, AVIF, WebP and GIF files remain in their original format.

GIF, TGA, BMP and TIF image types will be converted to JPG or PNG on import. If the imported image has transparency, it will be converted to PNG. Otherwise, it will be converted to JPG.

HDR and EXR are [high dynamic range formats](https://en.wikipedia.org/wiki/High-dynamic-range_imaging) formats. Images of these types are converted to PNG on import and marked as being stored in RGBM format. RGBM essentially stores a multiplier for RGB values in the PNG's alpha channel, enabling the compression of an HDR format into a low dynamic range format.

By default, imported images will be resized to the nearest power of two. For example, an image that is 323x414 will be resized to 256x512 on import. This is done because the graphics engine cannot utilize mipmapping with non-power of two textures. However, this behavior can be overridden by disabling the 'Textures POT' setting in the Asset Tasks panel before importing a non-power of two texture.

## Texture Properties

Selecting a texture's thumbnail in the Assets panel will load it into the Inspector panel. Note that you can multi-select textures and edit the whole selection simultaneously in the Inspector.

A texture shares the standard set of asset properties (ID, name, tags and so on). But it also has some texture-specific properties.

[Image: Texture Properties]

### Texture Filtering

Texture filtering gives control over how the color of a texture mapped pixel is calculated. 'Point' applies no filtering whereas 'Linear' will interpolate the color of a texel with those of its neighbors. This produces better visual results, particularly as a texture is minimized (where the texture occupies fewer pixels on the screen than it has texels).

### Anisotropy

When textures are viewed on surfaces at an oblique angle, quality can suffer and they can appear blurred. To fix this problem, you can set a value for anisotropy. See how different anisotropy values can affect the appearance of a texture:

[Image: Anisotropy]

Note that as anisotropy increases, the cost of sampling the texture on the GPU also increases.

### Texture Addressing

The texture addressing properties give you control over how a texture is sampled for texture coordinates outside the range 0 to 1. See how the different modes affect the sprite below:

[Image: Addressing]

## Max Texture Size

Different devices can support different texture sizes. Using [WebGL report](https://webglreport.com/) on the device and browser, we can see the max size supported.

For example, this is from a MacBook Pro 16 inch (2020) laptop with Chrome which shows support up to 16384x16384.

<img loading="lazy" src="/img/user-manual/assets/textures/mac-webgl-report.png" alt="Macbook Pro WebGL report" width="600" />

Whereas on a Samsung S7 mobile device, only 4096x4096 is supported.

<img loading="lazy" src="/img/user-manual/assets/textures/samsung-s7-webgl-report.jpg" alt="Samsung S7 WebGL report" width="600" />

If the engine attempts to utilize a texture that exceeds the max texture size reported by WebGL, it will resize it down to this maximum size at runtime. Note that this is only done for texture loaded from images (PNG, JPG, AVIF, WebP, GIF). Compressed textures cannot be resized at runtime and will simply fail to render if they are too large for the device.

If you would like to avoid downsizing at runtime, at the time of writing (Fri 23 Oct 2020), 4096x4096 is very widely supported with some developers even opting for 2048x2048 which is guaranteed to work everywhere.

--------------------------------------------------------------------------------

## WASM Module

URL: https://developer.playcanvas.com/user-manual/editor/assets/inspectors/wasm/

Wasm Modules (also known as WebAssembly Modules) contain compiled executable code for the web.

A Wasm module comprises three parts:

* the binary executable file
* the JavaScript glue code file
* an optional fallback asm.js

These files can be added to the project either by dragging and dropping the files into the Assets Panel or by selecting 'Upload' from the Assets Panel context menu.

Please note that PlayCanvas Editor currently supports [Emscripten](https://emscripten.org/) compiled Wasm Modules only.

## Wasm Module Properties

Once the files have been added to the project, select the Wasm Module to display its properties in the Inspector Panel:
[Image: Wasm Module Properties]

### Name

Name must match the module name defined in the glue and fallback script. This name is used to instantiate the module at load time.

### Glue script

This is the JavaScript glue code required to execute Wasm code.

### Fallback script

This is the optional fallback asm.js script to use when WebAssembly is not supported.

--------------------------------------------------------------------------------

## Asset Viewers

URL: https://developer.playcanvas.com/user-manual/editor/assets/viewers/

Some of the assets have viewer tools that can help inspect them more closely. Assets that can be viewed in these will have the option to 'Open in Viewer' in the inspector and context menu.

[Image: Open in Viewer option]

Clicking on either of these options will open the viewer with the asset in a new tab.

## Model Viewer

[Image: Model Viewer Preview]

(Model: Copyright 2021 Wayfair LLC. [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/) International)

The [open source Model Viewer](https://github.com/playcanvas/model-viewer) is available for the following asset types:

- Model (Source) that are GLBs
- Model
- Container
- Animation

The Model Viewer is useful for inspecting the model node hierarchy, morph targets and debugging animation issues.

## Texture Tool

[Image: Texture Tool Preview]

The [open source Texture Tool](https://github.com/playcanvas/texture-tool) is available for the following asset types:

- Texture
- Texture Atlas

--------------------------------------------------------------------------------

## Editor API

URL: https://developer.playcanvas.com/user-manual/editor/editor-api/

:::warning

The Editor API is a beta feature. Please use caution when using on live projects.

:::

The Editor has a user accessible API that is currently in beta which can be used to help automate and extend the base functionality.

The API is not yet considered stable and may change in the future as we continue development. However, it is unlikely that API will change a lot from its current state.

The Editor API is open sourced on [GitHub](https://github.com/playcanvas/editor-api). The API documentation can be found [here](https://api.playcanvas.com/editor/).

## Automation Example

The API can be accessed via the browser's devtools console allowing for automation of repetitive tasks. In the example below, we are using the Editor to find all the Entities that have the tag 'red' and disabling them by running code in the browser console.

Editor API code:

```javascript
(function() {
    const entities = editor.api.globals.entities.root.listByTag('red');
    for (const entity of entities) {
        entity.set('enabled', false);
    }
})();
```

## Extending Editor Functionality

It is possible to add extra functionality to the Editor and create custom interfaces such as adding buttons. This can be done via a number of ways such as browser extensions or [user scripts](https://en.wikipedia.org/wiki/Userscript).

User scripts is the more accessible out of the two which allows you to run custom code on top of the Editor.

Below we have an example where a button has been added to the [Viewport](/user-manual/editor/interface/viewport) to generate and randomly place boxes into the scene.

The PlayCanvas team are currently using the [Violentmonkey](https://violentmonkey.github.io/) open source browser extension to manage user scripts.

Once the browser extension has been installed, adding your own or another developer's user script is very straightforward. Steps can be found in [Violentmonkey's documentation](https://violentmonkey.github.io/guide/creating-a-userscript/).

The code for the above user script is:

```javascript
// ==UserScript==
// @name        Example Script
// @namespace   Violentmonkey Scripts
// @match       https://playcanvas.com/editor/scene/*
// @grant       none
// @version     1.0
// @author      -
// @description 20/10/2021, 11:40:21
// ==/UserScript==

(function() {
    async function generateBoxes(count, position, radius) {
        // create box entity
        const box = editor.api.globals.entities.create({ parent: editor.api.globals.entities.root });
        // find box material asset
        const boxMaterial = editor.api.globals.assets.findOne(asset => asset.get('name') === 'boxMaterial');
        // add render component
        box.addComponent('render', {
            type: 'box',
            materialAssets: [boxMaterial.get('id')]
        });

        // add a number of boxes around a point in the scene
        let offset = new pc.Vec3();
        let rotation = new pc.Quat();
        const result = [];

        for (let i = 0; i < count; i++) {
            const boxCopy = await box.duplicate();
            boxCopy.set('name', 'Box ' + (i + 1));
            offset.set(1, 0, 0);
            rotation.setFromEulerAngles(0, pc.math.random(-360, 360), 0);
            offset = rotation.transformVector(offset);
            offset.scale(pc.math.random(-radius, radius));
            boxCopy.set('position', [position.x + offset.x, position.y + offset.y, position.z + offset.z]);

            result.push(boxCopy);
        }

        // delete original box
        box.delete();

        return result;
    }

    function createButton() {
        const btn = new pcui.Button({ text: 'Generate Boxes' });
        btn.style.position = 'absolute';
        btn.style.bottom = '10px';
        btn.style.right = '10px';
        editor.call('layout.viewport').append(btn);

        let boxes;

        btn.on('click', () => {
            // delete existing boxes
            if (boxes) {
                editor.api.globals.entities.delete(boxes);
                boxes = null;
            }

            generateBoxes(10, new pc.Vec3(), 10).then(result => {
                boxes = result;
            });
        });
    }

    // Wait until the Editor is available before adding the button
    editor.once('load', () => createButton());
})();
```

Let's break down the important areas of the script:

At the top is the informational header about the script if you do share the script with other users. The important line is the `@match` attribute which controls which URLs the script is loaded on. In this case, it is set to load on any PlayCanvas scene. More information on how to change this can be found in [Violentmonkey's documentation](https://violentmonkey.github.io/api/matching/).

```javascript
// ==UserScript==
// @name        Example Script
// @namespace   Violentmonkey Scripts
// @match       https://playcanvas.com/editor/scene/*
// @grant       none
// @version     1.0
// @author      -
// @description 20/10/2021, 11:40:21
// ==/UserScript==
```

This is private Editor API to wait for an event when the Editor has fully loaded. Using the event ensures that the Editor API is accessible before the code to extend Editor features is run.

```javascript
    // Wait until the Editor is available before adding the button
    editor.once('load', () => createButton());
```

The button created is from the [PCUI](https://github.com/playcanvas/pcui) framework library that the Editor is also using. Again, there is some private API use to get the Viewport DOM to attach the button to.

```javascript
    function createButton() {
        const btn = new pcui.Button({ text: 'Generate Boxes' });
        btn.style.position = 'absolute';
        btn.style.bottom = '10px';
        btn.style.right = '10px';
        editor.call('layout.viewport').append(btn);

        let boxes;

        btn.on('click', () => {
            // delete existing boxes
            if (boxes) {
                editor.api.globals.entities.delete(boxes);
                boxes = null;
            }

            generateBoxes(10, new pc.Vec3(), 10).then(result => {
                boxes = result;
            });
        });
    }
```

--------------------------------------------------------------------------------

## Engine Compatibility

URL: https://developer.playcanvas.com/user-manual/editor/engine-compatibility/

## Introduction

The Editor supports two major release streams of the engine:

- The latest release of engine v1.x.x (**Engine V1**)
  - This supports WebGL1 and WebGL2, and receives critical bug fixes. It does not receive any new features.
- The latest release of engine v2.x.x (**Engine V2**)
  - This supports WebGL2 and WebGPU but not WebGL1 for rendering. It receives both new features and fixes.

You are free to switch between these at any time.

### Switching projects between Engine V1 and V2

This process will involve migration and updating of scripts given differences in our Engine API between Engine V1 and V2. All details regarding the engine changes can be found [here](/user-manual/engine/migrations)

To initiate this navigate to the settings panel and click the `SWITCH TO ENGINE V2` button to convert your project to using Engine V2 (This will require a confirmation before converting). If you wish to switch back, there is a `SWITCH TO ENGINE V1` button in the same location for Project V2.

:::important

It is recommended to create a checkpoint before switching engines.

:::

:::warning

Once the switch has been confirmed, **all users** in the current project will be reloaded.

:::

<img src='/img/user-manual/editor/editor-v2/switch-engine.png' width='400' />

#### Scripting

Given the changes in API for the engine you may wish to test your project in the launcher with Engine V1 or V2. You can do this by enabling the Force Engine V2 or Force Engine V1 checkboxes in the launcher options:

<img src='/img/user-manual/editor/editor-v2/launcher-options.png' width='600' />

Additionally, conditionally checking the Engine version at runtime will allow your script to be made compatible with both versions of the engine during the transition time.

<img src='/img/user-manual/editor/editor-v2/scripting-engine.png' width='300' />

#### Gamma and Tonemap

These settings were previously found under the rendering section of the Settings panel. However, in Engine V2, these have been moved to be set per camera. Each camera component will now have these additional fields:

<img src='/img/user-manual/editor/editor-v2/gamma-tonemap.png' width='400' />

The viewport settings are now located inside the EDITOR section of the Settings panel:

<img src='/img/user-manual/editor/editor-v2/viewport-camera.png' width='400' />

:::note

If you change the settings per camera and switch back to Project V1, your per-camera settings will be lost.

:::

#### sRGB Textures

<img src='/img/user-manual/editor/editor-v2/srgb-texture.png' width='400' />

For Engine V2, textures must be set as sRGB or not, depending on their use case. The textures that store color data, such as diffuse and emissive maps, should use sRGB for accurate color representation. This is found under the texture asset panel as shown above. These will be automatically set. However, if there are conflicts, the console will display them:

<img src='/img/user-manual/editor/editor-v2/console-texture.png' width='600' />

Click on the console message to open up the conflicting reference. If you wish to use the same texture for both sRGB and not, it is advised to duplicate your texture to cover both cases.

--------------------------------------------------------------------------------

## Common Questions

URL: https://developer.playcanvas.com/user-manual/editor/faq/

## How do I add a component?

To add a **component** to an Entity, select the Entity and then click **Add Component** in the **Inspector** or right click on the Entity and select a component from the Add Component context menu.

[Learn more](/user-manual/editor/scenes/components/)

## How do I add a script?

<img src="https://playcanvas.com/static-assets/instructions/add-new-script.gif" />

You can use JavaScript to control the behavior of entities. Select any entity, add a script component and create a new script asset.

[Learn more](/user-manual/editor/scripting/managing-scripts/)

## How do I change the background color?

To change the background color of your scene, you should update the Clear Color property of the camera in your scene.

You could also try adding a [skybox](/user-manual/editor/assets/inspectors/cubemap/) to your scene.

[Learn more](/user-manual/editor/scenes/components/camera/)

## How do I change the material of a model?

<img src="https://playcanvas.com/static-assets/instructions/change_material.gif" />

Every surface on a 3D model is rendered using a **material**. The material defines the properties of that surface, such as its color, shininess, bumpiness etc.

You can create a new material and drag and drop it on your model or you can select its existing materials and edit their properties in the Inspector.

[Learn more](/user-manual/editor/assets/inspectors/material/)

## How do I create a cubemap?

<img src="https://playcanvas.com/static-assets/instructions/new_cubemap.gif" />

Cubemaps are a special type of texture asset. They are formed from 6 texture assets where each texture represents the face of a cube.

To create a cubemap click on the **<span class="pc-icon">&#57632;</span> Add** button in the Assets panel and select **New Cubemap**. Then drag 6 textures in the cubemap inspector. To take advantage of Physically Based Rendering make sure you click **Prefilter** after setting the 6 textures.

[Learn more](/user-manual/editor/assets/inspectors/cubemap/)

## How do I create an Entity?

<img src="https://playcanvas.com/static-assets/instructions/new_entity.gif" />

You can create a new Entity by clicking on the <span class="pc-icon">&#57632;</span> Add button in the Hierarchy panel or right click on an Entity and use the **New Entity** menu item.

## How do I create a light?

You can create a light by adding a **Light** component to an Entity. You can also right click on an Entity and select New Entity / Directional Light to create a new directional light and similarly for spot lights and omni lights.

[Learn more](/user-manual/editor/scenes/components/light/)

## How do I create a material?

Every surface on a 3D model is rendered using a **material**. The material defines the properties of that surface, such as its color, shininess, bumpiness etc.

To create a material click on the **<span class="pc-icon">&#57632;</span> Add** button in the Assets panel and then select **New Material**.

[Learn more](/user-manual/editor/assets/inspectors/material/)

## How do I create a new shader?

You can create a new shader asset from the asset panel. Click **Add Asset -> Shader**.

[View tutorial](http://developer.playcanvas.com/tutorials/custom-shaders/)

## How do I create a shape like a box or sphere?

<img src="https://playcanvas.com/static-assets/instructions/new_box.gif" />

You can add primitive shapes like boxes, spheres and others by adding a **Render Component** on an Entity and changing its type to the desired shape.

You can also right click on an Entity and select New Entity/Box to add a box (similarly for other shapes).

[Learn more](/user-manual/editor/scenes/components/render/)

## How do I create a skybox?

To create a skybox for your scene you first need to create a [Cubemap asset](/user-manual/editor/assets/inspectors/cubemap/). Then you can drag and drop the Cubemap inside the 3D viewport, or you can go to the Scene Settings and drag the Cubemap in the Skybox field.

[Learn more](/user-manual/editor/interface/settings/rendering/)

## How do I delete an asset?

To delete an asset select it and press **Delete** or right click on it and select Delete.

## How do I delete an Entity?

You can delete an Entity by selecting it and pressing **Delete**. Alternatively you can click on the <span class="pc-icon">&#57636;</span> Delete button in the Hierarchy panel or right click on the Entity and select Delete from the context menu.

## How do I duplicate an Entity?

To duplicate an Entity use the <span class="pc-icon">&#57638;</span> Duplicate button in the Hierarchy panel or press **Ctrl+D**.

You can also copy and paste an Entity. To copy press **Ctrl+C** and to paste **Ctrl+V**.

## How do I play an animation?

To play an animation of a 3D model you need to create an Entity with a [Model Component](/user-manual/editor/scenes/components/model/) and an [Animation Component](/user-manual/editor/scenes/components/animation/). The **Model Component** will render your model and the **Animation Component** will play animations.

To render the model drag a model Asset in the Asset field of the Model Component. To play animations drag Animation Assets on the Assets field of the Animation Component.

## How do I play a sound?

To play sounds you need to add a [Sound component](/user-manual/editor/scenes/components/sound/) to an Entity. Then you can create slots to play [Audio assets](/user-manual/editor/assets/inspectors/audio/). Simply click "Add Slot" and drag an Audio Asset on the Asset field. In order to hear the sounds you also need to add an [AudioListener component](/user-manual/editor/scenes/components/audiolistener/) to an Entity - usually to the Camera Entity.

You can create Audio assets by dragging audio files from your computer into the Assets panel.

[View tutorial](/tutorials/basic-audio/)

## How do I remove a component?

<img src="https://playcanvas.com/static-assets/instructions/remove_component.jpg" />

To remove a component, select the Entity and then click on the **<span class="pc-icon">&#57636;</span> Delete** button in the Inspector next the component’s title.

## How do I render a 3D model?

<img src="https://playcanvas.com/static-assets/instructions/new_model.gif" />

To render a 3D model you need to add a **Model Component** to an Entity and drag a **Model Asset** on the Asset field. Alternatively you can drag and drop a Model Asset from the Assets Panel into the 3D Viewport.

[Learn more](/user-manual/editor/scenes/components/model/)

## How do I move / rotate / scale an Entity?

<img src="https://playcanvas.com/static-assets/instructions/transform.gif" />

To move an Entity, select it and then move it using the **<span class="pc-icon">&#57617;</span> Translate tool**. To rotate it use the **<span class="pc-icon">&#57619;</span> Rotate tool** and to scale it use the **<span class="pc-icon">&#57618;</span> Scale tool**.

Switch between the Translate / Rotate / Scale tools by pressing 1 / 2 / 3 respectively.

## How do I upload assets?

To upload Assets simply drag and drop files from your computer into the Assets panel. Your files will be processed by the server and will appear shortly after in the Assets Panel.

[Learn more](/user-manual/editor/assets/importing/)

## How do I use physics?

To give physical properties to an Entity you need to add a [Collision component](/user-manual/editor/scenes/components/collision/) to it and a [RigidBody component](/user-manual/editor/scenes/components/rigidbody/). The Collision component gives a physical shape to the Entity and the RigidBody component makes the Entity be simulated by the physics engine.

You can change the default [gravity](/user-manual/editor/interface/settings/physics/) in the Scene Settings.

[View tutorial](/tutorials/collision-and-triggers/)

## How do I use real-time shadows?

Real-time shadows are rendered for each light source that has **Cast Shadows** enabled. To enable shadows, select an Entity with a Light component and enable Cast Shadows. You also need to enable Cast Shadows on any Model components in your scene.

[Learn more](/user-manual/graphics/lighting/shadows/)

--------------------------------------------------------------------------------

## Getting Started

URL: https://developer.playcanvas.com/user-manual/editor/getting-started/

If you are new to the PlayCanvas Editor, this section should get you up to speed quickly.

--------------------------------------------------------------------------------

## The Editor Workflow

URL: https://developer.playcanvas.com/user-manual/editor/getting-started/workflow/

Building 3D web apps with the PlayCanvas Editor is easy.

## Create and upload assets

[Image: assets]

PlayCanvas supports a wide variety of industry standard asset formats. For example, upload images, 3D models, audio files or custom text or binary file formats.

## Construct your scene

[Image: scene]

The PlayCanvas Editor is a visual building tool which is used to construct scenes. Build a hierarchy of entities using built in components like 3D models, collision, particle effects and more.

## Add interactivity

[Image: script]

Using web standard Javascript attach custom behaviors to your entities. Add interactivity on any scale from a simple click handler or orbit camera to a full massively multiplayer online game.

## Publish your application

[Image: publish]

When your application is ready for sharing a simple one-click publish will get your application live on the internet hosted by PlayCanvas for free. Alternatively download your application for self-hosting on your own web server.

--------------------------------------------------------------------------------

## Your First App

URL: https://developer.playcanvas.com/user-manual/editor/getting-started/your-first-app/

Developing applications in the PlayCanvas Editor is easy and fun. Let's spend a few minutes learning the basics. We'll recreate the following simple 3D app:

[Interactive Demo]

*Use the arrow keys to move the red ball around.*

To begin, navigate to your PROJECTS page.

[Image: Projects Page]

Select the NEW button to create a new project:

[Image: New Project]

Enter `My First App` as the project name. The description is optional and can be filled out later. Hit CREATE.

With your project created, we can dive right into the Editor - simply hit the EDITOR button:

[Image: Editor Button]

When you first open the Editor, you should find that a minimal scene has been created for you:

[Image: Editor]

Specifically, you have a camera (to render the scene), a box sitting on a plane and a light (to illuminate the 3D objects).

You can easily change the box to another shape, like a sphere. To do this, select the Box entity in the HIERARCHY panel. In the Inspector panel on the right, change the Type property of the MODEL component from Box to Sphere. To be complete, rename the Entity from Box to Sphere too!

[Image: Box To Sphere]

But our sphere is a slightly boring gray color. Let's color it red! To do this, we must create a material by hitting the **+** icon in the ASSETS panel:

[Image: Create Material]

Select the material and its properties will appear in the Inspector panel to the right. Expand the DIFFUSE section and click on the color swatch to edit the color to red:

[Image: Red Material]

Next, we must assign the material to the sphere. You can do this with a simple drag and drop operation:

[Image: Drag And Drop Material]

Now let's set up the Camera entity. Select it in the HIERARCHY panel:

[Image: Editor Camera Selected]

Notice the little preview window that has appeared for the selected camera. Let's edit the camera's position and rotation to view the cube from directly in front. Edit the camera's position and rotation with the values below:

[Image: Camera Transform]

Next, let's make the sphere controllable with the arrow keys on the keyboard. To do this, we must create a script. Right click the Sphere entity and select `Add Component -> Script`.

[Image: Add Script Component]

A script component will appear in the Inspector on the Sphere entity. To create a script asset on the script component, enter the name `movement.js`.

[Image: Create Script Asset]

Hit Enter and your script will be created.

[Image: Edit Script]

Now hit EDIT to open the Code Editor. You'll see the following skeleton script:

To make the sphere controllable, let's add some code to the `update` function. Copy and paste the code below:

When the code is updated, save the script using CTRL+S (or CMD+S on Mac) and close the Code Editor tab.

You are now ready to preview your app by opening the Launch page. To do this, hit the Launch button in the top right of the Editor's 3D view.

[Image: Launch Button]

The Launch page opens in a new tab. When it opens, try pressing the 4 arrow keys to move the sphere around.

[Image: Launch Page]

:::tip

There is a 'Live Link' between the Editor and the Launch page. Any change you make in the Editor will be reflected in the Launch page in real time! It can be convenient to place the Launch page side by side with the Editor while you are working.

[Image: Editor Live Link]

:::

The final step is to publish your app so you can share it with others. To do this, click on the <span class="pc-icon">&#57911;</span> button in the left hand side toolbar.

[Image: Publish Button]

This will open up the Publishing dialog. Here you can publish a build (or even download a build for self-hosting).

[Image: Publish]

Click the PUBLISH TO PLAYCANVAS option. You can then configure your published app.

[Image: Publish New Build]

Leave the defaults, scroll down and select PUBLISH NOW.

[Image: Builds]

You now have a URL for your published build! Feel free to share it with the world on Twitter or Facebook!

And so, that is the process from start to finish for building and publishing a PlayCanvas application. In this quick introduction, we have touched on the basics. Now explore the rest of the User Manual to learn some more advanced topics. Good luck and have fun!

--------------------------------------------------------------------------------

## Editor Interface

URL: https://developer.playcanvas.com/user-manual/editor/interface/

[Image: Editor Interface]

This is the main interface to the PlayCanvas Editor. You can see labeled the main areas:

* [**Toolbar**](toolbar.md) Commonly used commands are available on the toolbar for quick access.
* [**Hierarchy**](hierarchy.md) A hierarchical view of the Entities that make up the open Scene. Use this to select, delete and re-parent Entities.
* [**Inspector**](inspector.md) Detailed properties of the selected [Entity](/user-manual/glossary#entity), [Asset](/user-manual/glossary#asset) or [Component](/user-manual/glossary#component).
* [**Viewport**](viewport.md) A 3D view onto your scene, use this view to select, position and orientate Entities.
* [**Assets**](assets.md) A view of all the Assets in the current Project. You can search for assets by name, filter by type and also drag and drop assets to various slots or in the Design View.

--------------------------------------------------------------------------------

## Assets Panel

URL: https://developer.playcanvas.com/user-manual/editor/interface/assets/

The Assets Panel manages all of the Assets that are available in your project. From here, you can create, upload, delete, inspect and edit any Asset.

[Image: Assets Panel]

The Assets Panel provides powerful features for organizing and managing your project's assets, including:

- **Folder organization** - Create a hierarchy of folders to organize assets
- **Search and filter** - Find assets by name, ID, tags, or type
- **Drag and drop** - Move assets between folders or into the viewport
- **Copy and paste** - Share assets between projects
- **Reference checking** - See where assets are used in your scene

For detailed documentation on all Assets Panel features, see the [Assets section](/user-manual/editor/assets/).

--------------------------------------------------------------------------------

## Hierarchy Panel

URL: https://developer.playcanvas.com/user-manual/editor/interface/hierarchy/

[Image: Hierarchy Panel]

The Hierarchy panel shows you a tree view of your entire Scene which is made up from a hierarchy of Entities. A Scene will always contain the Root Entity at the top of the tree. All the other Entities you see here on the right have been added by the developer.

The Hierarchy panel is great for quickly finding Entities as all the Entities in your Scene will be visible all the time. Clicking on an Entity in the Hierarchy will select it.

## Creating and Destroying Entities

Once an Entity is selected in the Hierarchy, you can create a new child Entity or delete the Selection. You can do this, either using the buttons in the top right of the Hierarchy panel, or by opening the context menu via a right click operation.

## Organizing the Hierarchy

The order in which Entities are listed in the tree view is important. As the Scene is a hierarchy, Entities can be *children* of another Entity -- their *parent* -- and the transform matrix of a parent Entity is applied to all of its children. If you move or rotate a parent Entity, all children will move or rotate underneath the parent. This is very useful for simplifying the manipulation of many Entities.

The structure of the Hierarchy can be edited via drag and drop. Simply select and drag any Entity in the Hierarchy. In this way, you can quickly reorder or reparent Entities. When you reparent an Entity in the Editor, its world-space transform (position, rotation, and scale) will be preserved so you won't see the Entity move, rotate, or change size after you change its parent. If you do not want that behavior, hold Ctrl (or Cmd on Mac).

:::note

Scale preservation works best when parent entities use uniform scaling. When non-uniform scale is combined with rotation in the hierarchy, the world scale becomes an approximation and may not be perfectly preserved.

:::

## Searching the Hierarchy

Near the top of the Hierarchy panel is a Search box which you can use to dynamically filter the content of the Entity tree.

[Image: Hierarchy Panel Search]

By default, the search will filter based on entity names with 'Smart Search' enabled. Smart Search is fuzzy, which means it matches names that are similar to the search string rather than exactly the same. This is useful if you can't quite remember the name of the Entity you are searching for.

You can customize the search by clicking the magnifying glass:

[Image: Hierarchy Panel Search Options]

## Duplicating Entities

You can duplicate Entities by selecting them and hitting Ctrl+D (or Cmd+D on Mac) or by right clicking and selecting Duplicate. A copy of each Entity will be created right next to its original.

## Copying / Pasting Entities

If you wish to paste Entities under a new parent you can select them and hit Ctrl+C (or Cmd+C on Mac) to copy them, then select the new parent and then Ctrl+V (or Cmd+V on Mac) to paste them under the selected parent. You can also use the Copy and Paste options when you right click on the selected Entities.

You can also copy and paste Entities between different Scenes or even different Projects. Just follow the same steps to copy Entities and then go to the desired Scene, select the desired parent and paste the Entities. The Editor will try to match any Asset references on the pasted Entities by path in the new Project. For example, if you are copy-pasting an Entity with a Model component that references a Model Asset called `mymodel.fbx` then when you paste the Entity in the other Project, the Editor will try to find an Asset named `mymodel.fbx` in the same folder. If a matching Asset is not found it will be left as missing so that you can fix it manually.

--------------------------------------------------------------------------------

## Inspector Panel

URL: https://developer.playcanvas.com/user-manual/editor/interface/inspector/

[Image: Inspector Panel]

The Inspector panel shows attribute values for the currently selected item.

Depending on what you have selected, you will see different inspector panels. Some available selections are:

* **Entity/Component Inspector**
* **Texture Inspector**
* **Material Inspector**
* **Cubemap Inspector**

Modifying these values is how you specify how your [Entity](/user-manual/glossary#entity) behaves. For example, you can set which model to render for a Model Component, or what color a light is.

Some attributes are simple text or numbers in which case a standard text field or slider control will be used. Other values may require a more specialized input method. For example, choosing an Asset highlights available Assets in the asset panel. Some values can be manipulated via the viewport, for example, the Translate, Rotate and Scale values can be edited by moving and Entity dragging the relevant [Gizmo](/user-manual/glossary#gizmo) around in the viewport.

When running both a game and the Editor simultaneously changes to attributes will be transmitted to Entities in the running application. An excellent way of iterating on values is to launch your game using the Play button, then place the Editor and the game tabs side by side and tweak values in the Editor as you watch and play the game.

## Copying and Pasting Attributes

The Inspector supports copying and pasting attribute values across entities and assets, making it easy to duplicate configurations and speed up your workflow.

<video autoPlay muted loop controls src='/video/editor-attribute-copy-paste.mp4' style={{width: '100%', height: 'auto'}} />

You can copy and paste any modifiable value in the Inspector in two ways:

1. Right click on an attribute to activate a context menu with Copy/Paste options.
2. Mouse over an attribute to activate Copy/Paste buttons to the right of the attribute label.

The copy/paste system enforces type matching to ensure data integrity. You can only paste to an attribute that matches the type of what was previously copied. For example, you cannot paste a string value to a number field, or a boolean value to a string field.

:::info[Multiselect Support]
When you have multiple items selected, copying will use the value from the first selected item. Pasting will apply the value to all selected items at once.
:::

:::note[Undo/Redo Support]
Copy/Paste of attributes is fully integrated with the Editor's undo/redo system.
:::

--------------------------------------------------------------------------------

## Controls and Keyboard Shortcuts

URL: https://developer.playcanvas.com/user-manual/editor/interface/keyboard-shortcuts/

## Camera Controls

The Editor's camera is controlled with the mouse and keyboard.

| Operation    | Controls                                                         |
| ------------ | ---------------------------------------------------------------- |
| Orbit        | Left Mouse Button + Drag                                         |
| Pan          | Middle Mouse Button + Drag<br />SHIFT + Left Mouse Button + Drag |
| Look Around  | Right Mouse Button + Drag                                        |
| Zoom / Dolly | Mouse Wheel                                                      |
| Move         | W-A-S-D                                                          |
| Move (fast)  | SHIFT + W-A-S-D                                                  |

## General Mouse Controls

| Operation                                              | Controls                  |
| ------------------------------------------------------ | ------------------------- |
| Select Entity                                          | Left Mouse Button         |
| Translate / Rotate / Scale Entity (according to gizmo) | Left Mouse Button + Drag  |

## General Modifier Keys

| Operation                                               | Controls                               |
| ------------------------------------------------------- | -------------------------------------- |
| Toggle Editor's Snap setting while operating gizmo      | Hold SHIFT while dragging the Entity   |
| Do not preserve the Entity's transform when reparenting | Hold CTRL when you reparent the Entity |

## Keyboard Shortcuts

Note that on a Mac, CMD should be used instead of CTRL.

| Operation             | Description                                                          | Keyboard Shortcut              |
| --------------------- | -------------------------------------------------------------------- | ------------------------------ |
| Launch                | Launch the scene in a new tab                                        | CTRL + ENTER                   |
| New Entity            | Creates a new entity as a child of the currently selected entity     | CTRL + E                       |
| Duplicate Entity      | Duplicates the selected entity and all children                      | CTRL + D                       |
| Rename Entity / Asset | Focuses on name field of the selected entity or asset                | N or F2                        |
| Copy Entity / Asset   | Copy the current entity/asset selection                              | CTRL + C                       |
| Paste Entity / Asset  | Paste copied entity/asset under the currently selected entity/folder | CTRL + P                       |
| Delete                | Delete the current selection                                         | DELETE<br />CTRL + BACKSPACE   |
| Frame Selection       | Focus on the current entity selection in the 3D view                 | F                              |
| Translate             | Activate the translate gizmo in the 3D view                          | 1                              |
| Rotate                | Activate the rotate gizmo in the 3D view                             | 2                              |
| Scale                 | Activate the scale gizmo in the 3D view                              | 3                              |
| World / Local         | Switch between local and world space gizmos                          | L                              |
| Toggle All Panels     | Hides or shows all the panels to maximize the 3D view                | SPACE                          |
| Bake                  | Re-bake lighting using the [Run-time Lightmapper](/user-manual/graphics/lighting/runtime-lightmaps)                 | CTRL + B                       |
| Previous Selection    | Select previously selected items                                     | SHIFT + Z                      |
| Undo                  | Undo the last action                                                 | CTRL + Z                       |
| Redo                  | Redo the last action                                                 | CTRL + Y<br />CTRL + SHIFT + Z |
| How do I...?          | Toggle search toolbar for mini-tutorials                             | CTRL + SPACE                   |
| Show Controls         | Display the Editor controls                                          | SHIFT + ?                      |

--------------------------------------------------------------------------------

## Launch Page

URL: https://developer.playcanvas.com/user-manual/editor/interface/launch-page/

The Editor's Viewport is where you visually construct your application. From there, you can press the Launch button to open the Launch Page and actually _run_ your application.

[Image: Launch Button]

## Working with the Launch Page

When you activate the Launch Page, it will open in a new tab. It is perfectly valid to switch back and forth between the Editor and the Launch Page. However, if you have a large screen or even a multi-monitor setup, consider tearing off the Launch Page tab and placing it side by side with the Editor.

[Image: Launch Page Side by Side]

This can make it easier and quicker to switch between the two.

## Real-Time Updates

A key feature of the Launch Page is that it maintains a real-time connection (or live-link) to the Editor. This means that any change made in the Editor will be immediately reflected in the Launch Page.

[Image: Launch Page Live Link]

And if any other users (with write permission) are present in the Editor with you, their edits can update your Launch Page as well.

## Running on Other Devices

Sometimes it can be useful to run the Launch Page on another device. For example, you might wish to test your application on a mobile device. To do this, ensure you are logged in to your PlayCanvas account on that device. Then, simply visit the Launch Page URL in your device's browser.

:::tip

Chrome can dynamically generate a QR code for any page. From the Launch Page on your desktop, open Chrome's main menu and select `Save and share` > `Create QR Code`. Then, scan the QR code on your mobile device to instantly visit the Launch Page.

:::

--------------------------------------------------------------------------------

## Using a Custom Engine

URL: https://developer.playcanvas.com/user-manual/editor/interface/launch-page/custom-engine/

By default, when you launch your PlayCanvas app from the Editor, it will use the latest stable build of the engine. Sometimes, there may be a reason why you may want to run your app with a different build of the engine. For example:

* To test your project against the latest dev build of the engine to test a new feature or bug fix, or detect any potential regressions before an upcoming engine release.
* To temporarily step back to a previous stable engine version to mitigate a regression in the current stable build.
* To develop and debug against your own fork of the [engine repo on GitHub](https://github.com/playcanvas/engine).

:::important[Loading the Correct Engine: Module vs UMD]

When loading a custom engine build, it's important to choose the correct format based on your project's script type:

* If your project contains any ESM Scripts (using `import`/`export`), you need to load the module build of the engine by using the `.mjs` extension in the URL.
* For projects that only contain Classic Scripts (using `pc.createScript`), use the UMD build with the `.js` extension.

:::

Now, let's look at how you handle each of these situations:

### Launch with the Dev Engine

When you launch your app from the Editor, a new tab is opened called the launch page. The URL of this page is of the form:

```none
    https://launch.playcanvas.com/<scene_id>
```

Simply edit the launch page's URL by adding the following string to the end:

```none
    https://launch.playcanvas.com/<scene_id>?use_local_engine=https://code.playcanvas.com/playcanvas-latest.js
```

To launch with the minified build, use:

```none
    https://launch.playcanvas.com/<scene_id>?use_local_engine=https://code.playcanvas.com/playcanvas-latest.min.js
```

To launch with Debug mode enabled, use:

```none
    https://launch.playcanvas.com/<scene_id>?debug=true&use_local_engine=https://code.playcanvas.com/playcanvas-latest.dbg.js
```

### Launch with a Previous Stable Engine

Previous stable builds of the PlayCanvas engine are archived on code.playcanvas.com. You can find all of the previous releases on [GitHub](https://github.com/playcanvas/engine/releases). The engine is named with the following convention:

```none
    playcanvas-<major>.<minor>.<patch>.js
```

So, for example:

```none
    playcanvas-0.225.0.js
```

To launch with this specific build of the engine, use the following launch URL:

```none
    https://launch.playcanvas.com/<scene_id>?use_local_engine=https://code.playcanvas.com/playcanvas-0.225.0.js
```

To launch with the minified build, use:

```none
    https://launch.playcanvas.com/<scene_id>?use_local_engine=https://code.playcanvas.com/playcanvas-0.225.0.min.js
```

:::tip

The Editor only officially supports the current Engine release and the previous minor version. While it is sometimes possible to use an older version of the Engine, it is not a long-term supported workflow. We strongly recommend keeping live projects updated with the current Engine release.

:::

### Launch with a Locally Built Engine

If you fork the engine repo on GitHub, you can build the engine yourself (via `npm run build`). To have the launch page launch your custom build, you need to start a local web server by running `npm run serve`.

Verify you can see your engine source at the URL:

```none
    http://localhost:51000/playcanvas.js
```

To use this engine in the launch page, edit the URL to:

```none
    https://launch.playcanvas.com/<scene_id>?use_local_engine=http://localhost:51000/playcanvas.js
```

--------------------------------------------------------------------------------

## Loading Screen

URL: https://developer.playcanvas.com/user-manual/editor/interface/launch-page/loading-screen/

All newly created PlayCanvas projects use the default loading screen:

[Image: Default Loading Screen]

It will be displayed in both the Launch Page and your published app.

## Customizing the Loading Screen

If you want to create a custom loading screen, load the Settings into the [Inspector](../../inspector) by clicking the 'cog' icon on the [Toolbar](../../toolbar) or in the [Viewport](../../viewport).

[Image: Settings]

Then, navigate to the `LOADING SCREEN` section:

[Image: Loading Screen Settings]

You have two options:

1. **CREATE DEFAULT** - Create a new loading screen script in the [Assets Panel](../../assets) that contains the full code for the default loading screen. You can customize this loading screen to your requirements.
2. **SELECT EXISTING** - Select a custom loading screen script from the Assets Panel.

Let's assume you don't have an existing script and instead create the default loading screen script. A very minimal loading screen just displaying a solid color looks like this:

```javascript
pc.script.createLoadingScreen((app) => {
    // Create the main loading screen div
    const div = document.createElement('div');
    div.style.backgroundColor = "#232323"; // Dark gray background
    div.style.position = "absolute";
    div.style.top = "0";
    div.style.left = "0";
    div.style.height = "100%";
    div.style.width = "100%";
    document.body.appendChild(div);

    // Hide the loading screen when the app starts
    app.once('start', () => {
        document.body.removeChild(div);
    });
});
```

However, your users will thank you if you display some kind of loading bar! Let's update the script with one:

```javascript
pc.script.createLoadingScreen((app) => {
    // Create the main loading screen div
    const div = document.createElement('div');
    div.style.backgroundColor = "#232323"; // Dark gray background
    div.style.position = "absolute";
    div.style.top = "0";
    div.style.left = "0";
    div.style.height = "100%";
    div.style.width = "100%";
    document.body.appendChild(div);

    // Create the progress bar div, centered on the screen
    const progressBar = document.createElement('div');
    progressBar.style.position = "absolute";
    progressBar.style.top = "50%";
    progressBar.style.left = "25%";
    progressBar.style.transform = "translateY(-50%)";
    progressBar.style.width = "50%";
    progressBar.style.height = "20px";
    progressBar.style.backgroundColor = "#d3d3d3"; // Light gray for the bar background
    div.appendChild(progressBar);

    // Create the filler for the progress bar
    const progressFiller = document.createElement('div');
    progressFiller.style.height = "100%";
    progressFiller.style.backgroundColor = "#4caf50"; // Green for the progress
    progressFiller.style.width = "0%";
    progressBar.appendChild(progressFiller);

    // Update the progress bar on preload progress
    app.on('preload:progress', (value) => {
        progressFiller.style.width = (value * 100) + '%';
    });
    app.once('preload:end', () => {
        app.off('preload:progress');
    });

    // Hide the loading screen when the app starts
    app.once('start', () => {
        document.body.removeChild(div);
    });
});
```

Feel free to get creative! Use whatever HTML and CSS you like to create the loading screen of your dreams.

--------------------------------------------------------------------------------

## Project Settings

URL: https://developer.playcanvas.com/user-manual/editor/interface/settings/

Project Settings control how your project runs and how the Editor behaves for your team. They include engine/runtime configuration (rendering, physics, input, network), asset import defaults, and Editor preferences. Some settings apply only to you (session or user-specific), while most affect all collaborators on the current branch.

## Opening Settings

Load the Settings into the [Inspector](../inspector) by clicking the 'cog' icon on the [Toolbar](../toolbar) or in the [Viewport](../viewport).

[Image: Settings]

## Settings Overview

Settings are grouped into categories, where each category is shown as a collapsible panel. These categories are listed in the Inspector in the following order:

| Panel | Description |
| --- | --- |
| [Engine](engine.md) | Engine settings, including the engine version. |
| [Editor](editor.md) | Editor settings such as camera near/far clip, zoom sensitivity, and more. |
| [Asset Import](asset-import.md) | Settings for controlling how assets are imported into your project. |
| [Physics](physics.md) | Physics settings to include the library and set gravity. |
| [Rendering](rendering.md) | Rendering settings such as skybox, clustered lighting, shadow settings, and more. |
| [Layers](layers.md) | Manage rendering layers and their order. |
| [Lightmapping](lightmapping.md) | Lightmapping settings such as resolution, mode, and ambient bake. |
| [Batch Groups](batch-groups.md) | Manage batch groups for this project. Batch groups reduce draw calls by batching similar models and elements together. |
| [Launch Page](launch-page.md) | Settings for the launch page. |
| [Input](input.md) | Enable or disable input devices (mouse, keyboard, etc.). |
| [Localization](localization.md) | Settings for adding localization assets. |
| [Network](network.md) | Network-related settings for the project. |

--------------------------------------------------------------------------------

## Asset Import Settings

URL: https://developer.playcanvas.com/user-manual/editor/interface/settings/asset-import/

Controls default behavior for imported assets.

:::note

These settings affect only you and are global for the whole project.

:::

Navigate to the `ASSET IMPORT` section and expand the panel:

[Image: Asset Import Settings]

Here is a breakdown of the available settings:

## General

| Setting | Description |
| --- | --- |
| **Search related assets** | If enabled, importing a source asset updates related target assets wherever they are located. If disabled, assets are updated only when in the same folder; otherwise, new assets are created. |
| **Assets default to preload** | Creates new assets with the preload option enabled. Script assets are always created with preload enabled. |

## Texture Import

| Setting | Description |
| --- | --- |
| **Textures POT** | When a texture is imported, it will be resized to the nearest power-of-two resolution. |
| **Create Atlases** | If enabled, imported textures are converted to Texture Atlas assets instead of Texture assets. |

## Model Import

| Setting | Description |
| --- | --- |
| **Preserve material mappings** | If enabled, when reimporting an existing source model, the Editor attempts to preserve existing user-defined material mappings. |
| **Overwrite Models** | When a model is imported, overwrites any previously imported model asset. |
| **Overwrite Animations** | When a model is imported, overwrites previously imported animation assets. |
| **Overwrite Materials** | When a model is imported, overwrites previously imported material assets. |
| **Overwrite Textures** | When a model is imported, overwrites previously imported texture assets. |
| **Convert to GLB** | Create model assets in GLB format. |
| **Import Hierarchy** | Generates a template asset when importing 3D assets (FBX, etc.). The template asset contains the full entity hierarchy from the imported file. |
| **Mesh Compression** | Specify the mesh compression to apply to imported models. |
| **Unwrap UV** | Generates a set of unwrapped UV coordinates. |
| **Texels Per Meter** | Specifies the number of texels per meter when UV unwrapping is enabled. Default: 16. |
| **Import Morph Normals** | Imports morph target normals when importing a model. Disable this if morph target normals look incorrect. |
| **Create FBX Folder** | Creates a new folder in the current directory when importing an FBX file to store the imported FBX contents. |

## Animation Import

| Setting | Description |
| --- | --- |
| **Naming Strategy** | Choose the naming strategy for imported animations:<ul><li><strong>Use Take Name</strong>: Name the animation after the take name assigned in the FBX file</li><li><strong>Use FBX Filename</strong>: Name the animation after the FBX filename</li></ul> |
| **Sample Rate** | The rate at which to sample animation curves (samples per second). Specify 0 to disable sampling and use input keys instead. |
| **Curve Tolerance** | The tolerance used when optimizing linear animation curve segments. Specify 0 to disable curve optimization. |
| **Cubic Curves** | Output cubic curves when they are encountered. Disable to convert all curves to linear segments. |

--------------------------------------------------------------------------------

## Batch Group Settings

URL: https://developer.playcanvas.com/user-manual/editor/interface/settings/batch-groups/

Batch Groups combine multiple meshes into a single draw call to reduce CPU overhead and improve rendering performance.

:::note

These settings affect all users on the currently active [branch](../../version-control/branches.md) of the project.

:::

Navigate to the `BATCH GROUPS` section and expand the panel:

[Image: Batch Groups Settings]

Here is a breakdown of the available settings:

## Adding a Batch Group

- Click **Add Group** to create a new batch group.
- Configure the properties:

| Setting | Description |
| --- | --- |
| **Name** | The name of the batch group. |
| **Dynamic** | Enable to allow objects in this batch group to move, rotate, or scale after being batched. If your objects are completely static, disable this setting. |
| **Max AABB** | The maximum size of any dimension of a bounding box around batched objects. Larger values batch more objects (fewer draw calls) but create bigger batched objects that are harder to cull. Smaller values create more draw calls but smaller, easier-to-cull batches. |
| **Layers** | The layers that this batch group belongs to. |

### Notes

- Use **static batching** whenever possible for optimal performance.
- Keep **Max AABB** reasonable to avoid large batches that reduce culling efficiency.
- Assign batch groups only to relevant layers to avoid unnecessary batching.

--------------------------------------------------------------------------------

## Editor Settings

URL: https://developer.playcanvas.com/user-manual/editor/interface/settings/editor/

By default, the Editor is configured with settings that should suit the majority of users. However, in some circumstances, you may wish to modify the default behavior of the Editor.

:::note

These settings affect only you and are global for the whole project. They are 'sticky' and persist over multiple sessions.

:::

Navigate to the `EDITOR` section and expand the panel:

[Image: Editor Settings]

Here is a breakdown of the available settings:

## Settings

| Setting | Description |
| --- | --- |
| **Grid Divisions**               | Divisions specifies the number of grid cells in each horizontal direction. Set to 0 to disable the grid. |
| **Grid Division Size**           | Size specifies the size of a cell. |
| **Snap**                         | Set the increment for [gizmo](../viewport.md#gizmos) snapping. Hold Shift or use the Snap toggle on the toolbar to enable snapping while using gizmos. |
| **Zoom Sensitivity**             | Change this value if you want to adjust the zoom sensitivity in the Editor viewport. |
| **Camera Depth Grabpass**        | Enable generating a depth map texture for the editor viewport. Required to preview certain material effects. |
| **Camera Color Grabpass**        | Enable generating a color map texture for the editor viewport. Required to preview certain material effects. |
| **Camera Clip Near**             | Adjust the editor camera Near clip value. This setting does not affect the game. |
| **Camera Clip Far**              | Adjust the editor camera Far clip value. This setting does not affect the game. |
| **Camera Clear Color**           | Set the editor camera clear color. This does not affect the game. |
| **Camera Tonemapping**           | Set the editor camera tone mapping. This setting does not affect the game. |
| **Camera Gamma**                 | Set the editor camera gamma correction. This setting does not affect the game. |
| **Show Fog**                     | Enable fog rendering in the viewport. |
| **Icons Size**                   | Size of icons displayed in the editor viewport. |
| **Locale**                       | The locale to preview in the editor and when you launch the application. This is only visible to you, not to other team members. |
| **Chat Notifications**           | Receive notifications for the Editor's built-in [real-time chat](../../realtime-collaboration.md#real-time-chat). |
| **Rename Duplicated Entities**   | When enabled, duplicated entities are renamed by appending an incrementing number to ensure uniqueness. For example, 'Box' becomes 'Box2'. |
| **Lightmapper Auto Bake**        | Controls whether the [runtime lightmapper](/user-manual/graphics/lighting/runtime-lightmaps) automatically rebakes lightmaps whenever the scene is updated. |

--------------------------------------------------------------------------------

## Engine Settings

URL: https://developer.playcanvas.com/user-manual/editor/interface/settings/engine/

Configure which PlayCanvas Engine version is used when launching, publishing, or downloading builds.

:::note

These settings affect all users on the currently active [branch](../../version-control/branches.md) of the project.

:::

Navigate to the `ENGINE` section and expand the panel:

[Image: Engine Settings]

Here is a breakdown of the available settings:

## Settings

| Setting | Description |
| --- | --- |
| **Engine Version** | The engine to use when you click Launch or when you publish or download a build. This setting is only valid during your current session and is not shared among team members. Options:<ul><li><strong>Previous</strong>: the latest patch of the previous minor build</li><li><strong>Current</strong>: the latest stable build</li><li><strong>Release Candidate</strong>: the forthcoming minor build yet to be promoted as the current stable build in the Editor.</li></ul> |

### Notes

- Engine versions may affect rendering features and behavior. Verify your project after switching.

--------------------------------------------------------------------------------

## Input Settings

URL: https://developer.playcanvas.com/user-manual/editor/interface/settings/input/

Enables or disables input device handling for the application.

:::note

These settings affect all users on the currently active [branch](../../version-control/branches.md) of the project.

:::

Navigate to the `INPUT` section and expand the panel:

[Image: Input Settings]

Here is a breakdown of the available settings:

## Settings

| Setting | Description |
| --- | --- |
| **Keyboard** | Enable keyboard input. Disable to ignore keyboard input in your application. |
| **Mouse** | Enable mouse input. Disable to ignore mouse input in your application. |
| **Touch** | Enable touch input. Disable to ignore touch input in your application. |
| **Gamepads** | Enable gamepad input. Disable to ignore gamepad input in your application. |

### Notes

- Disabling unused input methods can reduce event handling overhead.

--------------------------------------------------------------------------------

## Launch Page Settings

URL: https://developer.playcanvas.com/user-manual/editor/interface/settings/launch-page/

The **Launch Page** settings control browser-level features used when running your project from the PlayCanvas Editor.

:::note

These settings affect all users on the currently active [branch](../../version-control/branches.md) of the project.

:::

Navigate to the `LAUNCH PAGE` section and expand the panel:

[Image: Launch Page Settings]

Here is a breakdown of the available settings:

## Settings

| Setting | Description |
| --- | --- |
| **Enable SharedArrayBuffer** | Adds the required headers on the launch page to enable SharedArrayBuffer. |

### Notes

- [`SharedArrayBuffer`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/SharedArrayBuffer) is required for some advanced features such as multithreaded physics.
- When enabled, ensure your hosting setup serves the correct HTTP headers:
  - `Cross-Origin-Opener-Policy: same-origin`
  - `Cross-Origin-Embedder-Policy: require-corp`

--------------------------------------------------------------------------------

## Layers Settings

URL: https://developer.playcanvas.com/user-manual/editor/interface/settings/layers/

Layers determine which objects are rendered together and in what order.

:::note

These settings affect all users on the currently active [branch](../../version-control/branches.md) of the project.

:::

Navigate to the `LAYERS` section and expand the panel:

[Image: Layers Settings]

Here is a breakdown of the available settings:

## Layers

- **Add Layer**: Creates a new layer for organizing scene objects.
- Each layer can be renamed and removed as needed.

Default layers:

- **World**
- **Depth**
- **Skybox**
- **Immediate**
- **UI**

## Render Order

Defines the sequence in which layers are drawn, and separates opaque and transparent rendering passes.

| Setting | Description |
| --- | --- |
| **Layer** | The name of the render layer. |
| **Pass** | Either:<ul><li><strong>Opaque</strong>: Renders opaque mesh instances</li><li><strong>Transparent</strong>: Renders semi‑transparent mesh instances</li></ul> |
| **Enabled** | Enables or disables this part of the layer. When a part is disabled, the mesh instances of that part will not be rendered. |

### Notes

- Transparent objects must be rendered after opaque objects to display correctly.
- Layers can be reordered by dragging in the **Render Order** list.

--------------------------------------------------------------------------------

## Lightmapping Settings

URL: https://developer.playcanvas.com/user-manual/editor/interface/settings/lightmapping/

Controls baked lighting resolution, filtering, and ambient occlusion.

:::note

These settings affect all users on the currently active [branch](../../version-control/branches.md) of the project.

:::

Navigate to the `LIGHTMAPPING` section and expand the panel:

[Image: Lightmapping Settings]

Here is a breakdown of the available settings:

## General

| Setting | Description |
| --- | --- |
| **Size Multiplier** | The resolution of auto-generated lightmap textures is based on the area of geometry in world space and the size multipliers of the model and scene. Changing this value affects lightmap resolution across the whole scene. |
| **Max Resolution** | Maximum resolution for auto-generated lightmap textures. |
| **Mode** | The lightmap baking mode:<ul><li><strong>Color Only</strong>: A single color lightmap</li><li><strong>Color and Direction</strong>: A color lightmap plus dominant light direction (used for bump/specular)</li></ul> |

## Filtering

| Setting | Description |
| --- | --- |
| **Filter** | Enable a bilateral filter on runtime-baked lightmaps. |
| **Range** | The range parameter of the bilateral filter. |
| **Smoothness** | The spatial parameter of the bilateral filter. |

## Ambient Bake

| Setting | Description |
| --- | --- |
| **Ambient Bake** | Bake ambient light into lightmaps. |
| **Samples** | Number of samples to use when baking ambient light. |
| **Sphere Part** | The portion of the sphere to include when baking ambient light. |
| **Occlusion Brightness** | Brightness of the baked ambient occlusion. |
| **Occlusion Contrast** | Contrast of the baked ambient occlusion. |

--------------------------------------------------------------------------------

## Localization Settings

URL: https://developer.playcanvas.com/user-manual/editor/interface/settings/localization/

Manages the JSON files for supporting multiple languages.

:::note

These settings affect all users on the currently active [branch](../../version-control/branches.md) of the project.

:::

Navigate to the `LOCALIZATION` section and expand the panel:

[Image: Localization Settings]

Here is a breakdown of the available settings:

## Settings

| Setting | Description |
| --- | --- |
| **Assets** | JSON assets that contain localization data. Assets in this list are automatically parsed for localization data when loaded. These are used to localize your text elements. |
| **Create New Asset** | Creates a new localization JSON asset with the default en-US format. |

### Notes

- Localization assets typically contain key-value pairs for translations.
- Switching languages at runtime requires code to update UI and scene content.

--------------------------------------------------------------------------------

## Network Settings

URL: https://developer.playcanvas.com/user-manual/editor/interface/settings/network/

Configures network retry behavior for asset loading.

:::note

These settings affect all users on the currently active [branch](../../version-control/branches.md) of the project.

:::

Navigate to the `NETWORK` section and expand the panel:

[Image: Network Settings]

Here is a breakdown of the available settings:

## Settings

| Setting | Description |
| --- | --- |
| **Asset Retries** | The maximum number of times to retry loading an asset if it fails to load. If an asset request fails, it will be retried with exponential backoff. |

### Notes

- A higher retry count improves resilience for poor connections, but can delay error reporting.

--------------------------------------------------------------------------------

## Physics Settings

URL: https://developer.playcanvas.com/user-manual/editor/interface/settings/physics/

Controls the global physics simulation settings.

:::note

These settings affect all users on the currently active [branch](../../version-control/branches.md) of the project.

:::

Navigate to the `PHYSICS` section and expand the panel:

[Image: Physics Settings]

Here is a breakdown of the available settings:

## Settings

| Setting | Description |
| --- | --- |
| **Physics Library** | Add the Ammo asm.js and WebAssembly modules to this project from the PlayCanvas Store. |
| **Gravity** | Gravity is the acceleration applied every frame to all rigid bodies in your scene. By default, it is set to -9.8 meters per second per second, which essentially approximates Earth's gravity. If you are making a game in space, you might want to set this to 0, 0, 0 (zero g). |

### Notes

- Without Ammo.js imported, physics features will not function.
- Gravity can be adjusted to simulate different worlds.

--------------------------------------------------------------------------------

## Rendering Settings

URL: https://developer.playcanvas.com/user-manual/editor/interface/settings/rendering/

Controls lighting, resolution, and rendering features.

:::note

These settings affect all users on the currently active [branch](../../version-control/branches.md) of the project.

:::

Navigate to the `RENDERING` section and expand the panel:

[Image: Rendering Settings]

Here is a breakdown of the available settings:

## Environment

| Setting | Description |
| --- | --- |
| **Ambient Color** | The color of the scene's ambient light, specified in sRGB color space. |
| **Skybox** | A cubemap asset rendered behind your 3D scene. Also used as the default environment map for physical materials when prefiltered. |
| **Type** | Select the projection used to render the skybox cubemap:<ul><li><strong>Infinite</strong>: Rendered at infinity</li><li><strong>Box</strong>: Mapped to a box mesh</li><li><strong>Dome</strong>: Mapped to a hemispherical dome</li></ul> |
| **Mesh Position / Rotation / Scale** | The position, rotation and scale of the sky mesh. |
| **Center** | The relative normalized offset of the sky from the ground. |
| **Intensity** | The skybox intensity, used to match exposure levels. |
| **Rotation** | Rotation of the skybox. |
| **Mip** | Mip level of the prefiltered skybox. Higher values select lower-resolution, more blurred mips. |

## Clustered Lighting

| Setting | Description |
| --- | --- |
| **Clustered Lighting** | Enable clustered lighting. |
| **Cells (X, Y, Z)** | Number of cells per world-space axis used to subdivide the space containing lights. |
| **Max Lights Per Cell** | Maximum number of lights a cell can store. |
| **Cookie Atlas Resolution** | Resolution of the atlas texture storing all non-directional cookie textures. |
| **Cookies Enabled** | Clustered lights support cookies. |
| **Shadows Enabled** | Clustered lights support shadows. |
| **Shadow Atlas Resolution** | Resolution of the atlas texture storing all non-directional shadow textures. |
| **Shadow Type** | The type of shadow filtering used by all shadows. |
| **Area Lights Enabled** | Clustered lights support area lights. |

## Exposure & Fog

| Setting | Description |
| --- | --- |
| **Exposure** | The exposure value tweaks the overall brightness of the scene. |
| **Fog** | Controls an approximation of ambient fog in your scene. Types:<ul><li><strong>None</strong>: Fog disabled</li><li><strong>Linear</strong>: Fades in linearly between Fog Start and Fog End distances</li><li><strong>Exp</strong>: Fades in from the view position according to an exponential function</li><li><strong>Exp2</strong>: Fades in from the view position according to an exponential squared function</li></ul> |
| **Fog Density** | Controls the rate at which fog fades in for Exp and Exp2 fog types. Larger values cause fog to fade in more quickly. Must be positive. |
| **Fog Start / End** | Distances, in scene units, where fog starts to fade in (start) and where it reaches maximum (end). |

## Resolution

| Setting | Description |
| --- | --- |
| **Resolution Width / Height** | The width/height of your application in pixels. |
| **Resolution Mode** | Decides whether the canvas resolution changes when it is resized. |
| **Fill Mode** | Decides how the canvas fills the browser window. |

## Device & API

| Setting | Description |
| --- | --- |
| **Device Order** | The order in which attempts are made to create the graphics devices. |
| **Enable WebGPU** | When enabled, the application will try to use WebGPU if available. |
| **Enable WebGL 2.0** | When enabled, the application will try to use WebGL 2.0 if available. |

## Rendering Options

| Setting | Description |
| --- | --- |
| **Power Preference** | Provides a hint to WebGL regarding the preferred power mode:<ul><li><strong>Default</strong>: Browser decides</li><li><strong>High Performance</strong>: Prioritize rendering performance</li><li><strong>Low Power</strong>: Prioritize power saving</li></ul> |
| **Anti-Alias** | When disabled, anti-aliasing is disabled for the back buffer. |
| **Device Pixel Ratio** | Multiplies the canvas back buffer resolution by the device pixel ratio (e.g., 2x on Retina). Increases sharpness and GPU/memory usage. |
| **Transparent Canvas** | Makes the canvas background transparent so the web page shows through. Useful for overlaying the app on custom page designs or UI. |
| **Preserve Drawing Buffer** | Preserves the drawing buffer until explicitly cleared. Useful for taking screenshots. |

## External Libraries

| Setting | Description |
| --- | --- |
| **Basis Library** | Add the necessary libraries to support Basis compression. |
| **Draco Library** | Add the necessary libraries to support Draco compression. |

--------------------------------------------------------------------------------

## Toolbar

URL: https://developer.playcanvas.com/user-manual/editor/interface/toolbar/

The Editor's main toolbar can be found on the left, vertical edge of the Editor. You can access many common operations via the toolbar. Here is a list of the functionality available:

| Button | Command | Description |
| ------ | ------- | ----------- |
| [Image: Menu] | **Menu** | Open the main menu to access the most commonly needed Editor functions. |
| [Image: Gizmo Translate] | **Translate** | Activate the translate [gizmo](../viewport#gizmos) in the [Viewport](../viewport). |
| [Image: Gizmo Rotate] | **Rotate** | Activate the rotate [gizmo](../viewport#gizmos) in the [Viewport](../viewport). |
| [Image: Gizmo Scale] | **Scale** | Activate the scale [gizmo](../viewport#gizmos) in the [Viewport](../viewport). |
| [Image: Element Scale] | **Resize Element** | Activate the User Interface [Element resizing gizmo](/user-manual/user-interface/elements#element-resizing) in the [Viewport](../viewport).  |
| [Image: World Local] | **World/Local** | Switch between local and world coordinate systems for the active [gizmo](../viewport#gizmos) in the [Viewport](../viewport). |
| [Image: Snap] | **Snap** | Enable snapping when using the [gizmos](../viewport#gizmos) in the [Viewport](../viewport). |
| [Image: Focus] | **Focus** | Zoom the [Viewport](../viewport) camera to the currently selected Entity. |
| [Image: Undo] | **Undo** | Undo the last operation. |
| [Image: Redo] | **Redo** | Redo the last operation. |
| [Image: Bake] | **Lightmapper** | Access the [Lightmapper](/user-manual/graphics/lighting/runtime-lightmaps) bake and auto-rebake controls. |
| [Image: Code Editor] | **Code Editor** | Open the [Code Editor](/user-manual/editor/scripting/code-editor). |
| [Image: Publish] | **Publish** | [Publish a build](/user-manual/editor/publishing/web/playcanvas-hosting#publishing-a-new-build) of your project. |
| [Image: How Do I] | **How Do I...?** | Toggle the 'How Do I...?' help widget in the [Viewport](../viewport). |
| [Image: Controls] | **Controls** | Show the list of [controls and keyboard shortcuts](../keyboard-shortcuts) supported by the Editor. |
| [Image: Feedback] | **Feedback** | Visit the [forum](https://forum.playcanvas.com/t/playcanvas-editor-feedback) to leave feedback about the Editor. |
| [Image: Settings] | **Settings** | Load Editor and Scene Settings into the [Inspector](../inspector). |

--------------------------------------------------------------------------------

## Viewport

URL: https://developer.playcanvas.com/user-manual/editor/interface/viewport/

[Image: Viewport]

The viewport shows your scene as currently rendered. You can freely move around the scene by manipulating the Editor's current camera.

## Cameras {#cameras}

Initially the Editor is set to use the **Perspective** camera. This camera is as if a movie camera was floating in your scene. You can use the camera dropdown menu to view the scene using various other cameras.

[Image: Camera Dropdown]

The **orthographic** cameras: Top, Bottom, Front, Back, Left, Right, let you view a version of the scene with no perspective. Useful for fine-tuning positions.

You can also use the camera menu to select any of the camera Entities in your scene. This way you can position your in-game camera exactly as required.

## Gizmos {#gizmos}

[Image: Gizmos]

The 3-Colored Axis you can see in the screenshot above is called a [Gizmo](/user-manual/glossary#gizmo). This is used to manipulate the transform matrix of the selected Entity. There are three types of Gizmo: Translate (with arrows on the ends of the axes); Rotate (which is made up of three colored rings) and Scale (with cubes on the ends of the axes).

## Render Mode {#render-mode}

You can modify the viewport render mode using this drop-down menu in the top-right of the viewport:

[Image: Viewport Render Mode Menu]

It allows you to toggle wireframe rendering:

[Image: Viewport Wireframe]

You can also visualize your scene in various debug render modes. This restricts the rendered scene to just show albedo, normals, AO, emission and more.

[Image: Viewport Render Modes]

--------------------------------------------------------------------------------

## Projects

URL: https://developer.playcanvas.com/user-manual/editor/projects/

## Overview

Projects are the foundation for everything you create in the PlayCanvas Editor. A project contains all the assets, scenes, scripts, and settings needed to build your application or game. This section covers all aspects of project management, from creation to configuration.

## Project Management Topics

### Getting Started

- **[Creating Projects](creating)** - How to create new projects.

### Working with Projects

- **[Project Dashboard](dashboard)** - Understanding and navigating the project dashboard interface.
- **[Project Settings](settings)** - Configure your project's name, description, privacy, and branding.
- **[Team Management](team-management)** - Adding collaborators and managing permissions.
- **[Development Logs](dev-logs)** - Using dev logs for project updates and team communication.

### Advanced Management

- **[Backup and Archiving](backup-archiving)** - Protecting your projects with backups and archives.
- **[Ownership and Transfers](ownership-transfers)** - Transferring projects between users and organizations.

## Quick Actions

Once you understand the basics of project management, you can:

- Access the [Editor](../interface) to build your project.
- Test your project using the [Launch Page](../interface/launch-page).
- Use [Real-time Collaboration](../realtime-collaboration) to build with others.
- Use [Version Control](../version-control) to manage your development workflow.

--------------------------------------------------------------------------------

## Backup and Archiving

URL: https://developer.playcanvas.com/user-manual/editor/projects/backup-archiving/

Protecting your PlayCanvas projects with regular backups is essential for safeguarding against accidental deletion, malicious actions, or data loss. This section covers all the methods available for backing up and restoring your projects.

## Why Backup Your Projects?

Regular backups protect against:

- Accidental deletion of assets or scenes
- Malicious team member actions
- Account security issues
- Data corruption or technical problems
- Development mistakes that break your project

## Backup Methods

### 1. Forking

The simplest way to create a backup is to fork your own project. Forking is [covered comprehensively](creating.md#fork-an-existing-project) earlier in this section.

### 2. Archiving (via playcanvas.com)

Archive files provide complete project backups that can be stored offline and imported later.

#### From the Projects List

[Image: Export Archive]

To export a project archive:

1. Ensure you are logged in
2. Visit the User Page that owns the project you wish to export
3. Locate the project in the project list
4. Click the down arrow next to the right of the project name
5. Select **"Export Project"**
6. Wait for the archive to be generated
7. Click **DOWNLOAD** to download the zip file

:::danger

Exported projects **do not** include:

- Version control history
- Any branches other than `main`
- The project's [Dev Log](dev-logs.md)

:::

### 3. Archiving (via REST API)

For automated and advanced backup scenarios, use the PlayCanvas REST API.

#### API Advantages

- **Automated Backups**: Set up scheduled backup scripts
- **Branch Selection**: Choose which branch to export
- **CI Integration**: Include in continuous integration workflows
- **Bulk Operations**: Backup multiple projects programmatically

#### Using the API

The [Project Archive API](/user-manual/api/project-archive/) allows you to:

```bash
# Export a project via REST API
curl -H "Authorization: Bearer {accessToken}" \
     -H "Content-Type: application/json" \
     -X POST \
     -d '{"branch_id": "99999999-9999-9999-9999-999999999999"}' \
     "https://playcanvas.com/api/projects/{projectId}/export"
```

#### Automation Tools

PlayCanvas provides an [open-source Node.js tool](https://github.com/playcanvas/playcanvas-rest-api-tools#archiving-a-project) to simplify automated backups:

- Command-line interface for easy scripting
- Support for multiple projects
- Configurable backup schedules
- Integration with cloud storage services

## Restoring Projects {#restoring-projects}

### From Archive Files

[Image: Import Archive]

To restore a project from an archive:

1. Go to your [Projects Dashboard](https://playcanvas.com/projects)
2. Click **"Import Project"** on the left side
3. Select your archive ZIP file
4. Choose a name for the restored project
5. Wait for the import to complete

#### Restore Considerations

- Creates a completely new project
- Original project remains unchanged
- All team members must be re-added
- Project settings may need reconfiguration
- Version control history is not restored

### From Forks

Since forks are independent projects, "restoring" from a fork involves:

1. Accessing your forked backup project
2. Manually copying changed assets back to the original
3. Or using the forked project as your new main project
4. Updating team access and settings as needed

## Backup Strategies

### Regular Backup Schedule

Establish a consistent backup routine:

📅 **Daily Backups** (for active development):

- Fork projects before major changes
- Export archives for critical milestones

📅 **Weekly Backups** (for ongoing projects):

- Create comprehensive archive exports
- Test restore procedures periodically

📆 **Monthly Backups** (for stable projects):

- Full project archives with documentation
- Long-term storage planning

### Backup Storage

Store your backups securely:

**Local Storage:**

- External hard drives or NAS devices
- Encrypted backup drives
- Regular verification of backup integrity

**Cloud Storage:**

- Google Drive, Dropbox, or OneDrive
- AWS S3 or similar cloud storage services
- Version-controlled backup repositories

**Distributed Storage:**

- Multiple backup locations
- Team member backup sharing
- Geographic distribution for disaster recovery

## Advanced Backup Techniques

### Version Control Integration

Coordinate backups with your [version control workflow](../version-control/index.md):

- **Branch-specific Backups**: Export different branches separately
- **Release Backups**: Archive every stable release
- **Feature Backups**: Backup before merging major features

### Automated Workflows

Set up automated backup systems:

**Continuous Integration:**

- Trigger backups on specific events
- Integrate with GitHub Actions or similar services
- Automatic backup validation and testing

**Scheduled Scripts:**

- Daily/weekly backup scripts using the REST API
- Cloud storage integration
- Backup rotation and cleanup

### Team Backup Coordination

For team projects:

- **Designated Backup Manager**: Assign backup responsibilities
- **Shared Backup Storage**: Team-accessible backup locations
- **Backup Verification**: Regular restore testing
- **Documentation**: Clear backup and restore procedures

## Recovery Planning

### Disaster Recovery

Prepare for worst-case scenarios:

1. **Identify Critical Assets**: Determine what must be preserved
2. **Recovery Time Objectives**: How quickly you need to restore
3. **Recovery Point Objectives**: How much data loss is acceptable
4. **Communication Plan**: How to notify team members
5. **Alternative Workflows**: Temporary development procedures

### Testing Restores

Regularly test your backup system:

- **Partial Restores**: Test individual asset recovery
- **Full Restores**: Complete project restoration
- **Team Training**: Ensure team members understand procedures
- **Documentation Updates**: Keep recovery procedures current

:::warning

Untested backups are unreliable backups. Regular testing ensures your backup strategy works when you need it most.

:::

--------------------------------------------------------------------------------

## Creating Projects

URL: https://developer.playcanvas.com/user-manual/editor/projects/creating/

You can create Editor projects in two ways:

1. Create a project based on a number of predefined starter kits.
2. Fork an existing project, using it as a starting point.

## Create a New Project from a Starter Kit

To create a new project:

1. Log in to your PlayCanvas account and navigate to your [User Page](https://playcanvas.com/)
2. Click the **NEW** button in the top right corner

[Image: New Project Button]

### Selecting a Starter Kit

All Editor projects are created based on a starter kit.

[Image: New Project Dialog]

PlayCanvas offers several starter kits to get you started quickly:

- **Blank Project**: A minimal project with a camera, light, sky and some basic shapes
- **Model Viewer**: A simple starter kit for viewing 3D models
- **VR Kit**: A starter kit showing how to enter and exit a WebXR-based VR session
- **Roll a Ball**: A simple physics-based 3D game
- **WebXR AR Starter Kit**: A starter kit showing how to initiate a WebXR-based AR session
- **First Person Shooter**: The basis for an FPS-type game or experience

Pick a starter kit, set a name, description and owner, and select whether your project should be public or private. Then hit **CREATE**.

:::note

You can only create private projects with a Personal or Organization plan.

:::

## Fork an Existing Project {#fork-an-existing-project}

Forking allows you to create a new project based on an existing public project (or a private project of which you are a team member). This is useful for:

- Using another project as a starting point for your own work
- Creating backups of your own projects
- Learning from example projects created by other developers
- Experimenting with modifications without affecting the original

### How to Fork a Project

To fork a project:

1. Navigate to the project's dashboard page
2. Click the **Fork** button in the dashboard header
3. Enter a name for your new project
4. Click **FORK**

[Image: Fork Project Button]

### What Gets Forked

When you fork a project, the entire main branch is copied to your new project:

- All scenes and their settings
- All assets (textures, models, scripts, etc.)
- Project settings

### What Doesn't Get Forked

Forking has some limitations:

- **Version control history** is not preserved
- **Only the main branch** is copied (other branches are ignored)
- **Team members** are not copied (you become the sole owner)
- **Development logs** and project activity are not copied

### After Forking

Once your fork is created:

- You have full ownership and control over the new project
- You can modify, delete, or further develop the project as needed
- The fork is completely independent from the original project
- You can add your own team members and configure project settings

Forked projects appear in your project list just like any other project you've created.

:::tip

The project dashboard for your fork has a **Forked From** property. Click the link to visit the dashboard of the upstream project that was originally forked.

:::

--------------------------------------------------------------------------------

## Project Dashboard

URL: https://developer.playcanvas.com/user-manual/editor/projects/dashboard/

The Project Dashboard is the control center for your project, providing access to all major project functions and information.

[Image: Dashboard]

## Dashboard Overview

From the Project Dashboard you can:

- Access the PlayCanvas Editor
- Run your project's primary app
- View the project's development log
- Monitor project activity
- Edit project settings
- Manage team members

## Dashboard Header

The Dashboard Header provides quick access to common project actions:

[Image: Dashboard Header]

### EDITOR Button

Click **EDITOR** to open the PlayCanvas Editor in a new tab, where you can:

- Create and edit scenes and entities
- Import and manage assets
- Configure components and settings
- Publish apps

### PLAY Button

Click **PLAY** to launch the Primary Build of your project in a new tab. This allows you to:

- Test your latest changes
- Experience your project as end users would
- Debug gameplay and user interface
- Share a playable version with others

### Watch Button

Click **Watch** to show Dev Log updates in the Global Feed when the **Watching** filter is enabled.

### Star Button

Click **Star** to like or favorite this project. Starring helps:

- Show appreciation to the developer
- Contribute to project popularity metrics

### Fork Button

Click **Fork** to create a copy of this project in your account. Forking:

- Creates a new project based on the current main branch
- Copies all assets and scenes (but not version history)
- Allows you to experiment without affecting the original
- Is useful for learning from other projects

--------------------------------------------------------------------------------

## Dev Logs

URL: https://developer.playcanvas.com/user-manual/editor/projects/dev-logs/

Development Logs (or Dev Logs for short) are a powerful feature for communicating project updates, milestones, and news with your team and community. They help track progress and keep stakeholders informed about your project's development.

[Image: Dev Log]

## Understanding Dev Logs

### What are Dev Logs?

Development logs are project-specific news posts that allow you to:

- Share project updates and milestones
- Communicate with team members and followers
- Document development progress
- Build community around your project
- Showcase new features and improvements

### Where Dev Logs Appear

Dev logs are displayed in two locations:

- **Project Dashboard Overview**: Chronological list of posts related to your project
- [**The Global Feed**](https://playcanvas.com/feed): Chronological list of all posts published by the entire community

## Writing Your First Dev Log

To create a development log post:

1. Navigate to your Project Dashboard
2. Go to the **OVERVIEW** section
3. In the center of the page, you will see the **DEV LOG FEED** section
4. Click in the text box showing the text **What changes have you made today?**
5. Add your title and content
6. Click **PREVIEW** to see how your post will appear when it is published
7. Click **POST** to finally share your update

:::important

When you create a new project, you are initially blocked from posting to your Dev Log. You must first publish a build of your application before you can post.

:::

:::tip

Dev Log posts support both [Markdown](https://daringfireball.net/projects/markdown/syntax) and [Emoji](https://www.webfx.com/tools/emoji-cheat-sheet/). Consider leveraging both to make your posts more engaging!

:::

### Content Ideas

Effective Dev Log posts might include:

- **Milestone Announcements**: Completed features or phases
- **Behind-the-Scenes**: Development process insights
- **Feature Showcases**: New gameplay mechanics or systems
- **Art and Asset Updates**: Visual progress and artwork
- **Technical Deep-dives**: Implementation details and solutions
- **Team Updates**: New members or role changes
- **Release Announcements**: Version updates or launch news

Dev Logs are a great tool to keep your community informed about your projects - so be sure to use them!

--------------------------------------------------------------------------------

## Ownership and Transfers

URL: https://developer.playcanvas.com/user-manual/editor/projects/ownership-transfers/

Project ownership determines who has ultimate control over a PlayCanvas project, including the ability to delete it, transfer ownership and control access. This section covers transferring ownership between users and organizations.

## Owner Responsibilities

Project owners have the following abilities (over and above what Admins are able to do):

- **Project Deletion**: Can permanently delete the project
- **Team Management**: Can add/remove any team member (including Admins)
- **Transfer Rights**: Can transfer ownership to others

## Initiating Ownership Transfers {#initiating-ownership-transfers}

### From the Projects List

[Image: Transfer Ownership Menu]

To transfer ownership of a project:

1. When logged in, go to your [User Page](https://playcanvas.com/)
2. Find the project you want to transfer
3. Click the arrow next to the project name
4. Select **"Transfer Ownership"** from the dropdown menu

### Transfer Dialog

[Image: Transfer Ownership Dialog]

In the transfer dialog:

1. **Enter the recipient's username** or organization name
2. Click **FIND** or press Enter to verify the recipient
3. Review the transfer details carefully
4. Click **TRANSFER** to send the transfer request

### Supported Recipients

You can transfer ownership to:

- **Individual Users**: Any PlayCanvas user account
- **Organizations**: PlayCanvas organization accounts

## The Transfer Process

### 1. Transfer Request

When you initiate a transfer:

- A transfer request is sent to the recipient
- The original owner retains control until accepted
- The request appears in the recipient's project list
- No changes occur until the transfer is accepted

### 2. Recipient Acceptance

[Image: Transfer Ownership Accept]

The recipient will see:

- Transfer request at the top of their project list
- Details about the project being transferred
- Options to **Accept** or **Decline** the transfer

### 3. Transfer Completion

Once accepted:

- **Ownership transfers immediately** to the new owner
- **All team members are removed** except the new owner
- **Original owner loses all access** unless re-added by new owner
- **Transfer cannot be reversed** without initiating a new transfer

:::warning

Transfer completion removes all existing team members from the project. The new owner must manually re-add team members if needed.

:::

--------------------------------------------------------------------------------

## Project Settings

URL: https://developer.playcanvas.com/user-manual/editor/projects/settings/

The Project Settings allow you to configure various aspects of your project including basic information, privacy settings, and branding options.

[Image: Project Settings]

## Accessing Project Settings

To access your project settings:

1. Open your project dashboard
2. Navigate to the **SETTINGS** tab

## Project Information

### Game Artwork

Assign a square 720x720 pixel image to represent your project. This image is used for:

- Project thumbnails in the dashboard
- Published game representations
- Social media sharing (Twitter, Facebook, etc.)
- PlayCanvas.com project listings

**Requirements:**

- Square aspect ratio (720x720 recommended)
- High quality and visually appealing
- Represents your project accurately
- Clear and readable at small sizes

:::tip

If you want your project to be featured on playcanvas.com, you must add a project image.

:::

### Project Name

Currently, it is not possible to change the project name after creation. Choose your project name carefully when creating a new project.

### Project Description

You can update your project description at any time. A good description should:

- Clearly explain what your project is about
- Include relevant keywords for discoverability
- Mention the target audience or platform
- Highlight unique features or selling points

:::note

When your project is shared to X or Facebook from playcanv.as, your project description is used for the text of the post.

:::

## Project Visibility

PlayCanvas Editor projects can either be public or private.

:::info

Only users with premium accounts can create and access private projects.

:::

### Public vs Private Projects

| Feature | Public Projects | Private Projects |
|---------|-----------------|------------------|
| **Visibility** | Visible to all PlayCanvas users | Only visible to project team members |
| **Discovery** | Can be discovered and forked by others | Cannot be discovered or forked |
| **Listings** | Appear in [public project listings](https://playcanvas.com/explore/featured) | Excluded from public listings |
| **Cost** | Free for all users | Require [premium account subscription](https://playcanvas.com/plans) |
| **Best For** | Portfolio pieces and open-source projects | Commercial projects and confidential work |

### Changing Project Visibility

To change your project's privacy:

1. Go to Project Settings
2. Toggle the "Private" setting
3. Confirm the change by hitting **SAVE**

--------------------------------------------------------------------------------

## Team Management

URL: https://developer.playcanvas.com/user-manual/editor/projects/team-management/

Effective team management is crucial for collaborative development in PlayCanvas. This section covers how to add team members, manage permissions, and coordinate with your development team.

## Adding Team Members

To add team members to your project:

1. Navigate to your Project Dashboard
2. Go to the **SETTINGS** section
3. Scroll to the **TEAM** section
4. In the edit box, enter the username or email of the person you want to add and click **SEND**

## User Permissions

PlayCanvas projects support three permission levels: **Read**, **Write** and **Admin**.

| Capability | Read Access | Write Access | Admin Access |
|------------| :---------: | :----------: | :----------: |
| **View scenes and assets** | ✅ | ✅ | ✅ |
| **View project settings** | ✅ | ✅ | ✅ |
| **Edit scenes and assets** | ❌ | ✅ | ✅ |
| **Publish builds** | ❌ | ✅ | ✅ |
| **Edit project settings** | ❌ | ❌ | ✅ |
| **Manage team members** | ❌ | ❌ | ✅ |
| **Delete project** | ❌ | ❌ | ✅ |
| **Transfer project ownership** | ❌ | ❌ | ✅ |
| **Best suited for** | Stakeholders, testers, observers | Developers, content creators | Project leads, owners |

### Changing Permissions

To modify a team member's permissions:

1. Go to Project **SETTINGS** > **Team**
2. Find the user in the team list
3. Click on their current permission level
4. Select the new permission level
5. Confirm the change by clicking **SAVE**

## Removing Team Members

To remove someone from your project:

1. Go to Project **SETTINGS** > **Team**
2. Find the user you want to remove
3. Mouse over the tick icon and it will switch to an **X** - click it
4. Confirm the removal by clicking **OK**

:::warning

When you remove a team member, they will lose all access to the project immediately. Make sure this is intentional before confirming.

:::

## Organization Team Management

### Organization Projects

If your project belongs to an organization, team management works slightly differently:

#### Organization Administrators

- Can add themselves to any organization project
- Can manage project teams across the organization
- Can access billing and seat management

#### Seat Management

For organization accounts with seat limits:

- Each team member on a private project occupies a seat
- Public projects don't require seats
- Organization owners can manage seat allocation
- Removing users from projects frees up seats

### Adding Organization Members

Organization administrators can add themselves to projects:

1. Go to the organization account page
2. Find the project in the projects list
3. Click the dropdown arrow
4. Select "Add me as admin"

--------------------------------------------------------------------------------

## Publishing

URL: https://developer.playcanvas.com/user-manual/editor/publishing/

Once you have built your game, you will no doubt want to publish it and allow people to enjoy its majesty! PlayCanvas makes things very easy for you to target just about any operating system or app store.

## Where can I publish PlayCanvas games?

* [Web](/user-manual/editor/publishing/web)
* [Mobile](/user-manual/editor/publishing/mobile)
* [Desktop](/user-manual/editor/publishing/desktop)
* [Playable Ads](/user-manual/editor/publishing/playable-ads)

--------------------------------------------------------------------------------

## Desktop

URL: https://developer.playcanvas.com/user-manual/editor/publishing/desktop/

It is possible to convert your PlayCanvas app to a native executable for each of the major desktop operating systems: Windows, Mac OS X and Linux. To do this, we recommend using [NW.js](https://nwjs.io/). Instructions for generating the executable can be found on the [NW.js Wiki](https://github.com/nwjs/nw.js/wiki/How-to-package-and-distribute-your-apps).

--------------------------------------------------------------------------------

## Mobile

URL: https://developer.playcanvas.com/user-manual/editor/publishing/mobile/

PlayCanvas games are just web pages. An index.html file and a collection of resources (JavaScript files, JSON files, images and so on). They work great in web browsers, of course, but if you want to submit your game to a mobile app store such as Google Play or the Apple App Store, then you somehow need to convert your PlayCanvas game to a native app. There are a few products available to simplify the process for you:

* [Cordova](/user-manual/editor/publishing/mobile/cordova)
* [GoNative](/user-manual/editor/publishing/mobile/gonative)

Alternatively, another option is to create an Android or iOS app that consists of a single fullscreen WebView that loads your PlayCanvas app from local app resources.

--------------------------------------------------------------------------------

## Apache Cordova

URL: https://developer.playcanvas.com/user-manual/editor/publishing/mobile/cordova/

[Apache Cordova](https://cordova.apache.org/) is an open-source mobile development framework. It allows you to use standard web technologies - HTML5, CSS3, and JavaScript for cross-platform development. Applications execute within wrappers targeted to each platform, and rely on standards-compliant API bindings to access each device's capabilities such as sensors, data, network status, etc.

You can use Cordova to natively wrap your PlayCanvas app. You can then publish it to the iOS App Store and Android's Google Play. Cordova can also generate executables compatible with macOS and Windows.

## Installing Cordova

To get started, follow the [instructions](https://cordova.apache.org/docs/en/latest/guide/cli/index.html#installing-the-cordova-cli) for installing Cordova on your computer.

## Creating a Project

To create a project, issue the following command:

```sh
cordova create hello com.example.hello HelloWorld
```

`hello` is the folder in which the project is created. `com.example.hello` is the reverse domain-style identifier for your app. `HelloWorld` is the human readable title of your app (it is the name of the generated app icon, for example).

So, for a game like [Master Archer](https://playcanv.as/p/JERg21J8/), an appropriate command would be:

```sh
cordova create masterarcher com.playcanvas.masterarcher "Master Archer"
```

Once your project is created, you will find a file called `config.xml` in the project's root folder. Here you can configure or edit certain characteristics of your app. For example, you can optionally [set up custom icons](https://cordova.apache.org/docs/en/latest/config_ref/images.html) for your app, either globally or per-platform.

## Adding your PlayCanvas App

When you create a new Cordova project, it generates a skeleton web app in a folder called `www`. You can go ahead and delete everything in the `www` folder. Next, copy your PlayCanvas app files to this location.

If you're building on the Engine without the Editor, copy your app files into `www` such that your `index.html` file is in the root.

:::note

Audio asset files will need to be in Base64 format to load and play correctly. This is due to iOS being restrictive about what files can be loaded in the WebView via local disk.

We recommend using a tool like [Base64 Guru](https://base64.guru/converter/encode/audio) or automating this via a script.

:::

If you have built your app in the PlayCanvas Editor, we have an official external tool that will build and prepare the project to be most compatible with Cordova. This includes automating tasks such as converting the audio files to Base64 so that they can be loaded on iOS.

The official external tool can be found on GitHub [here](https://github.com/playcanvas/playcanvas-rest-api-tools#cordova-publish).

Follow the [setup steps](https://github.com/playcanvas/playcanvas-rest-api-tools#setup) from the readme in the GitHub repo.

And run the command for the [Cordova Publish script](https://github.com/playcanvas/playcanvas-rest-api-tools#cordova-publish) as shown in the readme.

This will create a ZIP of the project ready for Cordova. Extract its contents to the root of the `www` folder.

## Building Executables

You are now ready to build your app for any of the platforms supported by Cordova.

### Building for iOS

Building for iOS is limited to macOS based computers. You must also ensure you have Xcode installed. You can install it from the [Mac App Store](https://apps.apple.com/us/app/xcode/id497799835?mt=12).

To build your app for iOS, add the Cordova iOS platform to your project. From the root of your project, issue the command:

```sh
cordova platform add ios
```

Ensure that the version is `6.0.0` or higher. This is because version `6.0.0` upgraded the wrapper to use WKWebView instead of UIWebView, which brings better performance. Read more on [Cordova's blog](https://cordova.apache.org/announcements/2020/06/01/cordova-ios-release-6.0.0.html).

By default, if you attempt to run a Cordova-based PlayCanvas app, you will encounter several errors/exceptions related to cross-origin HTTP requests. To fix this, add the following to your project's `config.xml`:

```html
    <platform name="ios">
        
        <preference name="scheme" value="app" />
        <preference name="hostname" value="localhost" />

        
    </platform>
```

### Testing for iOS

You are now ready to test your app. For iOS, you can use Simulator or run on a physical iOS device. Simulator is installed as part of the Xcode tools, runs on your Mac and simulates the various iOS based devices. To run your app in Simulator, issue the following command:

```sh
cordova run ios
```

Once the executable has been generated in the build process, Simulator will start and load it. You should see something like the following:

[Image: Master Archer in Simulator]

To run on a physical device:

1. Connect your iOS device to your Mac via USB.
2. Open `platforms/ios/<my-project-name>.xcworkspace` in Xcode.
3. Navigate to the Signing & Capabilities settings for your project's Target and select a valid Team so that your app can be digitally signed before being deployed to your device.
4. Select your iOS device in Xcode's Scheme drop-down list.

    [Image: Xcode Scheme drop-down]

5. Press the Run button to build, deploy and run the application on your device.

    [Image: Xcode Run button]

Once you are happy with your app, you can ship it to [App Store Connect](https://developer.apple.com/app-store-connect/).

--------------------------------------------------------------------------------

## GoNative

URL: https://developer.playcanvas.com/user-manual/editor/publishing/mobile/gonative/

[GoNative](https://gonative.io/) is a paid third party service that can take a URL and create a native mobile application that uses a native WebView to run on mobile in a few clicks.

As it is using a WebView, the user has to be online to use the app and you will have to host your PlayCanvas application somewhere such as [AWS Web Hosting services](https://aws.amazon.com/websites/).

GoNative also provides simple integration of mobile native plugins that can be accessed from your PlayCanvas application such as Google AdMob and In-App Purchases. A full list of [plugins can be found here](https://gonative.io/plugins).

--------------------------------------------------------------------------------

## Playable Ads

URL: https://developer.playcanvas.com/user-manual/editor/publishing/playable-ads/

We are currently supporting the following playable ad networks:

* [Facebook](/user-manual/editor/publishing/playable-ads/fb-playable-ads)
* [Snapchat](/user-manual/editor/publishing/playable-ads/snapchat-playable-ads)

Other ad networks may be supported but have not yet been tested by the PlayCanvas team.

--------------------------------------------------------------------------------

## Facebook Playable Ad

URL: https://developer.playcanvas.com/user-manual/editor/publishing/playable-ads/fb-playable-ads/

PlayCanvas supports the [Facebook Playable Ad](https://www.facebook.com/business/ads/playable-ad-format) formats and requirements via an [official external tool on GitHub](https://github.com/playcanvas/playcanvas-rest-api-tools).

[Image: Facebook Playable Ads]

The tool can create both the single 2MB (uncompressed) HTML file and the 5MB (uncompressed) ZIP formats via the configuration options. Full specifications for Facebook Playable Ads can be found on their [help center](https://www.facebook.com/business/help/412951382532338).

## Example project

The [Cube Jump project](https://playcanvas.com/project/354998/overview/cube-jump-playable-ad-for-fb) is ready to be exported for the Facebook Playable Ad format and the expected [HTML output can be found here](pathname:///downloads/fb-playable-ad-cube-jump-html.zip).

[Interactive Demo]

## File size tips

As there is a strict file size limit, you will have to plan and budget the usage of assets for the ad.

The minified PlayCanvas Engine code is **\~1.2MB** uncompressed and due to the need to encode the asset files into Base64 strings, it adds **\~30%** to the size of each asset file.

This means that for a single HTML format, this leaves \~500KB for assets before they are encoded into Base64 strings. For the ZIP format, this would be about \~3MB for assets before encoding.

Try to keep images as small as possible in dimensions and use tools like [TinyPNG](https://tinypng.com/) to reduce file size even further.

## Playable ad checklist

* Have you added the function call `FbPlayableAd.onCTAClick()` as part of your call to action callback?

## How to export

Follow the [setup steps](https://github.com/playcanvas/playcanvas-rest-api-tools#setup) from the readme in the GitHub repo.

### Single HTML

Set the following options in the `config.json` as shown below. This will produce a single HTML file in the output directory.

```json
    "one_page": {
        "patch_xhr_out": true,
        "inline_game_scripts": true,
        "extern_files": false
    }
```

### ZIP file

Set the following options in the `config.json` as shown below. This will produce a ZIP file with the asset data and PlayCanvas Engine code as separate files from the `index.html`.

```json
    "one_page": {
        "patch_xhr_out": true,
        "inline_game_scripts": true,
        "extern_files": true
    }
```

And run the command:

```sh
npm run one-page
```

Full details of options and commands can be found in the readme section for '[Converting a project into a single HTML file](https://github.com/playcanvas/playcanvas-rest-api-tools#converting-a-project-into-a-single-html-file)'.

### How to test

Follow the steps [here](https://www.facebook.com/business/help/338940216641734) to create a Facebook ad and at the time where the files for the ad are uploaded, there is an opportunity to test within the manager.

[Image: Test Ad]

Please ignore the warning about the source may contain an `XMLHttpRequest` as the code path has been removed by this tool.

Facebook also allows testing on device via the ad manager but requires you to publish the ad first. This is a strange limitation by Facebook but is required at the moment.

[Image: Preview Ad]

--------------------------------------------------------------------------------

## Snapchat Playable Ad

URL: https://developer.playcanvas.com/user-manual/editor/publishing/playable-ads/snapchat-playable-ads/

PlayCanvas supports the Snapchat Playable Ad format and requirements via an [official external tool on GitHub](https://github.com/playcanvas/playcanvas-rest-api-tools#converting-a-project-into-a-single-html-file).

Snapchat playable ads uses the [MRAID 2.0 API](https://www.iab.com/guidelines/mraid/) standard and requires the assets for the ad to be external from the `index.html` rather than embedded.

The external assets will need to be in a folder that is uniquely named (which the tool handles for you) so that they can be uploaded to Snapchat's CDN servers.

There are some limitations to be aware of with the tool which can be found in the documentation from [GitHub](https://github.com/playcanvas/playcanvas-rest-api-tools#converting-a-project-into-a-single-html-file).

## Example project

The [Cube Jump project](https://playcanvas.com/project/796932/overview/cube-jump-snapchat-ad) is ready to be exported to the Snapchat Playable Ad format and the expected HTML output can be found [here](pathname:///downloads/sc-playable-ad-cube-jump.zip).

[Interactive Demo]

## File size tips

As there is a soft limit of 5MB (uncompressed), you will have to plan and budget the usage of assets for the ad.

The minified PlayCanvas Engine code is **\~1.2MB** uncompressed and due to the need to encode the asset files into Base64 strings, it adds **\~30%** to the size of each asset file.

This means that you would have about \~3MB for assets before the Base64 encoding.

Try to keep images as small as possible in dimensions and use tools like [TinyPNG](https://tinypng.com/) to reduce file size even further.

## Playable ad checklist

The Snapchat ad network requires the call to action function to be in the `index.html` where the network can replace it with a unique tracking version when it is served to the user. The URL will be set in the Snapchat Ad campaign tool.

The tool wraps this logic in a global function: `snapchatCta();` that should be called in the playable ad project.

* Have you called the function `snapchatCta();` as part of the call to action callback?

## How to export

Follow the [setup steps](https://github.com/playcanvas/playcanvas-rest-api-tools#setup) from the readme in the GitHub repo.

### Dry run test

As Snapchat does not yet have an official tool to test with, we will have to do a dry run to test with first before exporting in a format that the Snapchat ad network expects.

Set the following options in the `config.json` as shown below. This will produce a ZIP file with the asset data and PlayCanvas Engine code as separate files from the `index.html`.

```json
    "one_page": {
        "patch_xhr_out": false,
        "inline_game_scripts": true,
        "extern_files": {
            "enabled": true,
            "folder_name": "78fb9255-3033-4fe2-b9e1-355b149229a1",
            "external_url_prefix": ""
        },
        "mraid_support": true,
        "snapchat_cta": true
    }
```

The `folder_name` needs to be a unique string. For this purpose, you can use a random GUID from a [GUID generator](https://www.guidgenerator.com/) or your Snapchat representative may have a specific string that you should use.

And run the command:

```sh
npm run one-page
```

Full details of options and commands can be found in the readme section for '[Converting a project into a single HTML file](https://github.com/playcanvas/playcanvas-rest-api-tools#converting-a-project-into-a-single-html-file)'.

To test the ad on a device, we can use the Android app [Creative Preview](https://play.google.com/store/apps/details?id=com.google.android.apps.audition&hl=en_GB&gl=US) but we need to host the ad on a https server.

Our recommended approach is to [host locally](/user-manual/editor/publishing/web/self-hosting/#running-a-downloaded-build) and use [ngrok](https://ngrok.com/) to create a https tunnel to your computer that the app can access.

<img loading="lazy" src="/img/user-manual/editor/publishing/playable-ads/snapchat-playable-ads/ngrok.png" width="600" />

Once this is setup, open Creative Preview app and create a new 'Display' ad with the following settings:

<img loading="lazy" src="/img/user-manual/editor/publishing/playable-ads/snapchat-playable-ads/creative-preview.png" width="300" />

### Export for Snapchat

When the ad is ready to be uploaded for Snapchat, we need to add Snapchat's CDN URL prefix to the asset references in `index.html`. We can do this via the options in `config.json` via the `external_url_prefix` property:

```json
    "one_page": {
        "patch_xhr_out": false,
        "inline_game_scripts": true,
        "extern_files": {
            "enabled": true,
            "folder_name": "78fb9255-3033-4fe2-b9e1-355b149229a1",
            "external_url_prefix": "https://rtb-ads.shadow.snapads.com/html5"
        },
        "mraid_support": true,
        "snapchat_cta": true
    }
```

The ZIP can then be given to your Snapchat representative to upload to the ad network.

--------------------------------------------------------------------------------

## Web

URL: https://developer.playcanvas.com/user-manual/editor/publishing/web/

PlayCanvas games are actually web pages and as such, are most at home in the browser. When publishing on the web, you have a number of options:

* [PlayCanvas Hosting](/user-manual/editor/publishing/web/playcanvas-hosting)
* [Self-hosting](/user-manual/editor/publishing/web/self-hosting)
* [Facebook](/user-manual/editor/publishing/web/facebook)
* [heyVR.io](/user-manual/editor/publishing/web/hosting-heyvr)

--------------------------------------------------------------------------------

## Communication with web pages

URL: https://developer.playcanvas.com/user-manual/editor/publishing/web/communicating-webpage/

One of the key advantages of using PlayCanvas and WebGL over other plugins or cross-compiled engines is the ability to interact directly between your application and the surrounding webpage. In this page we'll talk about some common ways of interfacing your PlayCanvas application with a web page or web application.

There are two ways you may find your PlayCanvas application communicating with the surrounding Web page. First, you may have embedded your application in a iframe in a page. Second you may be serving your own HTML page which loads a PlayCanvas page. These two methods require very different ways of communicating between web page and application.

## Defining an API

Common to both methods of hosting you should think about what features of your PlayCanvas application you need to expose to the web page. Perhaps you need to change the color of something based on a button click or a slider; or you might need to send some text input into the application to be rendered to a texture. Decide in advance what features you need to expose and in your PlayCanvas application write an explicit API or set of functions which are the only functions that your web page will call.

Here is a simple example where we show a couple of different ways of exposing an API from a PlayCanvas application to a web page.

Method one defines a global function which can be called anywhere in your page to access your application. Method two defines an application event which you can fire from your page. The application listens for this event and performs actions in response to the event. Both are valid methods of defining an API with your application.

### Embedded in IFrame

Embedding a PlayCanvas application in an iframe is a quick and easy way to get your PlayCanvas content in a page. It also means that you can make use of our optimized hosting and don't need to worry about serving all the PlayCanvas content. However, the downside is that you can not call javascript functions in the PlayCanvas application directly from the hosting page because they are running on different pages.

To communicate between a parent page and an iframe you will need to use the [postMessage](https://developer.mozilla.org/en-US/docs/Web/API/Window/postMessage) javascript API to send data between your page and the PlayCanvas application.

In your host page, use the iframeless URL for the iframe. The default publish link has the build in an iframe to include the social sharing bar at the bottom. This can cause problems with [postMessage](https://developer.mozilla.org/en-US/docs/Web/API/Window/postMessage) as there are now two iframes to communicate through.

If you add `/e` after `https://playcanv.as` in the URL, this will give you a version of the build without the iframe and social sharing bar.

```html
<iframe id="app-frame" src="https://playcanv.as/e/p/example/">
<script>
const iframe = document.getElementById("app-frame");
iframe.contentWindow.postMessage({
    score: 10,
}, "https://playcanv.as");
</script>
```

In your application

### Serve your own HTML

When you download your PlayCanvas application for self-hosting. This is the index.html page that we include to run your application.

```html
<!doctype html>
<html>
<head>
    <meta name='viewport' content='width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no'>
    <meta charset='utf-8'>
    <link rel="stylesheet" type="text/css" href="styles.css">
    <title>Application Title</title>
    <script src="playcanvas-stable.min.js"></script>
    <script>
        SCENE_PATH = "12346.json";
        CONTEXT_OPTIONS = {
            'alpha': false,
            'preserveDrawingBuffer': false
        };
    </script>
</head>
<body>
    <script src="__start__.js"></script>
    <script src="__loading__.js"></script>
</body>
</html>
```

It is absolutely possible and even recommended, that you start from this page as the basis of your web page and you can modify it to add any additional content that is required for your page.

When it comes to communicating with your PlayCanvas application, for example from a button push. You can call the APIs we defined above directly from your script. There is no need for the `postMessage` calls.

Note, it is important that you run any custom code after the `__start__.js` scripts as this creates the PlayCanvas application. In many cases you may wish to wait until after all the asset loading has finished, but before the application starts. You can do this by responding to the `start` event.

For example:

```html
<!doctype html>
<html>
<head>
    <meta name='viewport' content='width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no'>
    <meta charset='utf-8'>
    <link rel="stylesheet" type="text/css" href="styles.css">
    <title>Application Title</title>
    <script src="playcanvas-stable.min.js"></script>
    <script>
        SCENE_PATH = "12346.json";
        CONTEXT_OPTIONS = {
            'alpha': false,
            'preserveDrawingBuffer': false
        };
    </script>
</head>
<body>
    <script src="__start__.js"></script>
    <script src="__loading__.js"></script>
    <script>
    const app = pc.Application.getApplication();
    app.on("start", function () {
        // get the root of the scene.
        const hierarchy = app.root.children[0];

        // do other stuff here
    });
    </script>
</body>
</html>
```

--------------------------------------------------------------------------------

## Facebook

URL: https://developer.playcanvas.com/user-manual/editor/publishing/web/facebook/

[Facebook](https://facebook.com) is an excellent place to publish PlayCanvas games in order to reach an enormous audience. Publishing games from PlayCanvas to Facebook is straightforward.

## How to publish to Facebook

**1.** Publish your game to PlayCanvas. (You can also publish self-hosted games to Facebook, it will be a similar process to below)

**2.** Visit the [Facebook Developer Portal](https://developers.facebook.com/) and create a new Facebook App for your game.

**3.** When requested choose **Facebook Canvas** as the type of application

[Image: Facebook Canvas]

**4.** For the Secure Canvas URL enter the `playcanv.as` URL for your game

[Image: Secure Canvas]

**5.** In the Application Settings section add the Website URL so that it looks like this: `https://s3-eu-west-1.amazonaws.com/apps.playcanvas.com/[BUILD_HASH]/index.html`. To find this URL take your `playcanv.as` game link e.g. `https://playcanv.as/p/JtL2iqIH/` and add an `e/` before the `p/` e.g. `https://playcanv.as/e/p/JtL2iqIH/`. This will redirect in your browser, use this URL for the Website URL

[Image: Website URL]

:::note

Adding this URL as your website URL is required because of the way PlayCanvas hosts games and the security requirements that Facebook implements to allow access to its API. We're working on a fix for this in PlayCanvas so that you only need to set the Secure Canvas URL.

:::

**6.** Finally, set up all the images and icons that are required for your game

[Image: Images]

**7.** You can test your game by visiting the App URL listed in the Settings page

[Image: App URL]

--------------------------------------------------------------------------------

## Hosting using a CDN

URL: https://developer.playcanvas.com/user-manual/editor/publishing/web/hosting-cdn/

When deploying your PlayCanvas application for self-hosting it is often necessary to separate the location your application is served from (the index.html) from the assets that the application loads. For example, a Content Delivery Network (CDN) is used to make sure that assets are delivered from a server that is geographically close to the user's computer. This makes your application load much more quickly. This guide will show you how to configure your PlayCanvas application to use a separate location for your assets.

The first step is to download a web build of your application following the steps in [the publishing guide](/user-manual/editor/publishing/web/self-hosting). Then we start by making changes to the `index.html` and `__settings__.js` files in the downloaded build.

## Serving Assets from another location

The simplest change to make is to load all assets from a different location. This is done by setting the `ASSET_PREFIX` property in your `__settings__.js`.

[Image: settings.js]

The `ASSET_PREFIX` will be prepended to all request that are made for an asset (including a scene) both during the preload phase and during runtime. For example, you should set this to the root folder of your CDN asset store.  In the above example, previously an asset that would have had a URL like `files/123456/1/texture.jpg` will now be loaded from `http://keepy-up-cdn.example.com/files/123456/1/texture.jpg`.

## Additional URLs

There are a few remaining files that are referenced directly by the `index.html`. In particular, the style sheet, the PlayCanvas javascript engine, the `__settings__.js`, `__loading__.js` and `__start__.js` application scripts. Update your index.html as seen below.

[Image: index.html]

In `__settings__.js`, the path to the application settings `config.json` would need to be changed as well. Update

[Image: settings.js]

## Copy Files to CDN

Next you should copy all the required files into the new location on your server. These files are loaded using the `ASSET_PREFIX`:

`__game_scripts.js`, scene json (e.g. `123456.json`), `config.json`, assets (everything inside the `files` folder) and `logo.png` the default loading screen logo.

And these files are referenced by the index.html:

- `playcanvas-stable.min.js`,
- `manifest.json`
- `__settings__.js`
- `__loading__.js`
- `__start__.js`
- `styles.css`

[Image: CDN files]

You should copy all these files onto your CDN hosting service.

## Setting up CORS

Your application is now ready to load files from a separate server. The final step to tackle is to ensure your server is correctly set up to serve Cross-Origin Resource Sharing (CORS) headers. CORS is a security feature of the browser which means that by default a web page on `http://example.com` can't download files from a web page on `http://keepy-up-cdn.example.com/` because they have a different "origin". To get around this, you must set the server at `http://keepy-up-cdn.example.com/` to serve CORS headers that tell the browser that other pages are allowed to download the content.

Setting up CORS is different depending on which CDN or server you are using. You will need to check the documentation of your server or CDN provider to find out how to set up CORS header. For example, the page for Amazon Web Services CORS settings is [here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html)

--------------------------------------------------------------------------------

## Hosting on heyVR.io

URL: https://developer.playcanvas.com/user-manual/editor/publishing/web/hosting-heyvr/

[heyVR.io](https://heyvr.io) is a publishing platform for WebXR games, giving you access to a global audience of hundreds of thousands of players across multiple devices. It offers a wide range of ready-to-use gameplay features to enhance your players' experience and enable you to start earning from your game through in-game content sales and advertisements.

## Publishing Flow

To publish your game on heyVR, you'll need to follow a few simple steps:

1. Create a developer account (assuming you don't have one yet)
2. Upload your game
3. Publish!

You can optionally integrate the [heyVR SDK](https://docs.heyvr.io/en/game-development/sdk) into your game, which will boost player engagement and grant you access to useful features such as leaderboards, cloud saves, matchmaking and more.

### 1. Create a developer account

Simply head over to the heyVR's [developer area](https://developer.heyvr.io) and register your developer account free of charge. After registration, you'll be able to create/manage your games. To read more, check out the [documentation](https://docs.heyvr.io).

### 2. Upload your game

Once you have created your developer account, you'll need to download a web build of your application. The steps for this process are explained in detail on the [Self-hosting](/user-manual/editor/publishing/web/self-hosting/#self-hosting-on-your-own-server) documentation page. Simply follow the steps for **Self-hosting on your own server** to acquire a **zip** file.

Once you have the zip build, you can follow the step-by-step guide on [publishing a game](https://docs.heyvr.io/en/developer-area/publish-a-game) on heyVR. The guide includes all the steps, requirements and common questions regarding the publishing process.

### 3. Publish

That's it! By now you should be able to see, view and test your game on heyVR. Once you're ready to make it publicly accessible, you can submit your game for review. The review process is to ensure that your game aligns with the [Publishing Guidelines](https://heyvr.io/hc/legal/heyvr-io-publishing-policies) and is fully functional.

### 4. SDK Implementation (optional)

To further enrich the user experience and boost engagement to your game, you can use the [heyVR SDK](https://docs.heyvr.io/en/game-development/sdk) to implement additional features in your game.

The SDK offers the following features and more:

- User Management
- Leaderboards
- Cloud Saves
- In-Game Purchases (both free & paid)
- In-Game Advertisements
- Multiplayer

A comprehensive guide on how to implement these features into your game is provided on heyVR's [Technical Documentation](https://docs.heyvr.io/en/game-development/sdk).

## Useful Links

- [Developer Area](https://developer.heyvr.io)
- [heyVR Documentation](https://docs.heyvr.io)
- [heyVR Support (Discord)](https://heyvr.io/discord)

--------------------------------------------------------------------------------

## PlayCanvas Hosting

URL: https://developer.playcanvas.com/user-manual/editor/publishing/web/playcanvas-hosting/

The fastest, easiest, most convenient way to publish your game is to publish it to playcanvas.com.

Publishing to PlayCanvas consists of two simple steps:

1. Publish a new **Build**
2. Set the build as the **Primary Build**

To publish you will need to have at least one scene in your project.

## Publishing a new Build {#publishing-a-new-build}

* Go to your Project and open the Editor.
* Click on the <span class="pc-icon">&#57911;</span> button in the left hand side toolbar or click Publishing in the top left Menu

[Image: Publish Button]

* This will open up the Publishing dialog. Here you can publish a build or download a build for self-hosting.

[Image: Publish]

* Click the PUBLISH button next to "Publish on PlayCanvas".

[Image: Publish New Build]

* You will then have to fill out some details about your new build. The available fields are:

  * *Image*: Click on the image to upload a different one if you want. This will be used when sharing the build on Social Media.
  * *Title*: The title of your build.
  * *Description*: The description of your build. This will be used when sharing the link of your build on Social Media.
  * *Version*: The version is there to help you distinguish one build from another. Use a value that makes sense to you.
  * *Release Notes*: Enter any new changes you made for this build. Again like the Version, this will only be shown to you so that you keep better track of your builds.
  * *Concatenate Scripts*: Check this to combine all of your PlayCanvas scripts into a single JavaScript file.
  * *Minify Scripts*: Check this to minify your PlayCanvas scripts to reduce the file size.
  * *Generate Source Map*: Include Source Maps with the build. This can be useful for debugging but is generally disabled when publishing a production version of your app.
  * *Optimize Scene Format*: Compresses the scene JSON up to 50%. Read more [here](/user-manual/optimization/optimizing-scene-format).

[Image: Select scenes]

* Select the Scenes you want to include in your build from the list. Notice that the Scene with the active banner icon will be loaded first when your build is launched if you include it in the build.
* Click 'PUBLISH NOW'
* You will see a new build created in the BUILDS tab. Wait until it finishes processing.
* Your web app is now live! Feel free to tweet it and share to Facebook right from the published URL.

Builds are given permanent link in the form `https://playcanv.as/b/BUILD_ID/`. This link will be valid until you delete the build. If you want to share a link to your game, you should probably use the published project link explained below.

## Selecting a Primary Build {#selecting-a-primary-build}

Click Builds on the same popup.

[Image: Builds]

Here you can see a list of existing published builds.

You can also set the Primary Build for your project, by clicking on the banner icon on the left of each build. This will be the build that users play when they click on the PLAY button on the project's home page.

It's useful to have a single link that will always refer to the latest version of your game. That way you don't have to worry about broken links when you delete old builds. This is why each project has a Primary Build link. This will always be in the form `https://playcanv.as/p/PROJECT_ID/`

:::note

If you are sharing a link to your game you should use the Primary Build link. That means you know it won't change when you re-publish.

:::

The first time you publish a build, it will automatically become the Primary Build. For any subsequent build, you can choose when to assign it to be the Primary Build. This means that you can publish builds and test them before finally publishing them to your audience.

--------------------------------------------------------------------------------

## Self-hosting for beginners

URL: https://developer.playcanvas.com/user-manual/editor/publishing/web/self-hosting-for-beginners/

This document is aimed at people who are complete beginners to web programming and describes at high level how the different parts of a web application combine to serve PlayCanvas applications, or other web pages to a user's browser.

## The Web Stack

In general you can separate a web application into 3 parts: Server-side, Client-side and Static content. In almost all cases you will have all three of these parts for your web application, though sometimes one or several parts will be either very simple or handled by another service.

### Server-side Code

When a user opens a link in their web browser the browser sends a request to a server somewhere on the internet asking for an HTML page. At its simplest this is a page of text that sits on a hard disk on the web server and is sent back over the internet to the browser.

For web pages which are documents for displaying text or images this works very well. But in order to provide a level of interactivity often a server will do some processing after receiving the request and then **render** a page of HTML including the results of that processing.

For example, in PHP you could show the current date and time, inside your page.

```php
<html>
    <head>
        <title>My Homepage</title>
    </head>
    <body>
        <h1>The date is</h1>
        <?php
            echo date(DATE_RFC2822);
        ?>
        <p>Have a nice day</p>
    </body>
</html>
```

This PHP code creates an HTML page like below and sends it to the browser to display.

```html
<html>
    <head>
        <title>My Homepage</title>
    </head>
    <body>
        <h1>The date is</h1>
        Mon, 11 Apr 2016 06:54:14 -0400
        <p>Have a nice day</p>
    </body>
</html>
```

There are many, many different languages and frameworks to choose from when writing server-side code. PHP, Python, Nodejs, Ruby are all popular choices. But ultimately they have the same outcome, they receive a request for an HTML page, they do some processing and then they return the HTML data.

When writing PlayCanvas applications, no code you write in PlayCanvas runs server-side and we don't provide any server-side code for your projects.

### Client-side Code

In the web stack, the server is not the only place where we can do programming and respond to user input. Client-side refers to code running inside your browser. This code is always in Javascript, which is the language that browsers run. With client-side javascript you can perform many different operations. In the simplest case, you can modify the HTML page that was downloaded from the server

```javascript
const title = document.getElementById("title");
title.innerHTML = "This is the new title";
```

Or in the most complex case, you can write a full 3D WebGL game using PlayCanvas. Because everything you write using PlayCanvas is client-side javascript.

### Static Content

Some parts of your web application are not dynamic and do not need to change. For example, images, audio files, text files and in the case of PlayCanvas applications 3D models and textures. You can think of this a bit like loading files of a local disk, except that of course it comes over the internet so it's a bit slower. Serving static content is done by a web server, very much like server-side code and in some cases it will be the same machine. As there is no interactivity, there is no processing done to fulfill the request. The web server just sends back the requested file.

The important thing about static content is making sure it downloads to the client as fast as possible.

## Hosting PlayCanvas Applications

When you build a PlayCanvas application your are writing client-side javascript code and creating assets which are static content. In PlayCanvas projects you will have no server-side code at all.

When you publish using PlayCanvas we handle the server-side by setting up a URL where your application is served from sending back the HTML page that starts your client-side application. When the HTML page loads it runs the client-side javascript of your PlayCanvas application this will load all your assets from our static content servers.

We have designed the PlayCanvas hosting to be optimized for serving your PlayCanvas applications to end users throughout the world as quickly as possible. This is why the easiest way to deploy a PlayCanvas application is by clicking the publish button and sending out the `playcanv.as` URL. However, in some cases you may be required to host your own application.

## Self-hosting PlayCanvas Applications

We have a more [detailed page](/user-manual/editor/publishing/web/self-hosting/) about self-hosting a PlayCanvas application. But in summary, to self-host you need to provide the three parts of the web stack described above.

### Server

For a standard PlayCanvas application there is no server-side code required. However, you will still need to supply a web-server because you need to serve the static content including the `index.html` file that is found in your downloaded application. It is this file that the browser will request to load your application. There are two common web server application for Linux: Apache and Nginx and one common web server for Windows: IIS.

### Caching and Compression

As described in **Server** you will be serving all your PlayCanvas content including the `index.html` file from your web server as static content.

To get good performance when serving the static content it is **critical** to enable gzip compression and set the correct cache headers for the content. See your web server documentation for how to enable compression and caching headers on static content.

### Content types

Not all servers have content types setup for common formats used in PlayCanvas and can give errors when attempting to load the application from the server.

If you do get errors or warnings about assets not being served correctly in the browser, you may need to add these types.

Here's a list of content types used in PlayCanvas applications:

| File type | Content type             |
| --------- | ------------------------ |
| AAC       | audio/aac                |
| BASIS     | application/octet-stream |
| BIN       | application/octet-stream |
| DDS       | image/dds                |
| GLB       | model/gltf-binary        |
| JPEG      | image/jpeg               |
| JS        | application/javascript   |
| JSON      | application/json         |
| MP3       | audio/mpeg               |
| MP4       | video/mp4                |
| OGG       | audio/ogg                |
| PNG       | image/png                |
| TEXT      | text/plain               |
| WASM      | application/wasm         |
| WAV       | audio/x-wav              |
| XML       | application/xml          |

### Client

The client-side code is everything you have written as part of your PlayCanvas application. When the browser loads the `index.html` file it will start your PlayCanvas application and run all the client-side code that you have written.

--------------------------------------------------------------------------------

## Self-hosting

URL: https://developer.playcanvas.com/user-manual/editor/publishing/web/self-hosting/

There are two options for self-hosting a PlayCanvas application on your own domain.

1. Embed a PlayCanvas hosted application using an [iframe](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe).
2. Download your application from the Editor and upload the files to your own site.

## iframe Embedding {#iframe-embedding}

When you [publish to playcanvas.com](/user-manual/editor/publishing/web/playcanvas-hosting), your application is assigned a URL. To embed your application in another page. You can simply include this URL as the `src` property of an iframe.

```html
<html>
    <head>
        <title>My Great Game</title>
    </head>
    <body>
        [Interactive Demo]
    </body>
</html>
```

## Self-hosting on your own server {#self-hosting-on-your-own-server}

In order to host your application independently of PlayCanvas' servers, do the following:

* Go to your Project and open the Editor.
* Click on the <span class="pc-icon">&#57911;</span> button in the left hand side toolbar or click Publishing in the top left Menu

[Image: Publish Button]

* This will open up the Publishing Dialog.

[Image: Publish]

* Click the DOWNLOAD .ZIP button

[Image: Download]

* Enter a name for your export. This will be the name of the .zip file that will be created for you.
* Select the Scenes you want to include in your export from the list. Notice that the Scene with the active banner icon will be the first scene loaded when your app is launched.
* Click on the 'DOWNLOAD' button on the bottom to download a zip file of your project ready for deployment.
* Extract the contents of the zip file to a location of your choosing. The file `index.html` will load your application.

## Self-hosting on GitHub pages {#self-hosting-on-github-pages}

As a PlayCanvas application is static content, [GitHub Pages](https://pages.github.com/) can be used to host your application using the same steps as [Self-hosting on your own server](#self-hosting-on-your-own-server).

You will also need to add an extra file named `.nojekyll` in the GitHub repo root directory to ensure that all files are copied to the final site. This is because some PlayCanvas published files start with an underscore and will be ignored without this file being added.

GitHub Pages Documentation can be found [here](https://docs.github.com/en/pages/getting-started-with-github-pages/about-github-pages#static-site-generators) about usage of the `.nojekyll` file.

## Running a Downloaded Build {#running-a-downloaded-build}

You cannot load your application by opening the `index.html` file in a browser or from a `file://` url. You must use a local webserver to load the `index.html` file using `http://` or `https://`.

There are many options for running a web server. Here are a few:

* *Easy:* Install [Python](https://www.python.org/) and run the command *python -m SimpleHTTPServer* from the same folder as your application's index.html. Then point your browser to `http://localhost:8000`.
* *Intermediate:* Install [NPM](https://www.npmjs.com/) and [http-server](https://www.npmjs.com/package/http-server) globally. Run the command *http-server -p 8000 --cors -c-1* from the same folder as your application's index.html. Then point your browser to `http://localhost:8000`.
* *Intermediate:* Install [XAMPP](https://www.apachefriends.org/index.html). Although this is a full PHP development environment, it includes an easy to configure Apache server.
* *Advanced:* Install [Apache](https://httpd.apache.org/) or [nginx](https://www.nginx.com/) as a standalone service.

--------------------------------------------------------------------------------

## Real-time Collaboration

URL: https://developer.playcanvas.com/user-manual/editor/realtime-collaboration/

Real-time collaboration is at the heart of the PlayCanvas Editor. This brings a number of benefits:

* 🧑‍🤝‍🧑 Multiple users can work together to build a scene.
* 🆘 One user can join another to offer advice or help fix an issue.
* 🔍 Stakeholders can drop by to see the latest state of a project.

Let's examine how real-time collaboration is surfaced in the interface.

## Presence Bar

In the bottom left corner of the [Viewport](../interface/viewport) (next to the CHAT button), you will find the Presence Bar.

[Image: Presence Bar]

Whenever a new user enters the scene, their user avatar will be added to the Presence Bar. Likewise, when they close the Editor, their avatar will be removed from the Presence Bar. You can hover any avatar to view the associated username. And if you click an avatar, it will take you to that user's profile page.

:::tip

Each user is assigned a unique 'user color' that is used throughout the interface to represent them.

:::

## Real-time Chat {#real-time-chat}

If you select the CHAT button, the Chat panel will expand and you can broadcast messages to other users present in the Editor with you.

[Image: Chat]

You can toggle browser notifications for chat messages in the [Settings](interface/settings/editor.md#settings).

:::tip

If you paste URLs into the chat, they will be formatted as clickable hyperlinks.

:::

## Viewport Cameras

Each user in the scene is represented in the [Viewport](../interface/viewport) by a colored, wireframe camera frustum.

[Image: Viewport Cameras]

Mouse over the shaded center plane of a user camera to view the associated username:

[Image: Viewport Camera Username]

## Selection Indicators

It can be useful to know what entities other users are selecting and potentially editing. The [Hierarchy](../interface/hierarchy) displays square indicators to the right of entities selected by other users (shaded according to their user color).

[Image: Selection Indicators]

Whenever an entity with a 3D model is selected by any user, its outline will be rendered in the [Viewport](../interface/viewport).

[Image: Viewport Selection]

--------------------------------------------------------------------------------

## Working with Scenes

URL: https://developer.playcanvas.com/user-manual/editor/scenes/

A **Scene** is the foundation of your PlayCanvas application. It defines the 3D world that users will experience, containing all the entities, components and settings that make up your game or interactive experience.

## What's in a Scene?

Every scene contains:

- **Entities** - The objects in your scene, organized in a hierarchy. Entities can represent characters, props, lights, cameras or any other element.
- **Components** - The building blocks that give entities their behavior and appearance. Components handle rendering, physics, audio, user interface and more.
- **Scene Settings** - Global properties that affect the entire scene, such as physics gravity, ambient lighting, fog and skybox.

## In This Section

| Topic | Description |
|-------|-------------|
| [Managing Scenes](managing-scenes.md) | Create, open, duplicate and delete scenes in your project. |
| [Components](components/index.md) | Learn about the different component types you can add to entities. |
| [Loading Scenes](loading-scenes.md) | Load scenes dynamically at runtime in your application. |

--------------------------------------------------------------------------------

## Components

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/

A component encapsulates functionality that can be added to or removed from entities. For example, a component might enable an entity to play sound, render a 3D model or run a script.

You can add a component to an entity using the PlayCanvas Editor or by using the Engine API. The properties exposed by components are listed in the [Inspector](/user-manual/editor/interface/inspector) when you select an Entity.

## Component Inspector

Each component appears as a panel in the Inspector with a common header containing the following controls:

[Image: Component Header]

| Control | Description |
|---------|-------------|
| Collapse/Expand | Click the arrow to collapse or expand the component panel. |
| Component Icon | Identifies the component type. |
| Enable Toggle | Enable or disable the component. Disabled components do not run or render. |
| Help | Opens the documentation for this component type. |
| Actions Menu | Click the cog icon to access Copy, Paste, and Delete options. |

## Component Types

There are many different components defined in the PlayCanvas Engine:

| Component                         | Description                                                                   |
| --------------------------------- | ----------------------------------------------------------------------------- |
| [Anim](anim)                      | Specifies the state graph and animations that can run on an entity hierarchy. |
| [Audio Listener](audiolistener)   | Specifies the location of the listener for 3D audio playback.                 |
| [Button](button)                  | Creates a user interface button.                                              |
| [Camera](camera)                  | Renders the scene from the location of the entity.                            |
| [Collision](collision)            | Assigns a collision volume to the entity.                                     |
| [Element](element)                | Defines a user interface text or image element.                               |
| [GSplat](gsplat)                  | Renders a 3D Gaussian Splat at the location of the entity.                    |
| [Layout Child](layoutchild)       | Overrides default Layout Group properties for one element.                    |
| [Layout Group](layoutgroup)       | Automatically sets position and scale of child user interface elements.       |
| [Light](light)                    | Attaches a dynamic light source to the Entity.                                |
| [Particle System](particlesystem) | Attaches a particle system to the Entity.                                     |
| [Rigid Body](rigidbody)           | Adds the entity to the scene's physical simulation.                           |
| [Render](render)                  | Renders a graphical primitive or a render asset.                              |
| [Screen](screen)                  | Defines the area and rendering of a user interface.                           |
| [Script](script)                  | Allows the entity to run JavaScript fragments to implement custom behavior.   |
| [Scrollbar](scrollbar)            | Defines a scrolling control for a Scroll View Component.                       |
| [Scroll View](scrollview)         | Defines a scrollable area in a user interface.                                |
| [Sound](sound)                    | Plays audio assets.                                                           |
| [Sprite](sprite)                  | Renders 2D graphics at the location of the entity.                            |

### Deprecated Components

PlayCanvas still provides some deprecated components. Use of these components is not recommended for new projects.

| Component              | Description                                                                                   |
| ---------------------- | --------------------------------------------------------------------------------------------- |
| [Animation](animation) | Specifies the animations that can run on the model specified by the entity's model component. |
| [Model](model)         | Renders a 3D model at the location of the entity.                                             |

--------------------------------------------------------------------------------

## Anim

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/anim/

The Anim Component is used to connect an animstategraph asset and all of its required animation assets to a single entity.

[Image: Anim Component]

## Properties

| Property          | Description |
|-------------------|-------------|
| Activate          | If enabled, the animation will start playing automatically when the scene is launched. |
| Speed             | Controls the playback speed of all animations in the state graph. A value of 1 is normal speed, 0.5 is half speed, and 2 is double speed. Range is 0 to 2. |
| Root Bone         | Optionally specify an entity to use as the root bone for the animation. This is useful when the model hierarchy is not at the top level of the entity. |
| Normalize Weights | If enabled, the weights of all layers will be normalized so they sum to 1. This affects how layer blending is calculated. |
| State Graph       | The animstategraph asset that defines the animation state machine for this entity. |

## Assigning Animation Assets

After selecting an animstategraph asset, the Anim Component will display a list of animation asset slots organized by layer. There will be one slot for each animation state in every layer of the state graph asset (excluding START, END, and ANY states). This is where actual animation data is connected to the previously created state graph.

[Image: Anim Component With Graph]

Multiple Anim Components can use the same animstategraph asset, each with their own set of animation assets. After all animation state slots have been filled, the Anim Component will become playable. The animations can be played via script by calling `entity.anim.playing = true` or automatically if the Activate option is enabled.

## Layer Masks

Each layer in the state graph can have an optional mask that limits which bones the layer's animations will affect. This is useful for scenarios like playing a walking animation on the lower body while playing a waving animation on the upper body.

[Image: Create Mask Button]

To create a mask for a layer, click the **CREATE MASK** button next to the layer name. This will open the mask inspector.

### Mask Inspector

The mask inspector displays a tree view of all bones in the model hierarchy. Each bone has a checkbox that determines whether the layer's animation will affect that bone.

[Image: Mask Inspector]

The mask inspector provides several controls:

| Control               | Description |
|-----------------------|-------------|
| ADD ALL / ADD SELECTED | Enables all bones in the mask, or only the selected bones if any are selected. |
| REMOVE ALL / REMOVE SELECTED | Disables all bones in the mask, or only the selected bones if any are selected. |
| Add hierarchy (context menu) | Right-click a bone to enable it and all of its children. |
| Remove hierarchy (context menu) | Right-click a bone to disable it and all of its children. |

To edit an existing mask, click the **EDIT MASK** button. To delete a mask, click the trash icon next to the layer.

## See Also

- [Animation](/user-manual/animation) - Learn more about the animation system

## Scripting Interface

You can control an Anim Component's properties using a [Script Component](script.md). The Anim Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/AnimComponent.html).

--------------------------------------------------------------------------------

## Animation (Legacy)

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/animation/

:::warning

The Animation Component is deprecated. It has been replaced by the [Anim](anim.md) Component.

:::

The Animation Component enables an entity to specify which animations can be applied to the model assigned to its Model Component.

[Image: Animation Component]

## Properties

| Property | Description |
|----------|-------------|
| Assets   | The animation assets that can be utilized by this entity. Multiple animations can be assigned via the picker control. |
| Speed    | A multiplier for animation playback speed. 0 will freeze animation playback, 1 represents the normal playback speed of the asset, and negative values will play the animation in reverse. Range is -2 to 2. |
| Activate | If checked, the component will start playing the animation on load. |
| Loop     | If checked, the animation will continue to loop back to the start on completion. Otherwise, the animation will come to a stop on its final frame. |

## See Also

- [Anim Component](anim.md) - The recommended replacement for this component

## Scripting Interface

You can control an Animation Component's properties using a [Script Component](script.md). The Animation Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/AnimationComponent.html).

--------------------------------------------------------------------------------

## Audio Listener

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/audiolistener/

The Audio Listener Component specifies the listener's position in 3D space. All 3D audio playback will be relative to this position.

[Image: Audio Listener Component]

## See Also

- [Sound Component](sound.md) - Play audio in 3D space
- [Audio Assets](/user-manual/editor/assets/inspectors/audio) - Learn more about audio assets

## Scripting Interface

You can control an Audio Listener Component's properties using a [Script Component](script.md). The Audio Listener Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/AudioListenerComponent.html).

--------------------------------------------------------------------------------

## Button

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/button/

The Button Component is a convenient shortcut for creating User Interface buttons for use with [Screen](screen.md) and [Element](element.md) Components.

The Button Component can be used in two Transition Modes. *Sprite Change*, which uses a different sprite or frame for each button state or *Tint*, which tints a single sprite with a different color for each state.

## Common Properties

| Property        | Description |
|-----------------|-------------|
| Active          | When enabled the button will respond to and fire events. When disabled the button is set to the Inactive State. |
| Image           | The Image Element Entity that is used to detect input events. |
| Hit Padding     | Additional space around the Image Element that will be included when testing for input events. Specified as left, bottom, right, top padding values. |
| Transition Mode | The type of effect to use when transitioning between states. Either Sprite Change or Tint. |

## Sprite Change Properties

[Image: Sprite Change Button]

| Property        | Description |
|-----------------|-------------|
| Hover Sprite    | The Sprite Asset used when the button is in the Hover State. |
| Hover Frame     | The Sprite Frame to display when the button is in the Hover State. |
| Pressed Sprite  | The Sprite Asset used when the button is in the Pressed State. |
| Pressed Frame   | The Sprite Frame to display when the button is in the Pressed State. |
| Inactive Sprite | The Sprite Asset used when the button is not active. |
| Inactive Frame  | The Sprite Frame used when the button is not active. |

## Tint Properties

[Image: Tint Button]

| Property      | Description |
|---------------|-------------|
| Hover Tint    | The color to tint the Image Element with when the button is in the Hover State. |
| Pressed Tint  | The color to tint the Image Element with when the button is in the Pressed State. |
| Inactive Tint | The color to tint the Image Element with when the button is in the Inactive State. |
| Fade Duration | The time in milliseconds to blend between the different state colors. |

## See Also

- [Element Component](element.md) - Required for button visuals
- [Screen Component](screen.md) - The root component for user interfaces
- [User Interface](/user-manual/user-interface) - Learn more about building user interfaces

## Scripting Interface

You can control a Button Component's properties using a [Script Component](script.md). The Button Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/ButtonComponent.html).

--------------------------------------------------------------------------------

## Camera

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/camera/

The Camera Component enables an entity to render a scene from a certain viewpoint.

[Image: Camera Component]

## Properties

| Property           | Description |
|--------------------|-------------|
| Clear Color Buffer | If enabled, the camera will explicitly clear its render target to the chosen clear color before rendering the scene. |
| Clear Depth Buffer | If enabled, the camera will explicitly clear the depth buffer of its render target before rendering the scene. |
| Clear Color        | The color used to clear the camera's render target. This property is only displayed if 'Clear Color Buffer' is enabled. |
| Depth Grabpass     | If enabled, the camera will render the scene's depth to a depth map texture which can be accessed in shaders. Useful for effects like soft particles or depth-based post-processing. |
| Color Grabpass     | If enabled, the camera will render the scene's color to a color map texture which can be accessed in shaders. Useful for effects like refraction or screen-space distortion. |
| Projection         | The projection type of the camera. Options are: <ul><li>Perspective</li><li>Orthographic</li></ul> |
| Frustum Culling    | If enabled, the camera will only render mesh instances whose axis-aligned bounding boxes intersect with the camera's view frustum. Otherwise, the entire scene will be rendered regardless of visibility. |
| Field of View      | The angle between top and bottom clip planes of a perspective camera. This property is only displayed if 'Projection' is set to 'Perspective'. |
| Ortho Height       | The distance in world units between the top and bottom clip planes of an orthographic camera. This property is only displayed if 'Projection' is set to 'Orthographic'. |
| Near Clip          | The distance in camera space from the camera's eye point to the near clip plane. |
| Far Clip           | The distance in camera space from the camera's eye point to the far clip plane. |
| Priority           | A number that defines the order in which camera views are rendered by the engine. Smaller numbers are rendered first. |
| Viewport           | A rectangle that specifies the viewport onto the camera's attached render target. This allows you to implement features like split-screen or picture-in-picture. It is defined by normalized coordinates (0 to 1) in the following format: X (lower left x), Y (lower left y), W (width), H (height). |
| Layers             | The layers that this camera will render. Only mesh instances on matching layers will be rendered by this camera. |
| Tonemapping        | The tonemapping algorithm to apply when rendering HDR content. Options are: Linear, Filmic, Hejl, ACES, ACES2, Neutral. This property is only available with Engine v2. |
| Gamma              | The gamma correction value to apply. Options are 1.0 (no correction) or 2.2 (standard gamma). This property is only available with Engine v2. |

## See Also

- [Cameras](/user-manual/graphics/cameras) - Learn more about cameras

## Scripting Interface

You can control a Camera Component's properties using a [Script Component](script.md). The Camera Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/CameraComponent.html).

--------------------------------------------------------------------------------

## Collision

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/collision/

The Collision Component assigns a collision volume to the entity. The component interface dynamically displays different attributes based on the 'Type' attribute.

#### Box

[Image: Collision Component (Box)]

#### Capsule

[Image: Collision Component (Capsule)]

#### Compound

[Image: Collision Component (Compound)]

#### Cone

[Image: Collision Component (Cone)]

#### Cylinder

[Image: Collision Component (Cylinder)]

#### Mesh

[Image: Collision Component (Mesh)]

#### Sphere

[Image: Collision Component (Sphere)]

If the entity also has a Rigid Body Component, the Collision Component determines the shape of the rigid body. If no Rigid Body Component is present, the Collision Component is treated as a trigger volume. The trigger volume cannot affect the simulation of other rigid bodies in the scene. Instead, you can add a Script Component and attach a script which responds to trigger events. For example, if another entity that has a Rigid Body Component enters or exits the trigger, your script can be notified.

## Properties

| Property        | Description |
|-----------------|-------------|
| Type            | The type of collision primitive. Can be: Box, Sphere, Capsule, Cylinder, Cone, Mesh, or Compound. |
| Half Extents    | Box only. The half-extents of the collision box. This is a 3-dimensional vector: local space half-width, half-height, and half-depth. |
| Radius          | Sphere, Capsule, Cylinder, and Cone only. The radius of the collision shape. |
| Height          | Capsule, Cylinder, and Cone only. The height of the collision shape along the selected axis. |
| Axis            | Capsule, Cylinder, and Cone only. Aligns the collision shape with the local-space X, Y, or Z axis of the entity. |
| Convex Hull     | Mesh only. If enabled, the collision mesh will be treated as a convex hull, which is more efficient for dynamic rigid bodies. If disabled, the mesh is used as a triangle mesh (concave), which only works with static or kinematic rigid bodies. |
| Model Asset     | Mesh only. The model asset that will be used as a source for the collision mesh. Either a Model Asset or Render Asset can be specified, but not both. |
| Render Asset    | Mesh only. The [render asset](/user-manual/editor/assets/inspectors/render) that will be used as a source for the collision mesh. Either a Model Asset or Render Asset can be specified, but not both. |
| Position Offset | The positional offset of the collision shape relative to the entity's position. |
| Rotation Offset | The rotational offset of the collision shape relative to the entity's rotation, specified in degrees. |

## See Also

- [Rigid Body Component](rigidbody.md) - Add physics simulation to the entity
- [Physics](/user-manual/physics) - Learn more about the physics system

## Scripting Interface

You can control a Collision Component's properties using a [Script Component](script.md). The Collision Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/CollisionComponent.html).

--------------------------------------------------------------------------------

## Element

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/element/

The Element Component is used to build user interfaces made up of 2D components such as images and text when in a hierarchy with a Screen Component ancestor. The Element Component provides layout properties such as anchors and a pivot point.

See the [User Interface](/user-manual/user-interface) section for more details.

## Group Element

The group element provides just the layout properties of the Element Component.

[Image: Group Element]

## Image Element

The image element displays an image using a texture asset, sprite asset, or a solid color.

[Image: Image Element]

## Text Element

The text element renders a string of text using a [font asset](/user-manual/editor/assets/inspectors/font/).

[Image: Text Element]

## Common Component Properties

| Property       | Description |
|----------------|-------------|
| Type           | The type of Element: Group, Image, or Text. |
| Preset         | Choosing a layout preset will automatically set the Anchor and Pivot properties to a preset value. |
| Anchor         | Determines where the element calculates its position in relation to. See the [Elements#Anchor](/user-manual/user-interface/elements/#anchor) section for more information. |
| Pivot          | Determines where the pivot point of the Element is. (0, 0) is bottom left, (1, 1) is top right. See the [Elements#Pivot](/user-manual/user-interface/elements/#pivot) section for more information. |
| Auto Width     | Text only. If enabled, the element's width will be automatically calculated based on the text content. |
| Auto Fit Width | Text only. If enabled, the font size will be automatically reduced to fit the text within the element's width. |
| Width          | The width of the Element in pixels. May be disabled when Auto Width is enabled or when the anchor is horizontally split. |
| Auto Height    | Text only. If enabled, the element's height will be automatically calculated based on the text content. |
| Auto Fit Height| Text only. If enabled, the font size will be automatically reduced to fit the text within the element's height. |
| Height         | The height of the Element in pixels. May be disabled when Auto Height is enabled or when the anchor is vertically split. |
| Margin         | The distance from the edge of the element to the Anchor. This is only available when the Anchor is split (non-equal in one axis). Specified as left, bottom, right, top. |
| Use Input      | If enabled, this Element is added to the list of elements that check for input and fire input related events. |
| Batch Group    | The Batch Group that this element belongs to. More on Batching [here](/user-manual/graphics/advanced-rendering/batching). |
| Layers         | The Layers to render this element into. More on Layers [here](/user-manual/graphics/layers). |

## Image Component Properties

| Property       | Description |
|----------------|-------------|
| Color          | The color to tint the element. |
| Opacity        | The transparency of the element, from 0 (fully transparent) to 1 (fully opaque). |
| Rect           | Defines the area of the texture asset to display, specified as U, V, Width, Height in normalized coordinates. Only shown when using a Texture. |
| Mask           | Switch Image Element into a mask. Masks do not render into the scene, but instead limit child elements to only be rendered where this element is rendered. |
| Texture        | The texture asset to display. Either a Texture, Sprite, or Material can be assigned, but not multiple. |
| Sprite         | The sprite asset to display. Either a Texture, Sprite, or Material can be assigned, but not multiple. |
| Frame          | The sprite frame index to display. Only shown when a Sprite is assigned. |
| Pixels Per Unit| The number of pixels that correspond to one unit in the component's coordinate system. Only shown when a Sprite is assigned. |
| Material       | A custom material asset to use for rendering. Either a Texture, Sprite, or Material can be assigned, but not multiple. |
| Fit Mode       | How the image should be fitted within the element bounds. Options are: Stretch (default), Contain (maintain aspect ratio, fit inside), Cover (maintain aspect ratio, fill). Only shown when a Texture or Sprite is assigned. |

## Text Component Properties

| Property          | Description |
|-------------------|-------------|
| Alignment         | Determines how the text is aligned within the element. (0, 0) is bottom left, (1, 1) is top right. |
| Font              | The font asset to use for rendering text. |
| Localized         | If enabled, the text will be looked up from the localization data using the Key property. |
| Text              | The text string to display. Only shown when Localized is disabled. |
| Key               | The localization key used to look up the translated text. Only shown when Localized is enabled. |
| Enable Markup     | If enabled, the text can contain markup tags for styling (e.g., color, bold). |
| Font Size         | The size in Screen Component pixels to render the font at. Only shown when Auto Fit Width and Auto Fit Height are disabled. |
| Min Font Size     | The minimum font size when using Auto Fit Width or Auto Fit Height. |
| Max Font Size     | The maximum font size when using Auto Fit Width or Auto Fit Height. |
| Line Height       | The size in Screen Component pixels to move down for a new line. |
| Wrap Lines        | If enabled, text that overflows the width of the text element will be wrapped to the next line. |
| Max Lines         | The maximum number of lines to display. Text beyond this limit will be clipped. Only shown when Wrap Lines is enabled. |
| Spacing           | A multiplier to apply to the amount of space between each character. |
| Color             | The color to tint the font. |
| Opacity           | The transparency of the element, from 0 (fully transparent) to 1 (fully opaque). |
| Outline Color     | The color of the text outline, including alpha for opacity. |
| Outline Thickness | The thickness of the text outline, from 0 (no outline) to 1 (maximum thickness). |
| Shadow Color      | The color of the text shadow, including alpha for opacity. |
| Shadow Offset     | The offset of the text shadow from the text, specified as horizontal and vertical offset. |

## See Also

- [Screen Component](screen.md) - The root component for user interfaces
- [Button Component](button.md) - Interactive button elements
- [LayoutGroup Component](layoutgroup.md) - Automatic layout of child elements
- [LayoutChild Component](layoutchild.md) - Override layout behavior for individual elements
- [User Interface](/user-manual/user-interface) - Learn more about building user interfaces

## Scripting Interface

You can control an Element Component's properties using a [Script Component](script.md). The Element Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/ElementComponent.html).

--------------------------------------------------------------------------------

## GSplat

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/gsplat/

The GSplat Component enables an entity to render a 3D Gaussian Splat.

[Image: GSplat Component]

## Properties

| Property | Description |
|----------|-------------|
| Asset    | The GSplat asset to be rendered by this GSplat Component. Only a single GSplat asset can be assigned to a GSplat Component. |
| Layers   | The [layers](/user-manual/graphics/layers) to render this element into. |

## See Also

- [Gaussian Splatting](/user-manual/gaussian-splatting) - Learn more about 3D Gaussian Splats

## Scripting Interface

You can control a GSplat Component's properties using a [Script Component](script.md). The GSplat Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/GSplatComponent.html).

--------------------------------------------------------------------------------

## Layout Child

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/layoutchild/

The LayoutChild Component enables an element that is controlled by a LayoutGroup Component to override the default behavior of the Layout Group.

See the [Layout Groups](/user-manual/user-interface/layout-groups) section for more details.

[Image: LayoutChild Component]

## Properties

| Property              | Description |
|-----------------------|-------------|
| Min Width             | Set the minimum width that the element can be rendered at. |
| Max Width             | Set the maximum width that the element can be rendered at. |
| Min Height            | Set the minimum height that the element can be rendered at. |
| Max Height            | Set the maximum height that the element can be rendered at. |
| Fit Width Proportion  | The proportion of additional space that the element will take up if the layout group is set to stretch or shrink. |
| Fit Height Proportion | The proportion of additional space that the element will take up if the layout group is set to stretch or shrink. |
| Exclude from Layout   | Completely ignore this element when calculating the layout. |

## See Also

- [LayoutGroup Component](layoutgroup.md) - The parent component that controls layout
- [Element Component](element.md) - Required for UI layout
- [Layout Groups](/user-manual/user-interface/layout-groups) - Learn more about layout groups

## Scripting Interface

You can control a LayoutChild Component's properties using a [Script Component](script.md). The LayoutChild Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/LayoutChildComponent.html).

--------------------------------------------------------------------------------

## Layout Group

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/layoutgroup/

The LayoutGroup Component enables an entity to specify the size and position of child Element Components.

See the [Layout Groups](/user-manual/user-interface/layout-groups) section for more details.

[Image: LayoutGroup Component]

## Properties

| Property       | Description |
|----------------|-------------|
| Orientation    | Whether the layout should run horizontally or vertically. Options are: Horizontal, Vertical. |
| Reverse X      | Reverses the order of children along the x axis. |
| Reverse Y      | Reverses the order of children along the y axis. |
| Alignment      | Specifies the horizontal and vertical alignment of child elements. Values range from 0 to 1 where (0, 0) is the bottom left and (1, 1) is the top right. |
| Padding        | Padding to be applied inside the container before positioning any children. Specified as left, bottom, right, and top values. |
| Spacing        | Spacing to be applied between each child element, specified as horizontal and vertical values. |
| Width Fitting  | Fitting logic to be applied when positioning and scaling child elements horizontally. Options are: None, Stretch, Shrink, Both. |
| Height Fitting | Fitting logic to be applied when positioning and scaling child elements vertically. Options are: None, Stretch, Shrink, Both. |
| Wrap           | Whether or not to wrap children onto a new row/column when the size of the container is exceeded. |

## See Also

- [LayoutChild Component](layoutchild.md) - Override layout behavior for individual children
- [Element Component](element.md) - Required for UI layout
- [Layout Groups](/user-manual/user-interface/layout-groups) - Learn more about layout groups

## Scripting Interface

You can control a LayoutGroup Component's properties using a [Script Component](script.md). The LayoutGroup Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/LayoutGroupComponent.html).

--------------------------------------------------------------------------------

## Light

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/light/

The Light Component attaches a dynamic light source to the Entity. The 'Type' property determines what kind of light is attached and what other properties are available.

#### Directional

[Image: Light Component (Directional)]

#### Omni

[Image: Light Component (Omni)]

#### Spot

[Image: Light Component (Spot)]

## Properties

| Property              | Description |
|-----------------------|-------------|
| Type                  | The type of light. Options are: Directional (uniform direction), Spot (cone from a point), Omni (all directions from a point). |
| Color                 | The color of the emitted light. |
| Intensity             | The intensity of the light, acts as a scalar value for the light's color. Range is 0 to 32. |
| Range                 | Omni and Spot only. The distance from the light source at which its contribution falls to zero. |
| Falloff Mode          | Omni and Spot only. Controls the rate at which a light attenuates from its position. Options are: Linear, Inverse Squared. |
| Inner Cone Angle      | Spot only. The angle in degrees from the spotlight direction at which the light starts to fall off. |
| Outer Cone Angle      | Spot only. The angle in degrees from the spotlight direction at which the light falls to zero. |
| Shape                 | The shape of the light source for area lighting. Options are: Punctual, Rectangle, Disk, Sphere. Only available when area lights are enabled in render settings. |

## Lightmap Properties

| Property              | Description |
|-----------------------|-------------|
| Static                | Mark this light as non-moving for optimization purposes. |
| Bake Lightmap         | Enable lightmap baking from this light. |
| Bake Direction        | Include directional information in baked lightmaps. |
| Bake Samples          | The number of samples used when baking lightmaps. Range is 1 to 255. |
| Bake Area             | The spread angle for baking soft shadows. Range is 0 to 180 degrees. |
| Affect Lightmapped    | If enabled, this light will affect lightmapped objects at runtime. |
| Affect Dynamic        | If enabled, this light will affect non-lightmapped (dynamic) objects. |
| Affect Specularity    | Directional only. If enabled, this light contributes to specular reflections on materials. |

## Shadow Properties

| Property              | Description |
|-----------------------|-------------|
| Cast Shadows          | If enabled, the light will cause shadow casting models to cast shadows. |
| Shadow Update Mode    | When the shadowmap is updated. Options are: Once (generated once), Realtime (updated every frame). |
| Resolution            | The resolution of the shadowmap. Options range from 16x16 to 4096x4096. Higher values produce more accurate shadows at the cost of performance. |
| Cascades              | Directional only. The number of shadow cascades. Options are: 1, 2, 3, 4. More cascades provide better shadow quality at different distances. |
| Cascade Distribution  | Directional only. Controls how the shadow cascades are distributed. Range is 0 to 1. Only shown when Cascades is greater than 1. |
| Distance              | The maximum distance from the camera beyond which shadows are no longer visible. |
| Shadow Intensity      | The darkness of the shadows. Range is 0 (no shadow) to 1 (fully dark). |
| Shadow Type           | The shadow mapping algorithm. Options are: Shadow Map PCF 1x1, Shadow Map PCF 3x3, Shadow Map PCF 5x5, Variance Shadow Map (16bit), Variance Shadow Map (32bit). |
| VSM Blur Mode         | VSM only. The blur algorithm for variance shadow maps. Options are: Box, Gaussian. |
| VSM Blur Size         | VSM only. The size of the blur kernel. Range is 1 to 25. |
| VSM Bias              | VSM only. Bias value to reduce shadow artifacts. |
| Shadow Bias           | PCF only. Bias value to reduce shadow acne artifacts. |
| Normal Offset Bias    | PCF only. Offset along normals to reduce peter-panning artifacts. |

## Cookie Properties

| Property              | Description |
|-----------------------|-------------|
| Cookie                | Omni and Spot only. A texture asset (or cubemap for Omni) to be projected from the light. |
| Cookie Intensity      | The strength of the cookie texture. Range is 0 to 1. |
| Cookie Angle          | Spot only. The rotation angle of the cookie texture in degrees. |
| Cookie Offset         | Spot only. The UV offset of the cookie texture. |
| Cookie Scale          | Spot only. The UV scale of the cookie texture. |
| Cookie Falloff        | Spot only. If enabled, applies the spotlight falloff to the cookie. |
| Cookie Channel        | The texture channel(s) to use for the cookie. Options are: R, G, B, A, RGB. |

## Other Properties

| Property              | Description |
|-----------------------|-------------|
| Layers                | The layers that this light will affect. |

## See Also

- [Lighting](/user-manual/graphics/lighting/) - Learn more about lighting

## Scripting Interface

You can control a Light Component's properties using a [Script Component](script.md). The Light Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/LightComponent.html).

--------------------------------------------------------------------------------

## Model (Legacy)

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/model/

:::warning

The Model Component is deprecated. It has been replaced by the [Render](render.md) Component.

:::

The Model Component enables an entity to render a primitive shape or a model asset.

[Image: Model Component]

## Properties

| Property                  | Description |
|---------------------------|-------------|
| Type                      | The type of the model to be rendered. Can be: Asset, Box, Capsule, Cone, Cylinder, Plane, Sphere. |
| Model                     | Asset type only. The model asset rendered by this model component. Only a single model can be rendered per model component. |
| Material                  | Primitive types only. The material asset used to render the primitive shape. |
| Cast Shadows              | If enabled, the model rendered by this component will cast shadows onto other models in the scene. |
| Cast Lightmap Shadows     | If enabled, the model rendered by this component will cast shadows into lightmaps. |
| Receive Shadows           | If enabled, the model rendered by this component will receive shadows cast by other models in the scene. |
| Static                    | If the model never moves, check this box as a hint to the engine to make certain optimizations. |
| Lightmapped               | If enabled, this model does not receive lighting from dynamic lights. Instead it receives lighting generated by lightmap lights. |
| Lightmap Size Multiplier  | A multiplier applied to the calculated lightmap size for this model. Increase for higher resolution lightmaps. Only shown when Lightmapped is enabled. |
| Custom AABB               | If enabled, allows specifying a custom axis-aligned bounding box for visibility culling instead of using the automatically calculated one. |
| AABB Center               | The center position of the custom bounding box. Only shown when Custom AABB is enabled. |
| AABB Half Extents         | The half-extents (half width, height, depth) of the custom bounding box. Only shown when Custom AABB is enabled. |
| Batch Group               | The Batch Group that this model belongs to. More on Batching [here](/user-manual/graphics/advanced-rendering/batching). |
| Layers                    | The layers to render this model into. |

## See Also

- [Render Component](render.md) - The recommended replacement for this component

## Scripting Interface

You can control a Model Component's properties using a [Script Component](script.md). The Model Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/ModelComponent.html).

## Customizing Materials

You can learn how to customize the materials of your model [here](/user-manual/editor/assets/inspectors/material/#assigning-materials).

--------------------------------------------------------------------------------

## Particle System

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/particlesystem/

The Particle System Component specifies a particle emitter in 3D space.

[Image: Particle System Component]

## Properties

### Basic Settings

| Property        | Description |
|-----------------|-------------|
| Auto Play       | If checked, the particle system will play immediately on creation. If this option is left unchecked, you will need to call the particle system component's play function from script. |
| Particle Count  | The maximum number of particles managed by this particle system. |
| Lifetime        | The length of time in seconds between a particle's birth and its death. |
| Emission Rate   | The lower bound of the time range defining the interval between particle births. The time for the next particle emission will be chosen at random between Emission Rate and Emission Rate 2. |
| Emission Rate 2 | The upper bound of the time range defining the interval between particle births. The time for the next particle emission will be chosen at random between Emission Rate and Emission Rate 2. |
| Start Angle     | The lower bound of the initial particle rotation specified in degrees. For each particle, this angle is chosen at random between Start Angle and Start Angle 2. |
| Start Angle 2   | The upper bound of the initial particle rotation specified in degrees. For each particle, this angle is chosen at random between Start Angle and Start Angle 2. |
| Loop            | If checked, the particle system will emit indefinitely. Otherwise, it will emit the number of particles specified by the Particle Count property and then stop. |
| Pre Warm        | If enabled, the particle system will be initialized as though it had already completed a full cycle. This option is only available for looping particle systems. |

### Lighting

| Property        | Description |
|-----------------|-------------|
| Lighting        | If checked, the particle will be lit by the directional and ambient light in the scene. In some circumstances, it may be advisable to set a normal map on the particle system in order to achieve more realistic lighting. |
| Half Lambert    | Enabling Half Lambert lighting avoids particles looking too flat when lights appear to be shining towards the back sides of the particles. It is a completely non-physical lighting model but can give more pleasing visual results. This option is only available when Lighting is enabled. |
| Intensity       | Scales the color of particles to allow them to have arbitrary brightness. |

### Rendering

| Property        | Description |
|-----------------|-------------|
| Depth Write     | If checked, the particles will write depth information to the depth buffer. If unchecked, the depth buffer is left unchanged and particles will be guaranteed to overwrite one another in the order in which they are rendered. |
| Depth Softening | Determines how much particles fade out as they get closer to another surface. This avoids the situation where particles appear to cut into surfaces. Setting this value to zero effectively disables depth softening. Setting a value greater than zero requires the scene to be rendered to a depth target for depth comparisons to be performed. This can have a significant performance impact by increasing the overall number of draw calls submitted every frame. |
| Sort            | Sorting mode gives you control over the order in which particles are rendered. Options: None (GPU simulated, best performance), Camera Distance (back to front), Newest First, Oldest First. |
| Blend Type      | The blending mode determines how particles are composited when written to the frame buffer. Options: Alpha, Additive, Multiply. |
| Stretch         | A value in world units that controls the amount by which particles are stretched based on their velocity. Particles are stretched from their center towards their previous position. |
| Align To Motion | Orient particles in their direction of motion. |

### Emitter Shape

| Property        | Description |
|-----------------|-------------|
| Emitter Shape   | The shape of the emitter volume. Options: Box, Sphere. |
| Emitter Extents | Box shape only. The half extents of the box-shaped emitter volume in local space within which particles are spawned at random positions. |
| Inner Extents   | Box shape only. The inner half extents of the box-shaped emitter volume. Particles will spawn between the inner and outer extents. |
| Emitter Radius  | Sphere shape only. The radius of the sphere-shaped emitter volume within which particles are spawned at random positions. |
| Inner Radius    | Sphere shape only. The inner radius of the sphere-shaped emitter volume. Particles will spawn between the inner and outer radius. |

### Space and Layers

| Property        | Description |
|-----------------|-------------|
| Wrap            | Enables wrap bounds. |
| Wrap Bounds     | World space AABB volume centered on the owner entity's position. If a particle crosses the boundary of one side of the volume, it teleports to the opposite side. You can use this to make environmental effects like rain by moving a wrapped emitter's owner entity. |
| Local Space     | If enabled, particles are simulated in local space instead of world space. This means they will move with the emitter entity. |
| Screen Space    | If enabled, particles are rendered in screen space, useful for 2D UI effects. |
| Layers          | The layers to render this particle system into. |

### Orientation

| Property        | Description |
|-----------------|-------------|
| Orientation     | Controls how particles are oriented. Options: Screen (face the camera), World Normal (face a fixed world direction), Emitter Normal (face the emitter's normal direction). |
| Particle Normal | The normal vector used for particle orientation when Orientation is set to World Normal or Emitter Normal. |

### Textures

| Property        | Description |
|-----------------|-------------|
| Color Map       | The color map texture to apply to all particles in the system. If no texture asset is assigned, a default spot texture is used. |
| Normal Map      | The normal map texture to apply to all particles in the system. Applying a normal map can make billboard particles appear more consistent with the scene's lighting. |

### Sprite Sheet Animation

These properties are shown when a Color Map or Normal Map is assigned.

| Property         | Description |
|------------------|-------------|
| Horizontal Tiles | The number of horizontal tiles (columns) in the sprite sheet. |
| Vertical Tiles   | The number of vertical tiles (rows) in the sprite sheet. |
| Animation Count  | The number of distinct animations in the sprite sheet. |
| Animation Index  | The index of the animation to play. Disabled when Randomize Index is enabled. |
| Randomize Index  | If enabled, each particle will randomly select an animation index. |
| Frame Count      | The number of frames in the animation. |
| Start Frame      | The frame index to start the animation from. |
| Animation Speed  | The speed at which the animation plays. |
| Animation Loop   | If enabled, the animation will loop. |

### Mesh

| Property        | Description |
|-----------------|-------------|
| Model Asset     | A model asset. The first mesh found in the model is used to represent all particles rather than a flat billboard. |
| Render Asset    | A render asset. The mesh is used to represent all particles rather than a flat billboard. Only one of Model Asset or Render Asset can be set. |

### Curves

| Property        | Description |
|-----------------|-------------|
| Local Velocity  | A curve defining how each particle's velocity with respect to the particle system's local coordinate system changes over time. If two curves are specified in the curve editor, local velocity will be a random lerp between both curves. |
| Velocity        | A curve defining how each particle's velocity with respect to the world coordinate system changes over time. If two curves are specified in the curve editor, velocity will be a random lerp between both curves. |
| Radial Speed    | A curve defining how each particle's radial velocity (away from the emitter center) changes over time. If two curves are specified, the radial speed will be a random lerp between both curves. |
| Rotation Speed  | A curve defining how each particle's angular velocity changes over time. If two curves are specified in the curve editor, the angular velocity will be a random lerp between both curves. |
| Scale           | A curve defining how each particle's scale changes over time. By default, a particle is 1 unit in width and height. If two curves are specified in the curve editor, the scale will be a random lerp between both curves. |
| Color           | A curve defining how each particle's color changes over time. |
| Opacity         | A curve defining how each particle's opacity changes over time. If two curves are specified in the curve editor, the opacity will be a random lerp between both curves. |

## See Also

- [Particle System](/user-manual/graphics/particles) - Learn more about particle systems

## Scripting Interface

You can control a Particle System Component's properties using a [Script Component](script.md). The Particle System Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/ParticleSystemComponent.html).

--------------------------------------------------------------------------------

## Render

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/render/

The Render Component enables an entity to render a primitive shape or a render asset.

[Image: Render Component]

## Properties

| Property                 | Description |
|--------------------------|-------------|
| Type                     | The type of the graphical object to be rendered. Can be: Asset, Box, Capsule, Cone, Cylinder, Plane, Sphere. |
| Asset                    | Asset type only. The render asset to be rendered by this render component. Only a single render asset can be assigned to a render component. |
| Root Bone                | Asset type only (skinned meshes). The entity to be used as the root bone for any skinned meshes that are rendered by this component. |
| Cast Shadows             | If enabled, the mesh instances rendered by this component will cast shadows onto other mesh instances in the scene. |
| Cast Lightmap Shadows    | If enabled, the mesh instances rendered by this component will cast shadows into lightmaps. |
| Receive Shadows          | If enabled, the mesh instances rendered by this component will receive shadows cast by other mesh instances in the scene. |
| Static                   | If the entity referencing this render component never moves, check this box as a hint to the engine to make certain optimizations. |
| Lightmapped              | If enabled, this component's mesh instances do not receive lighting from dynamic lights. Instead they receive lighting generated by lightmap-baking lights. |
| Lightmap Size Multiplier | A multiplier applied to the calculated lightmap size for this mesh. Increase for higher resolution lightmaps. Only shown when Lightmapped is enabled. |
| Custom AABB              | If enabled, allows specifying a custom axis-aligned bounding box for visibility culling instead of using the automatically calculated one. This is an optimization for skinned characters to avoid per-frame bounding box computations based on bone positions. |
| AABB Center              | The center position of the custom bounding box. Only shown when Custom AABB is enabled. |
| AABB Half Extents        | The half-extents (half width, height, depth) of the custom bounding box. Only shown when Custom AABB is enabled. |
| Batch Group              | The batch group that this component's mesh instances belong to. |
| Layers                   | The layers to render this component's mesh instances into. |
| Materials                | The material assets that will be used to render the mesh instances of this component. Each material corresponds to the respective mesh instance. |

## See Also

- [Render Assets](/user-manual/editor/assets/inspectors/render) - Learn about render assets

## Scripting Interface

You can control a Render Component's properties using a [Script Component](script.md). The Render Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/RenderComponent.html).

--------------------------------------------------------------------------------

## Rigid Body

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/rigidbody/

The Rigid Body Component enables an entity to participate in the scene's physics simulation. This allows the movement of an entity to be simulated realistically. The component interface dynamically displays different attributes based on the 'Type' attribute.

#### Static

[Image: Rigid Body Component (Static)]

#### Dynamic

[Image: Rigid Body Component (Dynamic)]

#### Kinematic

[Image: Rigid Body Component (Kinematic)]

Note that you must add a [Collision Component](collision.md) to the same entity in order to define the shape of the rigid body. Otherwise, the Rigid Body Component has no effect and will not participate in the physics simulation.

## Properties

| Property        | Description |
|-----------------|-------------|
| Type            | The type of the body. Options: Static, Dynamic, Kinematic. |
| Mass            | Dynamic only. The mass of the body in kilograms (when world units are meters). |
| Linear Damping  | Dynamic only. Specifies the proportion of linear velocity lost per second (0 to 1). |
| Angular Damping | Dynamic only. Specifies the proportion of angular velocity lost per second (0 to 1). |
| Linear Factor   | Dynamic only. Multiplier for the body's linear movement in each world axis (X, Y, Z). If set to 0 for any axis, no movement will occur in that axis - useful for creating 2D games or constrained movement. |
| Angular Factor  | Dynamic only. Multiplier for the body's angular (rotational) movement about each world axis (X, Y, Z). If set to 0 for any axis, no rotation will occur around that axis. |
| Friction        | Controls how quickly a body loses velocity when in contact with other bodies (0 to 1). |
| Restitution     | A measure of the bounciness of a body (0 to 1). Warning: setting to 1 means a moving body will never come to a stop (unless colliding with other bodies with restitutions below 1, or unless a stop is scripted). |

## See Also

- [Collision Component](collision.md) - Required to define the rigid body's shape
- [Physics](/user-manual/physics) - Learn more about the physics system

## Scripting Interface

You can control a Rigid Body Component's properties using a [Script Component](script.md). The Rigid Body Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/RigidBodyComponent.html).

--------------------------------------------------------------------------------

## Screen

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/screen/

The Screen Component defines the area and rendering of a user interface. Children added to a Screen Component should all have an Element Component.

See the [User Interface](/user-manual/user-interface) section for more details.

[Image: Screen Component]

## Properties

| Property         | Description |
|------------------|-------------|
| Screen Space     | When enabled, the contents of the screen are rendered in 2D as an overlay to the canvas. |
| Resolution       | When Screen Space is disabled. The resolution of the screen coordinates (width and height). Screen coordinates increase as you move right and up. |
| Ref Resolution   | Screen Space only, when Scale Mode is Blend. The reference resolution used to calculate the scale factor (width and height). |
| Scale Mode       | Screen Space only. Determines how the user interface scales when the window size does not match the screen size. Options: None (no scaling), Blend (scales by ratio of reference to actual resolution). |
| Scale Blend      | Screen Space only, when Scale Mode is Blend. The weighting of scaling between horizontal (0) and vertical (1). |
| Priority         | Determines the order in which Screen Components in the same layer are rendered. Higher priority is rendered on top. Must be an integer between 0 and 127. |

## See Also

- [Element Component](element.md) - UI elements that are children of the screen
- [Button Component](button.md) - Interactive button elements
- [User Interface](/user-manual/user-interface) - Learn more about building user interfaces

## Scripting Interface

You can control a Screen Component's properties using a [Script Component](script.md). The Screen Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/ScreenComponent.html).

--------------------------------------------------------------------------------

## Script

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/script/

The Script Component enables an entity to run user-supplied scripts. In this way, the user can write scripts (using JavaScript or TypeScript) that run when the entity is instantiated and updated on a per-frame basis.

[Image: Script Component]

## Adding Scripts

To create a new script, click on the **Add Script** dropdown in the Script Component and either:

- Select an existing script from the list
- Type a new script name and click **Create Script** to create a new script asset

Alternatively, you can drag a script asset from the Assets Panel onto the Script Component.

[Image: Add Script Dialog]

## Script Panel Controls

Each script added to the component displays as a collapsible panel with the following controls:

| Control      | Description |
|--------------|-------------|
| Script Name  | Click to select the script asset in the Assets Panel. |
| On/Off       | Toggle to enable or disable this individual script. |
| Edit         | Opens the script in the Code Editor. |
| Parse        | Re-parses the script to update its attributes. Use after modifying script attribute definitions. |
| Remove       | Removes the script from the component (click the X button). |

## Script Ordering

When multiple scripts are attached to an entity, their order matters. Scripts are executed from top to bottom. You can reorder scripts by dragging and dropping them within the component.

## Script Attributes

Scripts can define custom attributes that appear in the Inspector. These attributes allow you to configure script behavior without modifying the code. Supported attribute types include:

- **boolean** - Checkbox
- **number** - Numeric input (with optional slider for min/max range)
- **string** - Text input
- **vec2**, **vec3**, **vec4** - Vector inputs
- **rgb**, **rgba** - Color picker
- **asset** - Asset picker
- **entity** - Entity picker
- **curve** - Curve editor
- **json** - Complex nested objects

See the [Script Attributes](/user-manual/scripting/script-attributes/) documentation for details on defining attributes in your scripts.

## See Also

- [Scripting](/user-manual/scripting) - Learn how to write scripts
- [Editor Scripting](/user-manual/editor/scripting) - Managing scripts in the Editor

## Scripting Interface

The Script Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/ScriptComponent.html).

--------------------------------------------------------------------------------

## Scrollbar

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/scrollbar/

The Scrollbar Component defines a scrolling control for a [Scroll View](scrollview.md) Component.

See the [User Interface](/user-manual/user-interface) section for more details.

[Image: Scrollbar Component]

## Properties

| Property    | Description |
|-------------|-------------|
| Orientation | Controls whether the scrollbar moves horizontally or vertically. Options: Horizontal, Vertical. |
| Value       | The current position value of the scrollbar, in the range 0 to 1. |
| Handle      | The entity to be used as the scrollbar handle. This entity must have an Element Component. |
| Handle Size | The size of the handle relative to the size of the track, in the range 0 to 1. For a vertical scrollbar, a value of 1 means that the handle will take up the full height of the track. |

## See Also

- [Scroll View Component](scrollview.md) - The scrollable area that uses this scrollbar
- [Element Component](element.md) - Required for the scrollbar handle
- [User Interface](/user-manual/user-interface) - Learn more about building user interfaces

## Scripting Interface

You can control a Scrollbar Component's properties using a [Script Component](script.md). The Scrollbar Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/ScrollbarComponent.html).

--------------------------------------------------------------------------------

## Scroll View

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/scrollview/

The Scroll View Component defines a scrollable area in a user interface. A Scroll View can be scrolled via [Scrollbar](scrollbar.md) Components.

See the [User Interface](/user-manual/user-interface) section for more details.

[Image: Scroll View Component]

## Properties

| Property                | Description |
|-------------------------|-------------|
| Scroll Mode             | Specifies how the scroll view should behave when the user scrolls past the end of the content. Options: Clamp (content stops at bounds), Bounce (content bounces back), Infinite (content scrolls forever). |
| Bounce                  | Bounce mode only. Controls how far the content should move before bouncing back (0 to 10). |
| Friction                | Controls how freely the content should move if thrown (e.g., by flicking on a phone or flinging the scroll wheel). A value of 1 means content stops immediately; 0 means content continues moving forever (or until bounds are reached, depending on scroll mode). |
| Use Mouse Wheel         | Whether to use mouse wheel for scrolling (horizontally and vertically) when the mouse is within bounds. |
| Mouse Wheel Sensitivity | Use Mouse Wheel only. Mouse wheel horizontal and vertical sensitivity. Setting a direction to 0 disables scrolling in that direction. Default is [1, 1]. |
| Viewport                | The entity to be used as the masked viewport area, within which the content will scroll. This entity must have an Element Component. |
| Content                 | The entity which contains the scrolling content itself. This entity must have an Element Component. |
| Horizontal              | Whether to enable horizontal scrolling. |
| Scrollbar (horizontal)  | Horizontal only. The entity to be used as the horizontal scrollbar. This entity must have a Scrollbar Component. |
| Visibility (horizontal) | Horizontal only. Controls scrollbar visibility. Options: Show Always, Show When Required (only when content exceeds viewport). |
| Vertical                | Whether to enable vertical scrolling. |
| Scrollbar (vertical)    | Vertical only. The entity to be used as the vertical scrollbar. This entity must have a Scrollbar Component. |
| Visibility (vertical)   | Vertical only. Controls scrollbar visibility. Options: Show Always, Show When Required (only when content exceeds viewport). |

## See Also

- [Scrollbar Component](scrollbar.md) - Controls for scrolling the view
- [Element Component](element.md) - Required for the viewport and content
- [User Interface](/user-manual/user-interface) - Learn more about building user interfaces

## Scripting Interface

You can control a Scroll View Component's properties using a [Script Component](script.md). The Scroll View Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/ScrollViewComponent.html).

--------------------------------------------------------------------------------

## Sound

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/sound/

The Sound Component controls playback of audio samples.

[Image: Sound Component]

Each Sound Component contains "Slots" that are responsible for playing sounds. Each Slot can be assigned a different audio Asset, and can be played independently from the other Slots.

You can add a new Slot by clicking on the "ADD SLOT" button. You can remove a Slot by clicking on the bin icon in the header of each slot.

## Component Properties

| Property        | Description |
|-----------------|-------------|
| Positional      | If checked, the component will play back audio assets as if played from the location of the entity in 3D space. |
| Volume          | The volume that will be multiplied with the volume of each Slot when playing an audio asset (0 to 1). |
| Pitch           | The pitch that will be multiplied with the pitch of each Slot when playing an audio asset. A value of 1 means the audio is played back at the original pitch. |
| Ref Distance    | Positional only. The reference distance for reducing volume as the sound source moves further from the listener. |
| Max Distance    | Positional only. The maximum distance from the listener at which audio falloff stops. Note the volume of the audio is not necessarily 0 after this distance, but just doesn't fall off anymore. |
| Distance Model  | Positional only. Determines which algorithm to use to reduce the volume of the sound as it moves away from the listener. Options: Linear, Exponential, Inverse. |
| Roll-off Factor | Positional only. The rate at which volume fall-off occurs. |

## Slot Properties

| Property   | Description |
|------------|-------------|
| Name       | The name of the Slot. This is the name that you will use to refer to this Slot in code. |
| Asset      | The audio asset to play. |
| Start Time | The starting point of the sound in the audio asset, in seconds. You can use this if you don't want to play the entire audio asset but just a portion of it instead. |
| Duration   | The duration of the sound to play starting from Start Time, in seconds. Leave empty to play until the end. E.g. you might have an audio asset with multiple sounds in it. To make this Slot play such a sound you can define its Start Time and Duration. |
| Auto Play  | If checked, the Slot will be played on load. Otherwise, the Slot will need to be played using script. |
| Overlap    | If checked, this Slot will play its sound without stopping first. Overlap should be used for one-shot sounds that need to be played repeatedly. Overlapping sounds do not stop when the Entity is destroyed, but only when they finish or when you manually stop them in script. |
| Loop       | If checked, the Slot will loop continuously. Otherwise, it will be played once to completion. |
| Volume     | The volume of the audio asset played back by the slot (0 to 1). |
| Pitch      | The pitch at which the audio is played. A value of 1 means the audio is played back at the original pitch. |

## See Also

- [Audio Listener Component](audiolistener.md) - Set the listener position for 3D audio
- [Audio Assets](/user-manual/editor/assets/inspectors/audio) - Learn more about audio assets

## Scripting Interface

You can control a Sound Component's properties using a [Script Component](script.md). The Sound Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/SoundComponent.html).

--------------------------------------------------------------------------------

## Sprite

URL: https://developer.playcanvas.com/user-manual/editor/scenes/components/sprite/

The Sprite Component renders and animates [Sprite Assets](/user-manual/editor/assets/inspectors/sprite) into the scene.

There are two types of sprite: Simple and Animated.

## Simple Sprites

Simple Sprite Components display a single frame from an atlas.

[Image: Simple Sprite]

## Simple Sprite Component Properties

| Property    | Description |
|-------------|-------------|
| Type        | Simple or Animated. |
| Sprite      | The Sprite Asset to display. |
| Frame       | The frame index of the Sprite to display. |
| Width       | The width of the sprite when using 9-slicing (only shown for sliced/tiled sprites). |
| Height      | The height of the sprite when using 9-slicing (only shown for sliced/tiled sprites). |
| Color       | A color to apply as a tint to the sprite. |
| Opacity     | The transparency of the sprite (0 to 1). |
| Flip X      | Flip the rendered sprite horizontally. |
| Flip Y      | Flip the rendered sprite vertically. |
| Batch Group | The Batch Group that this sprite belongs to. More on Batching [here](/user-manual/graphics/advanced-rendering/batching). |
| Layers      | The Layers in which to render the sprite. |
| Draw Order  | The order in which this sprite is rendered. Lower numbers are rendered first. |

## Animated Sprites

Animated Sprite Components have multiple Sprite Animation Clips attached which can play back different Sprite Assets.

[Image: Animated Sprite]

## Animated Sprite Component Properties

| Property   | Description |
|------------|-------------|
| Type       | Simple or Animated. |
| Color      | A color to apply as a tint to the sprite. |
| Opacity    | The transparency of the sprite (0 to 1). |
| Flip X     | Flip the rendered sprite horizontally. |
| Flip Y     | Flip the rendered sprite vertically. |
| Speed      | Multiplier applied to the speed at which sprite animation clips on this sprite component are animated. |
| Layers     | The Layers in which to render the sprite. |
| Draw Order | The order in which this sprite is rendered. Lower numbers are rendered first. |
| Auto Play  | The name of a sprite animation clip to play when the sprite is enabled. Select from available clips or None. |

## Sprite Animation Clip Properties

| Property          | Description |
|-------------------|-------------|
| Name              | The name of the Sprite Animation Clip. Used to reference an individual clip. |
| Loop              | If true the animation clip will loop back to the start when it reaches the end. |
| Frames Per Second | The speed at which the clip is played in Frames Per Second. |
| Sprite            | The Sprite Asset which is used to play this clip. |

## See Also

- [Sprite Assets](/user-manual/editor/assets/inspectors/sprite) - Learn about creating sprite assets

## Scripting Interface

You can control a Sprite Component's properties using a [Script Component](script.md). The Sprite Component's scripting interface is [here](https://api.playcanvas.com/engine/classes/SpriteComponent.html).

--------------------------------------------------------------------------------

## Loading Scenes

URL: https://developer.playcanvas.com/user-manual/editor/scenes/loading-scenes/

This page covers how to load scenes programmatically and different approaches for using scenes in your projects.

There are two main approaches: changing scenes completely and loading scenes additively.

## Changing Scenes Completely

This is the most common approach that developers take where each scene is a self-contained part of the game. For example, one scene would be the title screen and then one scene per level.

[Here is an example](https://playcanvas.com/project/924351/) where the user can move to and from the title screen to other levels.

[Interactive Demo]

This is done by simply calling [`SceneRegistry.changeScene`](https://api.playcanvas.com/engine/classes/SceneRegistry.html#changescene) with the name of the scene.

```javascript
this.app.scenes.changeScene('Some Scene Name');
```

If the scene data is not already loaded, this function will:

- Make the asynchronous network request for the new scene data.
- When the scene data is loaded, it will delete all child entities from the application root node (destroying the existing scene hierarchy).
- Call `loadSceneSettings` which is now synchronous as the scene data is loaded.
- Call `loadSceneHierarchy` which is now synchronous as the scene data is loaded.

If you want to know when the scene is loaded or if there are errors, you will need to provide a callback:

```javascript
this.app.scenes.changeScene('Some Scene Name', (err, loadedSceneRootEntity) => {
    if (err) {
        console.error(err);
    } else {
        // Scene hierarchy has successfully been loaded
    }
});
```

To avoid the asynchronous network request when calling `changeScene`, you can call [`SceneRegistry.loadSceneData`](https://api.playcanvas.com/engine/classes/SceneRegistry.html#loadscenedata) ahead of time. This makes `changeScene` synchronous, immediately calling `loadSceneSettings` and `loadSceneHierarchy`.

Common use cases would include knowing that the user would load level 2 when level 1 is completed. In this case, you can load the scene data for level 2 when the user is in level 1. When they complete level 1, they won't have to wait for data to be loaded and immediately enter level 2.

## Loading Scenes Additively

It is possible to load multiple scene hierarchies in an additive manner rather than completely switching scenes. The common use cases for this are to split up a large world so that it can be loaded over time rather than loading it all at once at the start.

A variant of the above would be for each scene to represent a section of the world that is loaded and destroyed as the player moves around. The system would only load the nearest connected sections of the world and related assets while destroying and unloading assets for any section that is not needed. This would help with managing resources such as memory and VRAM.

Sometimes developers use this approach to ensure that certain code and entities are created before the actual game loads and have them globally accessible throughout the game session.

[Below is a simplified example](https://playcanvas.com/project/685077/) of additively loading scenes where the UI in the top left is the 'main' scene and different scene hierarchies are loaded/destroyed.

[Interactive Demo]

:::warning
Multiple instances of the same scene hierarchy cannot be loaded at once. Entities have unique GUIDs assigned in the Editor, and loading multiple instances causes GUID conflicts.

If you need multiple instances of an entity hierarchy, use [Templates](/user-manual/editor/templates/) instead. Templates generate unique GUIDs on instantiation.
:::

## Understanding How Scenes Work

To use scenes effectively, it is important to understand how they are loaded when used in a project. This section goes into detail about how scenes are structured and loaded.

Scenes are separate from [assets](/user-manual/assets/) and have their own properties and APIs.

Scenes are represented by [Scene Registry Items](https://api.playcanvas.com/engine/classes/SceneRegistryItem.html) stored in the [Scene Registry](https://api.playcanvas.com/engine/classes/SceneRegistry.html), accessible via the [Application](https://api.playcanvas.com/engine/classes/AppBase.html#scenes) object. You can find a Scene Registry Item by the scene's name and use it to load the hierarchy or settings.

:::note

The [application root node](https://api.playcanvas.com/engine/classes/AppBase.html#root) is not the scene hierarchy root entity that is named 'Root' by default that you see in the scene with the Editor. The scene hierarchy root entity will be a child of the application root node.

:::

There are two APIs to load the scene hierarchy and settings:

- [`SceneRegistry.loadSceneHierarchy`](https://api.playcanvas.com/engine/classes/SceneRegistry.html#loadscenehierarchy) - Loads a scene hierarchy
- [`SceneRegistry.loadSceneSettings`](https://api.playcanvas.com/engine/classes/SceneRegistry.html#loadscenesettings) - Loads settings from a scene

Here is a code example to load the scene hierarchy or settings:

```javascript
// Find the Scene Registry Item by the name of the scene
const sceneItem = this.app.scenes.find('Some Scene Name');

// Load the scene hierarchy with a callback when it has finished
this.app.scenes.loadSceneHierarchy(sceneItem, (err, loadedSceneRootEntity) => {
    if (err) {
        console.error(err);
    } else {
        // Scene hierarchy has successfully been loaded
    }
});

// Load the scene settings with a callback when it has finished
this.app.scenes.loadSceneSettings(sceneItem, (err) => {
    if (err) {
        console.error(err);
    } else {
        // Scene settings have successfully been loaded
    }
});
```

Both `loadSceneHierarchy` and `loadSceneSettings` have similar behavior in how they get the data needed to load the hierarchy or settings.

When the function is called, it performs an asynchronous network request to the server for the scene data. This means that there will be a delay (depending on network speed, the network connection and size of the scene) between the request to load the scene and the browser completing the network request where the application is still updating.

Once the network request has been completed, the engine will do the following:

`loadSceneHierarchy`

- Creates the entities and components from the loaded scene and adds the hierarchy to the [application root node](https://api.playcanvas.com/engine/classes/AppBase.html#root).
- Calls `initialize` and `postInitialize` functions on the ScriptTypes in the loaded scene.
- Calls the callback that was passed into the `loadSceneHierarchy` function.
- (Optional) The [callback](https://api.playcanvas.com/engine/types/LoadHierarchyCallback.html) receives the loaded scene root entity as a parameter, which can be modified or reparented as needed.

`loadSceneSettings`

- Applies the loaded scene settings to the application.
- Calls the [callback](https://api.playcanvas.com/engine/types/LoadSettingsCallback.html) that was passed into the `loadSceneSettings` function.

By default, `loadSceneHierarchy` will always load additively and it's up to the developer to remove/destroy the existing loaded scene to change scenes completely.

There are several ways to approach this with pros and cons:

### Destroying all children under application root node first

This approach has discrete steps that make it easier to manage where the currently loaded scene is destroyed before loading and creation of the new scene.

```javascript
// Find the Scene Registry Item by the name of the scene
const sceneItem = this.app.scenes.find('Some Scene Name');

// Destroy all children under application root to remove the currently loaded scene hierarchy
const rootChildren = this.app.root.children;
while(rootChildren.length > 0) {
    rootChildren[0].destroy();
}

// Load the scene hierarchy with a callback when it has finished
this.app.scenes.loadSceneHierarchy(sceneItem, (err, loadedSceneRootEntity) => {
    if (err) {
        console.error(err);
    } else {
        // Scene hierarchy has successfully been loaded
    }
});
```

However, as mentioned above, there is a delay between calling `loadSceneHierarchy` and the scene data actually being loaded. This means that there will be a few frames where the application will be rendering a blank screen while it's waiting for the network request to complete which brings us to the alternative.

### Destroying the old scene root entity after the new scene is loaded

This would mean that the old scene hierarchy will be destroyed in the callback after the new scene hierarchy has been added to hierarchy which ensures that the old scene would be present while the scene data is loaded from network.

```javascript
// Find the Scene Registry Item by the name of the scene
const sceneItem = this.app.scenes.find('Some Scene Name');

// Assume the old scene hierarchy's root entity is named 'Root' which is the default name
const oldSceneRootEntity = this.app.root.findByName('Root');

// Load the scene hierarchy with a callback when it has finished
this.app.scenes.loadSceneHierarchy(sceneItem, (err, loadedSceneRootEntity) => {
    if (err) {
        console.error(err);
    } else {
        // Scene hierarchy has successfully been loaded
        oldSceneRootEntity.destroy();
    }
});
```

However, the old scene will be present in the hierarchy while the new scene's scriptTypes call `initialize` and `postInitialize`. This can cause issues if there is some dependency or assumptions in the scripts that it's the only scene hierarchy that is loaded. Examples would be searching for an entity by name in `initialize` and there is also an entity with the same name in the old scene hierarchy. The script would then have a reference to the old scene hierarchy's entity instead of the new scene's which will cause unexpected behavior once the old scene's hierarchy is destroyed.

To help mitigate these potential issues, we have an API that allows the separation of loading the scene data from the creation of the scene hierarchy in the scene, [`SceneRegistry.loadSceneData`](https://api.playcanvas.com/engine/classes/SceneRegistry.html#loadscenedata).

## Managing Assets in Scenes

A common question with scenes is if the assets used in the scene will be loaded as part of the scene load. With PlayCanvas, the assets and scenes are separate and will need to be loaded separately which gives the developer a large degree of flexibility.

The recommended practice is to tag assets with the scene name they belong to. When loading a scene, load the tagged assets first, then load the scene once all assets are ready.

More information about asset tags and asset loading can be found on [this page](/user-manual/assets/preloading/).

The [example project](https://playcanvas.com/project/926754/) below loads the assets when loading the scene and unloads when returning the main menu.

[Interactive Demo]

--------------------------------------------------------------------------------

## Managing Scenes

URL: https://developer.playcanvas.com/user-manual/editor/scenes/managing-scenes/

The Scenes dialog allows you to create, open, duplicate and delete scenes in your project.

## Opening the Scenes Dialog

There are two ways to open the Scenes dialog:

1. **Menu** - Click the PlayCanvas logo in the top-left corner and select **Scenes**.

   [Image: Menu - Scenes option]

2. **Scene Name** - Click on the current scene name in the viewport toolbar.

   [Image: Scene name in toolbar]

## Scenes Dialog Overview

[Image: Scenes dialog]

The Scenes dialog displays all scenes in your project:

- **Filter** - Use the search field to filter scenes by name.
- **Scene List** - Scenes are sorted alphabetically, showing the scene name and last modified date.
- **Current Scene** - The currently loaded scene is highlighted and its name is selectable for copying.

## Opening a Scene

Click on any scene in the list to open it.

## Creating a New Scene

1. Click the **+ ADD NEW SCENE** button at the bottom of the dialog.
2. Enter a name for your scene.
3. Press **Enter** to create and open the new scene.

## Scene Actions Menu

Click the dropdown button on the right side of any scene to access additional actions:

[Image: Scene dropdown menu]

| Action | Description |
|--------|-------------|
| **Duplicate Scene** | Creates a copy of the scene. The duplicate is automatically named with an incremented number (e.g., "My Scene 2"). |
| **Delete Scene** | Permanently deletes the scene after confirmation. |
| **Item History** | Opens the version control history for the scene. |
| **Open in New Tab** | Opens the scene in a new browser tab. |

:::note
Creating, duplicating and deleting scenes requires write access to the project.
:::

--------------------------------------------------------------------------------

## Scripting

URL: https://developer.playcanvas.com/user-manual/editor/scripting/

The PlayCanvas Editor provides a tightly integrated environment for writing and managing scripts. This section covers the Editor-specific tools and workflows that will enhance your scripting experience.

:::tip Learn the Basics First
If you're new to scripting in PlayCanvas, we recommend starting with the [Scripting](/user-manual/scripting/) section to learn the fundamentals—script lifecycle, events, attributes, and the Engine API—before diving into the Editor-specific features covered here.
:::

## What the Editor Provides

As an Editor user, you benefit from a tightly integrated development environment:

- **Asset Management** — Scripts are assets, managed alongside your models, textures, and materials in the Assets panel.
- **Visual Configuration** — Script Attributes let you customize script behavior directly in the Inspector without changing code.
- **Rapid Iteration** — Hot Reloading lets you see code changes in real-time without restarting your app.
- **Seamless Integration** — The Editor bridges the gap between your code and your scene, making it easy to connect scripts to Entities and Components.

## In This Section

- [Managing Scripts](./managing-scripts.md) — Create, organize, and manage script assets in the Editor.
- [Import Maps](./import-maps.md) — Configure module resolution for ESM scripts.
- [Code Editor](./code-editor.md) — Use the built-in code editor for quick edits.
- [VS Code Extension](./vscode-extension.md) — Set up VS Code for a full-featured development experience.
- [Hot Reloading](./hot-reloading.md) — Iterate rapidly with live code updates.
- [Loading Order](./loading-order.md) — Control the order in which scripts are loaded.

--------------------------------------------------------------------------------

## Code Editor

URL: https://developer.playcanvas.com/user-manual/editor/scripting/code-editor/

The Code Editor is an online real-time collaborative editor that allows you to edit your Script assets and also all the other text based assets like JSON, HTML, CSS etc.

## Opening The Code Editor

You can open the Code Editor from inside the [PlayCanvas Editor](/user-manual/editor/) by pressing Ctrl + I. You can also click on this toolbar icon:

[Image: Code Editor Toolbar]

Alternatively you can double click on a text-based asset like a Script to open the Code Editor focused on that asset.

## Interface

[Image: Code Editor]

### Menu

The menu contains all the different tools and options available along with their keyboard shortcuts.

### Files

Here you can see all the assets that can be edited by the Code Editor. Click on an asset to select it. Selecting an asset will open it in the Text Editor for editing.

Single clicking an asset will open it in Preview Mode. This means that if you open another asset it will be shown in the same Tab. If you want to open an asset permanently double click on the asset or start editing it.

By right clicking on an asset you will see the context menu with various asset-related options. You can also drag and drop assets into folders.

### Tabs

Each open asset has a corresponding tab in the tab view. You can rearrange tabs by dragging and dropping them and you can also close tabs by clicking on the X button that appears when you put your mouse cursor on a tab. If you right click on a tab you will see the tab context menu with various tab-related options.

### Text Editor

Here you can actually edit the contents of your assets. The editor uses [Monaco](https://github.com/Microsoft/monaco-editor) which is the same text editor library used by Visual Studio Code.

In addition, the editor supports auto-complete. When you type a letter or hit Ctrl+Space you will see a list of auto-complete suggestions.

The editor will also lint your JavaScript code. This means that while you type your code the editor will scan your code for possible errors or suspicious usage. This is done with the help of [JSHint](https://jshint.com/), which also supports special comments which control its behavior. For a list of these special comments check out the [docs](https://jshint.com/docs/).

### Collaborators

The Editor supports real-time collaborative editing by all users with permission to edit the code. You will see other user's avatars here.

### Status Bar

Here you can see various helpful messages while you are interacting with the Code Editor.

## Saving and Reverting

The code editor is collaborative which means that every user who has the document open in the editor will see changes as they are made by other developers. However, changes are not saved automatically and when the application is run from the Editor the saved version is loaded. This means you can choose at which point to commit your change to be used by the application. Any user can save the document at any time.

If you have unsaved changes the Revert option in the File menu will cancel these changes and restore the document back to the saved version. If all users exit the document before saving the unsaved changes will be preserved for a short time before the document is automatically reverted to the saved version. So you should save your work before exiting the document.

## Command Palette

All the text editing features can be found using the command palette via Ctrl + Shift + P (Windows) or Cmd + Shift + P (Mac).

## Hot Keys

The editor supports various shortcuts to help you when editing code. You can find all those options in the Menu or the command palette. The most common can be found below:

| Command                         | PC                       | Mac                        |
|---------------------------------|--------------------------|----------------------------|
| Save                            | Ctrl + S                 | Cmd + S                    |
| Undo                            | Ctrl + Z                 | Cmd + Z                    |
| Redo                            | Ctrl + Y                 | Cmd + Shift + Z or Cmd + Y |
| Find in file                    | Ctrl + F                 | Cmd + F                    |
| Find next match                 | F3                       | Cmd + G                    |
| Find previous match             | Shift + F3               | Cmd + Shift + G            |
| Replace                         | Ctrl + H                 | Cmd + Alt + F              |
| Find in Files                   | Ctrl + Shift + F         | Cmd + Shift + F            |
| Comment line or selection       | Ctrl + /                 | Cmd + /                    |
| Block Comment line or selection | Alt + Shift + A          | Alt + Shift + A            |
| Indent line or selection        | Tab                      | Tab                        |
| Un-Indent line or selection     | Shift + Tab              | Shift + Tab                |
| Jump to declaration             | Ctrl + F12               | Cmd + F12                  |
| Show autocomplete               | Ctrl + Space or Ctrl + I | Ctrl + Space or Cmd + I    |

## Searching

You can search for text in a file by pressing Ctrl + F (Windows) or Cmd + F (Mac). This will open up the Find panel on the top right:

Enter your search term here and press Enter. You can navigate between matches by continuously hitting Enter (or Shift + Enter to go backwards).

If you want to replace a match enter the text you want to replace it with in the Replace input field on the right. Then hit Enter to replace the match and keep hitting Enter to replace each subsequent match (or Shift + Enter to replace backwards).

### Find in Files

You can also find a term by searching in all your assets. To do this hit Ctrl + Shift + F to bring up the Find in Files panel on the bottom. Like before enter your search term and hit Enter. This will open up a new tab where the search results will be displayed:

On the left side of the input fields there are various search options. These allow you to search using a regular expression, do a case-sensitive search or search for whole words. If you would like to learn and experiment more with regex, then [RegExr](https://regexr.com/) is great site with cheat sheets, examples and an online editor for testing regex expressions.

[Image: Find in Files]

You can double click on a line in the search results to go that particular line in the asset.

## Quick Open

You can quickly open an asset by hitting Ctrl + P (or Cmd + P on Mac) which will open a panel where you can search for the asset by its name. Start typing the name of the asset and the panel will perform a fuzzy search to find the asset you are looking for. Hit Enter to open the selected asset.

[Image: Go to Anything]

## Preferences

You can edit your preferences by clicking on Edit -> Preferences. Here you can change the font size for the text editor and also edit other editor related options.

[Image: Preferences]

## Extra Tips

### Quick Searching

Highlighting a word in the code editor will highlight all other matches in the file which makes it easier to check where a variable or function is being used.

[Image: Quick Searching]

### Multiple Selection Editing

Holding down Alt and clicking left mouse button will add another cursor to the editor. This allows you to quickly make the same changes in multiple areas of the file.

[Image: Multiple Selection Editing]

### Rectangular Selection

Holding down Alt + Shift and dragging left mouse button will create a rectangular selection which is useful for selecting and editing columns of text.

[Image: Rectangular Selection]

--------------------------------------------------------------------------------

## Hot Reloading

URL: https://developer.playcanvas.com/user-manual/editor/scripting/hot-reloading/

When you are iterating on a complex project it can be frustrating to have to do a full page refresh every time you make a change to a script. Especially if it takes you a long time to get to the point where you are testing your code. That is where hot-swapping of code comes in.

## How to Use Hot-swapping

Hot-swapping is enabled on a per-script basis. To enable it, all you need to do is implement the `swap()` method in your script.

When a script with a `swap` function is changed in the code editor, any launched applications will reload the script and add it to script registry. Then it creates brand new script instances to swap with the old ones, calling the `swap` method during that process per each instance. The `initialize` method of the script is *not* called again. Instead, the old script instance is passed into the `swap` method and it is up to the developer to ensure that the state of the old script is copied into the new one. Declared script attributes are automatically copied over into the new script instance. It is also important to remove any event listeners from the old instance and re-attach them to the new one.

For example:

Try changing logic within the `update` method and save the code. The launched application will automatically swap `rotator` script instances with new ones and your application will keep working with the new logic.

The `swap` method is called regardless of the running state of a script instance, so if it was disabled due to an error it can be re-enabled in the `swap` method.

--------------------------------------------------------------------------------

## Import Maps

URL: https://developer.playcanvas.com/user-manual/editor/scripting/import-maps/

With ESM Scripts, you can use Import Maps to control how module specifiers are resolved in your project. Import Maps allow you to define aliases or remap module paths so you can write cleaner and more flexible imports, especially when working with shared libraries or internal tools.

For example, instead of writing:

```js

```

You can use an import map to simplify this to:

```js

```

## Create an Import Map

In the Settings Panel find the "Import Map" section. Click "Create Default" and the editor will create a new Import Map in the asset registry and assign it to the project.

Open the file in the code editor. If you want to map a module like "/utils/math.mjs" to "math", update the import map with the following;

```json
{
  "imports": {
    "math": "./utils/math.mjs"
  }
}
```

Now in your code you can call `import x from "math"` and it will resolve to your module.

:::tip

You can also use Import Maps as shorthand for npm packages. For example, add `"tweenjs": "https://esm.sh/tween.js"` which will resolve to tween.js on a CDN.

:::

## Things to know

- Import Maps only apply to ESM Builds. Classic Scripts on their own don't support them.
- The paths you define in the map must match files that exist in your project or point to valid external URLs.
- Only one import map can be active at a time.
- If you're using external modules (e.g. from a CDN), ensure they are ESM-compatible and CORS-accessible.

## When to use Import Maps

- To avoid brittle relative paths in large projects.
- To create clean, alias-based paths for shared utilities.
- To reference third-party external modules without installing them locally.

Using import maps helps keep your codebase organized and easier to maintain, especially as your project grows.

--------------------------------------------------------------------------------

## Script Loading Order

URL: https://developer.playcanvas.com/user-manual/editor/scripting/loading-order/

## ESM Scripts

ESM Scripts do not have an explicit loading order, and should not be relied upon to load in a specific order. Instead, you should use module import statements to declare dependencies between modules.

## Classic Scripts

Generally all scripts are loaded at the beginning of your application. The loading order is determined by a setting in your project which you can access from the main Editor menu or Scene Settings:

[Image: Loading Order]

The loading order panel shows all Classic scripts marked as `preload` and the order that they are loaded and executed in.

[Image: Loading Order List]

You can click-and-drag to move individual scripts around to edit the order.

When scripts are first loaded, they are immediately executed. That means that the scripts are first executed in the order that they are loaded. However, the loading order of the script **does not** affect the execution of order of script methods within script component. For example, the `initialize` methods of scripts on the same entity are called in the order that they are listed on the Entity, not the loading order.

### Preloading

By default, as with other assets in PlayCanvas, a script asset is marked as `preload`. This means that it will be loaded before the application starts. If you disable preloading on a script, it will not be loaded under normal circumstances. This way, you can include a script in your project but prevent it from loading by unchecking `preload`. You can trigger a non-preloading script to load dynamically by using the regular asset API (see [`AssetRegistry#load`](https://api.playcanvas.com/engine/classes/AssetRegistry.html#load)).

It is possible to subscribe to dynamic changes to script registry:

```javascript
this.app.scripts.on('add', (name, scriptType) => {
    console.log('script', name, 'has been loaded');
});
```

### Concatenation

By default, when you publish or export your application, all preloaded Classic scripts are concatenated into a single script file. This optimizes load time by reducing the number of requests that are needed to load all your scripts.

--------------------------------------------------------------------------------

## Managing Your Scripts

URL: https://developer.playcanvas.com/user-manual/editor/scripting/managing-scripts/

In the PlayCanvas Editor, your JavaScript code lives in Script Assets. These are `.js` or `.mjs` files stored within your project's Asset panel, just like your models, textures, and materials. Effectively managing these Script Assets is key to an organized and efficient workflow. This page covers how to create, organize, delete, import, and assign scripts using the Editor interface.

Key Concepts:

* **Script Asset:** The `.js` or `.mjs` file containing your script's code. It resides in the Asset Panel.
* **Script Component:** An instance of one or more Script Assets attached to an Entity. This is what makes your script "run" on that specific Entity.

## Creating New Scripts

You can create new Script Assets directly within the Editor:

1. Navigate to the Asset Panel.
2. Choose a Folder (Optional but Recommended): It's good practice to organize your scripts into folders. You can create a new folder by right-clicking in the Asset Panel and selecting New Asset > Folder.
3. Create the Script:
    * Right-click within the desired folder (or the root of the Asset Panel).
    * Select New Asset > Script.  
    [Image: New Script]
    * In the popup, provide a filename for your new script.

## Importing Scripts

You can bring scripts into your PlayCanvas project from external sources:

* **Importing Scripts from Your Computer:**
    1. Locate the `.js`/`.mjs` file(s) on your computer's file system.
    2. Drag the file(s) directly from your file explorer/finder into the desired folder in the PlayCanvas Editor's Asset Panel.
    3. The Editor will upload and process the script, making it available as a Script Asset.
* **Importing Scripts from the PlayCanvas Asset Store:**
    1. Open the [Asset Store](../assets/asset-store/index.md) (accessible via a button in the Asset Panel header).
    2. Browse or apply the SCRIPT filter.
    3. Once you find a script/package you want, click to view its details and select IMPORT.
    4. The imported script(s) will appear in your Asset Panel, typically within a new folder named after the Asset Store package. These are now regular Script Assets you can manage and use.

## Assigning Scripts to Entities (via Script Components)

A Script Asset itself doesn't do anything until it's attached to an Entity via a Script Component.

1. **Select an Entity:** In the Hierarchy panel, select the Entity you want to add a script to.
2. **Add a Script Component (if one doesn't exist):**
    * In the Inspector panel (on the right), click the Add Component button.
    * Select Script from the list. A new Script Component will be added to the Entity.
3. **Assign Your Script Asset(s) to the Script Component:**
    * The Script Component has an edit box (with the text + ADD SCRIPT). Click within the edit box to focus.
    * A dropdown/search box will appear. Start typing the name of your script, or browse the list, and select it.
    [Image: Select Script]
    * You can add multiple Script Assets to a single Script Component on an Entity. They will generally execute their lifecycle methods (like initialize, update) in the order they appear in the "Scripts" array, though dependencies are better managed using postInitialize or events.

## Organizing Scripts

As your project grows, so will your number of scripts. Good organization is crucial:

* **Folders:** Use folders within the Asset Panel to categorize and group your scripts. You can drag-and-drop scripts between folders as necessary.
* **Naming Conventions:** Stick to consistent and descriptive naming for your script files. This makes them easier to find and understand.

## Deleting Scripts

To delete a Script Asset:

1. Select the Script Asset in the Asset Panel.
2. Press the Delete key, or right-click and select Delete.
3. A confirmation dialog will appear. Click DELETE to confirm.

:::note

If the Script Asset is currently assigned to any Script Components on Entities in your scenes, deleting the asset will remove it from those components. You may want to delete those components if they are no longer required.

:::

:::warning

There is no undo for Asset Deletion so be cautious. Using [version control](../../editor/version-control/index.md) to regularly set checkpoints is highly recommended to recover accidentally deleted files.

:::

--------------------------------------------------------------------------------

## VS Code Extension

URL: https://developer.playcanvas.com/user-manual/editor/scripting/vscode-extension/

The PlayCanvas VS Code Extension provides a powerful, real-time editing environment for working with text-based assets from the PlayCanvas Editor. Designed for developers who prefer modern tooling such as IntelliSense, source control, GitHub Copilot, and AI-enabled workflows, the extension integrates VS Code directly with your PlayCanvas projects.

[Image: VS Code Extension Demo]

The extension is fully [open-source on GitHub](https://github.com/playcanvas/vscode-extension) and licensed under MIT.

## Features

* **Realtime Asset Syncing**  
  Changes made in VS Code are instantly reflected in the PlayCanvas Editor, without needing manual file uploads or page refreshes.

* **Live File Collaboration**  
  See other collaborators who are editing the same file to avoid conflicts and improve team coordination.

* **Full Script Type Checking**  
  Enjoy robust TypeScript-powered type checking, IntelliSense, and autocomplete for PlayCanvas script types.

* **Disk-Mapped File System**  
  Your PlayCanvas project structure is mirrored locally, enabling deep integration with external tools, including AI-assisted development workflows.

* **Enhanced Developer Experience**  
  Integrates tightly with VS Code’s full feature set: refactoring tools, Git integration, snippets, and extensions.

## Installation

1. Install [Visual Studio Code](https://code.visualstudio.com/download).  
2. Install the [PlayCanvas VS Code Extension](https://marketplace.visualstudio.com/items?itemName=playcanvas.playcanvas) from the VS Code Marketplace.  
3. Sign in with your PlayCanvas account when prompted.  
4. Open the Command Palette (`Ctrl`/`Cmd` + `P`) and run **“PlayCanvas: Open Project”** to link a PlayCanvas project.

### Supported Editors

| Editor  | Supported |
| ------- | --------- |
| VS Code | ✅        |
| Cursor  | ✅        |

## Using the Extension

* **Open Project**  
  Use the Command Palette and run **PlayCanvas: Open Project** to begin editing your project locally.

* **Edit**  
  Modify scripts, shaders, and other text assets using the complete set of VS Code editing capabilities.

* **Sync**  
  All saved changes automatically sync to your PlayCanvas project in real time.

* **Collaboration**  
  View other users editing the same file and pull the latest changes when needed.

Integrating VS Code with PlayCanvas provides a sophisticated environment tailored for advanced development workflows, giving developers the flexibility and tools needed to build complex and high-performance web-based applications.

--------------------------------------------------------------------------------

## Templates

URL: https://developer.playcanvas.com/user-manual/editor/templates/

Templates (or prefabs) allow you to speed up your development by creating Entities that are reusable. You can place multiple instances of a Template in your Scene and if you make any changes and apply them to the Template Asset, all instances of that Template will be updated.

[Interactive Demo]

## Creating Templates {#creating-templates}

To create a new Template Asset you can right-click on any Entity in your Scene and select Template &rarr; New Template. This will create a new Template Asset and add it to your currently selected folder in the Asset Panel. The Entity you right clicked will become an instance of that new Template Asset automatically.

## Adding Templates in your Scene {#adding-templates-in-your-scene}

You can drag & drop a Template Asset in your scene or right click under an Entity in the Hierarchy and select Template &rarr; Add Instance. Then you can select the Template Asset and that will add an instance of it under the clicked Entity.

When an Entity is an instance of a Template it will have a slightly difference appearance in the Hierarchy:

[Image: Template Instance]

The root Entity of the Template Instance and all its children will have a different icon showing that they are part of the same Template.

When you select the root of the Template Instance you will notice the following properties in the Entity Inspector:

[Image: Template Inspector]

## Updating Templates {#updating-templates}

In order to make changes to a Template Asset you first have to add an instance of it in the Editor. Then you can change the instance as you see fit and in the end apply the changes to the Template Asset.

### Template Overrides {#template-overrides}

When you make changes to a Template instance or its children this will generate Template Overrides. There are various types of overrides:

* *Field override*: An override where the value of a field of the Entity or its Components differs to the one in the Template Asset.
* *New Entity override*: An Entity that you have added as a child to the Template instance that does not exist in the Template Asset.
* *Deleted Entity override*: An child Entity that you have deleted from the Template instance.

When you apply overrides to the Template Asset then they stop being overrides and become part of the Template Asset.

You can see the summary of the overrides by selecting the root of the Template Instance and looking at the Entity Inspector:

[Image: Overrides]

Also notice the different color of the label of the field that has been overridden. You can also see a more detailed list of all the overrides by clicking View Diff. See [Overrides Diff View](/user-manual/editor/templates/diff) for more information.

If you hover over the colored label of the overridden field you can view more details about that specific override:

[Image: Override Hover]

### Applying Overrides {#applying-overrides}

To apply an override you can click APPLY in the tooltip shown when you hover over the overridden field.

To apply multiple overrides select the root of the Template Instance and click Apply All on the Entity Inspector. You can also right click on the root of the Template Instance and select Template &rarr; Apply To Template.

Alternatively you can open the [Override Diff View](/user-manual/editor/templates/diff) and apply overrides from there too.

Any overrides you apply to the Template Asset will propagate to other instances of the Template Asset in any scene that these might be.

:::note

You cannot currently undo the action of applying overrides to a Template Asset.

:::

### Reverting Overrides {#reverting-overrides}

To revert a specific override, click REVERT in the tooltip shown when you hover over the overridden field.

To revert all overrides select the root of the Template Instance and click Revert All on the Entity Inspector.

Alternatively you can open the [Override Diff View](/user-manual/editor/templates/diff) and revert overrides from there too.

### More details on Overrides {#more-details-on-overrides}

When you create an override then the property that it overrides gets protected from updates to the Template Asset. For example. say your Template Instance looks like so:

[Image: Template Instance]

You modify the position of Tree1/Sphere. This creates an override on the position of the Sphere entity. You then create another instance of the Template Asset and you change the position of the Sphere to something else. If you apply the override from the second instance then that will not update the position of the first instance since it is overridden and protected. In order for the first instance to pick up position changes for the Sphere Entity you have to revert the override first.

## Instantiating At Runtime {#instantiating-at-runtime}

You can instantiate Template Assets at runtime like so:

```javascript
const templateAsset = this.app.assets.get(templateAssetId);
const instance = templateAsset.resource.instantiate();
this.app.root.addChild(instance);
```

You can also use Script Attributes to pass Template Assets to your scripts instead of searching for them by ID.

## When do I need to load Template Assets? {#when-do-i-need-to-load-template-assets}

Templates store the Entity hierarchy and data, similar to scenes. When a template instance is added to the scene in the Editor, the Entity hierarchy and data is included in the Scene data.

At runtime, the Template instance is not linked to the asset and you can reduce the download size by not preloading/loading the asset.

You only need Template assets to be loaded if you are instantiating instances at runtime.

--------------------------------------------------------------------------------

## Override Diff View

URL: https://developer.playcanvas.com/user-manual/editor/templates/diff/

The Override Diff View shows a detailed list of all the Template Overrides for a Template Instance. In here you can compare the overridden value to the value in the Template Asset and you can select which overrides to apply or revert.

[Image: Diff View]

## Applying Overrides

Click on the dot next to each override to see a dropdown menu to apply the override. If you have nested Templates then you will have the option to apply the change to any parent Template.

## Reverting Overrides

Click on the dot next to each override and click Revert to revert the override.

--------------------------------------------------------------------------------

## Nested Templates

URL: https://developer.playcanvas.com/user-manual/editor/templates/nested/

PlayCanvas also supports Nested Templates. These are Templates that have instances of other Templates as children. For example imagine a Tree Template where each fruit is another Template.

This allows you to structure complex Template hierarchies with a lot of versatility avoiding copy pasting Entities.

## Nested Overrides

Let's use the following Template Instance as an example:

[Image: Nested Example]

In this example Tree is a Template that consists of Branches which are instances of the Branch Template. Each Branch Template consists of instances of the Apple Template.

Let's say we modify the position of the Tree/Branch 1. This will create an override on the Tree Template.

Now let's say we modify the position of Tree/Branch 1/Apple. This will create an override on the Tree Template and another override on Tree/Branch 1.

If you apply the override to the Branch Template then ALL branches everywhere will pick up the update. If you apply the override to the Tree Template then other Branch Templates will remain unmodified but all Tree Templates will pick up the change.

Overrides are always relative to the selected Template Instance. So if you apply the override to the Tree Template and then select Tree/Branch 1, you will see that the Branch still has the override for the Apple, because we have not applied it to the Branch Template.

--------------------------------------------------------------------------------

## Troubleshooting

URL: https://developer.playcanvas.com/user-manual/editor/troubleshooting/

This is a list compiled of all the most commonly asked questions and answers for the Editor powered by Engine V2 for projects running Engine V1 or V2.

## 1. My scene looks brighter/darker in the Editor when it was powered by Engine V1 vs V2

### Check Camera settings

- Check **gamma** and **tone mapping** for scene under **Settings -> Rendering**
- Check **gamma** and **tone mapping** for viewport under **Settings -> Editor**
- Check **gamma** and **tone mapping** for each `CameraComponent` (**Engine V2 PROJECT ONLY**)

#### Scene Settings

<img src='/img/user-manual/editor/editor-v2/settings-rendering.png' width='400px' />

#### Viewport Settings

<img src='/img/user-manual/editor/editor-v2/settings-editor.png' width='400px' />

#### Camera Settings

<img src='/img/user-manual/editor/editor-v2/camera-settings.png' width='400px' />

### Check Texture sRGB flags

- Check if you have any audit fixes **Status Bar -> N audits found**
  - Fixes can be applied automatically **Status Bar -> N audits found -> Fix Issues**
  - Conflicts have to be resolved case-by-case:
        1. Refer to **Console Output** for which textures/texture atlases are affected and where they are used
        2. Click each warning/error to jump to where the texture/sprite is used

#### Asset Auditor

<video autoPlay muted loop controls src='/video/asset-auditor.mp4' style={{width: '100%', height: 'auto'}} />

## 2. My camera makes objects look brighter/darker in the Editor compared to the Launcher

If the camera is created by a script, make sure the **gamma** and **tone mapping** settings are explicitly set on the camera component.

--------------------------------------------------------------------------------

## Version Control

URL: https://developer.playcanvas.com/user-manual/editor/version-control/

Once you've moved beyond the simplest of projects, you will find that version control becomes an important part of your application development process. Version Control is a catch-all term for a system that performs the following functions

- Allows you to turn back time to a previous version of your code and assets
- Allows you to see changes that have been made to a project over time
- Allows you to trial new changes in a safe place, isolated from other developers, and merge changes when ready

PlayCanvas has version control tools built directly into the Editor which can be broken down into three main features. **Checkpoints** are a snapshot of your project at a single point in time, they form a timeline of changes to your project; **Branches** are a single line of development perhaps representing the changes required to create one feature or by one developer. Changes to assets in one branch will not affect changes in another branch; **Merging & Resolving conflicts**, merging is the process of combining one branch into another branch, conflicts occur when both branches edit the same data. After resolving conflicts the destination branch should contain the changes from both branches.

## Version Control in PlayCanvas

You may be familiar with other version control systems (VCS), if so this summary will help you get familiar with how PlayCanvas works relative to other version control systems.

You can think of a checkpoint like you would a **commit** in a VCS like Git or Mercurial. Each checkpoint is a point-in-time snapshot of the project with an associated message that describes what changes were made in this checkpoint. While you are editing your project your current (un-checkpointed) changes are similar to the **working directory** i.e. you can think of these as your **local changes** (even though in PlayCanvas your local changes are shared with anyone else in the same branch as you).

You cannot delete a checkpoint, but you can restore a previous checkpoint. Restoring a checkpoint works a little like a `git checkout <commit>` or `hg update -r <commit>`. However, in PlayCanvas we don't allow branches unless they have been explicitly created (no detached heads or similar). When you restore a previous checkpoint future changes will be children of the latest checkpoint in the branch.

**Branches** in PlayCanvas work like branches in other systems. A branch forms an isolated line of development made up of checkpoints. All users who have set a branch as their current branch will see their changes in a real-time collaborative fashion. Branches cannot be deleted, but when you are finished working on a branch it can be *closed*.

Merging works in a similar way to other VCSs. However, in PlayCanvas it's important to note that merging occurs *between checkpoints* so any changes that you have not committed in a checkpoint will not be included in your merge. PlayCanvas automatically creates checkpoints in the destination branch of a merge to prevent you accidentally losing changes when merging.

More details can be found on the specific pages for [checkpoints](/user-manual/editor/version-control/checkpoints), [branches](/user-manual/editor/version-control/branches), and [merging](/user-manual/editor/version-control/merging).

## How Version Control Affects Your Storage

Using Version Control will use more storage for checkpoints and branches.

Every checkpoint created only stores the changes from the previous checkpoint. The amount of data used is dependent on the changes such as new assets, texture changes etc.

Creating a branch from a checkpoint makes a copy of the project state at that checkpoint. This can increase storage usage significantly depending on the project.

Unfortunately, it is not possible to delete branches or checkpoints except in these specific cases:

- [Hard reset to a checkpoint](/user-manual/editor/version-control/checkpoints/#hard-reset-to-a-checkpoint)
- [Deleting a branch](/user-manual/editor/version-control/branches/#deleting-a-branch)

If you need more storage space, please email us at [support@playcanvas.com](mailto:support@playcanvas.com). In most cases, we can increase your storage allowance with no extra charge.

--------------------------------------------------------------------------------

## Branch Workflows

URL: https://developer.playcanvas.com/user-manual/editor/version-control/branch-workflows/

There are many different ways that you can use branches to suit your project needs. Below we describe a few methods that are commonly used for different types of project.

## Feature-based branches

[Image: Feature branches]

With a feature-based workflow each feature you are developing is started by creating a new branch out of the main branch. Then development work for the feature is done in your feature branch. When your feature is complete you merge any new changes from the main branch back into your branch. Perform a final test to make sure changes from master haven't affected your feature and then merge your feature branch into the main branch.

## Release branches

If your production cycle features shipping numbered versions of your application, perhaps with extended periods of testing for each version, you might choose to use a release branch workflow.

[Image: Release branches]

With this workflow new features are merged into the main branch and each time you are ready to release a version you take a new branch named after the version you are releasing. A build is published from this release branch and any fixes needed for the release are added into the release branch. Once the release is ready to go, you can merge any fixes back into master and continue development on the next release.

## Continuous delivery

If your application is a long-lived product which will be continuously updated, for example, a new release every week, you may wish to use a continuous delivery workflow.

[Image: Continuous Delivery]

In a continuous delivery workflow rather than having branches for each release, several long-lived branches are used to prepare the application for release. For example, features are merged into the main branch and after every feature is merged the main branch is merged into a branch called "staging". A build is published from staging to a sample environment where testing can be performed. Any required fixes are made into master and then merged into staging again. When staging is deemed ready, it is merged into another branch called "prod" (production). A build is made from prod and this is published to the live environment.

--------------------------------------------------------------------------------

## Branches

URL: https://developer.playcanvas.com/user-manual/editor/version-control/branches/

A branch is an isolated line of development. Every checkpoint created belongs to a branch and a series of checkpoints in a branch can track the development of an application or a particular feature. A PlayCanvas project will always have at least one branch, the main branch, and will often have multiple branches. You can merge the changes from one branch into any other branch using the version control panel in the Editor.

## Main branch {#main-branch}

Every project has a branch called "main" which is always present and cannot be deleted. In most respects this branch is no different from any other branch. However, in some cases (for example, the REST API) the "main" branch will be used as a default if no other branch is specified. A common scenario is to treat the main branch as the current development state of your application; to use another branch for stable releases and still more branches for feature development. However, you should feel free to use or not use the main branch as best suits your needs.

## Current branch {#current-branch}

For each project you work on you will always have a single branch set to be your **current branch**. This is the branch that you are actively working on and whenever you open the editor or edit a code file your changes will apply to your current branch.

## Creating a new branch {#creating-a-new-branch}

[Image: Create branch]

To create a branch open the version control panel, select the checkpoint that you wish to start the branch from and choose the "New Branch" option in the checkpoint's drop-down menu.

[Image: New branch dialog]

You will be asked to name your branch. Try to give your branch a descriptive name like `fix-player-bug` or `refactor-sound-effects`. After creating the branch you will automatically be switched to the new branch you just created.

Branches created by you will automatically be favorited so you can quickly find them in the [branch filter](#filtering-branch-list) for favorites.

## Filtering branch list {#filtering-branch-list}

[Image: Branch filter]

You can filter branches between:

- Favorites - Branches that are favorited by you.
- Open - All branches that are open.
- Closed - All branches that are closed.

Favoriting a branch can be done with the following options:

### Drop down menu {#drop-down-menu}

[Image: Favorite a branch via menu]

### Selected branch button {#selected-branch-button}

[Image: Favorite a branch via button]

## Searching for a branch {#searching-for-a-branch}

[Image: Searching for a branch]

To help find a branch in the current list filter, you can use the search bar at the top.

## Switching to a branch {#switching-to-a-branch}

[Image: Switch branch]

To switch branch open the version control panel, select the branch you wish to switch to and choose the "Switch to this branch" option in the branch's drop-down menu.

The editor will reload with your current branch switched to the chosen branch.

## Closing a branch {#closing-a-branch}

[Image: Close branch]

If you have completed work on a branch, you can close it which will remove it from the open branches list.

To close a branch, open the version control panel, select the branch you wish to close and choose the "Close this branch" option in the branch's drop-down menu. Note that you cannot close your current branch or the main branch. Switch to a different branch first if you wish to close your current branch.

[Image: Close branch dialog]

You will be asked to confirm the closing of the branch and you have an option to create a checkpoint before closing. This is enabled by default. If you wish to discard these changes you can untick the option here.

:::warning

Unticking this checkbox will lose any work you have made in the branch since you last made a checkpoint.

:::

Closed branches can also be reopened at a later date.

## Deleting a branch {#deleting-a-branch}

Deletion of branches are only supported if the following conditions are met:

- The branch has not been merged into another branch
- No branches have been created from this branch

To delete a branch open the version control panel, select the branch you wish to delete and choose the "Delete this branch" option in the branch's drop-down menu.

[Image: Delete a branch]

You will be asked to confirm the deletion of the branch by typing the name of the branch in the dialog box.

**Note, deleted branches cannot be recovered after deletion! If in doubt, please close the branch instead.**

[Image: Delete a branch warning]

--------------------------------------------------------------------------------

## View Changes

URL: https://developer.playcanvas.com/user-manual/editor/version-control/changes/

It is possible to view the difference between the current state of the project and a checkpoint using the View Changes options. A useful feature to run before starting a merge or taking a checkpoint. Or just to check on the state of the project since you last logged in.

The first way of viewing changes it to select the view changes button in the checkpoints view. This will show you the changes made in your branch since the most recent checkpoint.

[Image: VC Panel]

The second way to view changes is in the drop-down menu for each checkpoint. This will show you the difference between your current state and the checkpoint that is selected. You can use this to view the difference between your branch and checkpoint in another branch.

[Image: View Changes]

Changes are views in a similar interface to the Conflict Manager used when merging except that this interface is only for viewing and cannot be edited.

[Image: View Changes Dialog]

--------------------------------------------------------------------------------

## Checkpoints

URL: https://developer.playcanvas.com/user-manual/editor/version-control/checkpoints/

A checkpoint is a snapshot of your project at a point in time. It contains the complete set of data for your project so that you can restore this state at any point in the future. Checkpoints are similar to *commits* in other version control systems. Checkpoints are identified by a unique id number and a description that you enter at the time you create the checkpoint.

[Image: Checkpoint]

A checkpoint is a permanent record of the state of your project and forms part of the graph that is used for branches and merging changes. As such, once created checkpoints cannot be deleted. This means that once you have committed your changes as part of a checkpoint they are safe forever in your project history.

## Creating a checkpoint {#creating-a-checkpoint}

Checkpoints are created from the Version Control panel.

[Image: VC Panel]

The New Checkpoint button opens the input form to create a checkpoint. You can also use the keyboard shortcut Ctrl+S (Cmd+S on OS X)

[Image: Create Checkpoint]

## Restoring a checkpoint {#restoring-a-checkpoint}

[Image: Restore Checkpoint]

If you'd like to restore the state of your project from a previous checkpoint you can do that from the version control panel. Open the panel, find the checkpoint that you'd like to restore to and choose "Restore checkpoint" from the checkpoint's drop down menu. The editor will reload the project at the checkpoint.

**Note, restoring a checkpoint brings the changes from the checkpoint into your current branch, but PlayCanvas does not allow branching unless you have explicitly created a branch. So if you restore a checkpoint and then create a new checkpoint it will be a child of the latest checkpoint in the branch.**

[Image: Restore checkpoint applied]

## Hard reset to a checkpoint {#hard-reset-to-a-checkpoint}

Hard reset allows you to delete all checkpoints after a selected checkpoint. This is useful if you need to 'undo' a merge (perhaps a branch was accidentally merged or the merge needs to be done differently).

Hard reset can only delete the checkpoints if the following conditions are met:

- No branches have been created from the checkpoints being deleted
- The checkpoints being deleted have not been created by a merge of branches

**Note, deleted checkpoints cannot be recovered.**

To hard reset, open the panel, find the checkpoint that you'd like to reset to and choose "Hard reset" from the checkpoint's drop down menu.

Type 'hard reset' in the text box to confirm that you wish to delete all checkpoints after the selected checkpoint.

The editor will reload the project at the checkpoint.

--------------------------------------------------------------------------------

## Graph View

URL: https://developer.playcanvas.com/user-manual/editor/version-control/graph-view/

## Overview

The Version Control Graph makes it easy to track progress and merge history of the branches of a PlayCanvas project.

While the Version Control Panel displays a simple list of the latest checkpoints of the selected branch, the Graph View shows a much larger (and expandable) view of the entire version control graph, including checkpoints, branches and merges:

## Launching the Graph View

To access the Graph View, either click the Graph button in the top menu of the Version Control panel (for the current Editor branch), or the 'Version Control Graph' entry in the branch menu dropdown.

## Graph Nodes

A graph node will contain the following information:

- The first (up to) two lines contain a (truncated) description of the checkpoint.
- The next line consists of the first four characters of the checkpoint GUID, its date and the name of the user who created it.
- The last line contains the name of the branch.

## Graph Node Context Menu

Clicking on a node will open up the context menu for some or all following options depending on the node and access permissions:

- **View Changes** - Launch a Diff View between this checkpoint and its immediate predecessor. ([More details](/user-manual/editor/version-control/changes/))

- **Select for Compare** - Select the first checkpoint of a pair to be compared.

- **Compare with Selected** - Launch a Diff View between the current node and a node marked via 'Select for Compare'.

- **New Branch** - Create a new branch from the selected checkpoint. ([More details](/user-manual/editor/version-control/branches/#creating-a-new-branch))

- **Copy Data** - Copies full (non-truncated) checkpoint data in JSON format to the clipboard such as the checkpoint GUID, the branch id, checkpoint message etc.

- **Restore** (only available for the current Editor branch) - Restore the branch state to that of the selected checkpoint. ([More details](/user-manual/editor/version-control/checkpoints/#restoring-a-checkpoint))

- **Hard Reset** (only available for the current Editor branch) - Perform a Hard Reset to the selected checkpoint. ([More details](/user-manual/editor/version-control/checkpoints/#restoring-a-checkpoint))

## Expandable Nodes

Initially the Graph View displays up to 20 checkpoints from each branch, and up to 60 checkpoints total.

A triangle in the top right corner of a node means that it has yet more incoming or outgoing edges (such as branches and merges), and can be expanded via the 'Expand Node' context menu entry.

## Closed Branches

The graph will also include checkpoints from closed branches so that they are able to always show both parents of a merge result checkpoint. They are marked with [x] after the branch name:

## Graph Node Layout Logic

To reduce the size of graph layout both vertically and horizontally and make it easier to navigate, the graph nodes are laid out in a specific way.

The branch that was selected when a Graph View button was clicked is shown on the left. All other branches are positioned to the right of it.

In general, each branch occupies its own column, but when possible, the graph places branches above one another, to prevent the graph nodes from being placed too far horizontally.

It also attempts to position parent checkpoints lower than their children, whenever possible.

At the same time it is trying to avoid large gaps between nodes in the branch originally selected for the Graph View, because it is more likely to need clear overview of all its checkpoints. This and similar considerations may sometimes lead to a parent checkpoint being shown higher than its child in another branch and an arrow pointing downwards.

The color of an edge helps to easily identify its direction, because it always matches the color of the source node.

The color of each branch remains the same whenever you open the Graph View and does not depend on which branch was selected to launch it.

The Graph View supports zooming in and out with the scroll wheel and panning via click and drag.

--------------------------------------------------------------------------------

## Item History

URL: https://developer.playcanvas.com/user-manual/editor/version-control/item-history/

Checkpoint history is available for every item in the project so you can easily view the changes in the Checkpoints.

Due to the extra information that needs to be stored per checkpoint to support this feature, only checkpoints made after Mar 2, 2022 can be seen with Item History.

See it in action below for all supported items. The `[=]` in the top left of the node means that a Checkpoint has been made but with no changes to the item.

More information on using the Graph View can be found [here](/user-manual/editor/version-control/graph-view/).

### Entity Item History

[Image: Entity Item History]

### Asset Item History

[Image: Asset Item History]

### Project Settings Item History

[Image: Project Settings Item History]

### Scene Item History

[Image: Scene Item History]

--------------------------------------------------------------------------------

## Merging and resolving conflicts

URL: https://developer.playcanvas.com/user-manual/editor/version-control/merging/

Merging is the process by which you combine work that has been performed in one branch with work that has been performed in another branch. Merging is a natural part of the branch based workflow described in the [branches documentation](/user-manual/editor/version-control/branches).

## Merging two branches

In PlayCanvas a merge takes two checkpoints from two different branches, calculates the changes that have occurred since their last shared ancestor checkpoint, combines these changes together (sometimes asking for help to resolve conflicting changes) and finally creates a new checkpoint containing the result of the merging of the two checkpoints.

First, it's important to note that when you perform a merge in PlayCanvas you are not actually merging two branches. You are, in fact, merging two checkpoints. This is relevant because you may have changes in one of the two branches that are more recent than the latest checkpoint. In the case of the branch you are merging into PlayCanvas will automatically create a checkpoint to ensure you do not lose any changes.

### Example merges

[Image: Merging checkpoints]
*Changes Y & Z are not included in merge result C and **are lost**.*

[Image: Merging checkpoints]
*By default PlayCanvas creates a new checkpoint in the destination branch of a merge, so changes Y are included*

[Image: Merging checkpoints]
*If changes Z are required, create a checkpoint in the source branch before starting the merge.*

## Starting a merge

[Image: Start merge]

To start a merge switch your current branch to the branch you wish to merge into. Then select the branch that you wish to merge from and choose "Merge into current branch" from the branch's drop-down menu.

[Image: Merge dialog]

You have a few options before starting the merge.

On the 'Merge from' branch, you can:

- **Create checkpoint first** - Tick this option to create a checkpoint before the merge. This is useful if there are changes in the branch that have not yet been checkpointed and you want to include them in the merge.
- **Close branch after merging** - Tick this option to close the 'Merge from' branch after the merge is completed. This is commonly used for feature branches where the feature is completed and to keep the branch list clean from stale/old branches.

On the 'Merge to' branch, you can:

- **Create checkpoint first** - This is ticked by default. As merges are based on checkpoints (i.e. it merges one checkpoint to another), this ensures that any changes on the 'Merge to' branch that aren't checkpointed, will be included in the merge. Unticking this option will discard any changes that are not checkpointed. It is recommended to not untick this option.

### Automatic Merging

When you merge two checkpoints PlayCanvas will try to automatically merge any changes from the two branches that don't conflict with each other. In many cases, two branches can be merged completely automatically. In an automatic merge once the merge is complete the Editor will reload with your merged changes in the current branch.

### Resolving Conflicts

Sometimes the two branches have conflicting changes which cannot be automatically merged. An example of a conflicting change is if both branches have changed the position on the same entity. It's not possible for the system to make a choice between the two final positions.

If one or more conflicts exist when merging two branches the Editor will show the conflict manager and you must resolve each conflict before the merge can be completed.

### Conflict Manager

[Image: Conflict Manager]

The conflict manager shows each resource that is in conflict on the left, and for each selected resource a comparison between the original version (the Base) and the version in each branch. For each conflicted property you can choose which version to accept. Once all properties for all resources have been resolved you click the "Complete Merge" button to finish the merge and create the new checkpoint.

[Image: Resolved Conflicts]

While the conflict manager and the merge is in progress your current branch is locked to further edits from other users. This prevents changes being overwritten by your merge when you finish resolving the conflicts. If another user has blocked a branch you need with a merge, you can forcibly end their merge from the editor.

--------------------------------------------------------------------------------

## PlayCanvas Engine

URL: https://developer.playcanvas.com/user-manual/engine/

The PlayCanvas Engine is the run-time framework that powers your PlayCanvas applications. It is written in JavaScript and is open sourced under an MIT license on [GitHub](https://github.com/playcanvas/engine). It is published on the [NPM registry](https://www.npmjs.com/package/playcanvas) and ships with full TypeScript declarations. It also comes with a comprehensive set of [code examples](https://playcanvas.github.io/).

As a developer, you have complete freedom as to how you use the Engine. The two options are:

1. Build applications against the Engine directly. [Learn](standalone) how to use the Engine standalone.
2. Build applications in the [Editor](../editor). In this case, consult the [Scripting](../scripting) section.

--------------------------------------------------------------------------------

## Engine Migrations

URL: https://developer.playcanvas.com/user-manual/engine/migrations/

The PlayCanvas Engine is continually evolving, and its updates occasionally introduce breaking changes that require users to adapt their scripts.

This guide provides an overview of all major breaking changes across releases, offering users a helpful resource for migrating their code to newer versions.

It’s advisable to use the debug version of the engine when troubleshooting issues, as it provides logs for deprecated messages, warnings, and errors related to incorrect usage.

## Migration from 2.4.0 to 2.5.0

### Breaking changes in 2.5.0

#### ShaderMaterial

`ShaderMaterial` previously streamlined gamma correction, tone mapping, and fog handling by automatically injecting the necessary code blocks into each fragment shader. This functionality has now been removed, making it the responsibility of individual shaders to include the relevant features manually.

If you encounter shader errors due to missing functions such as `gammaCorrectOutput`, `toneMap`, or `addFog`, ensure that you explicitly include the required functions in your shader. For more details, see [this update](https://github.com/playcanvas/engine/pull/7331).

## Migration from 1.75.0 to 2.4.0

The migration from major version 1 to major version 2 is a substantial update, introducing numerous breaking changes.

### Breaking changes

#### WebGL 1

Support for WebGL1 has been discontinued; the engine now exclusively supports WebGL2 and WebGPU (in beta). If your application relies on WebGL1, you will need to continue using version 1 of the engine.

#### AudioSourceComponent

The AudioSourceComponent, which was replaced by the SoundComponent some time ago, has now been completely removed.

#### Legacy Scripts

The legacy scripting system, deprecated since 2016 and maintained in a read-only state for several years, is now being removed entirely.

#### Deprecated functions

Numerous deprecated functions that provided backward compatibility have been removed. If your application displays deprecation warnings when using the debug version of Engine 1, these issues should be resolved before migrating to Engine 2. In Engine 2, deprecated warnings are no longer displayed, and the backward compatibility code has been eliminated.

#### Basic Material

The BasicMaterial has been removed. To achieve equivalent functionality, you can use a StandardMaterial with the emissive color or emissive map set as a replacement.

#### Shader Material

If you've received the error `Material class cannot be instantiated, use ShaderMaterial instead`, this indicates the need to modify your code to use [ShaderMaterial](/user-manual/graphics/shaders/) instead. Note that as the engine shaders now use [Linear Workflow](/user-manual/graphics/linear-workflow/), notice how `gammaCorrectOutput` is used to apply final gamma correction in the [fragment shader](/user-manual/graphics/shaders/#fragment-shader).

#### Rendering to a texture

When rendering to a texture, the deprecated method of configuring the RenderTarget on the Layer has been fully removed. The RenderTarget must now be set directly on the camera instead.

#### Engine rendering callbacks

The engine previously executed multiple callbacks per frame for camera and layer rendering. These have been replaced with an event-driven system that supports multiple subscribers. The new events are now emitted by the Scene class. For more details, refer to [this pull request](https://github.com/playcanvas/engine/pull/7156).

#### StandardMaterial tint flags

The tint options for StandardMaterial have been confusing and inconsistent, so we've removed the flags for Ambient, Diffuse and Emissive tint. Previously, these flags only affected certain cases, such as when a texture was applied. With this update, tint colors are now always applied. To disable tinting, set the color to a neutral value (the default tint colors used when creating a new material):

Here is the list of default colors per tint type:

* **Ambient**: `new Color(1, 1, 1)` (white)
* **Diffuse**: `new Color(1, 1, 1)` (white)
* **Emissive**: `new Color(0, 0, 0)` (black)

There is one somewhat confusing behavior to note. By default, the emissive tint is set to black to ensure the material doesn't emit any color. When you assign an emissive texture, it's important to set the emissive color to white; otherwise, the black tint will override the emissive contribution from the texture, resulting in no visible emission.

#### Black movie texture

If your movie texture appears black, check the warnings in the debug version of the engine. It likely indicates that the material’s tint emissive color is set to black, causing the movie to render as black. To fix this, change the emissive color to white.

#### Gamma-correction, tone-mapping and fog settings

Previously, the `gammaCorrection` and `toneMapping` settings were applied globally on the `Scene` and affected all cameras' rendering. Now, these settings are available directly on each camera, allowing for unique configurations and rendering for each individual camera.

Previously, fog settings were accessed directly on the `Scene`, such as `Scene.fog`, `Scene.fogColor`, and `Scene.fogEnd`. These settings have now been moved under the `Scene.fog` property, and can be set using `Scene.fog.type`, `Scene.fog.color`, `Scene.fog.end`, and similar properties.

For more details, refer to [this pull request](https://github.com/playcanvas/engine/pull/7101).

#### Gamma space textures

Textures that represent colors, such as Diffuse, Emissive, Specular, and Sheen, are typically stored in sRGB space to maintain color range and reduce banding. When used by the engine, these textures are converted from sRGB to linear space for accurate lighting calculations. Previously, this conversion was handled by shader math, which impacted performance. With the removal of WebGL1 support, we can now leverage hardware to perform this conversion efficiently at no extra cost. The only requirement is that the texture must be created using sRGB format:

* When loading a Texture asset that represents colors in sRGB space, it's important to specify an sRGB encoding. For details, see [this pull request](https://github.com/playcanvas/engine/pull/6739).
* When creating a Texture instance that represents color in sRGB space, it is essential to use an sRGB pixel format, such as `PIXELFORMAT_SRGBA8`.

#### Instancing

If your code includes customizations to the instancing section of the `transformVS` chunk, you’ll need to update it by moving these customizations to the `transformInstancingVS` chunk. Additionally, configure the material to specify which attributes are in use. For further details, refer to [this pull request](https://github.com/playcanvas/engine/pull/6867).

### Other changes

For detailed information on the changes, refer to the release notes for each individual engine version:

* [2.0.0](https://github.com/playcanvas/engine/releases/tag/v2.0.0)
* [2.1.0](https://github.com/playcanvas/engine/releases/tag/v2.1.0)
* [2.2.0](https://github.com/playcanvas/engine/releases/tag/v2.2.0)
* [2.3.0](https://github.com/playcanvas/engine/releases/tag/v2.3.0)
* [2.4.0](https://github.com/playcanvas/engine/releases/tag/v2.4.0)

--------------------------------------------------------------------------------

## Running the Engine in Node.js

URL: https://developer.playcanvas.com/user-manual/engine/running-in-node/

[Node.js](https://nodejs.org/) is a cross-platform, open-source JavaScript runtime environment that can run on Windows, Linux, macOS, and more. It runs on the V8 JavaScript engine, and executes JavaScript code outside a web browser. Node.js lets developers use JavaScript to write command line tools and for server-side scripting.

The PlayCanvas Engine fully supports running in Node.js. This can be useful for:

* creating multiplayer servers
* creating tools for processing asset data
* writing unit tests for your application

:::note

The PlayCanvas Engine runs its own [unit tests](https://github.com/playcanvas/engine/blob/main/test/README.md) using Node.js.

:::

## Installation

Before you begin, verify you have Node.js 18+ installed. Then you can install the PlayCanvas Engine and `jsdom` using npm.

```bash
npm install jsdom playcanvas --save-dev
```

:::info

The `jsdom` package is used to simulate a DOM environment in Node.js. This is required because the PlayCanvas Engine uses the DOM API in a number of places.

:::

## Configuring jsdom

Let's create a function that uses `jsdom` to configure the DOM environment so that the PlayCanvas Engine can run successfully.

```javascript

let jsdom;

    const html = '<!DOCTYPE html><html><head></head><body></body></html>';

    jsdom = new JSDOM(html, {
        resources: 'usable',         // Allow the engine to load assets
        runScripts: 'dangerously',   // Allow the engine to run scripts
        url: 'http://localhost:3000' // Set the URL of the document
    });

    // Copy the window and document to global scope
    global.window = jsdom.window;
    global.document = jsdom.window.document;

    // Copy the DOM APIs used by the engine to global scope
    global.ArrayBuffer = jsdom.window.ArrayBuffer;
    global.Audio = jsdom.window.Audio;
    global.DataView = jsdom.window.DataView;
    global.Image = jsdom.window.Image;
    global.KeyboardEvent = jsdom.window.KeyboardEvent;
    global.MouseEvent = jsdom.window.MouseEvent;
    global.XMLHttpRequest = jsdom.window.XMLHttpRequest;

    // Copy the PlayCanvas API to global scope (only required for 'classic' scripts)
    jsdom.window.pc = pc;
}
```

Once you have called `jsdomSetup()`, you can create your PlayCanvas application as normal.

## Creating a PlayCanvas Application

When running a PlayCanvas application in Node.js, you are unlikely to require rendering. In this case, you can create a Null graphics device which will not output any graphics.

```javascript

    const canvas = document.createElement('canvas');
    const graphicsDevice = new NullGraphicsDevice(canvas);
    return new Application(canvas, { graphicsDevice });
}
```

--------------------------------------------------------------------------------

## Using the Engine Standalone

URL: https://developer.playcanvas.com/user-manual/engine/standalone/

It is possible to build applications on the PlayCanvas Engine without using the Editor. Some examples of applications built directly against the Engine are:

* [glTF Viewer](https://playcanvas.com/viewer) \[[GitHub](https://github.com/playcanvas/model-viewer)\]
* [SuperSplat](https://playcanvas.com/supersplat/editor) \[[GitHub](https://github.com/playcanvas/supersplat)\]
* ...and, of course, the [PlayCanvas Editor](../../editor) itself!

This page guides you in how to get started.

:::note

Before you begin, ensure you have [Node.js](https://nodejs.org/) installed.

:::

When setting up your project, there are two main options to consider.

## Option 1: Build Tool and NPM

This is the recommended set up that should suit most developers.

A build tool can bundle your application into an optimized package that can run on a wide range of browsers. There are many build tools such as [webpack](https://webpack.js.org/), [Rollup](https://rollupjs.org/) and [esbuild](https://esbuild.github.io/), and PlayCanvas will work with all of them. Here, we will use [Vite](https://vitejs.dev/), a popular build tool that aims to provide a faster and leaner development experience for modern web projects.

First, select whether you prefer to develop in JavaScript or TypeScript:

## Option 2: Import Map and CDN

An [import map](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/script/type/importmap) can resolve module specifiers in JavaScript modules. Consider this import statement:

```javascript

```

An import map can resolve `playcanvas` to a CDN-hosted build of the engine that can be dynamically loaded by the browser. This means that we can skip the build step described in Option 1.

First, select whether you prefer to develop in JavaScript or TypeScript:

--------------------------------------------------------------------------------

## Supported Browsers

URL: https://developer.playcanvas.com/user-manual/engine/supported-browsers/

The PlayCanvas Engine requires a browser with [WebGL 2.0](https://en.wikipedia.org/wiki/WebGL#WebGL_2) support. The minimum browser versions are:

| Browser                                     | Version | Win | macOS | Linux | Chrome OS | Android | iOS |
| ------------------------------------------- | ------- | --- | ----- | ----- | --------- | ------- | --- |
| [Chrome](https://www.google.com/chrome/)    | 56+     | ✔️  | ✔️    | ✔️    | ✔️        | ✔️      | ✔️  |
| [Safari](https://www.apple.com/safari/)     | 15+     |     | ✔️    |       |           |         | ✔️  |
| [Firefox](https://www.mozilla.org/firefox/) | 51+     | ✔️  | ✔️    | ✔️    |           | ✔️      | ✔️  |
| [Edge](https://www.microsoft.com/edge)      | 79+     | ✔️  | ✔️    | ✔️    |           | ✔️      | ✔️  |
| [Opera](https://www.opera.com/)             | 43+     | ✔️  | ✔️    | ✔️    |           | ✔️      |     |

:::tip

We recommend using an up-to-date browser for the best performance and access to newer features like [WebGPU](https://en.wikipedia.org/wiki/WebGPU).

:::

## Checking WebGL 2.0 Support

To verify your browser supports WebGL 2.0, visit [webglreport.com](https://webglreport.com/?v=2). 🎉

## Build Target

The PlayCanvas Engine is built targeting ES2020 JavaScript, which is supported by all browsers that support WebGL 2.0.

--------------------------------------------------------------------------------

## Gaussian Splatting

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/

3D Gaussian Splatting is a revolutionary technique for capturing, representing, and rendering photorealistic 3D scenes. Unlike traditional polygonal meshes, Gaussian Splatting uses millions of small, semi-transparent elliptical splats to reconstruct detailed environments with exceptional visual fidelity.

[Interactive Demo]

## What Makes Gaussian Splatting Special?

Gaussian Splatting excels at capturing real-world environments through photogrammetry, making it incredibly quick and affordable to generate high-quality 3D content. The technique is particularly powerful for:

- **Photorealistic environments** - Capture real locations with stunning visual detail
- **Rapid content creation** - Generate complex 3D scenes from simple photo/video capture
- **Volumetric representation** - Handle translucent materials, fine details, and complex lighting naturally
- **Real-time rendering** - Optimized for interactive frame rates in web browsers

## PlayCanvas Gaussian Splatting Workflow

PlayCanvas provides a complete ecosystem for working with Gaussian Splats:

1. **[Creating Splats](creating)** - Methods for creating your own splat data
2. **[Viewing Splats](viewing)** - Preview and evaluate splats using the PlayCanvas Model Viewer
3. **[Editing Splats](editing)** - Clean up and prepare splats for optimal rendering
4. **[Building Splat-based Apps](building)** - Integrate splats into your PlayCanvas projects

--------------------------------------------------------------------------------

## Building Splat-based Apps

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/building/

This section shows how you can build Gaussian splat-based apps using PlayCanvas.

--------------------------------------------------------------------------------

## Engine Features

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/building/engine-features/

The PlayCanvas Engine includes a powerful, high-performance renderer for Gaussian Splats. This section covers a number of topics related to the capabilities of this renderer.

## The GSplatComponent

The [GSplatComponent](https://api.playcanvas.com/engine/classes/GSplatComponent.html) is what grants PlayCanvas apps the ability to render 3D Gaussian Splats. To use it:

1. Create an Entity
2. Add a GSplatComponent to it
3. Set a GSplat Asset on the component

--------------------------------------------------------------------------------

## Custom Shaders

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/building/engine-features/custom-shaders/

The PlayCanvas Engine supports custom shaders for Gaussian Splats, allowing you to create advanced visual effects and customize the rendering behavior beyond the standard implementation.

## Introduction

There are two ways to customize Gaussian Splat rendering with shaders:

1. **Shader Chunk Customization (Recommended)** - Override the `gsplatModifyVS` shader chunk to customize splat position, size, and color. This allows you to override only the relevant parts of the shader while leaving the core shader functionality intact.

2. **Full Shader Replacement** - Replace the entire vertex and fragment shaders for complete control. This provides maximum flexibility but requires understanding the full shader implementation.

Most use cases can be accomplished with shader chunk customization, which is covered in detail below.

**[View Live Example](https://playcanvas.vercel.app/#/gaussian-splatting/multi-splat)** - See shader chunk customization in action with animated splats.

## API Reference

The `gsplatModifyVS` shader chunk allows you to override three functions that customize how splats are rendered:

### modifySplatCenter

Transform the position of splat centers in model space.

**GLSL:**

```glsl
void modifySplatCenter(inout vec3 center)
```

**WGSL:**

```wgsl
fn modifySplatCenter(center: ptr<function, vec3f>)
```

**Parameters:**

- `center` - The splat center position in model space

**Example:**

```glsl
// Offset all splats up by 1 unit
void modifySplatCenter(inout vec3 center) {
    center.y += 1.0;
}
```

### modifySplatRotationScale

Modify the splat size and shape by adjusting the rotation quaternion and scale vector. This is more efficient than working with covariance matrices directly.

**GLSL:**

```glsl
void modifySplatRotationScale(vec3 originalCenter, vec3 modifiedCenter, inout vec4 rotation, inout vec3 scale)
```

**WGSL:**

```wgsl
fn modifySplatRotationScale(originalCenter: vec3f, modifiedCenter: vec3f, rotation: ptr<function, vec4f>, scale: ptr<function, vec3f>)
```

**Parameters:**

- `originalCenter` - The original splat center position before modification
- `modifiedCenter` - The splat center position after `modifySplatCenter()` was applied
- `rotation` - Quaternion (x, y, z, w) representing the splat's rotation
- `scale` - Scale vector representing the splat's size in each axis

**Example:**

```glsl
// Scale all splats by 2x
void modifySplatRotationScale(vec3 originalCenter, vec3 modifiedCenter, inout vec4 rotation, inout vec3 scale) {
    scale *= 2.0;
}
```

### modifySplatColor

Transform splat colors and opacity.

**GLSL:**

```glsl
void modifySplatColor(vec3 center, inout vec4 color)
```

**WGSL:**

```wgsl
fn modifySplatColor(center: vec3f, color: ptr<function, vec4f>)
```

**Parameters:**

- `center` - The splat center position (after `modifySplatCenter()` was applied)
- `color` - The splat color (RGBA)

**Example:**

```glsl
// Darken all splats by 50%
void modifySplatColor(vec3 center, inout vec4 color) {
    color.rgb *= 0.5;
}
```

## Usage Examples

### Basic Setup

To apply a custom shader chunk to a Gaussian Splat material:

```javascript
// Get the shader language for the current device
const shaderLanguage = device.isWebGPU ? 'wgsl' : 'glsl';

// Define your custom shader code
const customShader = `
    // Your shader functions here
`;

// Set the custom shader chunk override on the gsplat material
gsplatMaterial.getShaderChunks(shaderLanguage).set('gsplatModifyVS', customShader);

// Update the material to recompile with the new shader
gsplatMaterial.update();
```

### GLSL Example: Position and Color Animation

```javascript
const customShader = `
uniform float uTime;

void modifySplatCenter(inout vec3 center) {
    // Create a wave effect based on height
    float heightIntensity = center.y * 0.2;
    center.x += sin(uTime * 5.0 + center.y) * 0.3 * heightIntensity;
}

void modifySplatRotationScale(vec3 originalCenter, vec3 modifiedCenter, inout vec4 rotation, inout vec3 scale) {
    // No modification to size
}

void modifySplatColor(vec3 center, inout vec4 color) {
    // Add a golden tint to the wave peaks
    float sineValue = abs(sin(uTime * 5.0 + center.y));
    vec3 gold = vec3(1.0, 0.85, 0.0);
    float blend = smoothstep(0.9, 1.0, sineValue);
    color.rgb = mix(color.rgb, gold, blend);
}
`;

// Set the custom shader chunk override on the gsplat material
const shaderLanguage = app.graphicsDevice.isWebGPU ? 'wgsl' : 'glsl';
gsplatMaterial.getShaderChunks(shaderLanguage).set('gsplatModifyVS', customShader);
gsplatMaterial.update();

// Update the uniform each frame
const uTime = app.graphicsDevice.scope.resolve('uTime');
let time = 0;
app.on('update', (dt) => {
    time += dt;
    uTime.setValue(time);
});
```

### WGSL Example: Position and Color Animation

```javascript
const customShader = `
uniform uTime: f32;

fn modifySplatCenter(center: ptr<function, vec3f>) {
    // Create a wave effect based on height
    let heightIntensity = (*center).y * 0.2;
    (*center).x += sin(uniform.uTime * 5.0 + (*center).y) * 0.3 * heightIntensity;
}

fn modifySplatRotationScale(originalCenter: vec3f, modifiedCenter: vec3f, rotation: ptr<function, vec4f>, scale: ptr<function, vec3f>) {
    // No modification to size
}

fn modifySplatColor(center: vec3f, color: ptr<function, vec4f>) {
    // Add a golden tint to the wave peaks
    let sineValue = abs(sin(uniform.uTime * 5.0 + center.y));
    let gold = vec3f(1.0, 0.85, 0.0);
    let blend = smoothstep(0.9, 1.0, sineValue);
    (*color) = vec4f(mix((*color).rgb, gold, blend), (*color).a);
}
`;

// Set the custom shader chunk override on the gsplat material
const shaderLanguage = app.graphicsDevice.isWebGPU ? 'wgsl' : 'glsl';
gsplatMaterial.getShaderChunks(shaderLanguage).set('gsplatModifyVS', customShader);
gsplatMaterial.update();
```

### Removing Custom Shaders

To remove a custom shader and revert to default rendering:

```javascript
// Remove the custom shader chunk override from the gsplat material
const shaderLanguage = app.graphicsDevice.isWebGPU ? 'wgsl' : 'glsl';
gsplatMaterial.getShaderChunks(shaderLanguage).delete('gsplatModifyVS');
gsplatMaterial.update();
```

## Helper Functions

The following helper functions are available in `modifySplatRotationScale()` for manipulating splat size and shape:

### gsplatGetSizeFromScale

Extract the current size of a splat from its scale vector.

**GLSL:**

```glsl
float gsplatGetSizeFromScale(vec3 scale)
```

**WGSL:**

```wgsl
fn gsplatGetSizeFromScale(scale: vec3f) -> f32
```

**Example:**

```glsl
// Clamp splat size to a specific range
float size = gsplatGetSizeFromScale(scale);
float newSize = clamp(size, 0.01, 0.5);
scale *= newSize / size;
```

### gsplatMakeSpherical

Make splats spherical with a specific radius.

**GLSL:**

```glsl
void gsplatMakeSpherical(inout vec3 scale, float radius)
```

**WGSL:**

```wgsl
fn gsplatMakeSpherical(scale: ptr<function, vec3f>, radius: f32)
```

**Example:**

```glsl
// Make all splats perfectly spherical with uniform size
float size = gsplatGetSizeFromScale(scale);
gsplatMakeSpherical(scale, size * 0.5);

// Or hide a splat by setting scale to zero
scale = vec3(0.0);
```

### Direct Scale Manipulation

Since the new API provides direct access to the scale vector, you can easily modify splat sizes:

```glsl
// Double the size of all splats
scale *= 2.0;

// Scale non-uniformly
scale.x *= 2.0;  // Stretch horizontally

// Hide a splat
scale = vec3(0.0);
```

## Examples

Here are some examples demonstrating different custom shader techniques:

### Animation Effects

[**Simple Sinusoidal Animation**](https://playcanvas.github.io/#/gaussian-splatting/multi-splat) - **Uses Shader Chunk Customization** - Applies a simple shader to animate Gaussian color and position using a sine wave. This example demonstrates how to create dynamic, procedural motion effects by modifying splat properties in real-time.

### Transition Effects

[**3D Gaussian Splat Statues**](https://playcanvas.com/project/1224723/overview/3d-gaussian-splat-statues) - **Uses Full Shader Replacement** - Uses custom shaders to transition splats on and off screen via a hot, plasma-type effect. This showcases how custom shaders can create dramatic visual transitions and material effects.

### Lighting and Relighting

[**3DGS with Physics and Relighting**](https://playcanvas.com/project/1358087/overview/3dgs-with-physics-and-relighting) - **Uses Full Shader Replacement** - Uses custom shaders to relight a splat to implement a night mode with multiple moving point light sources. This example demonstrates advanced lighting techniques and how to dynamically modify splat appearance based on scene lighting conditions.

--------------------------------------------------------------------------------

## Draw Order and Sorting

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/building/engine-features/draw-order/

## How Gaussians Are Sorted

Individual Gaussians within a GSplatComponent are sorted back to front based on camera depth. This sorting happens asynchronously in a Web Worker to avoid blocking the main thread. The calculated sort order is then passed back to the main thread and uploaded to the GPU for rendering.

:::note

Because the sorting occurs asynchronously, you may notice some visual lag if the camera transform changes significantly over a very short time period, as the Web Worker needs time to recalculate the new sort order.

:::

## Multiple GSplatComponents

GSplatComponents are rendered back to front based on their bounding boxes, and each component's Gaussians are sorted independently within that component.

:::info Global sorting

By default, the PlayCanvas Engine does not support "global sorting" across multiple GSplatComponents (where all Gaussians from all components would be sorted together). However, you can enable [Global Sorting](/user-manual/gaussian-splatting/building/engine-features/global-sorting), a beta feature that allows all Gaussians from multiple components to be sorted together, eliminating visibility and popping artifacts.

:::

## Depth Buffer Considerations

GSplatComponents do not write to the depth buffer during rendering. This limitation means you cannot use functionality that relies on reading back or leveraging depth buffer data in your application. For example, a Depth of Field post effect, which typically requires the depth buffer, would not generally be compatible with splat rendering.

As a workaround, you can generate a mesh-based approximation of your splat and render it to the depth buffer in a separate layer (without writing to the framebuffer). This technique allows depth-dependent effects while maintaining the visual quality of the Gaussian splat rendering.

--------------------------------------------------------------------------------

## Global Sorting

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/building/engine-features/global-sorting/

Global Sorting is a rendering feature that allows multiple Gaussian splat components in a scene to be sorted together, eliminating visibility and popping artifacts when splat components overlap.

:::info Beta Feature

Global Sorting is currently in beta. If you encounter any issues, please report them on the [PlayCanvas Engine GitHub repository](https://github.com/playcanvas/engine/issues).

:::

## The Problem

Without global sorting, multiple GSplatComponents are rendered independently. Each component's Gaussians are sorted separately, and the components themselves are rendered based on their bounding boxes. This approach can lead to:

- **Visibility artifacts** when splat components overlap
- **Popping effects** as the camera moves and component render order changes
- **Incorrect depth sorting** between Gaussians from different components

## The Solution: Global Sorting

Global sorting solves these issues by sorting all Gaussians from all splat components together in a single unified sort. This ensures correct rendering order across the entire scene, regardless of how many splat components you have or how they overlap.

## Enabling Global Sorting

To enable global sorting, set the [`unified`](https://api.playcanvas.com/engine/classes/GSplatComponent.html#unified) property to `true` on your GSplat components:

```javascript
entity.gsplat.unified = true;
```

:::note

The `unified` property can only be changed when the component is disabled.

:::

## Live Example

Check out the [Global Sorting example](https://playcanvas.vercel.app/#/gaussian-splatting/global-sorting) which demonstrates the difference between global sorting enabled and disabled. The example allows you to toggle unified mode on and off to observe how global sorting eliminates artifacts when rendering multiple overlapping splat components.

## Benefits

- **Improved Visual Quality**: Eliminates artifacts when rendering multiple overlapping splat components
- **Consistent Rendering**: Maintains correct depth sorting regardless of camera position
- **Better Scene Composition**: Enables complex scenes with many splat components

## See Also

- [GSplatComponent API](https://api.playcanvas.com/engine/classes/GSplatComponent.html)
- [Draw Order and Sorting](/user-manual/gaussian-splatting/building/engine-features/draw-order)
- [LOD Streaming](/user-manual/gaussian-splatting/building/engine-features/lod-streaming)
- [Global Sorting Example](https://playcanvas.vercel.app/#/gaussian-splatting/global-sorting)

--------------------------------------------------------------------------------

## LOD Streaming

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/building/engine-features/lod-streaming/

LOD (Level of Detail) Streaming enables efficient rendering of large Gaussian splat scenes by dynamically loading appropriate levels of detail based on the camera's distance. This feature dramatically reduces memory usage and improves rendering performance for large-scale splat scenes.

:::info Beta Feature

LOD Streaming is currently in beta. If you encounter any issues, please report them on the [PlayCanvas Engine GitHub repository](https://github.com/playcanvas/engine/issues).

:::

## How It Works

LOD streaming works by:

1. Pre-generating multiple versions of your splat at different detail levels
2. Organizing them into an octree structure for efficient streaming
3. Dynamically loading and unloading detail levels based on camera distance
4. Rendering only the appropriate level of detail for each region of the scene

This approach allows you to render massive splat scenes that would otherwise be impossible due to memory constraints.

## Enabling LOD Streaming

To enable LOD streaming, set the [`unified`](https://api.playcanvas.com/engine/classes/GSplatComponent.html#unified) property to `true` on your GSplat component and load a streaming LOD format asset:

```javascript
entity.gsplat.unified = true;
```

:::note

The `unified` property can only be changed when the component is disabled.

:::

## Creating LOD Streaming Data

To use LOD streaming, you need to generate the streaming format from multiple splat files with different levels of detail. The tool takes your pre-generated LOD files and creates an optimized streaming format.

See the [Generating LOD Format](/user-manual/gaussian-splatting/editing/splat-transform#generating-lod-format) section in the SplatTransform documentation for detailed instructions on how to create the required `lod-meta.json` format.

:::tip

You must create the different LOD levels yourself (LOD 0 = highest detail, higher numbers = lower detail). The tool organizes these into a streaming-optimized format but doesn't create the simplified versions.

:::

## Live Examples

Explore these live examples to see LOD streaming in action:

- [LOD Streaming (Basic)](https://playcanvas.vercel.app/#/gaussian-splatting/lod-streaming) - Demonstrates basic LOD streaming with different detail levels
- [LOD Streaming with Spherical Harmonics](https://playcanvas.vercel.app/#/gaussian-splatting/lod-streaming-sh) - Shows LOD streaming with spherical harmonic data

## Controlling LOD Behavior

You can control and fine-tune LOD streaming using the following APIs:

### Component-Level Control

Use the [`lodDistances`](https://api.playcanvas.com/engine/classes/GSplatComponent.html#loddistances) property to set the distance thresholds for switching between LOD levels:

```javascript
// Set LOD distance thresholds (in world units)
entity.gsplat.lodDistances = [10, 20, 40, 80];
```

### Scene-Level Control

The [`Scene.gsplat`](https://api.playcanvas.com/engine/classes/Scene.html#gsplat) property provides access to scene-wide settings for unified gsplat rendering. This includes options for:

- Performance tuning parameters
- Debug visualization settings
- Memory management controls
- Stream loading behavior

```javascript
// Access scene-level gsplat settings
const gsplatSettings = app.scene.gsplat;

// Configure settings as needed
// (See API documentation for available properties)
```

## Benefits

- **Better Performance**: LOD streaming reduces memory usage and improves rendering performance for large scenes
- **Scalability**: Enables rendering of much larger Gaussian splat scenes by dynamically loading appropriate detail levels
- **Flexibility**: Provides fine-grained control over LOD distances and streaming behavior
- **Optimized Loading**: Only loads the data needed for the current view

## See Also

- [GSplatComponent API](https://api.playcanvas.com/engine/classes/GSplatComponent.html)
- [Scene.gsplat API](https://api.playcanvas.com/engine/classes/Scene.html#gsplat)
- [SplatTransform CLI Tool](/user-manual/gaussian-splatting/editing/splat-transform)
- [Generating LOD Format](/user-manual/gaussian-splatting/editing/splat-transform#generating-lod-format)
- [Streaming LOD using Editor](/user-manual/gaussian-splatting/building/streaming-lod-editor)
- [Global Sorting](/user-manual/gaussian-splatting/building/engine-features/global-sorting)

--------------------------------------------------------------------------------

## Performance

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/building/engine-features/performance/

Rendering splats can be expensive on both the CPU and GPU. Here are some strategies to achieve good performance:

## Limit Gaussian Count

Be mindful of the number of Gaussians in your scene since every Gaussian is sorted on camera depth every frame. You can check the number contained within a particular GSplat asset by using the [SPLAT DATA Panel](https://github.com/playcanvas/supersplat/wiki/Inspecting-Splat-Data) in the [SuperSplat Editor](https://superspl.at/editor). Use SuperSplat to trim unwanted Gaussians from your PLY files.

## Fill Rate Considerations

3D Gaussian Splatting is particularly expensive in terms of fill rate (fragment operations). This is because:

- **High Overdraw**: Each Gaussian splat is rendered as a textured billboard (quad) that often overlaps with many other splats
- **Transparency Blending**: Splats use alpha blending to achieve smooth appearance, requiring expensive per-fragment blending operations
- **Fragment Density**: Dense splat clouds can result in dozens or even hundreds of fragments being processed for each final pixel

This high fragment cost is why optimizing pixel count and rendering settings is crucial for 3DGS performance.

### Configure Scene Settings

Given the fragment-heavy nature of Gaussian splatting, these settings have a significant impact on performance:

- **Disable `Anti-Alias`**: Anti-aliasing multiplies the number of fragments processed per pixel, which is especially costly for splat rendering
- **Disable `Device Pixel Ratio`**: This reduces the overall pixel resolution, directly reducing the number of fragments that need to be processed

Both settings help reduce the fragment processing load, which is the primary bottleneck in 3DGS rendering.

--------------------------------------------------------------------------------

## Picking

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/building/engine-features/picking/

The PlayCanvas Engine provides a [Picker API](https://api.playcanvas.com/engine/classes/Picker.html) which can query the mesh instance rendered at a specified pixel. The picker works with splats in the same way that it does for meshes.

Check out the [Engine Splat Picking Example](https://playcanvas.github.io/#/gaussian-splatting/picking) to see how it's done.

--------------------------------------------------------------------------------

## Shadows

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/building/engine-features/shadows/

Splats can cast shadows onto meshes (see the GSplatComponent's [castShadows](https://api.playcanvas.com/engine/classes/GSplatComponent.html#castshadows) property).

[Image: Splat Shadows]

View the [Splat Shadows Engine Example](https://playcanvas.github.io/#/gaussian-splatting/simple).

Splats cannot directly receive shadows (although you can fake it with an invisible mesh that approximates the splat that is written to the depth buffer).

--------------------------------------------------------------------------------

## Streaming LOD using Editor

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/building/streaming-lod-editor/

The PlayCanvas Engine supports [LOD Streaming](/user-manual/gaussian-splatting/building/engine-features/lod-streaming) for Gaussian splats. Native support for this feature in the PlayCanvas Editor will be added in the near future. In the meantime, you can use the Engine API in scripts to enable streaming LOD functionality in your Editor projects.

## Sample Project

We've created a sample project that demonstrates how to use streaming LOD with Gaussian splats in the PlayCanvas Editor:

**[Church of Saints Peter and Paul](https://playcanvas.com/project/1408991/overview/church-of-saints-peter-and-paul)**

This project showcases a large-scale Gaussian splat scene with LOD streaming, including custom reveal shader effects.

## Using the Streamed GSplat Script

The sample project includes a `streamed-gsplat.mjs` script that can be added to any Entity to enable LOD streaming:

### Setup Steps

1. Add the script to an Entity in your scene
2. Configure the `splatUrl` property to point to an externally hosted LOD splat format file

:::info Creating LOD Format

To generate the required LOD streaming format, see the [Generating LOD Format](/user-manual/gaussian-splatting/editing/splat-transform#generating-lod-format) section in the SplatTransform documentation.

:::

:::note External Hosting

Currently, the LOD splat data needs to be hosted externally (not as an Editor asset). This limitation will be removed in the future when native Editor support for streaming LOD format is added.

:::

### Quality Settings

The `streamed-gsplat.mjs` script provides four different quality/performance presets, allowing you to specify:

- Which LOD levels to load
- At what distances each LOD level should be displayed

These settings enable fine-tuned control over the balance between visual quality and rendering performance, making it easy to optimize for different target platforms and devices.

## Custom Shader Effects

The sample project also demonstrates how to create custom shader effects for Gaussian splats. It includes scripts from the [PlayCanvas Engine GSplat Scripts](https://github.com/playcanvas/engine/tree/main/scripts/esm/gsplat) repository.

### Reveal Effect Example

Specifically, the project uses the [Reveal Radial](https://github.com/playcanvas/engine/blob/main/scripts/esm/gsplat/reveal-radial.mjs) shader effect (along with its base class) to create an animated reveal of the splat scene. This effect:

- Creates radial waves emanating from a center point
- First shows small colored dots progressively
- Then lifts particles up with a highlight effect before settling to their original state

This demonstrates the flexibility of the PlayCanvas Engine's shader system for creating compelling visual effects with Gaussian splats.

## Future Improvements

As native Editor support for streaming LOD is added, the following improvements are planned:

- **Direct Asset Import**: Upload LOD splat files directly as Editor assets (no external hosting needed)
- **Visual Configuration**: Configure LOD settings through the Editor UI instead of script properties
- **Preview in Editor**: View and test streaming LOD behavior directly in the Editor viewport

Stay tuned for updates as these features are rolled out!

## See Also

- [LOD Streaming](/user-manual/gaussian-splatting/building/engine-features/lod-streaming)
- [Global Sorting](/user-manual/gaussian-splatting/building/engine-features/global-sorting)
- [SplatTransform CLI Tool](/user-manual/gaussian-splatting/editing/splat-transform)
- [Generating LOD Format](/user-manual/gaussian-splatting/editing/splat-transform#generating-lod-format)
- [Custom Shaders](/user-manual/gaussian-splatting/building/engine-features/custom-shaders)
- [Sample Project: Church of Saints Peter and Paul](https://playcanvas.com/project/1408991/overview/church-of-saints-peter-and-paul)

--------------------------------------------------------------------------------

## Your First Splat App

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/building/your-first-app/

Welcome to building your first Gaussian splat application! This section provides step-by-step tutorials to create a simple application that loads a splat of a cute toy cat that users can orbit around, pan, and zoom.

## Choose Your Path

We offer **four different approaches** to build the same app. Pick the one that matches your background and project needs:

### 🔧 [Using the Engine API](./engine)

**Best for:** Experienced JavaScript developers who want full control

- **What:** Direct PlayCanvas Engine API usage with ES6 modules
- **Files:** `index.html` + `main.js`
- **Pros:** Maximum flexibility, full engine access, smallest footprint
- **Cons:** More code to write, requires engine knowledge

### 🎨 [Using the Editor](./editor)

**Best for:** Designers, artists, and visual learners

- **What:** Point-and-click interface with Asset Store integration
- **Files:** PlayCanvas project (cloud-based)
- **Pros:** No coding required, visual tools, team collaboration, publishing features
- **Cons:** Requires PlayCanvas account, less programmatic control

### ⚛️ [Using PlayCanvas React](./react)

**Best for:** React developers and teams using React ecosystems

- **What:** JSX components that map to PlayCanvas entities and components
- **Files:** React component
- **Pros:** React integration, state management, TypeScript support, component composition
- **Cons:** Requires React knowledge and build setup

### 🌐 [Using Web Components](./web-components)

**Best for:** Web developers familiar with HTML and modern standards

- **What:** Declarative HTML using `<pc-app>`, `<pc-entity>`, `<pc-splat>` tags
- **Files:** Single HTML file
- **Pros:** No build tools needed, familiar HTML syntax, quick prototyping
- **Cons:** Limited to web component capabilities

--------------------------------------------------------------------------------

## Using the Editor

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/building/your-first-app/editor/

Let's build a simple Gaussian splat application step by step using the [PlayCanvas Editor](/user-manual/editor). We'll create a scene with an interactive 3D toy cat splat that you can rotate around.

:::tip

The project we will be building can be found [here](https://playcanvas.com/project/1372123/overview/my-first-splat-app). Instead of building it from scratch, feel free to simply [fork it](/user-manual/editor/projects/creating/#fork-an-existing-project) instead. Or use it as a reference if you get stuck while following the steps below.

:::

## Video Walk-Through

The complete process to build the app is in this short video:

<video autoPlay muted loop controls src='/video/my-first-splat-editor.mp4' style={{width: '100%', height: 'auto'}} />

Here are the actions demonstrated in the video as a set of steps you can follow:

## Creating a New Project

First, let's create a new PlayCanvas project:

1. Go to [playcanvas.com](https://playcanvas.com) and sign in to your account
2. Click **NEW** to create a new project
3. Select the **Blank Project** template
4. Enter a name like "My First Splat"
5. Click **CREATE** to create the project

PlayCanvas takes you to the new project's dashboard. Click the **EDITOR** button and the Editor opens with a blank scene.

## Preparing the Scene

Let's make some small modifications to the project to get started:

1. In the **HIERARCHY** panel, select and delete the **Light**, **Box** and **Plane** entities
2. Click the **Settings** icon (cog) in the Viewport
3. In the **INSPECTOR** panel, navigate to **SETTINGS → RENDERING**:
   * Unset the **Skybox** by clicking the **x** icon (this removes the default blue sky)
   * Uncheck both **Anti-Alias** and **Device Pixel Ratio**

:::info Performance Optimization

We uncheck **Anti-Alias** and **Device Pixel Ratio** to reduce the fragment processing load, which is the primary bottleneck in Gaussian splat rendering. This helps to achieve optimal splat rendering performance. Learn more in the [Performance](../engine-features/performance.md) guide.

:::

You should now see a clean dark gray viewport with just the Camera entity remaining in the hierarchy.

## Uploading the Splat Asset

First, download the toy cat splat to your local file system: [`https://developer.playcanvas.com/assets/toy-cat.sog`](https://developer.playcanvas.com/assets/toy-cat.sog)

Now let's add the downloaded splat to the project:

1. In the **ASSETS** panel (bottom of the screen), click the **+** icon
2. Select **Upload** from the popup menu
3. In the file open dialog, locate and select the downloaded `toy-cat.sog` and click **Open**

The Editor processes the `.sog` file and displays it in your **ASSETS** panel as a `gsplat` asset (with name `toy-cat.sog`).

## Creating the Splat Entity

Let's create an entity to display our splat:

1. Drag and drop the `toy-cat.sog` asset from the **ASSETS** panel into the Viewport.

The Editor creates a new entity under the root in the **HIERARCHY** panel and you should now see the toy cat in the viewport.

## Positioning the Splat

The splat is not centered on the origin so let's adjust its transform:

1. Select the newly created splat entity in the **HIERARCHY** panel
2. In the **INSPECTOR** panel, set **Position** to `X: 0, Y: -0.7, Z: 0`

You should now see the toy cat centered on the origin.

## Adding Camera Controls

To make the scene interactive, let's assign a script to our camera entity:

1. Right-click this link and select **Save link as...**: [`camera-controls.mjs`](https://raw.githubusercontent.com/playcanvas/engine/main/scripts/esm/camera-controls.mjs)
2. In the **ASSETS** panel (bottom of the screen), click the **+** icon
3. Select **Upload** from the popup menu
4. In the file open dialog, locate and select the downloaded `camera-controls.mjs` script and click **Open**

Now let's attach the script to our camera:

1. Select the **Camera** entity in the **HIERARCHY** panel
2. In the **INSPECTOR**, click **ADD COMPONENT** and select **Script**
3. In the Script component, click the **Add Script** dropdown
4. Select **cameraControls** from the list

You've now attached the camera controls script and it's ready to use!

## Testing the Scene

Now let's test our interactive splat scene:

1. Click the **LAUNCH** button in the Viewport's toolbar to run the project
2. You should see the toy cat splat displayed in your browser
3. Try interacting with it:
   * **Left mouse drag**: Orbit around the splat
   * **Mouse wheel**: Zoom in and out

## Final Result

Congratulations! You've successfully created an interactive Gaussian splat application using the PlayCanvas Editor.

[Interactive Demo]

:::tip Next Steps

Now that you have a working splat app, try experimenting with:

* Swapping the toy cat `.sog` file for one of your own
* Adding a user interface
* Building more complex interactions with scripts

:::

--------------------------------------------------------------------------------

## Using the Engine API

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/building/your-first-app/engine/

Let's build a simple Gaussian splat application step by step using the [PlayCanvas Engine](/user-manual/engine) directly. We'll create a scene with an interactive 3D toy cat splat that you can rotate around.

## Starting Point

Let's set up our project with two files: an HTML file and a JavaScript file.

First, create an `index.html` file:

```html title="index.html"
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <style>
            body { margin: 0; overflow: hidden; }
        </style>
        <script type="importmap">
        {
            "imports": {
                "playcanvas": "https://cdn.jsdelivr.net/npm/playcanvas/+esm"
            }
        }
        </script>
    </head>
    <body>
        <script type="module" src="main.js"></script>
    </body>
</html>
```

Next, create a `main.js` file with the basic PlayCanvas application setup:

```javascript title="main.js"

// Create application
const canvas = document.createElement('canvas');
document.body.appendChild(canvas);

const app = new Application(canvas, {
    graphicsDeviceOptions: {
        antialias: false
    }
});
app.setCanvasFillMode(FILLMODE_FILL_WINDOW);
app.setCanvasResolution(RESOLUTION_AUTO);
app.start();

window.addEventListener('resize', () => app.resizeCanvas());
```

This creates an empty 3D scene using the [`Application`](https://api.playcanvas.com/engine/classes/Application.html) class with a canvas that:

- Fills the entire browser window (`FILLMODE_FILL_WINDOW`)
- Automatically adjusts resolution based on device pixel ratio (`RESOLUTION_AUTO`)
- Properly resizes when the window changes size

We can't see anything rendered yet though - we need to load assets and add a camera and content.

:::warning Performance Optimization

We've disabled `antialias` in the graphics device options for optimal splat rendering performance. This setting helps reduce the fragment processing load, which is the primary bottleneck in Gaussian splat rendering. Learn more in the [Performance](../engine-features/performance.md) guide.

:::

## Loading Assets

Before we can display a splat or add camera controls, we need to load the assets our app will use. We'll use the [`Asset`](https://api.playcanvas.com/engine/classes/Asset.html) class to define our assets and the [`AssetListLoader`](https://api.playcanvas.com/engine/classes/AssetListLoader.html) to load them efficiently. Add this code to `main.js` where the comment says "We'll add our code here step by step":

```javascript
// Load assets
const assets = [
    new Asset('camera-controls', 'script', {
        url: 'https://cdn.jsdelivr.net/npm/playcanvas/scripts/esm/camera-controls.mjs'
    }),
    new Asset('toy', 'gsplat', {
        url: 'https://developer.playcanvas.com/assets/toy-cat.sog'
    })
];

const loader = new AssetListLoader(assets, app.assets);
await new Promise(resolve => loader.load(resolve));
```

We're loading two assets:

- A camera controls script that will let us orbit around the scene
- A `.sog` file containing a toy cat splat

The [`AssetListLoader`](https://api.playcanvas.com/engine/classes/AssetListLoader.html) loads all assets efficiently and we use `await` to ensure they're fully loaded before proceeding.

## Adding a Camera

To view our scene, we need to create a camera entity using the [`Entity`](https://api.playcanvas.com/engine/classes/Entity.html) class and add a [camera component](https://api.playcanvas.com/engine/classes/CameraComponent.html) to it. Add this code to `main.js` after the asset loading:

```javascript
// Create camera entity
const camera = new Entity('Camera');
camera.setPosition(0, 0, 2.5);
camera.addComponent('camera');
app.root.addChild(camera);
```

We've positioned the camera 2.5 units down the Z axis. By default, a camera looks down the negative Z axis, so our camera is now looking toward the origin where we'll place our splat.

## Adding Camera Controls

Now let's make the camera interactive by attaching the camera controls script using the [script component](https://api.playcanvas.com/engine/classes/ScriptComponent.html). Add this code to `main.js` after the camera creation:

```javascript
// Add camera controls
camera.addComponent('script');
camera.script.create('cameraControls');
```

Since we've already loaded the camera controls script using [`AssetListLoader`](https://api.playcanvas.com/engine/classes/AssetListLoader.html), we can directly create the script component. The camera controls will allow you to:

- **Left mouse drag**: Orbit around the target
- **Right mouse drag**: Pan the camera
- **Mouse wheel**: Zoom in and out

## Adding the Splat

Now let's add our toy cat splat to the scene using the [gsplat component](https://api.playcanvas.com/engine/classes/GSplatComponent.html). Add this code to `main.js` after the camera controls:

```javascript
// Create splat entity
const splat = new Entity('Toy Cat');
splat.setPosition(0, -0.7, 0);
splat.setEulerAngles(0, 0, 180);
splat.addComponent('gsplat', { asset: assets[1] });
app.root.addChild(splat);
```

We reference the splat asset using `assets[1]` (the second asset in our array). We've positioned the splat slightly below the origin (-0.7 on the Y axis) and rotated it 180 degrees around the Z axis to orient it properly.

## Complete Code

Here are the complete files with all the code from the steps above:

**index.html:**

```html title="index.html"
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <style>
            body { margin: 0; overflow: hidden; }
        </style>
        <script type="importmap">
        {
            "imports": {
                "playcanvas": "https://cdn.jsdelivr.net/npm/playcanvas/+esm"
            }
        }
        </script>
    </head>
    <body>
        <script type="module" src="main.js"></script>
    </body>
</html>
```

**main.js:**

```javascript title="main.js"

// Create application
const canvas = document.createElement('canvas');
document.body.appendChild(canvas);

const app = new Application(canvas, {
    graphicsDeviceOptions: {
        antialias: false
    }
});
app.setCanvasFillMode(FILLMODE_FILL_WINDOW);
app.setCanvasResolution(RESOLUTION_AUTO);
app.start();

window.addEventListener('resize', () => app.resizeCanvas());

// Load assets
const assets = [
    new Asset('camera-controls', 'script', {
        url: 'https://cdn.jsdelivr.net/npm/playcanvas/scripts/esm/camera-controls.mjs'
    }),
    new Asset('toy', 'gsplat', {
        url: 'https://developer.playcanvas.com/assets/toy-cat.sog'
    })
];

const loader = new AssetListLoader(assets, app.assets);
await new Promise(resolve => loader.load(resolve));

// Create camera entity
const camera = new Entity('Camera');
camera.setPosition(0, 0, 2.5);
camera.addComponent('camera');
camera.addComponent('script');
camera.script.create('cameraControls');
app.root.addChild(camera);

// Create splat entity
const splat = new Entity('Toy Cat');
splat.setPosition(0, -0.7, 0);
splat.setEulerAngles(0, 0, 180);
splat.addComponent('gsplat', { asset: assets[1] });
app.root.addChild(splat);
```

## Final Result

After completing the steps above, you should see an interactive 3D toy cat splat that you can orbit around, pan, and zoom!

:::tip Try it yourself

Create both files above (`index.html` and `main.js`) in the same directory and open `index.html` in your browser to see your first splat app in action! Then extend it in any way you like using the full power of the PlayCanvas Engine!

:::

--------------------------------------------------------------------------------

## Using PlayCanvas React

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/building/your-first-app/react/

const { asset } = useSplat('/assets/toy-cat.sog');
    if (!asset) return null;

    return (
         
    );
}

    return (
        <div style={{ width: '100%', height: '400px', marginBottom: '2rem', borderRadius: '8px', overflow: 'hidden' }}>
            
        </div>
    );
}

Let's build a simple Gaussian splat application step by step using [PlayCanvas React](/user-manual/react). We'll create a scene with an interactive 3D toy cat splat that you can rotate around.

## Starting Point

First, let's set up a basic React component structure. We'll start with the essential PlayCanvas React components:

```jsx

    return (
        
    );
}
```

This creates an empty 3D scene with optimal settings for web applications. However, we can't see anything rendered yet. We need a camera and some content.

:::warning Performance Optimization

We've configured the `Application` with `graphicsDeviceOptions={{ antialias: false }}` for optimal splat rendering performance. Setting `antialias` to `false` reduces the fragment processing load, which is the primary bottleneck in Gaussian splat rendering. Learn more in the [Performance](../engine-features/performance.md) guide.

:::

## Adding a Camera

To view our scene, we need a camera which we can add using the `Entity` component with `Camera` and `CameraControls`:

```jsx {2-3,8-11}

    return (
        
    );
}
```

We've positioned the camera 2.5 units down the Z axis. By default, a camera looks down the negative Z axis, so our camera is now looking toward the origin where we'll place our splat. The `CameraControls` will allow you to:

- **Left mouse drag**: Orbit around the target
- **Right mouse drag**: Pan the camera
- **Mouse wheel**: Zoom in and out

## Adding the Splat

Now let's add our toy cat splat as a component using the `useSplat` hook and `GSplat` component:

```jsx {2,4,6-15,24}

function ToyCat() {
    const { asset } = useSplat('toy-cat.sog');
    if (!asset) return null;

    return (
        
    );
}

    return (
        
    );
}
```

We've added several important elements:

- **`useSplat` hook**: Loads the splat asset from the URL
- **Conditional rendering**: `if (!asset) return null;` ensures we don't render until the asset is loaded
- **GSplat positioning**: The splat is positioned slightly below the origin (-0.7 on the Y axis) and rotated 180 degrees around the Z axis to orient it properly
- **Single Entity**: The ToyCat component returns a single Entity containing the GSplat

## Complete Code

Here's the complete React component with all the code from the steps above:

```jsx

function ToyCat() {
    const { asset } = useSplat('toy-cat.sog');
    if (!asset) return null;

    return (
        
    );
}

    return (
        
    );
}
```

## Final Result

After completing the steps above, you should see an interactive 3D toy cat splat that you can orbit around, pan, and zoom!

:::tip Try it yourself

Add the complete JSX code above to your React component and run your application to see your first splat app in action! Then extend it in any way you like using the full power of PlayCanvas React!

:::

--------------------------------------------------------------------------------

## Using Web Components

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/building/your-first-app/web-components/

Let's build a simple Gaussian splat application step by step using [PlayCanvas Web Components](/user-manual/web-components). We'll create a scene with an interactive 3D toy cat splat that you can rotate around.

## Starting Point

Begin by creating a new file called `index.html` and copy the [Web Components boilerplate HTML](/user-manual/web-components/getting-started/#boilerplate-html) into it.

Now, let's add the basic structure of our application to our HTML `body` using the [`<pc-app>`](/user-manual/web-components/tags/pc-app) and [`<pc-scene>`](/user-manual/web-components/tags/pc-scene) elements.

```html
<pc-app antialias="false" high-resolution="false">
    <pc-scene>
    </pc-scene>
</pc-app>
```

This creates an empty 3D scene. However, we can't see anything rendered yet. For that, we will need to add a camera and a splat.

:::warning Performance Optimization

We've disabled `antialias` and `high-resolution` on the `<pc-app>` element for optimal splat rendering performance. These settings help reduce the fragment processing load, which is the primary bottleneck in Gaussian splat rendering. Learn more in the [Performance](../engine-features/performance.md) guide.

:::

## Loading Assets

Before we can display a splat or add camera controls, we need to define the assets our app will use. Let's add a camera controls script and a splat asset using the [`<pc-asset>`](/user-manual/web-components/tags/pc-asset) element.

```html {2-3}
<pc-app antialias="false" high-resolution="false">
    <pc-asset src="https://cdn.jsdelivr.net/npm/playcanvas/scripts/esm/camera-controls.mjs"></pc-asset>
    <pc-asset id="toy" src="https://developer.playcanvas.com/assets/toy-cat.sog"></pc-asset>
    <pc-scene>
    </pc-scene>
</pc-app>
```

We've added two assets:

- A camera controls script that will let us orbit around the scene
- A compressed PLY file containing a toy cat splat

## Adding a Camera

To view our scene, we need a camera which we can add using the [`<pc-entity>`](/user-manual/web-components/tags/pc-entity) and [`<pc-camera>`](/user-manual/web-components/tags/pc-camera) elements.

```html {5-7}
<pc-app antialias="false" high-resolution="false">
    <pc-asset src="https://cdn.jsdelivr.net/npm/playcanvas/scripts/esm/camera-controls.mjs"></pc-asset>
    <pc-asset id="toy" src="https://developer.playcanvas.com/assets/toy-cat.sog"></pc-asset>
    <pc-scene>
        <pc-entity position="0 0 2.5">
            <pc-camera></pc-camera>
        </pc-entity>
    </pc-scene>
</pc-app>
```

We've positioned the camera 2.5 units down the Z axis. By default, a camera looks down the negative Z axis, so our camera is now looking toward the origin where we'll place our splat.

## Adding Camera Controls

Now let's make the camera interactive by adding the camera controls script using the [`<pc-scripts>`](/user-manual/web-components/tags/pc-scripts) and [`<pc-script>`](/user-manual/web-components/tags/pc-script) elements.

```html {7-9}
<pc-app antialias="false" high-resolution="false">
    <pc-asset src="https://cdn.jsdelivr.net/npm/playcanvas/scripts/esm/camera-controls.mjs"></pc-asset>
    <pc-asset id="toy" src="https://developer.playcanvas.com/assets/toy-cat.sog"></pc-asset>
    <pc-scene>
        <pc-entity position="0 0 2.5">
            <pc-camera></pc-camera>
            <pc-scripts>
                <pc-script name="cameraControls"></pc-script>
            </pc-scripts>
        </pc-entity>
    </pc-scene>
</pc-app>
```

The camera controls script will allow you to:

- **Left mouse drag**: Orbit around the target
- **Right mouse drag**: Pan the camera
- **Mouse wheel**: Zoom in and out

## Adding the Splat

Now let's add our toy cat splat to the scene using the [`<pc-splat>`](/user-manual/web-components/tags/pc-splat) element.

```html {11-13}
<pc-app antialias="false" high-resolution="false">
    <pc-asset src="https://cdn.jsdelivr.net/npm/playcanvas/scripts/esm/camera-controls.mjs"></pc-asset>
    <pc-asset id="toy" src="https://developer.playcanvas.com/assets/toy-cat.sog"></pc-asset>
    <pc-scene>
        <pc-entity position="0 0 2.5">
            <pc-camera></pc-camera>
            <pc-scripts>
                <pc-script name="cameraControls"></pc-script>
            </pc-scripts>
        </pc-entity>
        <pc-entity position="0 -0.7 0" rotation="0 0 180">
            <pc-splat asset="toy"></pc-splat>
        </pc-entity>
    </pc-scene>
</pc-app>
```

We've positioned the splat slightly below the origin (-0.7 on the Y axis) and rotated it 180 degrees around the Z axis to orient it properly. The `asset="toy"` attribute references the splat asset we defined earlier.

## Final Result

After completing the steps above, you should see an interactive 3D toy cat splat that you can orbit around, pan, and zoom!

<CodePenEmbed id="MYgGZax" title="<pc-splat> example" />

:::tip Try it yourself

Copy the final HTML code above into an HTML file and open it in your browser to see your first splat app in action! Then extend it in any way you like using the full power of the PlayCanvas Engine!

:::

--------------------------------------------------------------------------------

## Creating Splats

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/creating/

Gaussian Splats are 3D scenes that have been reconstructed from photogrammetry. This photogrammetry can be photographs or individual frames extracted from video. It can also be 'synthetic', where images are rendered by 3D packages such as [Blender](https://www.blender.org/), for example.

Once you have captured your photogrammetry, you are in a position to convert it into a 3D Gaussian Splat. This process can be broken down into two main steps:

1. **Structure from Motion (SfM):** Input images are analyzed to estimate original camera poses and create an initial 3D point cloud, identifying key points in the scene.
2. **Training:** The algorithm performs the following over many thousands of iterations:
    - **Adds splats** in areas that need more detail
    - **Removes splats** that don't contribute significantly
    - **Adjusts splat parameters** to better match the input images

--------------------------------------------------------------------------------

## Recommended Tools

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/creating/recommended-tools/

PlayCanvas does not itself provide a tool to convert photogrammetry into a Gaussian splat. But there is a healthy variety of third-party tools for you to choose from, each with their strengths and weaknesses.

## Quick Comparison

| Tool | Difficulty | Platforms | Cost | Open Source | Primary Use | Requirements |
|------|------------|-----------|------| :-----: |-------------|--------------|
| [**Polycam**](https://poly.cam/) | Easy | iOS, Android, Web | Freemium | ❌ | Capture + Training | Mobile device |
| [**Luma AI**](https://lumalabs.ai/app) | Easy | iOS, Android, Web | Freemium | ❌ | Capture + Training | Mobile device |
| [**COLMAP**](https://colmap.github.io/) | Advanced | Win, Linux, macOS | Free | ✔️ | Camera Poses | |
| [**RealityScan**](https://www.realityscan.com/) | Advanced | Win | Free* | ❌ | Camera Poses | CUDA GPU |
| [**Postshot**](https://www.jawset.com/) | Advanced | Win | Paid | ❌ | Camera Poses + Training | CUDA GPU |
| [**Brush**](https://github.com/ArthurBrussee/brush) | Advanced | Win, Linux, macOS, Web | Free | ✔️ | Camera Poses + Training | |
| [**nerfstudio**](https://docs.nerf.studio/) | Advanced | Win, Linux, macOS | Free | ✔️ | Research/Training | |
| [**INRIA Tools**](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/) | Advanced | Win, Linux | Free | ✔️ | Research/Reference | CUDA GPU |

*_Free for non-commercial use_

## Easy/Consumer Tools

These tools are designed for users who want to create Gaussian splats quickly without technical expertise:

[**Polycam**](https://poly.cam/) (iOS, Android, Web)  
Commercial platform with user-friendly interface for creating Gaussian splats. Features include free tier with limitations, pro tiers for higher quality, support for up to 2000 images, built-in editing tools, and easy sharing and export.

:::important

Select **splat PLY** on export from Polycam.

:::

[**Luma AI**](https://lumalabs.ai/app) (iOS, Android, Web)  
AI-powered cloud service with mobile app for easy capture. Offers mobile app for capture, cloud processing, high-quality results, video input support, and game engine integration.

:::important

Select **Gaussian Splat** on export from Luma and extract the PLY file from the downloaded ZIP file.

:::

## Advanced/Pro Tools

These tools offer more control and customization but require technical knowledge and are suitable for professional workflows:

[**COLMAP**](https://colmap.github.io/) (Windows, Linux, macOS)  
Open source Structure-from-Motion (SfM) pipeline for camera alignment and sparse point cloud generation. Provides cross-platform compatibility, high-quality reconstruction, command-line and GUI interfaces, and serves as the foundation for splat training in many workflows. Particularly valuable for users on non-Windows systems.

[**RealityScan**](https://www.realityscan.com/) (Windows)  
Desktop application for camera alignment and sparse point cloud generation, which become the foundation for splat training in tools like PostShot. Free for non-commercial use. Requires a CUDA-enabled GPU.

[**Postshot**](https://www.jawset.com/) (Windows)  
Commercial desktop application for creating Gaussian splats with advanced features. Provides on-device processing, quick results, and a user-friendly interface. Requires a CUDA-enabled GPU.

[**Brush**](https://github.com/ArthurBrussee/brush) (Windows, Linux, macOS, Android, Web)  
Open source, cross-platform engine with broad device compatibility. Uses WebGPU-based rendering, offers real-time training visualization, has no CUDA dependency, supports browsers, and works on mobile devices.

[**nerfstudio**](https://docs.nerf.studio/) (Windows, Linux, macOS)  
Open source research framework for training various splat models. Features command-line interface, multiple model types, highly customizable settings, research-oriented approach, and active development community.

[**INRIA Tools**](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/) (Windows, Linux)  
Original reference implementation from the 3D Gaussian Splatting paper. Has a dependency on COLMAP. Provides research-grade quality, CUDA acceleration, full parameter control, though requires complex setup and is best used for experimentation.

## Outputting a PLY File

All of these tools can output trained Gaussian splat scenes in the PLY file format. The PLY format serves as the standard interchange format for 3D Gaussian Splats, making it possible to move your creations between different applications and workflows. To better understand what these tools are producing and how to work with the resulting files effectively, let's explore the [PLY format](../formats/ply.md) in detail.

--------------------------------------------------------------------------------

## Taking Photos

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/creating/taking-photos/

The quality of your Gaussian splat is fundamentally determined by the quality of your source photos. This guide covers essential techniques for capturing images that will produce high-quality, detailed splats.

## Why Photo Quality Matters

Unlike traditional 3D modeling where you can fix issues in post-production, Gaussian splats are trained directly from your source imagery. This means that every problem in your source photos becomes a problem in your final splat.

:::warning Poor Photo Quality Consequences

Bad capture techniques result in:

- **Floaters** - Stray splats in wrong locations
- **Blurry or missing details** - Areas with insufficient coverage  
- **Incorrect lighting** - Inconsistent illumination across the scene
- **Training failures** - Algorithms unable to converge on good solutions

:::

Getting the capture right is your best investment in final quality - it's much easier to shoot it right than to fix it later.

## Camera Equipment

Choosing the right camera equipment depends on your experience level, budget, and the complexity of subjects you plan to capture. Here's a comparison of the main options:

| Camera Type | Best For | Key Advantages | Main Limitations | Recommended Models |
|-------------|----------|----------------|------------------|-------------------|
| 📱 **Smartphone** | Beginners, small objects | Always available, computational photography, easy handling | Fixed aperture, limited zoom, smaller sensors | iPhone 13 Pro+, Google Pixel 7+, Samsung Galaxy S22+ |
| 🎥 **Action Camera** | Intermediate users, limited budgets | Stabilization, wide angle, durability | Small sensors, limited control | DJI Osmo Pocket |
| 📷 **DSLR/Mirrorless** | Professional work, large scenes | Full manual control, larger sensors, interchangeable lenses | More complex, heavier, requires photography knowledge | Any modern DSLR/mirrorless with 20MP+ |

:::tip For Beginners

Start with your smartphone! Modern phones produce excellent results and let you learn the fundamentals without equipment investment. You can always upgrade to a dedicated camera once you understand the process.

:::

### Lens Considerations

The focal length and aperture you choose significantly impact reconstruction quality. For optimal results, stick to **35-85mm equivalent focal length** - this range provides natural perspective without the distortion of ultra-wide lenses or the compressed depth perception of telephoto lenses.

When it comes to aperture, **f/8-f/11** represents the sweet spot where most lenses achieve maximum sharpness across the entire frame. This ensures all parts of your subject are equally detailed for the reconstruction algorithm.

:::caution Avoid These Settings

- **Ultra-wide lenses (&lt;24mm)**: Cause barrel distortion that confuses reconstruction algorithms
- **Telephoto lenses (&gt;135mm)**: Compress depth perception, making 3D reconstruction more difficult  
- **Wide apertures (f/1.4-f/2.8)**: Create shallow depth of field, leaving parts of your subject out of focus
- **Tiny apertures (f/16+)**: Introduce diffraction that softens the entire image

:::

## Camera Settings

Proper camera settings are crucial for consistent, high-quality captures. The key principle is consistency - maintain identical settings across your entire capture session to avoid confusing the reconstruction algorithms.

### File Format and Quality

Always shoot in **RAW format** when possible, as it preserves maximum image data for processing flexibility. If RAW isn't available, use the highest quality JPEG setting your camera offers. Avoid heavily compressed formats that introduce artifacts.

Use your camera's **maximum resolution** - more pixels provide more detail for the reconstruction algorithm to work with. As a minimum, aim for 12MP for small objects and 20MP+ for room-scale scenes.

### ISO and Image Quality

Keep your ISO as low as possible (ISO 100-400) to minimize noise. Clean images with low noise help reconstruction algorithms identify features more accurately.

:::info ISO Guidelines

| Lighting Condition | Recommended ISO | Maximum ISO |
|-------------------|-----------------|-------------|
| Bright daylight | ISO 100-200 | ISO 400 |
| Indoor/Overcast | ISO 200-400 | ISO 800 |
| Low light | ISO 400-800 | ISO 1600 |
| Very low light | ISO 800-1600 | ISO 3200* |

*Only if absolutely necessary - consider adding artificial lighting instead

:::

### Focus Strategy

Manual focus is strongly preferred to ensure consistent sharpness across all images. If you must use autofocus, switch to single-point mode and focus on the same feature consistently throughout your capture session.

### Exposure Consistency

The most critical aspect of camera settings is maintaining **identical exposure** throughout your entire capture session. Use manual mode to lock your aperture, shutter speed, and ISO settings.

:::tip Exposure Lock Technique

If you're not comfortable with manual mode:

1. Use your camera's auto-exposure on the first shot
2. Note the settings it chose
3. Switch to manual and dial in those exact settings
4. Don't change them for the rest of the session

:::

For shutter speed, ensure it's fast enough to prevent motion blur - at minimum **1/125s when handheld**. Use a tripod for longer exposures in low light, and enable image stabilization if your camera or lens has it.

## Photos vs Video

The choice between still photography and video extraction depends on your subject matter and shooting conditions. Each approach has distinct advantages and optimal use cases.

### Still Photography

Still photography offers the highest quality per frame and maximum control over each capture. This approach works best for **static subjects** in **controlled environments** where you can take your time to ensure each shot is perfect.

The advantages of stills include access to higher resolutions (often 40MP+ vs 4K video's 8MP), full manual control over each individual frame, superior low-light performance, and no compression artifacts from video encoding.

:::tip When to Choose Stills

Use still photography for:

- Small objects requiring maximum detail
- Professional work where quality is paramount  
- Studio or controlled lighting situations
- Subjects that can remain stationary during capture

:::

### Video Capture

Video capture excels when dealing with **moving subjects** or when you need to cover large areas quickly. It's particularly useful in challenging environments where setting up individual shots would be difficult or time-consuming.

For video capture, always shoot in **4K resolution minimum** - 1080p doesn't provide sufficient detail for quality splats. Use a high frame rate (60fps+) for smooth motion, and critically important: **lock your exposure settings** to prevent flicker between frames.

:::warning Video Capture Requirements

Essential video settings:

- **4K resolution minimum** (8MP effective)
- **60fps+ frame rate** for motion handling
- **Manual exposure** - no auto-adjustments
- **Maximum bitrate** available
- **Stable color temperature** throughout

:::

When extracting frames from video, take every 2nd-5th frame depending on your movement speed, and always check extracted frames for motion blur before processing - blurry frames will hurt your final reconstruction quality.

:::tip

Consider using tools like [Sharp Frames](https://sharp-frames.reflct.app/) to automatically select the best frames from your video.

:::

## Number of Photos

The number of photos you need varies dramatically based on your subject size and complexity. More photos generally lead to better results, but there are practical guidelines that balance quality with capture time.

| Subject Size | Recommended Photos | Examples | Key Considerations |
|--------------|-------------------|----------|-------------------|
| **Small (&lt;1 foot)** | 50-100 photos | Desktop items, products, collectibles | Higher density needed for fine details |
| **Medium (1-10 feet)** | 100-200 photos | Furniture, statues, vehicles | Multiple elevation levels essential |
| **Large (10+ feet)** | 200-500+ photos | Rooms, buildings, landscapes | Systematic grid approach required |

### Overlap Requirements

Proper overlap between images is critical for successful reconstruction. Think of it like overlapping puzzle pieces - each photo needs to share enough content with its neighbors for the algorithm to understand how they connect.

Aim for **70-80% overlap** between adjacent photos in the same ring or row. When moving between different elevation levels or rings around your subject, maintain **60-70% overlap** to ensure vertical connections.

:::note Angle Guidelines

- **15-30 degrees** between adjacent viewpoints
- **Multiple elevations** for every subject (high, medium, low angles)  
- **360-degree coverage** for objects viewable from all sides
- **When in doubt, shoot more** - extra photos rarely hurt, missing coverage always does

:::

## Scene Coverage Techniques

### Object Photography

**Turntable Method:**

1. Place object on rotating surface
2. Take photos every 15-20 degrees (18-24 photos per ring)
3. Capture 3-4 rings at different heights
4. Include top-down and bottom-up views if possible

**Orbital Method:**

1. Walk around static object in circles
2. Maintain consistent distance
3. Vary height between passes
4. Overlap each orbit by 30-40%

### Room/Scene Photography

**Grid Pattern:**

1. Plan systematic grid across the space
2. Take photos from each grid point
3. Vary heights (standing, crouching, elevated)
4. Include corner and edge details

**Feature-Based Coverage:**

1. Identify key features and surfaces
2. Ensure each feature visible from multiple angles
3. Pay extra attention to transitions and corners
4. Capture ceiling and floor connections

## Lighting Considerations

### Consistent Lighting

**Natural Light:**

- **Overcast days:** Soft, even illumination
- **Avoid direct sunlight:** Creates harsh shadows
- **Golden hour:** Warm but avoid dramatic shadows

**Artificial Light:**

- **Soft, diffused lighting:** Multiple light sources
- **Avoid single hard lights:** Creates strong shadows
- **Color consistency:** Same color temperature throughout

### Challenging Lighting

**Mixed Lighting:**

- Try to maintain consistent color temperature
- Use manual white balance
- Consider HDR techniques for extreme contrast

**Low Light:**

- Use tripod for stability
- Increase ISO carefully
- Consider additional lighting equipment

## Common Mistakes to Avoid

Learning from common pitfalls can save you hours of frustration and failed reconstructions. Here are the most frequent issues that sabotage otherwise good photogrammetry sessions.

:::danger Coverage Problems

**Insufficient overlap**: Gaps in reconstruction where photos don't connect  
**Missing viewpoints**: Blind spots create holes in your final model  
**Single height level**: Results in flat, incomplete geometry  
**Too few photos**: Sparse coverage leads to low-quality, incomplete results

:::

:::warning Technical Issues

**Motion blur**: Even slight camera shake makes images unusable for reconstruction  
**Focus inconsistency**: Some areas sharp while others are soft confuses algorithms  
**Exposure changes**: Brightness variations between shots break feature matching  
**Reflective surfaces**: Mirrors, glass, and chrome can create false features and artifacts

:::

:::caution Environmental Challenges

**Moving objects**: People, vehicles, or wind-blown foliage create ghosting artifacts  
**Changing conditions**: Clouds or shadows shifting during capture session  
**Cluttered backgrounds**: Unnecessary complexity that doesn't add value to your subject

:::

## Quality Control Checklist

Before processing your photogrammetry, invest time in reviewing your captures. This quality control step can save hours of processing time and prevent disappointing results.

:::tip Pre-Processing Checklist

✅ **Review all images** for sharpness and exposure consistency  
✅ **Check coverage** - ensure no major blind spots in your subject  
✅ **Verify overlap** - sufficient redundancy between adjacent photos  
✅ **Confirm consistency** - lighting and camera settings remained stable  
✅ **Remove bad shots** - delete blurry, over/underexposed, or redundant images

:::

Remember: good photogrammetry is the foundation of excellent Gaussian splats. While it may seem time-consuming initially, developing systematic capture habits will dramatically improve your results and reduce the time spent troubleshooting failed reconstructions.

Take time to plan your capture strategy before you start shooting, and execute it methodically. The extra effort invested in proper photogrammetry pays dividends in the quality of your final splats.

--------------------------------------------------------------------------------

## Editing and Publishing Splats

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/editing/

Raw Gaussian Splat files often need editing and optimization before they're ready for production use. This process involves removing unwanted elements, compressing file sizes, and optimizing performance for real-time rendering in PlayCanvas.

## Why Edit Gaussian Splats?

Generated splat files typically have several issues that need addressing:

- **Floaters** - Stray splats in wrong locations from reconstruction errors
- **Background noise** - Unwanted environmental elements captured during scanning
- **Oversized files** - Too many splats for real-time rendering
- **Poor performance** - Suboptimal splat distribution affecting frame rates
- **Visual artifacts** - Rendering glitches that need manual cleanup

## Editing Tools Overview

PlayCanvas provides both visual and command-line tools for editing Gaussian Splats:

### [SuperSplat Editor](supersplat)

The **SuperSplat Editor** is a powerful, browser-based visual editor designed specifically for interactive splat editing. It's perfect for:

- Manual cleanup and artistic editing
- Removing unwanted background elements
- Fine-tuning splat positioning and colors
- Creating camera animations and flythroughs
- Publishing splats to the web

### [splat-transform CLI](splat-transform)

The **splat-transform CLI** is a command-line tool for batch processing and automated transformations. It excels at:

- Scripted, reproducible transformations
- Batch processing multiple files
- Automated filtering and optimization
- Integration into build pipelines
- Combining and merging splat files

--------------------------------------------------------------------------------

## SplatTransform

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/editing/splat-transform/

[SplatTransform](https://github.com/playcanvas/splat-transform) is an open source library and CLI tool for converting and editing Gaussian splats. Whether you need to convert between formats, apply transformations, or analyze splat data, SplatTransform provides the tools developers need for precise control over their Gaussian splat workflows. The library is platform-agnostic and can be used in both Node.js and browser environments.

:::note Open Source

SplatTransform is [open-sourced under an MIT license on GitHub](https://github.com/playcanvas/splat-transform)

:::

## Why Use SplatTransform?

SplatTransform solves important problems developers face when working with Gaussian splats:

🔄 **Broad Format Support** — seamlessly convert between PLY, SPLAT, KSPLAT, SOG and even CSV  
🛠️ **Powerful Transformations** — translate, rotate, and scale your splats with precision  
🧹 **Smart Filtering** — remove NaN values, filter by properties, and strip unnecessary data  
📊 **Statistical Analysis** — generate per-column statistics for data analysis  
📦 **Scene Merging** — combine multiple splat files into a merged scene  
⚡ **Production Ready** — optimized for maximum performance  
🆓 **Open Source** — completely free and available on GitHub

## Installation

Install or update to the latest version:

```bash
npm install -g @playcanvas/splat-transform
```

For library usage, install as a dependency:

```bash
npm install @playcanvas/splat-transform
```

Verify your CLI installation:

```bash
splat-transform --version
```

## Basic Usage

The general syntax for SplatTransform is:

```bash
splat-transform [GLOBAL] input [ACTIONS] ... output [ACTIONS]
```

**Key points:**

- Input files become the working set; ACTIONS are applied in order
- The last file is the output; actions after it modify the final result
- Use `null` as output to discard file output (useful with `--summary`)

### Format Conversion

Convert between commonly used splat formats with simple commands:

```bash
# Simple format conversion
splat-transform input.ply output.csv

# Convert from .splat format
splat-transform input.splat output.ply

# Convert from .ksplat format
splat-transform input.ksplat output.ply

# Convert to compressed PLY
splat-transform input.ply output.compressed.ply

# Uncompress a compressed PLY back to standard PLY
# (compressed .ply is detected automatically on read)
splat-transform input.compressed.ply output.ply

# Convert to SOG bundled format
splat-transform input.ply output.sog

# Convert to SOG unbundled format
splat-transform input.ply output/meta.json

# Convert from SOG (bundled) back to PLY
splat-transform scene.sog restored.ply

# Convert from SOG (unbundled folder) back to PLY
splat-transform output/meta.json restored.ply

# Convert to standalone HTML viewer (bundled, single file)
splat-transform input.ply output.html

# Convert to unbundled HTML viewer (separate CSS, JS, and SOG files)
splat-transform -U input.ply output.html

# Convert to HTML viewer with custom settings
splat-transform -E settings.json input.ply output.html
```

SplatTransform detects file format based on extension. Supported formats are shown below:

| Format | Input | Output | Description |
| ------ | ----- | ------ | ----------- |
| `.ply` | ✅ | ✅ | Standard PLY format |
| `.sog` | ✅ | ✅ | Bundled super-compressed format (recommended) |
| `meta.json` | ✅ | ✅ | Unbundled super-compressed format (accompanied by `.webp` textures) |
| `.compressed.ply` | ✅ | ✅ | Compressed PLY format (auto-detected and decompressed on read) |
| `.lcc` | ✅ | ❌ | LCC file format (XGRIDS) |
| `.ksplat` | ✅ | ❌ | Compressed splat format (mkkellogg format) |
| `.splat` | ✅ | ❌ | Compressed splat format (antimatter15 format) |
| `.spz` | ✅ | ❌ | Compressed splat format (Niantic format) |
| `.mjs` | ✅ | ❌ | Generate a scene using an mjs script (Beta) |
| `.csv` | ❌ | ✅ | Comma-separated values spreadsheet |
| `.html` | ❌ | ✅ | HTML viewer app (single-page or unbundled) based on SOG |

## Actions

Actions can be repeated and applied in any order to transform and filter your splats:

```none
-t, --translate        <x,y,z>          Translate splats by (x, y, z)
-r, --rotate           <x,y,z>          Rotate splats by Euler angles (x, y, z) in degrees
-s, --scale            <factor>         Uniformly scale splats by factor
-H, --filter-harmonics <0|1|2|3>        Remove spherical harmonic bands > n
-N, --filter-nan                        Remove Gaussians with NaN or Inf values
-B, --filter-box       <x,y,z,X,Y,Z>    Remove Gaussians outside box (min, max corners)
-S, --filter-sphere    <x,y,z,radius>   Remove Gaussians outside sphere (center, radius)
-V, --filter-value     <name,cmp,value> Keep splats where <name> <cmp> <value>
                                          cmp ∈ {lt,lte,gt,gte,eq,neq}
-p, --params           <key=val,...>    Pass parameters to .mjs generator script
-l, --lod              <n>              Specify the level of detail of this model, n >= 0
-m, --summary                           Print per-column statistics to stdout
```

## Global Options

```none
-h, --help                              Show this help and exit
-v, --version                           Show version and exit
-q, --quiet                             Suppress non-error output
-w, --overwrite                         Overwrite output file if it exists
-i, --iterations       <n>              Iterations for SOG SH compression (more=better). Default: 10
-L, --list-gpus                         List all available GPU adapters and exit
-g, --gpu              <n|cpu>          Select device for SOG compression: GPU adapter index | 'cpu'
-E, --viewer-settings  <settings.json>  HTML viewer settings JSON file
-U, --unbundled                         Generate unbundled HTML viewer with separate files
-O, --lod-select       <n,n,...>        Comma-separated LOD levels to read from LCC input
-C, --lod-chunk-count  <n>              Approx number of Gaussians per LOD chunk in K. Default: 512
-X, --lod-chunk-extent <n>              Approx size of an LOD chunk in world units (m). Default: 16
```

:::note

See the [SuperSplat Viewer Settings Schema](https://github.com/playcanvas/supersplat-viewer?tab=readme-ov-file#settings-schema) for details on how to pass data to the `-E` option.

:::

## Transformations

### Apply Spatial Transformations

Transform your splats during conversion with intuitive command-line options:

```bash
# Scale and translate
splat-transform bunny.ply -s 0.5 -t 0,0,10 bunny_scaled.ply

# Rotate by 90 degrees around Y axis
splat-transform input.ply -r 0,90,0 output.ply

# Chain multiple transformations
splat-transform input.ply -s 2 -t 1,0,0 -r 0,0,45 output.ply
```

## Filtering and Optimization

### Smart Filtering

Remove unwanted data and optimize your splats for production:

```bash
# Remove entries containing NaN and Inf
splat-transform input.ply --filter-nan output.ply

# Filter by opacity values (keep only splats with opacity > 0.5)
splat-transform input.ply -V opacity,gt,0.5 output.ply

# Strip spherical harmonic bands higher than 2
splat-transform input.ply --filter-harmonics 2 output.ply
```

## Scene Merging

Combine multiple splat files into a single scene with individual transformations:

```bash
# Combine multiple files with different transforms
splat-transform -w cloudA.ply -r 0,90,0 cloudB.ply -s 2 merged.compressed.ply

# Apply final transformations to combined result
splat-transform input1.ply input2.ply output.ply -t 0,0,10 -s 0.5
```

## Statistical Summary

Generate per-column statistics for data analysis or validation:

```bash
# Print summary, then write output
splat-transform input.ply --summary output.ply

# Print summary without writing a file (discard output)
splat-transform input.ply -m null

# Print summary before and after a transform
splat-transform input.ply --summary -s 0.5 --summary output.ply
```

The summary includes min, max, median, mean, stdDev, nanCount and infCount for each column in the data.

## CSV Export for Data Analysis

One of SplatTransform's most powerful features is CSV export, enabling data science workflows:

```bash
# Export splat data to CSV
splat-transform scene.ply data.csv

# Pre-filter before exporting for analysis
splat-transform input.ply --filter-nan -V opacity,gt,0.1 analysis.csv
```

### Why CSV Export Matters

- **Spreadsheet Analysis** — Import directly into Excel, Google Sheets, or any data analysis tool
- **Statistical Insights** — Calculate distributions, correlations, and quality metrics
- **Custom Filtering** — Use spreadsheet formulas to identify outliers or segment data
- **Visualization** — Create charts and graphs to understand splat data patterns
- **Integration** — Feed splat data into machine learning pipelines or custom workflows

CSV export transforms your splats from opaque binary files into readable, analyzable datasets perfect for research and optimization.

## Device Selection for SOG Compression

When compressing to SOG format, you can control which device (GPU or CPU) performs the compression:

```bash
# List available GPU adapters
splat-transform --list-gpus

# Let WebGPU automatically choose the best GPU (default behavior)
splat-transform input.ply output.sog

# Explicitly select a GPU adapter by index
splat-transform -g 0 input.ply output.sog  # Use first listed adapter
splat-transform -g 1 input.ply output.sog  # Use second listed adapter

# Use CPU for compression instead (much slower but always available)
splat-transform -g cpu input.ply output.sog
```

:::note

When `-g` is not specified, WebGPU automatically selects the best available GPU. Use `-L` to list available adapters with their indices and names. The order and availability of adapters depends on your system and GPU drivers.

:::

:::warning

CPU compression can be significantly slower than GPU compression (often 5-10x slower). Use CPU mode only if GPU drivers are unavailable or problematic.

:::

## Generators (Beta)

Generator scripts can be used to synthesize gaussian splat data. This allows you to procedurally create splat scenes using JavaScript:

```bash
splat-transform gen-grid.mjs -p width=10,height=10,scale=10,color=0.1 scenes/grid.ply -w
```

See the [example generator scripts](https://github.com/playcanvas/splat-transform/tree/main/generators) in the GitHub repository for more details.

## Common Workflows

### Production Optimization Pipeline

```bash
# Clean, limit spherical harmonic bands, and apply a scale for production
splat-transform raw_capture.ply \
  --filter-nan \
  --filter-harmonics 2 \
  -s 0.8 \
  production/capture.sog
```

### Format Migration

```bash
# Convert existing KSPLAT assets to PlayCanvas SOG
for file in *.ksplat; do
  splat-transform "$file" "${file%.ksplat}.sog"
done
```

### Quality Analysis

```bash
# Export for quality analysis in spreadsheet
splat-transform scene.ply \
  --filter-nan \
  -V opacity,gt,0.05 \
  quality_analysis.csv
```

### Multi-Scene Composition

```bash
# Combine multiple scenes with precise positioning
splat-transform \
  environment.ply -t 0,0,0 \
  character.ply -t 2,0,1 -r 0,180,0 \
  props.ply -t -3,0,2 -s 1.2 \
  complete_scene.ply
```

### Generating LOD Format

The LOD (Level of Detail) format enables efficient streaming and rendering of large gaussian splat scenes. The tool takes multiple pre-generated LOD files as input and generates an optimized streaming format with an octree structure for optimal download performance.

**Note:** The tool does NOT create the LOD levels themselves - you must supply multiple LOD files with progressively fewer gaussians (LOD 0 = highest detail, higher numbers = lower detail).

```bash
# Generate LOD streaming format from multiple input files
# Each input file represents a different detail level (LOD 0 is highest quality)
splat-transform \
  lod0.ply -l 0 \
  lod1.ply -l 1 \
  lod2.ply -l 2 \
  lod3.ply -l 3 \
  output/lod-meta.json \
  --filter-nan \
  --filter-harmonics 0

# Generate LOD with custom chunk settings for better performance
splat-transform \
  -C 1024 \
  -X 32 \
  lod0.ply -l 0 \
  lod1.ply -l 1 \
  lod2.ply -l 2 \
  output/lod-meta.json \
  --filter-nan

# For very large scenes, increase Node.js memory allocation
node --max-old-space-size=32000 node_modules/.bin/splat-transform \
  lod0.ply -l 0 \
  lod1.ply -l 1 \
  lod2.ply -l 2 \
  lod3.ply -l 3 \
  output/lod-meta.json \
  --filter-nan \
  --filter-harmonics 0
```

**Tips:**

- Use `--filter-nan` to remove invalid gaussians before processing
- Use `--filter-harmonics 0` to reduce file size if color detail is less critical
- Use `-C` to control the number of generated SOG files containing splats
- Use `-X` to control the size of each node. Increase for very large scenes to avoid generating a huge number of nodes to manage
- For very large scenes, use Node's `--max-old-space-size` flag to give it more memory

## Getting Help

Get help for any command:

```bash
# General help
splat-transform --help

# Get version information
splat-transform --version
```

For issues, feature requests, or contributions, visit the [GitHub repository](https://github.com/playcanvas/splat-transform). The project welcomes bug reports and pull requests from the community.

## Library Usage

SplatTransform exposes a programmatic API for reading, processing, and writing Gaussian splat data.

### Basic Import

```typescript

    readFile,
    writeFile,
    getInputFormat,
    getOutputFormat,
    DataTable,
    processDataTable
} from '@playcanvas/splat-transform';
```

### Key Exports

| Export | Description |
| ------ | ----------- |
| `readFile` | Read splat data from various formats |
| `writeFile` | Write splat data to various formats |
| `getInputFormat` | Detect input format from filename |
| `getOutputFormat` | Detect output format from filename |
| `DataTable`, `Column` | Core data structures for splat data |
| `combine` | Merge multiple DataTables into one |
| `transform` | Apply spatial transformations |
| `processDataTable` | Apply a sequence of processing actions |
| `computeSummary` | Generate statistical summary of data |

### File System Abstractions

The library uses abstract file system interfaces for maximum flexibility:

**Reading:**

- `UrlReadFileSystem` - Read from URLs (browser/Node.js)
- `MemoryReadFileSystem` - Read from in-memory buffers
- `ZipReadFileSystem` - Read from ZIP archives

**Writing:**

- `MemoryFileSystem` - Write to in-memory buffers
- `ZipFileSystem` - Write to ZIP archives

### Example: Reading and Processing

```typescript

    readFile,
    writeFile,
    getInputFormat,
    getOutputFormat,
    processDataTable,
    UrlReadFileSystem,
    MemoryFileSystem
} from '@playcanvas/splat-transform';

// Read a PLY file from URL
const fileSystem = new UrlReadFileSystem();
const inputFormat = getInputFormat('scene.ply');

const dataTables = await readFile({
    filename: 'https://example.com/scene.ply',
    inputFormat,
    options: { iterations: 10 },
    params: [],
    fileSystem
});

// Apply transformations
const processed = processDataTable(dataTables[0], [
    { kind: 'scale', value: 0.5 },
    { kind: 'translate', value: new Vec3(0, 1, 0) },
    { kind: 'filterNaN' }
]);

// Write to in-memory buffer
const memFs = new MemoryFileSystem();
const outputFormat = getOutputFormat('output.ply', {});

await writeFile({
    filename: 'output.ply',
    outputFormat,
    dataTable: processed,
    options: {}
}, memFs);

// Get the output data
const outputBuffer = memFs.files.get('output.ply');
```

### Processing Actions

The `processDataTable` function accepts an array of actions:

```typescript
type ProcessAction =
    | { kind: 'translate'; value: Vec3 }
    | { kind: 'rotate'; value: Vec3 }       // Euler angles in degrees
    | { kind: 'scale'; value: number }
    | { kind: 'filterNaN' }
    | { kind: 'filterByValue'; columnName: string; comparator: Comparator; value: number }
    | { kind: 'filterBands'; value: 0 | 1 | 2 | 3 }
    | { kind: 'filterBox'; min: Vec3; max: Vec3 }
    | { kind: 'filterSphere'; center: Vec3; radius: number }
    | { kind: 'lod'; value: number }
    | { kind: 'summary' };

type Comparator = 'lt' | 'lte' | 'gt' | 'gte' | 'eq' | 'neq';
```

### Custom Logging

Configure the logger for your environment:

```typescript

logger.setLogger({
    log: console.log,
    warn: console.warn,
    error: console.error,
    debug: console.debug,
    progress: (text) => process.stdout.write(text),
    output: console.log
});

logger.setQuiet(true); // Suppress non-error output
```

--------------------------------------------------------------------------------

## SuperSplat

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/editing/supersplat/

[SuperSplat](https://superspl.at/editor) is PlayCanvas's free, [open-source](https://github.com/playcanvas/supersplat) Gaussian Splat editor designed specifically for production workflows. It runs entirely in the browser with no downloads required, making it accessible from anywhere.

[Image: SuperSplat Interface]

## Video Tutorials

### Introduction to SuperSplat

A great way to learn the basics on SuperSplat is to watch this video introduction:

[Interactive Demo]

### In-Depth Tutorial

For a more comprehensive guide to using SuperSplat, check out this in-depth tutorial:

[Interactive Demo]

## Getting Started

### Accessing SuperSplat

1. **Open your browser** - Navigate to [superspl.at/editor](https://superspl.at/editor)
2. **Load your PLY file** - Drag and drop or use the File menu
3. **Begin editing** - Use the interface controls to navigate and edit

### System Requirements

- **Modern web browser** - Chrome, Firefox, Safari, or Edge
- **WebGL 2.0 support** - Available in all modern browsers
- **GPU acceleration** - Recommended for large splat files
- **No installation** - Everything runs in the browser

SuperSplat continues to evolve with new features and improvements. Stay updated with the latest releases and join the community to share your creations and learn from others.

--------------------------------------------------------------------------------

## Camera Controls

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/editing/supersplat/camera-controls/

SuperSplat provides intuitive camera controls for navigating the 3D view, supporting mouse, touch, and keyboard inputs. There are two control modes: **Orbit** (default) and **Fly**.

- **Orbit mode** - Camera rotates around a focal point. Best for inspecting objects from all angles.
- **Fly mode** - Camera moves freely through the scene. Best for navigating large environments.

Press **V** to toggle between modes, or use the WASD keys to automatically switch to fly mode.

## Mouse Controls

| Action | Orbit Mode | Fly Mode |
|--------|------------|----------|
| Left drag | Orbit around focal point | Look around |
| Middle drag | Zoom | Zoom |
| Right drag | Pan | Pan |
| Right + Ctrl/Shift drag | Orbit | Look around |
| Right + Alt drag | Zoom | Zoom |
| Mouse wheel | Zoom | Move forward/backward |
| Mouse wheel + Shift | Pan (trackpad) | — |
| Double-click | Set focal point | Set focal point (switches to orbit) |

## Touch Controls

| Action | Orbit Mode | Fly Mode |
|--------|------------|----------|
| Single finger drag | Orbit | Look around |
| Two-finger drag | Pan | — |
| Two-finger pinch/spread | Zoom | Move forward/backward |

## Keyboard Controls

| Key | Action |
|-----|--------|
| **W/A/S/D** | Fly forward/left/backward/right |
| **Q/E** | Fly down/up |
| **Shift** | 10× faster fly speed |
| **Ctrl** | 0.1× slower fly speed |
| **V** | Toggle between orbit and fly modes |
| **C** | Reset camera to default position |
| **F** | Focus camera on selection |

## Camera Settings

Access camera settings through the **View Options** panel (gear icon):

| Setting | Range | Default | Description |
|---------|-------|---------|-------------|
| **Field of View** | 10° - 120° | 75° | Camera viewing angle |
| **Fly Speed** | 0.1 - 30 | 1 | Speed of WASD navigation |

--------------------------------------------------------------------------------

## Data Panel

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/editing/supersplat/data-panel/

The SPLAT DATA panel provides statistical visualization and analysis tools for understanding the distribution of data properties across your Gaussian splats. It displays an interactive histogram that allows you to view and select splats based on their attribute values.

[Image: SPLAT DATA Panel]

## Overview

The panel consists of three main sections:

1. **Data Selector** - Choose which splat attribute to visualize
2. **Histogram Display** - Interactive visualization of the selected data distribution  
3. **Statistics** - Total counts for splat states

The panel is located at the bottom of the interface and is collapsed by default.

:::tip
The panel can be toggled open/closed by pressing the **D** key or by clicking the panel header.
:::

## Data Selector

The dropdown menu allows you to select from various splat properties for visualization:

| Property | Category | Description | Range/Calculation |
|----------|----------|-------------|-------------------|
| X | Position | Position coordinate along the X axis | - |
| Y | Position | Position coordinate along the Y axis | - |
| Z | Position | Position coordinate along the Z axis | - |
| Scale X | Scale | Size of the Gaussian along the X axis | - |
| Scale Y | Scale | Size of the Gaussian along the Y axis | - |
| Scale Z | Scale | Size of the Gaussian along the Z axis | - |
| Red | Color | Red channel intensity | 0-1 |
| Green | Color | Green channel intensity | 0-1 |
| Blue | Color | Blue channel intensity | 0-1 |
| Hue | Color | Color hue | 0-360° |
| Saturation | Color | Color saturation | 0-1 |
| Value | Color | Color brightness/value | 0-1 |
| Opacity | Color | Transparency of each splat | 0-1 |
| Distance | Derived | Distance from the origin (0,0,0) | √(x² + y² + z²) |
| Volume | Derived | Calculated volume of each Gaussian | scale_x × scale_y × scale_z |
| Surface Area | Derived | Approximate surface area | scale_x² + scale_y² + scale_z² |

## Histogram Visualization

The histogram displays the distribution of the selected property across all non-deleted splats in the scene, with blue bars representing unselected splats and yellow bars representing selected splats. Locked and deleted splats are not shown in the histogram.

### Interactive Features

**Hover Information** - Hover over any histogram bar to see a tooltip displaying:

- **value** - The data value for this bin
- **cnt** - Total count of splats in this bin  
- **percentage** - Percentage of total splats
- **sel** - Number of selected splats in this bin

**Selection by Range** - Click and drag on the histogram to highlight a range of values. A dashed yellow rectangle will appear showing your selection. Release to select all splats within that value range. Note that locked and deleted splats cannot be selected through the histogram.

Use modifier keys to control the selection operation:

| Modifier Key | Action | Description |
|--------------|--------|-------------|
| None | New Selection | Replaces the current selection with the histogram range |
| Shift | Add to Selection | Adds the histogram range to the existing selection |
| Ctrl | Remove from Selection | Removes the histogram range from the existing selection |

**Log Scale** - Enable the Log Scale checkbox to view the histogram with a logarithmic Y-axis. This is useful when data has a wide range of values, when most splats cluster around certain values with few outliers, or when you want to better visualize the distribution of sparse data.

## Statistics (Totals)

The bottom section displays real-time statistics about the current splat:

| Statistic | Description |
|-----------|-------------|
| Splats | Total number of non-deleted splats |
| Selected | Number of currently selected splats |
| Locked | Number of locked splats |
| Deleted | Number of deleted splats |

These values update automatically when splats are selected, deselected, locked, unlocked, deleted, or reset.

## Use Cases

**Finding Outliers** - Select a property like "Distance" or "Opacity", look for isolated bars at the extremes, then click and drag to select those ranges for examination or deletion.

**Color-Based Selection** - Choose "Hue", "Saturation", or "Value" and select a range to isolate splats of similar colors, useful for separating objects by color in the scene.

**Size-Based Selection** - Select "Volume" or "Surface Area", enable "Log Scale" for better visualization, then select ranges to find problematic splats for cleanup or adjustment.

**Quality Control** - Review the distribution of properties to check if opacity is reasonable, verify scale values are within expected ranges, and identify areas that may need cleanup.

## Tips

- Keep the panel collapsed when not in use to improve editor performance
- Check different properties to understand your data better  
- Use histogram selection together with other selection tools for precise control
- Understanding your splat distribution helps with optimization and quality improvements

## Technical Notes

### Data Transformations

Some properties are transformed for visualization to ensure the histogram displays human-readable values rather than the internal storage format:

| Property Type | Transformation |
|---------------|----------------|
| Scale values | Exponentiated: exp(value) |
| Color values (RGB) | Converted from spherical harmonics: 0.5 + value × 0.28209479177387814 |
| Opacity | Sigmoid function: 1 / (1 + exp(-value)) |

### Suppressed Properties

The following properties are not available for histogram visualization: **state** (internal state flag), **transform** (internal transformation data), and **f_rest_0** through **f_rest_44** (higher-order spherical harmonic coefficients).

--------------------------------------------------------------------------------

## Editing Splats

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/editing/supersplat/editing-splats/

## Edit Modes

SuperSplat functions in one of two **_edit modes_**:

- Centers Mode
- Rings Mode

These modes affect how selections work and what you see in the viewport.

### Centers Mode

In centers mode:

- Gaussians are overlaid with a blue dot at their center.
- Selections apply to all Gaussian centers independent of their screen depth.
- Centers are colored depending on their selection state. By default, blue is used for unselected Gaussians and yellow for selected Gaussians.
- You can control the size that centers are rendered in the VIEW OPTIONS panel.

<img width="1224" alt="Screenshot 2025-01-06 at 08 51 53" src="/img/user-manual/gaussian-splatting/editing/supersplat/centers-mode.png" />

### Rings Mode

In rings mode:

- Gaussians are overlaid with a ring at their outer boundary.
- Selections apply to the top-most layer of gaussian rings only.
- Selected Gaussians are colored yellow (by default).

<img width="1224" alt="Screenshot 2025-01-06 at 08 51 58" src="/img/user-manual/gaussian-splatting/editing/supersplat/rings-mode.png" />

### Disabling the Overlay

The mode overlay can be disabled entirely (using space bar shortcut), so neither dots nor rings are displayed.

However, please note that the selection behavior is still determined by the active mode.

<img width="1224" alt="Screenshot 2025-01-06 at 08 51 48" src="/img/user-manual/gaussian-splatting/editing/supersplat/disable-overlay.png" />

## Selecting and Deleting Splats

Cropping splats or deleting unwanted Gaussians is a key function of SuperSplat. To help with this, there are 8 selection tools available:

<div class="no-wrap-first-col">

| Tool | Description |
|------|-------------|
| [Image: Picker Select] **Picker Select** | Click to select a single splat, or click + drag to create a rectangular selection area. This is the default selection tool. |
| [Image: Lasso Select] **Lasso Select** | Click and drag to draw a freeform shape. Splats within the shape's outline will be selected. This is a 2D screen-space selection tool. |
| [Image: Polygon Select] **Polygon Select** | Click to place points that define the vertices of a polygon. Double-click to close the shape. Splats within the polygon will be selected. This is useful for precise selections with straight edges. |
| [Image: Brush Select] **Brush Select** | Click and drag to paint a selection using a circular brush. Adjust the brush size with the `[` (decrease) and `]` (increase) keys. Ideal for organic selection work. |
| [Image: Flood Select] **Flood Select** | Click on the viewport to generate a 2D selection mask based on a flood fill algorithm. A threshold slider (0-1) controls the sensitivity of the flood fill. This tool is particularly useful for selecting and deleting stray Gaussians (also known as floaters) that appear isolated in the scene. |
| [Image: Eyedropper Select] **Eyedropper Select** | Click on the viewport to select splats based on color similarity. A threshold slider (0-1) controls the sensitivity of the color matching. This tool is useful for selecting groups of splats that share similar colors. |
| [Image: Sphere Select] **Sphere Select** | Creates a 3D spherical volume for volumetric selection. Double-click anywhere in the scene to position the sphere center. Use the translate gizmo to move the sphere, and adjust the radius value in the toolbar. Click **Set** to replace the current selection, **Add** to add to the selection, or **Remove** to subtract from the selection. |
| [Image: Box Select] **Box Select** | Creates a 3D rectangular volume for volumetric selection. Double-click anywhere in the scene to position the box center. Use the translate gizmo to move the box, and adjust the dimensions (LenX, LenY, LenZ) in the toolbar. Click **Set** to replace the current selection, **Add** to add to the selection, or **Remove** to subtract from the selection. This is ideal for selecting splats within a specific region of 3D space. |

</div>

### Selection Modifiers

All 2D selection tools (Picker, Lasso, Polygon, Brush, Flood, Eyedropper) support modifier keys to control how the selection is applied:

| Modifier | Action |
|----------|--------|
| **None** | Replace the current selection with the new selection |
| **Shift** | Add to the current selection |
| **Ctrl / Cmd** | Remove from the current selection |

The 3D selection tools (Sphere Select, Box Select) have **Set**, **Add**, and **Remove** buttons in their toolbar instead of using modifier keys.

### Deleting Splats

Once you are happy with your selection, you can delete it with the **Delete** key.

## Transforming Splats

SuperSplat can translate, rotate and scale splats. To do this, select a splat in the Scene Manager and activate one of the gizmos via the horizontal icon bar.

To achieve fine grain control over the transform of the selected splat, you can use the TRANSFORM panel (below the SCENE MANAGER panel).

To set the origin of the currently active gizmo, double click anywhere in the 3D view.

## Measuring and Rescaling Splats

SuperSplat provides a measurement tool to help you measure distances within your splat scene and rescale it based on real-world measurements. This is accessible via the Measurement icon in the bottom toolbar.

**To use the measurement tool:**

1. Click the Measurement icon in the bottom toolbar to activate the tool.
2. Click in the Viewport to place the first marker.
3. Click again to place the second marker.
4. Click on either marker to activate the translation gizmo and tweak its location.
5. The length between the two markers is displayed in a popup above the bottom toolbar.

**To rescale the scene:**

- Edit the length value in the popup to rescale the entire scene based on that modified length.

**Keyboard shortcuts:**

- Press `Delete` to remove the placed markers.
- Press `Escape` to deactivate the Measurement tool.

## Merging Splats

It is possible to merge multiple `.ply` files together and output a single, combined `.ply` file. Simply load any number of `.ply` files into Scene Manager, perform whatever transformations and edits you require, and then save the result via the `File` > `Save` menu item.

--------------------------------------------------------------------------------

## Import and Export

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/editing/supersplat/import-export/

SuperSplat's import and export capabilities are essential for working with Gaussian Splat data throughout your entire workflow. Import allows you to bring in splat scenes from various capture tools and formats for editing, cleanup, and optimization. Once your editing work is complete, export enables you to save your refined splats in the optimal format for your target platform - whether that's a compressed format for web deployment, a full-quality PLY for archival, or a standalone HTML viewer for easy sharing. This flexibility ensures SuperSplat can integrate seamlessly into any Gaussian Splat production pipeline.

## Supported File Formats {#supported-file-formats}

SuperSplat works with several file formats for Gaussian Splat scenes:

| Format | Import | Export | Description |
| ------ | ------ | ------ | ----------- |
| `.ply` | ✅ | ✅ | Standard PLY format - most common interchange format, widely supported but heavyweight |
| `.compressed.ply` | ✅ | ✅ | Compressed PLY format - far smaller than uncompressed PLY, quantizes data. [Learn more](https://blog.playcanvas.com/compressing-gaussian-splats/) |
| `.sog` | ✅ | ✅ | Bundled super-compressed format (a zip file containing `meta.json` and `.webp` textures). Recommended for runtime applications |
| `meta.json` | ✅ | ❌ | Unbundled super-compressed format (accompanied by `.webp` textures). Use [SplatTransform](../splat-transform.md) CLI tool to export |
| `.splat` | ✅ | ✅ | Legacy compressed splat format (antimatter15) - less efficient than compressed PLY |
| `.lcc` | ✅ | ❌ | XGRIDS proprietary format which contains multiple levels-of-detail. Imports the highest LOD that contains less than 20 million Gaussians |
| `.html` / `.zip` | ❌ | ✅ | Standalone HTML viewer app - embeds compressed splat data for web sharing |

:::warning

Only `.ply` files containing 3D Gaussian Splat data can be loaded - other PLY file types will fail to import.

:::

## Importing Splats

SuperSplat can import Gaussian Splat scenes in `.ply`, `.compressed.ply`, `.splat`, `.lcc`, `.sog` (bundled SOG) and `meta.json` (unbundled SOG) formats.

There are four ways to load a Gaussian Splat file:

1. **Drag and drop** - Drop one or more splat files from your file system into SuperSplat's client area. For multi-file formats (such as `.lcc` or unbundled SOG), drag the parent folder containing those files.
2. **File menu** - Select `File` > `Import` and choose one or more splat files from your file system.
3. **Direct file opening** - If you have installed SuperSplat as a PWA, you can double-click a splat file in File Explorer (Windows) or Finder (macOS).
4. **URL loading** - Use the `load` query parameter in the form: `https://superspl.at/editor?load=<PLY_URL>`. For example:

    https://superspl.at/editor?load=https://raw.githubusercontent.com/willeastcott/assets/main/biker.ply

    This is particularly useful for sharing splats with others on social platforms like X and LinkedIn.

### Importing PLY Sequences {#ply-sequences}

SuperSplat supports importing sequences of PLY files to create splat animations. This allows you to view animated Gaussian Splats where each PLY file represents a single frame in the animation.

To import a PLY sequence:

1. Ensure your PLY files follow a naming convention with sequential frame numbers appended, such as:
   - `animation_0001.ply`
   - `animation_0002.ply`
   - `animation_0003.ply`
   - etc.

2. Load the sequence into SuperSplat by either:
   - **Dragging and dropping** all the PLY files from your file system into SuperSplat
   - **Dragging and dropping** a folder containing the PLY files onto SuperSplat
   - Using **File > Import** and selecting multiple PLY files

Once loaded, SuperSplat will automatically recognize the sequence and enable the [Timeline](timeline.md) panel, allowing you to:

- Step through frames using the arrow buttons
- Play the animation using the play button
- Scrub through the animation using the timeline slider

:::note

PLY sequences are memory-intensive since each frame loads a complete splat scene. For optimal performance, consider the file sizes and number of frames when working with animated splats.

:::

## Exporting Splats

To export your currently loaded scene, open the `File` > `Export` submenu and select your desired format. All formats with export support listed in the [Supported File Formats](#supported-file-formats) table above are available.

For information about exporting and hosting HTML viewers for your splats, see the [Publishing](publishing.md#self-hosting-the-supersplat-viewer) guide.

--------------------------------------------------------------------------------

## Interface Overview

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/editing/supersplat/interface/

The SuperSplat Editor's interface appears as follows:

[Image: supersplat-interface]

The key elements of the interface are labeled:

## Menu Bar

The Menu Bar gives you access to the most commonly used functions in the SuperSplat Editor.

* **File Menu:** Load and save your projects. Import and export to/from popular file formats.
* **Select Menu:** Perform common selection based operations on your loaded scene.
* **Render Menu:** Render an image or a video of your currently loaded scene.
* **Help Menu:** Access developer resources related to SuperSplat.

### Scene Manager

The Scene Manager allows you to manage multiple loaded Gaussian Splat scenes (typically loaded from imported PLY files). You can also use this panel to set the transform of the selected scene.

### Viewport

The 3D viewport where you can visually edit your scenes. The viewport has a 2D grid to help orient yourself in the scene. By default, the grid lies in the XZ plane (with the world Y axis pointing upwards). The X axis is colored red and the Z axis is blue. Major grid divisions occur at 1 meter intervals (and smaller grid divisions are at 10cm intervals).

### View Cube

The View Cube is a gizmo that gives added control over the viewport camera. You can click on any of the circles to switch to an orthographic view of the scene from one of six directions. This can make it easier to make accurate selections in certain circumstances.

### Right Toolbar

This vertical toolbar contains icons related to splat visualization, camera controls and application settings.

### Bottom Toolbar

This horizontal toolbar contains icons related to undo/redo, splat selection and transformation.

### Animation Timeline

The Timeline allows you to create and delete keyframes for the viewport camera. It also allows you to play back (or scrub through) the animation you have configured.

### Histogram Panel

This panel allows you to inspect the Gaussian data in your scenes, visualized as a histogram along with a set of numerical statistics.

--------------------------------------------------------------------------------

## Managing Projects

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/editing/supersplat/managing-projects/

SuperSplat supports the `.ssproj` file format, allowing you to save and reload your work, preserving all settings and animations. This format provides an efficient way to manage projects, share work, and continue editing at a later time.

## Features of `.ssproj` Files

* Full Project Save: Stores all relevant project data, including app settings, timeline settings and more.
* Easy Reloading: Load a saved project file to restore the exact state of your work.
* Portability: Share `.ssproj` files with others to collaborate on projects.

## Saving a Project

To save your current work as a `.ssproj` file:

* Open the `File` menu.
* Select `Save As`.
* Choose a location and enter a name for your project.
* Click Save. Your project will be stored as a `.ssproj` file.

## Loading a Project

To load a previously saved `.ssproj` file:

* Open the `File` menu.
* Select `Open`.
* Browse to the `.ssproj` file you wish to open.
* Click `Open`. SuperSplat will restore your project to its last saved state.

## File Structure

The `.ssproj` format is actually a ZIP archive containing:

* A JSON-based file that stores project-specific metadata, such as app settings and timeline settings.
* One or more uncompressed `.ply` files (these are listed in the `Scenes` panel on load).

## Best Practices

* Save Regularly: To prevent data loss, save your project frequently.
* Use Versioning: When working on major changes, save multiple versions of your project to avoid accidental overwrites.
* Backup Your Files: Store backups of important `.ssproj` files in a cloud storage service or external drive.

By utilizing the `.ssproj` format, SuperSplat users can efficiently manage and share their projects, enhancing workflow and collaboration.

--------------------------------------------------------------------------------

## Publishing

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/editing/supersplat/publishing/

SuperSplat provides two ways to share your splat scenes with the world:

1. **Publishing to SuperSplat** - Host your splats on [superspl.at](https://superspl.at) for easy sharing
2. **Self-Hosting the SuperSplat Viewer** - Export a standalone HTML viewer to host anywhere

## Publishing to SuperSplat

The SuperSplat Editor allows you to publish your splat scenes to the web at [https://superspl.at/](https://superspl.at/).

[Image: SuperSplat Website]

### Ensuring you are Logged In

Publishing splats requires you to be logged in.

Before you begin, you must have [created a PlayCanvas account](/user-manual/account-management/user-accounts/account-creation) and be logged in at [playcanvas.com](https://playcanvas.com). You must then also log in at [superspl.at](https://superspl.at) (via the `Login` button in the top right). Verify you are logged in by checking that your account avatar is displayed in the top right of the page on [superspl.at](https://superspl.at).

### Publishing your Splats

To publish your splat:

1. Open the `File` menu.
2. Select `Publish`.
3. Fill out the options in the Publish dialog:

   [Image: Publish Settings]

   | Option | Description |
   |--------|-------------|
   | **Publish to** | Select where to publish your scene. Choose **New Scene** (default) to publish a brand new scene on a new URL, or select one of your existing published scenes from the dropdown to overwrite it |
   | **Title** | A short title that will appear below your splat's thumbnail once it is published |
   | **Description** | A textual description of your splat that will be displayed under the splat on its viewer page |
   | **Listed** | If checked, the splat will be returned in searches on the SuperSplat website. If unchecked, the splat will only be discoverable by anyone who has the link |
   | **Start Position** | The starting position to use for the viewer's camera:<br/>• **Default**: The viewer does its best to pick a suitable start point<br/>• **Current Viewport**: Use the current camera position as set in the SuperSplat Editor's viewport<br/>• **1st Camera Frame**: Use the first camera's position as defined by the first frame of the Timeline |
   | **Animation** | The animation to apply to the viewer's camera:<br/>• **None**: No animation<br/>• **Track**: Animate the camera using keyframes set on the SuperSplat Editor's Timeline |
   | **Background** | The background color of the viewer |
   | **Field of View** | The vertical field of view of the viewer's camera in degrees |
   | **SH Bands** | The number of spherical harmonics bands to be written out to the published compressed PLY file |

4. Select `Publish`.

:::note

It may take several minutes to compress your splat to SOG format during the publishing process, so be patient! ⏳

:::

Once the publish process is complete, a modal dialog will show with the URL of your published splat. Copy it and share it with whoever you like.

### Managing your Published Splats

After publishing splats to [superspl.at](https://superspl.at), you can manage them by visiting your [Manage page](https://superspl.at/manage). From here, you can perform the following actions on each published splat:

| Action | Description |
|--------|-------------|
| **Edit Title** | Update the splat's main title |
| **Edit Description** | Update the description shown on the splat's viewer page |
| **List/Unlist** | Toggle whether the splat appears in public searches on the SuperSplat website |
| **Delete** | Permanently remove the splat from superspl.at (this cannot be undone) |

## Self-Hosting the SuperSplat Viewer

The viewer used on the SuperSplat website is [open source](https://github.com/playcanvas/supersplat-viewer) and can be directly exported by the SuperSplat Editor should you wish to self-host your splat content. This viewer runs in any web browser with easy-to-use camera controls and even supports AR and VR visualization for devices that support WebXR.

### Exporting the HTML Viewer

To export your splat as an HTML viewer:

1. Open the `File` > `Export` submenu.
2. Select `Viewer App…`.

### Export Options

The viewer export can be configured via several options:

[Image: Viewer Export]

| Option | Description |
|--------|-------------|
| **Export Type** | Controls the format of the exported viewer:<br/>• **HTML**: A single-page `.html` file where the splat is Base64 encoded and embedded directly into the file. Very convenient since everything is packed into a single file that can be double-clicked to run in your browser using the `file://` protocol. However, the base64 encoding means it will be roughly 30% larger than the ZIP Package format, and browsers impose different limits on the maximum size (below 32MB should load everywhere, but above this you could encounter problems)<br/>• **ZIP Package**: A zip file containing the viewer `.html` file and a separate `.compressed.ply` containing the splat. Smaller, loads faster and guaranteed to load everywhere. However, it will only load over `http://`, so you will need to run a local web server (e.g. Node's [`serve`](https://www.npmjs.com/package/serve) or Python's [`SimpleHTTPServer`](https://docs.python.org/2/library/simplehttpserver.html)) |
| **Start Position** | The starting position to use for the viewer's camera:<br/>• **Default**: The viewer does its best to pick a suitable start point<br/>• **Current Viewport**: Use the current camera position as set in the SuperSplat Editor's viewport<br/>• **1st Camera Frame**: Use the first camera's position as defined by the first frame of the Timeline |
| **Animation** | The animation to apply to the viewer's camera:<br/>• **None**: No animation<br/>• **Track**: Animate the camera using keyframes set on the SuperSplat Editor's Timeline |
| **Background** | The background color of the viewer |
| **Field of View** | The vertical field of view of the viewer's camera in degrees |
| **SH Bands** | The number of spherical harmonics bands to be written out to the published compressed PLY file |

### Web Hosting for the HTML Viewer

Once exported, you can host the HTML Viewer file somewhere to make it accessible. One easy option is to use GitHub Pages:

1. Create a new repository on [GitHub](https://github.com).
2. Add the exported HTML file (and the `.compressed.ply` file if you exported the viewer as a ZIP package).
3. Visit your repository's `Settings` page. Select `Pages` on the left. Ensure `Source` is set to `Deploy from a branch` and set `Branch` to `main` and hit `Save`.
4. It will take a few moments for your viewer to be published. The URL will be in the form:

   `https://<github-username>.github.io/<repository-name>/<html-filename>`

--------------------------------------------------------------------------------

## Rendering Media

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/editing/supersplat/rendering/

SuperSplat enables you to render high-quality images and videos directly from your 3D Gaussian Splats. Whether you want to create a single frame for social media, showcase your creations with animated videos, embed them on your website, or keep them locally, this guide will help you get started.

:::important

Both images and videos are rendered entirely locally on your device. No render data or splat information is uploaded to any external server.

:::

## Getting Started

To render from your splat:

1. Open your splat in the [SuperSplat Editor](https://superspl.at/editor).
2. Position your camera for the desired view (for images) or set up camera animations on the [Timeline](timeline.md) (for videos).
3. Click the `Render` menu in the menu bar and select either `Image` or `Video`.

## Image Rendering

Image rendering creates a single high-quality frame from your current camera position.

### Image Settings

When you select `Image` from the Render menu, a dialog will appear with settings to customize your image:

- **Preset:** Choose from the following preset options:
  - `Current` (default) - Uses the current viewport resolution
  - `HD` - `1920x1080` resolution
  - `QHD` - `2560x1440` resolution  
  - `4K` - `3840x2160` resolution
  - `Custom` - Allows manual resolution selection
- **Resolution:** Only active when `Custom` preset is selected. Allows you to specify custom width and height values.
- **Transparent Background:** If checked, the rendered image will have a transparent background instead of the scene's background color. This is useful for creating images that can be composited over other content or used as overlays.
- **Show Debug Overlays:** If checked, whatever splat visualization mode (centers or rings) is active in the Editor will be rendered to the image.

### Rendering the Image

Once you've configured your settings:

1. Click the `Render` button.
2. The image will be rendered at the current camera position.
3. On completion, your image will auto-download.

## Video Rendering

Video rendering creates an animated sequence using camera animations from the Timeline.

### Video Settings

When you select `Video` from the Render menu, a dialog will appear with several settings to customize your video:

- **Resolution:** Choose from the following resolutions:
  - `960x540`
  - `1280x720`
  - `1920x1080` (default)
  - `2560x1440`
  - `3840x2160`
- **Format:** Choose the output video format:
  - `MP4` (default) - Most widely compatible format, ideal for general use
  - `WebM` - Open format, excellent for web use
  - `MOV` - Apple QuickTime format, ideal for professional video editing workflows
  - `MKV` - Flexible open container format supporting various codecs
- **Codec:** Choose the video codec for compression. Available options depend on the selected format:
  - For MP4 and MOV: `H.264` (default), `H.265/HEVC`
  - For WebM: `VP9` (default), `AV1`
  - For MKV: `H.264` (default), `H.265/HEVC`, `VP9`, `AV1`
- **Frame Rate:** Choose from the following frame rates:
  - `12 fps`
  - `15 fps`
  - `24 fps`
  - `25 fps`
  - `30 fps` (default)
  - `48 fps`
  - `60 fps`
  - `120 fps`
- **Bitrate:** Adjust the quality of your video by choosing a higher or lower bitrate. Options are:
  - `Low`
  - `Medium`
  - `High` (default)
  - `Ultra`
- **Frame Range:** Specify the start and end frames from the Timeline to include in your video. By default, the entire Timeline is rendered.
- **Portrait Mode:** If checked, the resolution will be flipped to be vertical. This is useful if you intend your video to be viewed in portrait on mobile.
- **Show Debug Overlays:** If checked, whatever splat visualization mode (centers or rings) is active in the Editor will be rendered to the video.

### Rendering the Video

Once you've configured your settings:

1. Click the `Render` button.
2. The Timeline pointer will animate through all frames until video encoding is complete.
3. On completion, your video will auto-download.

:::tip Browser Recommendation for Video Rendering

We recommend using a Chromium-based browser (Chrome, Edge, Opera, etc.) for rendering videos. Browsers that support the [FileSystem API](https://caniuse.com/native-filesystem-api) will ask you to pick a destination filename and will then stream out the file during the encoding process instead of having to do it in memory. This allows for the recording of much larger videos without running into memory limitations.

:::

## Tips for Best Results

- **For Images:** Position your camera carefully and consider the composition before rendering. Higher resolutions will provide more detail for print or web use.
- **For Videos:** Before rendering, ensure your Timeline animations are smooth and camera movements are polished.
- **For Social Media:** Portrait-mode renders often yield better engagement on mobile platforms.
- **Debug Overlays:** Use the debug overlay option when you need to show splat data for educational or technical presentations.

Happy Rendering! 🎬📸

--------------------------------------------------------------------------------

## Timeline

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/editing/supersplat/timeline/

The Timeline panel allows you to create camera animations for your Gaussian Splat scenes. You can set keyframes to define camera positions and movements, creating smooth animated transitions that can be played back in the editor.

[Image: Timeline Panel]

## Timeline Controls

The Timeline panel includes the following controls:

- **Play/Stop button** - Play or stop the animation playback
- **Next/Previous Key buttons** - Jump to the next or previous keyframe
- **Add Key button (`+`)** - Create a new keyframe at the current timeline position
- **Remove Key button (`-`)** - Delete the keyframe at the current playhead position
- **Timeline slider** - Scrub through the animation by dragging the playhead (which displays the current frame number)
- **FPS setting** - Set the frames per second for the animation
- **Timeline length** - Set the total duration of the animation in frames
- **Smoothness** - Adjust the interpolation smoothness between keyframes (0-1)

## Creating Keyframes

To create a camera animation keyframe:

1. Position the timeline playhead at the desired frame (by clicking on the timeline or using the frame counter)
2. Move and orient the viewport camera to the desired position
3. Click the `+` (Add Keyframe) button on the Timeline

The keyframe will appear as a yellow diamond on the timeline at the current frame position. Keyframes store the camera's position and rotation at that moment in time.

## Importing Camera Poses as Keyframes

SuperSplat can import camera poses from [COLMAP](https://colmap.github.io/) reconstructions to automatically create timeline keyframes. This feature allows you to recreate the approximate camera path that was used during the original scanning of your Gaussian Splat scene.

To import camera poses:

1. Obtain the [`images.txt`](https://colmap.github.io/format.html#images-txt) file from your COLMAP reconstruction output
2. In SuperSplat, select **File > Import** (or drag and drop the `images.txt` file)
3. SuperSplat will create keyframes from the camera poses in the file

Each camera pose in the `images.txt` file will be placed as a keyframe on sequential frames of the timeline, in the order they appear in the file. This creates an animation that follows the original capture path.

:::note

The `images.txt` file contains camera poses expressed as quaternions and translation vectors in COLMAP's coordinate system. SuperSplat automatically converts these to the appropriate camera positions and orientations for the timeline.

:::

## Editing Keyframes

Once keyframes are created, you can modify them in two ways:

### Changing Keyframe Timing

Click and drag the yellow diamond keyframes along the timeline to adjust when each camera position occurs in your animation.

### Updating Camera Position/Rotation

To change the camera position or rotation stored in an existing keyframe:

1. Move the playhead to the frame containing the keyframe you want to update
2. Adjust the viewport camera to the new desired position and rotation
3. Click the `+` (Add Key) button - this will overwrite the existing keyframe with the updated camera values

## Deleting Keyframes

To delete a keyframe:

1. Move the playhead to the frame containing the keyframe you want to delete
2. Click the `-` (Remove Key) button

## Configuring Timeline Settings

### Frames Per Second (FPS)

The FPS setting controls the playback speed of your animation. Available FPS values are:

- **1 FPS** - Very slow, single frame per second
- **6 FPS** - Low frame rate
- **12 FPS** - Medium frame rate
- **24 FPS** - Standard film frame rate
- **30 FPS** - Common video frame rate
- **60 FPS** - Smooth, high frame rate playback

### Timeline Length

You can set the total duration of your animation by adjusting the timeline length setting. This defines how many frames your animation spans and determines when the animation loops back to the beginning.

### Smoothness

The Smoothness setting controls how the camera interpolates between keyframes. This value ranges from 0 to 1:

- **0** - Linear interpolation, creating direct straight-line motion between keyframes
- **1** - Maximum smooth interpolation, creating more curved, cinematic camera movements
- **Values in between** - Blend between linear and smooth interpolation

Adjusting smoothness allows you to control the feel of your camera animation, from mechanical and precise to flowing and organic.

## Playing Animations

Use the play button to preview your animation. The camera will interpolate between keyframes based on your smoothness setting, creating fluid motion. The animation will loop continuously from the first frame back to the beginning.

### Animation Looping

The animation automatically loops from the last frame back to the first frame. To create seamless loops, it's important to avoid setting a keyframe on both the first and last frames of the timeline, as this will cause a sudden snap when the animation loops.

:::tip[Best Practice for Smooth Loops]

Instead of placing keyframes on both the first and last frames, place your final keyframe before the last frame and let the animation interpolate smoothly back to the beginning.

For example, with a 100-frame timeline:

- **Good:** Set keyframes at frames 0, 10, 20, 30, 40, 50, 60, 70, 80, 90
  - The animation smoothly interpolates from frame 90 back to frame 0
- **Avoid:** Setting keyframes at frames 0, 10, 20, ... 90, 100
  - This causes a snap/jerk when looping from frame 100 back to frame 0

This technique allows the camera to smoothly transition from your last keyframe back to the starting position, creating a seamless loop.

:::

## Saving Timeline Animations

:::important

Timeline animations are only preserved when you save your project as an `.ssproj` file. The timeline configuration is not saved when exporting to PLY or other splat formats.

See [Managing Projects](managing-projects.md) for more information on saving and loading SuperSplat project files.

:::

--------------------------------------------------------------------------------

## Splat File Formats

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/formats/

PlayCanvas supports two formats for 3D Gaussian Splat data:

## [PLY Format](./ply.md) - Source & Interchange

The industry standard for Gaussian splat data. Uncompressed, full precision, and universally compatible.

- **Use for**: Training, editing, archival storage
- **File size**: Large (anything up to several GB)
- **Quality**: Lossless

## [SOG Format](./sog.md) - Runtime & Delivery

Compressed format optimized for web delivery. 15-20× smaller than PLY with lossy compression.

- **Use for**: Web apps, real-time rendering, CDN delivery
- **File size**: Small (compressed)
- **Quality**: Visually optimized

## Quick Comparison

| | PLY | SOG |
|---|---|---|
| **Size** | Large | Small (15-20× compression) |
| **Quality** | Lossless | Lossy |
| **Use** | Source/editing | Runtime/delivery |
| **Speed** | Slow loading | Fast loading |

## Workflow

1. Train and edit with **PLY**
2. Convert to **SOG** for production using [SplatTransform](../editing/splat-transform.md)
3. Deploy SOG files for optimal performance

--------------------------------------------------------------------------------

## The PLY Format

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/formats/ply/

[PLY](https://en.wikipedia.org/wiki/PLY_(file_format)) (Polygon File Format) is the standard file format for storing 3D Gaussian Splat data. While PLY has been around since the 1990s as a format for storing 3D mesh data, its use in Gaussian Splatting represents a specialized application with unique characteristics and considerations.

## What is the PLY Format?

PLY is a simple, flexible file format originally designed by Stanford University for storing 3D scanner data. It can store various types of 3D geometry including vertices, faces, colors, and custom properties. The format supports both ASCII (human-readable text) and binary encoding.

## File Structure

A PLY file containing 3D Gaussian Splat data consists of three main sections:

### Header

```none
ply
format binary_little_endian 1.0
element vertex 500000
property float x
property float y
property float z
property float scale_0
property float scale_1
property float scale_2
property float rot_0
property float rot_1
property float rot_2
property float rot_3
property float opacity
property float f_dc_0
property float f_dc_1
property float f_dc_2
property float f_rest_0
property float f_rest_1
...
property float f_rest_44
end_header
```

### Element Definitions

The header defines the structure and properties of elements (typically vertices representing individual Gaussian splats).

### Data Section

The actual binary or ASCII data follows the header, containing the values for each property of every element.

## How 3DGS PLYs Differ from Regular PLYs

Standard PLY files typically store simple mesh geometry with basic properties like position and color. 3D Gaussian Splat PLY files are fundamentally different:

### Extended Properties

3DGS PLY files contain specialized properties for each Gaussian splat:

- **Position** (`x`, `y`, `z`): 3D location of the splat center
- **Scale** (`scale_0`, `scale_1`, `scale_2`): Size of the Gaussian along each axis
- **Rotation** (`rot_0`, `rot_1`, `rot_2`, `rot_3`): Quaternion representing splat orientation
- **Opacity**: Transparency/alpha value
- **Spherical Harmonics Coefficients**: View-dependent color encoding using two sets of properties:
  - **Direct Color Component** (`f_dc_0`, `f_dc_1`, `f_dc_2`): The base color values (RGB) representing the 0th-order spherical harmonic coefficients. These define the primary color of the splat.
  - **Higher-Order Coefficients** (`f_rest_0` through `f_rest_44`): Additional spherical harmonic coefficients that encode how the color changes based on viewing direction. These 45 coefficients are distributed across higher-order bands:
    - **1st order**: 3 coefficients × 3 color channels = 9 coefficients
    - **2nd order**: 5 coefficients × 3 color channels = 15 coefficients  
    - **3rd order**: 7 coefficients × 3 color channels = 21 coefficients

### No Traditional Geometry

Unlike regular PLY files that contain vertices and faces defining mesh topology, 3DGS PLY files contain only point data with no connectivity information. Each "vertex" represents an independent Gaussian splat.

### Massive Point Counts

3DGS PLY files typically contain hundreds of thousands to millions of points, far exceeding typical mesh vertex counts.

## PLY as a Source Format

Think of PLY files in 3DGS workflows as you would PSD files for images or project files for video editing:

### Uncompressed and Complete

- Contains full-precision data with no quality loss
- Preserves all Gaussian splat parameters in their original form
- Allows for re-processing and optimization without degradation

### Archive and Backup

- Essential for long-term storage of your 3DGS captures
- Enables future re-processing with improved algorithms
- Serves as the canonical version of your 3D scene

### Quality Reference

- Maintain PLY files as your highest-quality reference
- Generate compressed formats from PLY for specific use cases
- Always keep the original PLY for quality comparisons

## PLY as an Interchange Format

The PLY format serves as the lingua franca of the 3DGS ecosystem:

### Universal Compatibility

- **Training Software**: Brush, nerfstudio, Postshot
- **Editors**: [SuperSplat Editor](../editing/supersplat/index.md)
- **Converters**: [SplatTransform](../editing/splat-transform.md)
- **Viewers**: [SuperSplat Viewer](https://github.com/playcanvas/supersplat-viewer), [Model Viewer](https://github.com/playcanvas/model-viewer)

### Cross-Platform Workflow

- Move assets between different 3DGS training pipelines
- Share datasets with collaborators regardless of their toolchain
- Maintain consistency across different processing stages

### Research and Development

- Standard format for academic research and paper submissions
- Enables reproducible results across different implementations
- Facilitates algorithm development and comparison

## Runtime Considerations

While PLY is excellent for source and interchange purposes, it has significant limitations for real-time applications:

### File Size Issues

- **Uncompressed**: No data compression leads to massive file sizes
- **Typical Sizes**: Can range from 50MB to several GB per scene
- **Network Transfer**: Impractical for web delivery without preprocessing
- **Storage Costs**: Expensive for cloud storage and CDN distribution

### Loading Performance

- **Parse Time**: Text parsing (ASCII PLY) is particularly slow
- **Memory Usage**: Entire file must be loaded into memory
- **Initialization**: No progressive loading or streaming capabilities

:::tip

Convert your PLYs to a more efficient run-time format using the [SplatTransform](../editing/splat-transform.md) tool. We currently recommend Self-Organizing Gaussians (AKA SOGS) to achieve the best compression ratios and fastest loading times.

:::

--------------------------------------------------------------------------------

## The SOG Format

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/formats/sog/

**SOG (Spatially Ordered Gaussians)** is a compact container for 3D Gaussian Splat data. It achieves high compression via quantization (lossy by design), typically yielding files **\~15–20× smaller** than an equivalent PLY.

You can create SOG files with **[SplatTransform](https://github.com/playcanvas/splat-transform)** and preview them in the **[PlayCanvas Viewer](https://playcanvas.com/viewer)**.

This document is the format specification.

## 1. File set

A SOG dataset is a set of images plus a metadata file:

| File                 | Purpose                             | Channels (8-bit) |
| -------------------- | ----------------------------------- | ---------------- |
| `meta.json`          | Scene metadata and filenames        | —                |
| `means_l.webp`       | Positions – lower 8 bits (RGB)      | R,G,B            |
| `means_u.webp`       | Positions – upper 8 bits (RGB)      | R,G,B            |
| `scales.webp`        | Per-axis sizes via codebook         | R,G,B            |
| `quats.webp`         | Orientation – compressed quaternion | R,G,B,A          |
| `sh0.webp`           | Base color (DC) + opacity           | R,G,B,A          |
| `shN_centroids.webp` | SH palette coefficients (optional)  | R,G,B            |
| `shN_labels.webp`    | Indices into SH palette (optional)  | R,G              |

:::note[Image formats]

* By default, images **should** be **lossless WebP** to preserve quantized values exactly.
* Each property in `meta.json` names its file, so other 8-bit RGBA-capable formats **may** be used.
* Do not use lossy encodings for these assets as lossy compression will corrupt values and can produce visible/structural artifacts.

:::

### 1.1 Image dimensions & indexing

All per-Gaussian properties are co-located: the same pixel (x, y) across all property images (except shN_centroids) belongs to the same Gaussian.

* Pixels are laid out **row-major**, origin at the **top-left**.
* For image width `W` and height `H`, the number of addressable Gaussians is `W*H`.
* `meta.count` **must** be `<= W*H`. Any trailing pixels are ignored.

**Indexing math (zero-based):**

* From index to pixel:
  `x = i % W`, `y = floor(i / W)`
* From pixel to index:
  `i = x + y * W`

### 1.2 Coordinate system

Right-handed:

* **x:** right
* **y:** up
* **z:** back (i.e., −z is “forward” in camera-looking-down −z conventions)

### 1.3 Bundled variant

A bundled SOG is a ZIP of the files above. Readers **should** accept either layout:

* **Multi-file directory** (recommended during authoring)
* **Single archive** (e.g., `scene.sog`) containing the same files at the archive root

Readers **must** unzip and then resolve files using `meta.json` exactly as for the multi-file version.

---

## 2. `meta.json`

```ts
interface Meta {
  version: 2;              // File format version (integer)
  count: number;           // Number of gaussians (<= W*H of the images)
  antialias: boolean;      // True iff scene was trained with anti-aliasing

  means: {
    // Ranges for decoding *log-transformed* positions (see §3.1).
    mins: [number, number, number];   // min of nx,ny,nz (log-domain)
    maxs: [number, number, number];   // max of nx,ny,nz (log-domain)
    files: ["means_l.webp", "means_u.webp"];
  };

  scales: {
    codebook: number[];    // 256 floats; see §3.3
    files: ["scales.webp"];
  };

  quats: {
    files: ["quats.webp"]; // §3.2
  };

  sh0: {
    codebook: number[];    // 256 floats; DC coefficients for gamma-space color (§3.4)
    files: ["sh0.webp"];
  };

  // Present only if higher-order SH exist:
  shN?: {
    count: number;         // Palette size (up to 65536)
    bands: number;         // Number of SH bands (1..3). DC (=band 0) lives in sh0.
    codebook: number[];    // 256 floats; shared for all AC coefficients (§3.5)
    files: [
      "shN_centroids.webp",// Palette of AC coefficients as pixels (§3.5)
      "shN_labels.webp"    // Per-gaussian palette index (0..count-1)
    ];
  };
}
```

:::note

* The scales codebook contains linear-space values. The sh0 and shN codebooks contain gamma-space DC coefficients.
* Image data **must** be treated as raw 8-bit integers (no gamma conversion).
* Unless otherwise stated, channels not mentioned are ignored.
* Filenames in `files` arrays are arbitrary, but the order is significant.

:::

---

## 3. Property encodings

### 3.1 Positions

> `means_l.webp`, `means_u.webp` (RGB, 16-bit per axis)

Each axis is quantized to **16 bits** across two images:

```ts
// 16-bit normalized value per axis (0..65535)
const qx = (means_u.r << 8) | means_l.r;
const qy = (means_u.g << 8) | means_l.g;
const qz = (means_u.b << 8) | means_l.b;

// Dequantize into *log-domain* nx,ny,nz using per-axis ranges from meta:
const nx = lerp(meta.means.mins[0], meta.means.maxs[0], qx / 65535);
const ny = lerp(meta.means.mins[1], meta.means.maxs[1], qy / 65535);
const nz = lerp(meta.means.mins[2], meta.means.maxs[2], qz / 65535);

// Undo the symmetric log transform used at encode time:
const unlog = (n: number) => Math.sign(n) * (Math.exp(Math.abs(n)) - 1);

const p = {
  x: unlog(nx),
  y: unlog(ny),
  z: unlog(nz),
};
```

### 3.2 Orientation

> `quats.webp` (RGBA, 26-bit “smallest-three”)

Quaternions are encoded with **3×8-bit components + 2-bit mode** (total **26 bits**) using the standard *smallest-three* scheme.

* **R,G,B** store the three kept (signed) components, uniformly quantized to `[-√2/2, +√2/2]`.
* **A** stores the **mode** in the range **252..255**. The mode is `A - 252` ∈ {0,1,2,3} and identifies which of the four components was the **largest by magnitude** (and therefore omitted from the stream and reconstructed).
* Let `norm = Math.SQRT2` (i.e., √2).

```ts
// Dequantize the stored three components:
const toComp = (c: number) => (c / 255 - 0.5) * 2.0 / Math.SQRT2;

const a = toComp(quats.r);
const b = toComp(quats.g);
const c = toComp(quats.b);

const mode = quats.a - 252; // 0..3 (R,G,B,A is one of the four components)

// Reconstruct the omitted component so that ||q|| = 1 and w.l.o.g. the omitted one is non-negative
const t = a*a + b*b + c*c;
const d = Math.sqrt(Math.max(0, 1 - t));

// Place components according to mode
let q: [number, number, number, number];
switch (mode) {
    case 0: q = [d, a, b, c]; break; // omitted = x
    case 1: q = [a, d, b, c]; break; // omitted = y
    case 2: q = [a, b, d, c]; break; // omitted = z
    case 3: q = [a, b, c, d]; break; // omitted = w
    default: throw new Error("Invalid quaternion mode");
}
```

#### Validity constraints

* `quats.a` **must** be in **252, 253, 254, 255**. Other values are reserved.

### 3.3 Scales

> `scales.webp` (RGB via codebook)

Per-axis sizes are **codebook indices**:

```ts
const sx = meta.scales.codebook[scales.r]; // 0..255
const sy = meta.scales.codebook[scales.g];
const sz = meta.scales.codebook[scales.b];
```

Interpretation (e.g., principal axis standard deviations vs. full extents) follows the source training setup; values are in **scene units**.

### 3.4 Base color + opacity (DC)

> `sh0.webp` (RGBA)

`sh0` holds the **DC (l=0)** SH coefficient per color channel and **alpha**:

* **R,G,B** are 0..255 indices into `sh0.codebook`.
* **A** is the **opacity** in `[0,1]` (i.e., `sh0.a / 255`).

To convert the DC coefficient to **gamma-space RGB** color:

```ts
// SH_C0 = Y_0^0 = 1 / (2 * sqrt(pi))
const SH_C0 = 0.28209479177387814;

const r = 0.5 + meta.sh0.codebook[sh0.r] * SH_C0;
const g = 0.5 + meta.sh0.codebook[sh0.g] * SH_C0;
const b = 0.5 + meta.sh0.codebook[sh0.b] * SH_C0;
const a = sh0.a / 255;
```

### 3.5 Higher-order SH (optional)

> `shN_centroids.webp`, `shN_labels.webp`

If present, higher-order (AC) SH coefficients are stored via a palette:

* `shN.count` ∈ **\[1,64k]** number of entries.
* `shN.bands` ∈ **\[1,3]** number of bands per entry.

#### Centroids (palette)

* `shN_centroids.webp` is an RGB image storing the SH coefficient palette.
* There are always 64 entries per row; entries are packed row-major with origin top-left.

The texture width is dependent on the number of bands:

| Bands | Coefficients | Texture width (pixels) |
|---|---|---|
| 1 | 3 | 64 * 3 = 96 |
| 2 | 8 | 64 * 8 = 512 |
| 3 | 15 | 64 * 15 = 960 |

Calculating the pixel location for spherical harmonic entry n and coefficient c:

```ts
const coeffs = [3, 8, 15];
const u = (n % 64) * coeffs[bands - 1] + c;
const v = Math.floor(n / 64);
```

#### Labels

* `shN_labels.webp` stores a **16-bit index** per gaussian with range (0..count-1).

```ts
const index = shN_labels.r + (shN_labels.g << 8);
```

---

## 4. Example `meta.json`

```json
{
  "version": 2,
  "count": 187543,
  "antialias": true,
  "means": {
    "mins": [-2.10, -1.75, -2.40],
    "maxs": [ 2.05,  2.25,  1.90],
    "files": ["means_l.webp", "means_u.webp"]
  },
  "scales": {
    "codebook": [/* 256 floats */],
    "files": ["scales.webp"]
  },
  "quats": { "files": ["quats.webp"] },
  "sh0": {
    "codebook": [/* 256 floats */],
    "files": ["sh0.webp"]
  },
  "shN": {
    "count": 128,
    "bands": 3,
    "codebook": [/* 256 floats */],
    "files": ["shN_centroids.webp", "shN_labels.webp"]
  }
}
```

---

## 5. Versioning & compatibility

* Readers **must** check `version`. This document describes **version 2**.
* Additional optional properties may appear in future versions; readers **should** ignore unrecognized fields.

---

--------------------------------------------------------------------------------

## Viewing Splats

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/viewing/

Once you've created a Gaussian splat, you'll want to preview and evaluate it before proceeding to editing or integration into your projects. The **PlayCanvas Model Viewer** provides a convenient way to quickly view and inspect your splat files without needing to set up a full PlayCanvas project.

## PlayCanvas Model Viewer

The [PlayCanvas Model Viewer](https://playcanvas.com/viewer) is a web-based tool that allows you to instantly preview 3D content, including Gaussian splats, directly in your browser.

<video autoPlay muted loop controls src='/video/playcanvas-splat-viewer.mp4' style={{width: '100%', height: 'auto'}} />

### Supported Splat Formats

The Model Viewer supports the following commonly used Gaussian splat formats:

| Format | File Extension | Description |
|--------|----------------|-------------|
| **PLY** | `.ply` | Standard uncompressed splat format |
| **Compressed PLY** | `.compressed.ply` | Compressed (quantized) format |
| **SOG (bundled)** | `.sog` | Super-compressed format in single file |
| **SOG (unbundled)** | `meta.json` + `.webp` images | Super-compressed format in multiple files |

### How to View Your Splats

1. **Visit** [playcanvas.com/viewer](https://playcanvas.com/viewer)
2. **Drag and drop** your splat from your file system onto the viewer

   :::info[Viewing unbundled SOG scenes]
   For **SOG (unbundled)** format: drag the **parent folder** containing `meta.json` and `.webp` images
   :::
3. **Navigate** the 3D scene:

   | Control | Action |
   |---------|--------|
   | Left double click | Set orbit point |
   | Left click + drag | Orbit around the splat |
   | Right click + drag | Look around |
   | Shift + click + drag | Pan the view |
   | Mouse wheel | Zoom in/out |
   | WASD or Arrow keys | Move forwards/backwards/left/right |

## Open Source and Customization

The PlayCanvas Model Viewer is **open source** and available on [GitHub](https://github.com/playcanvas/model-viewer). This means you can:

- **Host your own version** - Use a local server or deploy to your own infrastructure for complete control
- **Add new functionality** - Add support for additional file formats or custom UI
- **Contribute back** - Submit issues and pull requests to help improve the viewer for everyone

## Next Steps

After previewing your splats in the Model Viewer:

- If cleanup is needed → continue to [Editing Splats](../editing) for optimization and refinement
- If the quality meets your needs → proceed directly to [Building Splat-based Apps](../building)

--------------------------------------------------------------------------------

## What is Gaussian Splatting?

URL: https://developer.playcanvas.com/user-manual/gaussian-splatting/what-is-gaussian-splatting/

3D Gaussian Splatting is a novel approach to representing and rendering 3D scenes that has revolutionized photorealistic reconstruction. Instead of using traditional geometric representations like meshes or voxels, Gaussian Splatting represents scenes as collections of 3D Gaussian functions (or "splats") that can be efficiently rendered to produce highly detailed, photorealistic images.

## The Fundamentals

### What is a Gaussian Splat?

A Gaussian splat is a 3D ellipsoid defined by:

- **Position (x, y, z)** - The center point of the splat in 3D space
- **Covariance matrix** - Defines the size, orientation, and shape of the ellipsoid
- **Color (RGB)** - The color contribution of the splat
- **Opacity (α)** - How transparent or opaque the splat is

Each splat can be thought of as a small, semi-transparent cloud of color that contributes to the final rendered image. When millions of these splats are combined, they create detailed 3D scenes.

### Why Gaussian Functions?

Gaussian functions are chosen because they have several advantageous properties:

1. **Smooth falloff** - They fade smoothly from the center to the edges
2. **Differentiable** - Essential for optimization during training
3. **Efficient rendering** - Can be rasterized quickly on modern GPUs
4. **Compact representation** - Each splat requires only a small amount of data

## Applications and Use Cases

### Ideal Scenarios

Gaussian Splatting excels in:

- **Real-world scene capture** - Museums, historical sites, architectural spaces
- **Product visualization** - Detailed object representation for e-commerce
- **Game environments** - Photorealistic backgrounds and static geometry
- **Virtual tourism** - Immersive exploration of real locations
- **Film and media** - Digital set extensions and virtual production

### Limitations

Gaussian Splatting is less suitable for:

- **Animated content** - Each frame would require separate training
- **Procedural geometry** - Traditional meshes are more flexible for generated content
- **Interactive objects** - Editing and deformation are challenging

Understanding these fundamentals will help you make informed decisions about when and how to use Gaussian Splatting in your PlayCanvas projects. Next, let's explore how to [create your own splat data](creating/index.md).

--------------------------------------------------------------------------------

## Getting Started

URL: https://developer.playcanvas.com/user-manual/getting-started/

## What is PlayCanvas?

PlayCanvas is a ✨ **web graphics creation platform** ✨

PlayCanvas is a collection of open-source products that you can use to build interactive, graphical web applications:

🦾 [**PlayCanvas Engine**](../engine) - Open source JavaScript run-time that powers all PlayCanvas applications  
🛠️ [**PlayCanvas Editor**](../editor) - Real-time collaborative visual editing environment for building PlayCanvas applications  
⚛️ [**PlayCanvas React**](../react) - Open source React-based declarative interface that wraps the PlayCanvas Engine  
🧩 [**PlayCanvas Web Components**](../web-components) - Open source Web Components-based declarative interface that wraps the PlayCanvas Engine

## Which Product is Right For You?

Every developer is different, so picking the right entry point to PlayCanvas will depend on your background and skills. Let us help you choose:

### 🦾 [**PlayCanvas Engine**](../engine) - *Full control development*

**Best for:** Experienced programmers, performance-critical applications, custom workflows  
**Choose if you:** Want maximum control, have specific architectural needs, or are building complex systems

### 🛠️ [**PlayCanvas Editor**](../editor) - *Visual-first development*

**Best for:** Beginners, teams, rapid prototyping, visual learners  
**Choose if you:** Want drag-and-drop scene building, real-time collaboration, or prefer visual tools over code

### ⚛️ [**PlayCanvas React**](../react) - *React developers*

**Best for:** React developers, component-based thinkers, existing React projects  
**Choose if you:** Already use React, want declarative 3D scenes, or need to integrate 3D into React apps

### 🧩 [**PlayCanvas Web Components**](../web-components) - *Standards-based development*

**Best for:** Vanilla JS developers, framework-agnostic projects, web standards enthusiasts  
**Choose if you:** Prefer native web technologies, want framework independence, or like custom HTML elements

### 😕 Still Confused?

Consult this simple questionnaire:

**New to 3D development?** → Start with the [**PlayCanvas Editor**](../editor)  
**React developer?** → Use [**PlayCanvas React**](../react)  
**Prefer vanilla HTML and JavaScript?** → Try [**PlayCanvas Web Components**](../web-components)  
**A coder that needs maximum control?** → Go with the [**PlayCanvas Engine**](../engine)  
**Working in a team?** → The [**PlayCanvas Editor**](../editor) offers the best collaboration  
**Coming from Unity or Unreal?** → The [**PlayCanvas Editor**](../editor) provides a similar end-user experience  
**Integrating into existing app or site?** → Choose [**React**](../react) or [**Web Components**](../web-components) based on your existing stack

And if you still have questions, come and chat to us on [Discord](https://discord.gg/RSaMRzg).

--------------------------------------------------------------------------------

## Join the PlayCanvas Community

URL: https://developer.playcanvas.com/user-manual/getting-started/community/

To get the most out of PlayCanvas, we recommend that you join our community!

🤝 **Make friends** with other community members including the dev team that builds PlayCanvas  
🎓 **Learn** from other PlayCanvas experts  
🎨 **Share your creations** with people who care  
📰 **Stay informed** about the latest PlayCanvas-related news

## Our Community Platforms

We focus on 5 key community platforms:

### [Reddit](https://reddit.com/)

Reddit is a great place to share PlayCanvas work, ask questions, and connect with people who've proactively opted into a PlayCanvas-related community.

To join:

* Visit our [subreddit](https://www.reddit.com/r/PlayCanvas/).
* Click the 'Join' button.
* Optional: Tap the **bell** to set notification frequency (we recommend **Low** which is Reddit's default).

:::tip[tips for posting]

* An **Image & Video** post will perform much better than a **Text** post.
* Try **crossposting** your PlayCanvas posts to related subreddits such as [WebGL](https://www.reddit.com/r/webgl/), [WebGPU](https://www.reddit.com/r/webgpu/) and [WebXR](https://www.reddit.com/r/WebXR/) - but always read each subreddit's rules first! To crosspost, select **Share** > **Crosspost** underneath your post.

:::

### [X](https://x.com/)

X (formerly Twitter) is ideal for quick updates, launch announcements, short clips, and real-time conversations with the wider web graphics community.

To join:

* Visit our [official X account](https://x.com/playcanvas).
* Tap the **Follow** button.
* Optional: Tap the bell to enable notifications.

:::tip[tips for posting]

* Attach a video to your post (GIF or short MP4, aim to stay below ~30s) for best reach.
* Tagging **@playcanvas** in your posts dramatically boosts the chance that the PlayCanvas team will repost you.
* Adding a link to your post will limit its reach. You can mitigate this by adding a link to the end of a thread.

:::

### [LinkedIn](https://linkedin.com/)

LinkedIn is perfect for professional networking, sharing career achievements, and connecting with industry professionals in the web graphics space.

To join:

* Visit our [company page](https://www.linkedin.com/company/playcanvas/).
* Tap the **+ Follow** button.
* Optional: Click **Turn on** to receive post notifications from us.

:::tip[tips for posting]

* Share professional milestones like project launches, career updates, or technical insights.
* Use industry-relevant hashtags like **#gamedev**, **#webgl**, **#javascript**, and **#playcanvas**.
* Professional posts with project screenshots or videos tend to generate good engagement and networking opportunities.

:::

### [Forum](https://forum.playcanvas.com/)

The PlayCanvas Forum is the best place for detailed technical discussions, troubleshooting complex issues, and sharing comprehensive tutorials with the community.

To join:

* Visit the [PlayCanvas Forum](https://forum.playcanvas.com/).
* Tap **Sign Up** to create your account.
* Optional: Enable email notifications for topics you're interested in following.

:::tip[tips for posting]

* **Search first** - Many questions have been answered before. Use the search function to check existing topics.
* **Be specific** - Include details like browser version, error messages, and relevant code snippets when asking for help.
* **Use proper categories** - Post in the appropriate section (Help & Support, Showcase, Jobs, etc.) to reach the right audience.

:::

### [Discord](https://discord.gg/RSaMRzg)

Discord provides real-time chat for quick questions, casual conversations, and connecting with community members and the PlayCanvas team in an informal setting.

To join:

* Visit our [Discord server](https://discord.gg/RSaMRzg).
* Tap **Accept Invite** to join the server.
* Optional: Customize your notification settings for different channels based on your interests.

:::tip[tips for chatting]

* **Use appropriate channels** - Post in the right channel to keep conversations organized. For example, we have a dedicated channel on Gaussian Splatting.
* **Be respectful** - We are proud to run a kind, friendly and helpful community.
* **Share screenshots and links** - Visual problems are much easier to diagnose when you can show what's happening.

:::

--------------------------------------------------------------------------------

## Made with PlayCanvas

URL: https://developer.playcanvas.com/user-manual/getting-started/made-with-playcanvas/

Below is our showcase video of some of the best WebGL browser games and experiences made with PlayCanvas.

Find more fantastic games and experiences made by our users in the [PlayCanvas Awesome List](https://github.com/playcanvas/awesome-playcanvas)!

## PlayCanvas Showcase 2022

Links to the content showcased can be found on the [blog post](https://blog.playcanvas.com/our-2022-developer-showreel-is-live/).

[Interactive Demo]

## PlayCanvas Showcase 2021

Links to the content showcased can be found on the [blog post](https://blog.playcanvas.com/playcanvas-showcase-2021/).

[Interactive Demo]

--------------------------------------------------------------------------------

## Open Source Mission

URL: https://developer.playcanvas.com/user-manual/getting-started/open-source/

## Mission Statement

> PlayCanvas is committed to democratizing 3D web development through open source technology. Our mission is to make powerful, interactive 3D experiences accessible to everyone, regardless of background or budget, by providing free, open, and well-documented tools that anyone can use, modify, and contribute to.

## Why Open Source?

At PlayCanvas, we believe that the future of the web is interactive and three-dimensional. By open sourcing our core technologies, we aim to:

**🌍 Democratize 3D Development**: Remove barriers to entry and make advanced 3D web technologies accessible to developers worldwide, regardless of their financial resources.

**🔧 Foster Innovation**: Enable the community to extend, modify, and improve our tools, leading to innovations we never could have imagined alone.

**🤝 Build Trust**: Transparency in our codebase allows developers to understand exactly how our tools work, contributing to trust and reliability.

**📚 Advance the Industry**: By sharing our knowledge and code, we help push the entire 3D web development ecosystem forward.

**🌱 Ensure Longevity**: Open source ensures that our tools will continue to exist and evolve, even as our company changes and grows.

## Our Home on GitHub

We use GitHub to manage our open source projects. Visit our [GitHub organization](https://github.com/playcanvas) to explore our complete open source ecosystem.

:::tip Stay informed

Hit the **Follow** button on our [organization page](https://github.com/playcanvas) to add our activity to your personal dashboard!

:::

Here are some of our key repositories:

| Repository | Description | License |
| ---------- | ----------- | ------- |
| [**engine**](https://github.com/playcanvas/engine) | JavaScript runtime for 3D web applications | MIT |
| [**editor**](https://github.com/playcanvas/editor) | Visual editing environment for building WebGL/WebGPU/WebXR applications | MIT |
| [**react**](https://github.com/playcanvas/react) | React components for building declarative 3D scenes | MIT |
| [**web-components**](https://github.com/playcanvas/web-components) | Web Components for standards-based 3D development | MIT |
| [**supersplat**](https://github.com/playcanvas/supersplat) | 3D Gaussian Splat editor | MIT |
| [**model-viewer**](https://github.com/playcanvas/model-viewer) | glTF and 3D Gaussian Splat viewer | MIT |
| [**pcui**](https://github.com/playcanvas/pcui) | Comprehensive UI library for building browser-based tools | MIT |

:::tip

Even this website is [open sourced under MIT](https://github.com/playcanvas/developer-site). Would you like to contribute and make it even better? Be our guest!

:::

## Contributing to PlayCanvas

We welcome contributions from developers of all skill levels! Whether you're fixing a typo in our documentation or implementing a major new feature, your contributions help make PlayCanvas better for everyone.

### Getting Started

1. **Choose Your Area of Interest**: Browse our repositories and find something that interests you
2. **Check Existing Issues**: If you are just starting out, look for ["good first issue"](https://github.com/search?q=org%3Aplaycanvas+label%3A%22good+first+issue%22&type=issues) labels across our repositories
3. **Start a Discussion**: Open an issue to discuss your ideas before implementing major changes
4. **Read the Guidelines**: Each repository has specific contributing guidelines in its `CONTRIBUTING.md` file

### Contribution Process

#### 1. **Log Issues**

- **Bug Reports**: Found a bug? [Create a detailed issue](https://github.com/playcanvas/engine/issues/new) with reproduction steps
- **Feature Requests**: Have an idea? Share it in the relevant repository's issues
- **Questions**: Use [GitHub Discussions](https://github.com/playcanvas/engine/discussions) for general questions

#### 2. **Submit Pull Requests**

- **Discussion First**: For significant changes, open an issue for discussion before coding
- **Follow Guidelines**: Each repository has specific code style and contribution guidelines
- **Stay Focused**: Keep PRs small and focused on a single feature or fix
- **Include Tests**: Add or update tests when appropriate
- **Complete CLA**: Sign our [Contributor License Agreement](https://docs.google.com/a/playcanvas.com/forms/d/1Ih69zQfJG-QDLIEpHr6CsaAs6fPORNOVnMv5nuo0cjk/viewform)

#### 3. **Review Process**

- Our maintainers will review your PR and provide feedback
- Address any requested changes promptly
- Be patient—quality reviews take time, but they ensure code quality

## Supporting PlayCanvas

### Star Our Repositories ⭐

One of the simplest ways to support PlayCanvas is by starring our repositories! Stars help:

- **Increase Visibility**: More stars mean more developers discover our tools
- **Show Appreciation**: Let us know which projects you find valuable
- **Track Popularity**: Help us understand which tools are most important to the community

**Our Top Repos to Star**:

- [PlayCanvas Engine](https://github.com/playcanvas/engine) ⭐
- [PlayCanvas React](https://github.com/playcanvas/react) ⭐
- [PlayCanvas Web Components](https://github.com/playcanvas/web-components) ⭐
- [SuperSplat](https://github.com/playcanvas/supersplat) ⭐

### Watch Repositories for Updates 👀

Stay informed about PlayCanvas development by watching repositories of interest:

#### What You'll Receive

- **Issue Notifications**: Get notified when new bugs are reported or feature requests are made
- **Pull Request Updates**: See when new features are being developed
- **Release Announcements**: Be the first to know about new versions and features
- **Discussion Alerts**: Participate in important design and architecture discussions

#### How to Watch

1. Visit any PlayCanvas repository on GitHub
2. Click the "Watch" button (👁️) at the top right
3. Choose your notification level:
   - **All Activity**: Get notified about everything
   - **Issues, PRs, Releases**: Only important updates
   - **Releases Only**: Just new version announcements

#### Recommended Watches

- [Engine](https://github.com/playcanvas/engine/subscription) - Core runtime updates
- ...and any other [repositories](https://github.com/playcanvas) you actively use in your projects!

## Final Thoughts

When you contribute to PlayCanvas, you're not just improving software — you're helping to build the future of interactive web experiences. Every bug fix, feature request, documentation improvement, and star helps make 3D web development more accessible to developers everywhere.

**Ready to contribute?** Start by exploring our [GitHub organization](https://github.com/playcanvas) and find a project that interests you. The PlayCanvas community is here to help you get started! 🙌

--------------------------------------------------------------------------------

## Glossary

URL: https://developer.playcanvas.com/user-manual/glossary/

Here is an overview of some of the terms we use to describe the PlayCanvas Engine and Tools.

## Animation {#animation}

An Animation is an [Asset](#asset) that contains keyframe data used to animate properties of [Entities](#entity) over time. Animations can control transforms (position, rotation, scale), material properties, and other animatable values. In PlayCanvas, animations are typically imported from 3D content creation tools or created using the Animation component and can be controlled via scripts using the Animation component API.

## Application {#application}

The `Application` class is the core container that manages all the essential systems and resources needed to run your PlayCanvas application. It serves as the central hub that coordinates:

* The `Scene` which includes the scene hierarchy and scene settings
* The `ComponentSystem`s that handle entity behavior
* Input devices (keyboard, mouse, touch and gamepad)
* Asset loading and management
* The main render loop and frame updates
* Audio context and 3D audio systems

The Application is accessible from any script function (`initialize`, `update` and so on) as `this.app` and provides the main interface for interacting with the engine's functionality.

## Asset {#asset}

An Asset is a unit of data that represents a resource used in your PlayCanvas application. Assets are typically imported from content creation applications (such as Blender, 3D Studio Max, or Photoshop) but can also be created directly in the editor. Common asset types include:

* 3D models and animations
* Textures and materials
* Audio files and music
* Scripts and JSON data
* Fonts and UI elements

Assets are managed through the [Asset Pipeline](#asset-pipeline) and can be assigned to [Components](#component) to give [Entities](#entity) their appearance and behavior. Also see [Source Assets](#source-asset) and [Target Assets](#target-asset).

## Asset Pipeline {#asset-pipeline}

The asset pipeline is a process run on the PlayCanvas servers which converts an uploaded [Source Asset](#source-asset) e.g. an FBX scene file into one or more [Target Assets](#target-asset) e.g. a model file, a material and some textures. The pipeline is designed to convert uploaded files into optimized versions ready to use in your game. To process an asset through the pipeline simply upload it to PlayCanvas from the dashboard or the PlayCanvas Editor.

## Attribute {#attribute}

An attribute is a property of a [Component](#component). Attributes are represented in the PlayCanvas Editor interface via user interface controls (pickers, check boxes, sliders, etc). Attributes can be tweaked via these controls and, once you have launched your game from the Editor's 'Launch' button, the updates are live-streamed to the connected game in real time.

## Collision {#collision}

Collision refers to the detection and response when two or more objects intersect in 3D space. PlayCanvas provides collision detection through the Collision component, which defines the shape used for collision detection, and the Rigidbody component, which handles the physics response. Common collision shapes include boxes, spheres, capsules, and triangle meshes.

## Component {#component}

Components describe the properties and functionality of an [Entity](#entity). In the PlayCanvas Engine, rather than defining game objects using a long chain of inherited classes, game objects are defined as an Entity and then a collection of Components.

Components are added to Entities via the Component menu in the PlayCanvas Editor, or they can be added at runtime via their respective `ComponentSystem` object.

## Cubemap {#cubemap}

A Cubemap is a special type of [Texture](#texture) that consists of six square textures arranged to form a cube. Cubemaps are commonly used for skyboxes to create distant background scenery and for environment mapping to simulate reflections on shiny surfaces. In PlayCanvas, cubemaps can be generated from HDR images or created from six individual texture faces.

## DOM {#dom}

The DOM (Document Object Model) is a way of representing an HTML document. Web browsers make an interface available for querying and modifying the DOM that makes up the HTML page that is displayed in the user's browser.

## Entity {#entity}

An Entity is one of the building blocks of your application in the [PlayCanvas Engine](#playcanvas-engine). Often an Entity will represent a single object in your game or application, though a single object may also be made of multiple Entities.

All Entities have a position, rotation and scale. They have a parent node from which they inherit a transform, and they may have child nodes to which they supply their transform.

## Framework {#framework}

The Framework is the high-level abstraction layer built on top of the core PlayCanvas Engine that provides a game-development-focused interface. It includes:

* The Entity-Component System for organizing game objects
* Asset management and loading systems
* Input handling and device abstraction
* Audio management and 3D spatial audio
* Graphics pipeline management
* Integration with the PlayCanvas Editor for live-link functionality

The Framework simplifies common game development tasks and provides a structured approach to building interactive 3D applications. See the [API Reference](https://api.playcanvas.com) for more details.

## Gizmo {#gizmo}

A Gizmo is a control that can be dragged around with the mouse in order to edit the attributes of an Entity. It is usually used to edit the transform matrix. A Gizmo consists of three color-coded parts, one for each axis in 3D space. Red is the X-axis, green is the Y-axis and blue is the Z-axis. Sometimes there will also be controls for manipulating more than one axis at once, e.g. the translate gizmo features clickable plane icons to allow translation in two dimensions at once, on the X and Y, Y and Z, Z and X planes.

## Hierarchy {#hierarchy}

The Hierarchy is a panel in the [PlayCanvas Editor](#playcanvas-editor) that displays the tree structure of [Entities](#entity) in your [Scene](#scene). It shows the parent-child relationships between entities and allows you to organize, select, rename, and reorder entities. The hierarchy reflects the transform inheritance structure where child entities inherit the transformations of their parents.

## High Dynamic Range {#high-dynamic-range}

High Dynamic Range or HDR refers to color information that is outside of the usual 0-1 range. In the standard range, 0 is black and 1 is the brightest color that the display device can show. In the real world there are no such limitations. For example, the sun can be many times brighter than the sky that surrounds it.

## Inspector {#inspector}

The Inspector is a panel in the [PlayCanvas Editor](#playcanvas-editor) that displays the properties and [Components](#component) of the currently selected [Entity](#entity) or [Asset](#asset). It provides an interface for editing attributes, adding or removing components, and configuring settings. The Inspector updates in real-time as you make changes, allowing for immediate feedback during development.

## Light {#light}

A Light is a [Component](#component) that illuminates the [Scene](#scene). PlayCanvas supports several types of lights including directional lights (like the sun), point lights (like light bulbs), and spot lights (like flashlights). Lights have properties such as color, intensity, range, and shadow casting capabilities that affect how they illuminate objects in the scene.

## Material {#material}

A Material is an [Asset](#asset) that defines how surfaces appear when rendered. Materials control the visual properties of 3D model surfaces through various parameters:

* **Albedo/Diffuse**: Base color and texture
* **Normal/Bump**: Surface detail and texture
* **Metalness**: How metallic the surface appears
* **Roughness**: How smooth or rough the surface is
* **Emission**: Self-illuminating properties
* **Opacity**: Transparency and alpha blending

Materials can reference [Texture](#texture) assets for detailed surface appearance and support both physically-based rendering (PBR) and traditional lighting models. In the [PlayCanvas Editor](#playcanvas-editor), materials can be edited by selecting them in the assets panel or by clicking on a material directly on a model in the 3D view.

## PlayCanvas Editor {#playcanvas-editor}

The PlayCanvas Editor is a visual editing tool which can be used by members of your development team to edit [Scenes](#scene). The PlayCanvas Editor is used to manipulate [Entities](#entity) and their [Components](#component), manage [Assets](#asset), and configure various aspects of your game or application. It provides an intuitive interface with panels for hierarchy management, asset organization, and real-time preview capabilities.

## PlayCanvas Engine {#playcanvas-engine}

The PlayCanvas Engine is a JavaScript library which provides all the functionality you will need to create an interactive 3D application or game.

Programs created using the PlayCanvas Engine will run directly in a modern web browser with no need for third-party plugins.

## Project {#project}

A Project is a collection of [Scenes](#scene) and [Assets](#asset) that belongs to a single user. Usually a single Project will only contain resources for a single application, though you can export multiple applications from a single Project.

## Rigidbody {#rigidbody}

A Rigidbody is a [Component](#component) that enables an [Entity](#entity) to participate in physics simulation. It gives the entity properties like mass, friction, and restitution, and allows it to respond to forces, gravity, and collisions. Rigidbodies can be configured as static (immovable), kinematic (movable but not affected by forces), or dynamic (fully simulated physics objects).

## Scene {#scene}

A Scene is a collection of Entity data, Art data and Code data which can be loaded as a unit. A Scene may represent your entire game, or just a single level or part of your game.

## Script {#script}

A Script is an [Asset](#asset). It is assigned to a Script [Component](#component) on an [Entity](#entity). Scripts are written in JavaScript. They have several predefined functions that can be overridden:

* `initialize` - called once on instantiation
* `postInitialize` - called once after all script `initialize` functions have been called
* `update` - called every frame
* `postUpdate` - called every frame after all script `update` functions have been called
* `swap` - called when a script is 'hot reloaded' (due to a save event in the Code Editor)

## Skybox {#skybox}

A Skybox is a background that surrounds the entire [Scene](#scene), typically used to create the illusion of distant scenery such as mountains, clouds, or stars. In PlayCanvas, skyboxes are created using [Cubemaps](#cubemap) and can be configured in the scene settings. They provide environmental lighting and serve as the backdrop when no other geometry is visible in a particular direction.

## Source Asset {#source-asset}

A source asset is the original file that has been uploaded into PlayCanvas. Source Assets are the input for the PlayCanvas asset pipeline which creates [Target Assets](#target-asset).

## Target Asset {#target-asset}

A target asset is a file that can be loaded into your game at runtime. It will be in a format ready to use in the Engine. Target Assets are usually the product of a [Source Asset](#source-asset) being uploaded and imported through the asset pipeline.

## Template {#template}

A Template is an [Asset](#asset) that contains a piece of an [Entity](#entity) hierarchy. It has a root Entity and can have any number of children. A Template is a reusable Entity that you can instantiate dynamically at runtime or place multiple instances of it in your [Scene](#scene). When you change the Template Asset all instances of the Template will also change.

## Texture {#texture}

A Texture is an [Asset](#asset). Typically, it contains image data that can be mapped onto 2D or 3D geometry. Textures can also be used to store other types of generic numeric data for processing on the GPU. PlayCanvas can load textures from standard web format images (JPG, PNG and GIF). The engine can also read super-compressed Basis textures that can be transcoded to natively supported GPU formats on load.

## Transformation Matrix {#transformation-matrix}

A Transformation Matrix is a mathematical matrix that represents a set of linear transforms. In particular: translation, rotation and scale. This means that a transformation matrix can be used to represent the position, orientation and size of an object in 3D space. In the PlayCanvas Engine each [Entity](#entity) has a transformation matrix accessible via the `getLocalTransform()` method.

## Viewport {#viewport}

The Viewport is the 3D view panel in the [PlayCanvas Editor](#playcanvas-editor) where you can visually see and interact with your [Scene](#scene). It provides a real-time rendered view of your game world, allowing you to navigate, select, and manipulate [Entities](#entity) using various tools and [Gizmos](#gizmo). The viewport supports multiple camera angles and rendering modes for different development needs.

--------------------------------------------------------------------------------

## Graphics

URL: https://developer.playcanvas.com/user-manual/graphics/

PlayCanvas incorporates an advanced graphics engine that delivers high-performance 3D rendering on the web. The engine provides both [WebGL](https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API) and [WebGPU](https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API) support, ensuring compatibility across all modern browsers while offering cutting-edge graphics capabilities.

## Graphics Engine Backends

The PlayCanvas engine supports multiple graphics backends:

* **WebGPU (Beta)** - Next-generation graphics API with reduced driver overhead and compute shader support
* **WebGL 2.0** - Mature and [widely supported](https://caniuse.com/webgl2) across all browsers and devices
* **Null** - For running the engine in headless environments such as Node.

:::note[Automatic fallback]

The engine will seamlessly fall back from WebGPU to WebGL based on browser support.

:::

## Key Rendering Features

### Physically Based Rendering (PBR)

* Comprehensive PBR support via metallic/roughness and specular/glossiness workflows
* Energy conservation and physically accurate lighting models
* Support for clearcoat, anisotropy, sheen, and transmission materials

### Advanced Lighting

* **Clustered lighting system** - Efficient handling of hundreds of dynamic lights
* **Directional, point, and spot lights** with configurable shadows and cookies
* **Area lights** - Rectangle, disk, and sphere-shaped light sources for realistic lighting
* **Image-based lighting (IBL)** with HDR environment maps
* **Runtime lightmap generation** for static lighting optimization

### High Dynamic Range (HDR) Rendering

* **Linear workflow** with automatic gamma correction
* **HDR display output** support on compatible devices
* **Advanced tone mapping** operators including ACES, Neutral, and Linear
* **CameraFrame system** for comprehensive post-processing pipeline

### Modern Rendering Pipeline

* **Render passes architecture** enabling advanced effects
* **Multiple render targets (MRT)** support
* **Depth pre-pass** and **temporal anti-aliasing (TAA)**
* **Hardware instancing** for efficient rendering of repeated geometry
* **Static and dynamic batching** to reduce draw calls

### Post-Processing Effects

The CameraFrame system provides a full suite of post-processing effects:

* **HDR Bloom** with physically accurate light bleeding
* **Screen Space Ambient Occlusion (SSAO)**
* **Depth of Field (DoF)** with bokeh effects
* **Temporal Anti-Aliasing (TAA)** for smooth edges
* **Vignette, sepia, brightness/contrast** and color grading

### Advanced Rendering Techniques

* **3D Gaussian Splatting** for photorealistic scene reconstruction
* **Hardware-accelerated particles** for special effects
* **Mesh skinning and morphing** for character animation
* **Procedural geometry generation** with optimized primitives
* **Texture compression** courtesy of Basis Universal

### Custom Shaders

* **Flexible shader system** supporting both GLSL (WebGL) and WGSL (WebGPU)
* **Automatic shader generation** with chunk-based composition
* **Preprocessor support** for shader variants and includes
* **WebGPU compute shaders** for GPU-accelerated computation

The graphics engine is continuously updated to leverage the latest web standards and hardware capabilities, ensuring PlayCanvas applications deliver exceptional visual quality and performance across all platforms.

--------------------------------------------------------------------------------

## Advanced Rendering

URL: https://developer.playcanvas.com/user-manual/graphics/advanced-rendering/



--------------------------------------------------------------------------------

## Batching

URL: https://developer.playcanvas.com/user-manual/graphics/advanced-rendering/batching/

Batching is the process of combining multiple mesh instances together into a single mesh instance, so that they can all be rendered in a single GPU draw call. PlayCanvas provides a handy feature on the [Model](/user-manual/editor/scenes/components/model), [Sprite](/user-manual/editor/scenes/components/sprite) and [Element](/user-manual/editor/scenes/components/element) components that lets you assign these components to batch groups which give the engine hints on how to combine meshes to reduce the overall draw call count.

There are a variety of rules which the engine will apply to see if mesh instances are able to be combined. The primary rule is that all mesh instances must share the same material.

Common batching use cases are:

- Combine together static geometry -- e.g. environments -- into a single mesh instance or multiple large instances to reduce draw calls, but still support camera culling.
- Combine together dynamic geometry -- e.g. a set of moving objects -- into a single mesh instance with dynamic properties that are applied on the GPU.

:::note

The use of batching is currently not compatible with [runtime lightmaps](/user-manual/graphics/lighting/runtime-lightmaps/) due to each lightmapped object requiring its own unique lightmap texture.

:::

## Creating Batch Groups

[Image: Creating Batch Groups]

Batch Groups can be created from the Batch Groups section of the [scene settings panel](/user-manual/editor/interface/settings/batch-groups/). Each batch group has a number of properties that are used to give the engine hints about how to create batches from this batch group.

### Batch Group Properties

- **Name**: Used to differentiate different batch groups, ideally it would describe the kinds of objects that this batch group will have. This name is available at runtime to retrieve the group.
- **Dynamic**: If enabled then objects inside the batch group can still move/rotate/scale. You can use this for objects that are similar to each other and use the same materials e.g. bullets. Static groups use less runtime resources so you should disable Dynamic if the contents of batch group will not move.
- **Max AABB size**: The maximum size of any one side of the bounding box that contains all objects in the batch group at the time when the batches are created. If the set of meshes are larger than the maximum size it will create multiple batches to be rendered. A larger bounding box will render in less draw calls, but will work less well with camera culling.

## Adding a component to a Batch Group

[Image: Selecting Batch Groups]

The Model component has a Batch Group property to assign a model into a batch group.

## Rules for combining mesh instances

The rules for whether the engine can combine mesh instances are fairly complicated but a good summary is that all mesh instances that belong to a single batch must obey the following:

- Have the same Batch Group ID
- Have the same material
- Have the same shader parameters
- Be within a bounding box with no side larger than the Max AABB Size
- Be in the same layer
- Each batch has a maximum vertex count of 65535
- For dynamic batches there is a maximum number of movable mesh instances. This hardware dependent but has a maximum of 1024.

If a batch group contains components or mesh instances that do not obey all of the rules the batch group will produce multiple batches such that each individual batch contains mesh instance that follow all the rules.

## Trigger re-batching

Based on Batch Groups the engine creates an optimized version of mesh instances. Further changes to many properties of the original mesh instances are not reflected in the optimized versions. To allow for good performance by using batching, while still allowing some further updates, you can request the engine to rebatch individual Batch Groups after you make changes to the original mesh instances. This is often useful with User Interface element components, where you might want to set up batching, but still need to do infrequent updates. Note that re-batching a group is a potentially expensive operation. In many cases, the impact of rebatching can be minimized by separating elements that need updating to separate Batch Group.

Here is an example of a simple script. The script updates `textureAsset` on an element, and marks the Batch Group as dirty.

```javascript
// change textureAsset on element
element.textureAsset = this.hoverAsset;

// if this element has Batch Group set, mark it dirty to rebuild the group in the next frame
if (element.batchGroupId)
    this.app.batcher.markGroupDirty(element.batchGroupId);
```

## Example - Batching a static environment

[Image: Western Scene]

In this scene we have created a static environment from 7 separate model files, some of which are repeated in the scene. For example, the road tile is used in 50 entities to create the long road through the center of the scene.

[Image: Western Animation]

You can see in the animation each draw call as it is made. In this environment the engine makes over 50 draw calls to draw each of the models individually. However, apart from the ground, all of these models use the same material and so they can be combined into batch groups.

[Image: Western Animation Batched]

In this animation we have created 4 batch groups for the buildings, the cacti, the road and the ground. Notice, that the road and the ground are not combined into single draw calls because the meshes are larger than the Max AABB Size defined on the batch group.

## Terminology

- **Batch Group** - A named group, created in the Editor, that defines some hints on how mesh instances should be combined. Components are assigned to a batch group
- **Batch** - An engine object created at runtime which is the set of mesh instances that are rendered in a single draw call. A batch group may result in multiple batches depending on the properties of the mesh instances that are added to the batch group.
- **Batch Manager** - The programmatic interface for creating and updating batches at runtime. See [API documentation](https://api.playcanvas.com/engine/classes/BatchManager.html).

--------------------------------------------------------------------------------

## Hardware Instancing

URL: https://developer.playcanvas.com/user-manual/graphics/advanced-rendering/hardware-instancing/

Hardware instancing is a rendering technique which allows the GPU to render multiple identical meshes in a small number of draw calls. Each instance of the mesh can have a different limited amount of state (for example, position or color). It's a technique suitable to drawing objects such as trees or bullets, say.

For its support on a device, check `pc.GraphicsDevice.supportsInstancing`. In general, it is supported on all WebGL2 devices and also on the majority of WebGL1 devices using the `ANGLE_instanced_arrays` extension.

Note that all instances are submitted for rendering by the GPU with no camera frustum culling taking place.

## How to use instancing

Populate a vertex buffer with per instance matrices to provide their world matrices for rendering.

```javascript
// store matrices for individual instances into array
const matrices = new Float32Array(instanceCount * 16);
const matrix = new pc.Mat4();
let matrixIndex = 0;
for (let i = 0; i < instanceCount; i++) {
    matrix.setTRS(pos, pc.Quat.IDENTITY, pc.Vec3.ONE);

    // copy matrix elements into array of floats
    for (let m = 0; m < 16; m++)
        matrices[matrixIndex++] = matrix.data[m];
}
```

Create a VertexBuffer which stores per-instance state and initialize it with the matrices. In the following example, we use [`pc.VertexFormat.getDefaultInstancingFormat`](https://api.playcanvas.com/engine/classes/VertexFormat.html#getdefaultinstancingformat) which allows us to store a per-instance Mat4 matrix. Then we enable instancing on a MeshInstance, which contains the mesh geometry we want to instance.

```javascript
const instanceCount = 10;
const vertexBuffer = new pc.VertexBuffer(
    this.app.graphicsDevice,
    pc.VertexFormat.getDefaultInstancingFormat(this.app.graphicsDevice),
    instanceCount,
    pc.BUFFER_STATIC,
    matrices
);
meshInst.setInstancing(vertexBuffer);
```

Note, that you can create a dynamic vertex buffer using `pc.BUFFER_DYNAMIC`, and update the contents of it per-frame like this:

```javascript
vertexBuffer.setData(matrices);
```

## Custom shader

When you write custom shader that uses instancing, you need to read and use per-instance state from vertex attributes. In the following example, we read a `mat4` using vertex attributes.

```glsl
attribute vec4 instance_line1;
attribute vec4 instance_line2;
attribute vec4 instance_line3;
attribute vec4 instance_line4;

mat4 getModelMatrix() {
    return mat4(instance_line1, instance_line2, instance_line3, instance_line4);
}
```

--------------------------------------------------------------------------------

## Indirect Drawing

URL: https://developer.playcanvas.com/user-manual/graphics/advanced-rendering/indirect-drawing/

Indirect drawing is a GPU-driven rendering technique where draw call parameters (such as vertex count, instance count, etc.) are stored in GPU buffer memory rather than being specified directly by the CPU. This allows compute shaders to dynamically generate or modify rendering parameters, enabling more efficient GPU-driven rendering workflows.

This feature is currently **only supported on WebGPU** (you can check WebGPU availability using [`GraphicsDevice.isWebGPU`](https://api.playcanvas.com/engine/classes/GraphicsDevice.html#iswebgpu)) and is ignored on other platforms.

## How Indirect Drawing Works

In traditional rendering, the CPU specifies draw parameters like vertex count and instance count for each draw call. With indirect drawing, these parameters are stored in a GPU buffer, and the GPU reads them during rendering. This enables:

- **GPU-driven culling**: Compute shaders can determine which objects to render
- **Dynamic instance counts**: Procedurally control how many instances to draw
- **Reduced CPU overhead**: Less CPU-GPU synchronization
- **Complex rendering effects**: Enable advanced techniques like GPU-driven LOD selection

## Basic Usage

### Setting Up Indirect Drawing

Indirect drawing requires allocating a slot in the indirect draw buffer and assigning it to a mesh instance. **Important**: slots must be allocated fresh each frame:

```javascript
// Get a fresh slot every frame
const indirectSlot = app.graphicsDevice.getIndirectDrawSlot();

// Configure the mesh instance to use indirect rendering
// First parameter: camera component (or null for all cameras)
// Second parameter: the allocated slot
meshInstance.setIndirect(null, indirectSlot);
```

### Configuring Buffer Size

Control the maximum number of indirect draw calls per frame:

```javascript
// Set maximum indirect draw calls per frame (default: 1024)
app.graphicsDevice.maxIndirectDrawCount = 2048;
```

### Understanding the Indirect Draw Buffer

The indirect draw buffer is a storage buffer that holds draw call parameters. It's automatically managed by the graphics device:

- Access via `app.graphicsDevice.indirectDrawBuffer`
- Size controlled by `maxIndirectDrawCount` property
- Each slot contains: `indexCount`, `instanceCount`, `firstIndex`, `baseVertex`, `firstInstance`

### Using with Compute Shaders

Indirect drawing is most powerful when combined with compute shaders that generate the draw parameters. The `getIndirectMetaData()` method returns essential mesh information for compute shaders:

```javascript
// Get mesh metadata needed for indirect rendering
// Returns Int32Array [count, base, baseVertex, 0]
const meshMetaData = meshInstance.getIndirectMetaData();

// Create compute shader to control rendering parameters
const compute = new pc.Compute(device, shader, 'IndirectDrawCompute');
compute.setParameter('indirectMetaData', meshMetaData);
compute.setParameter('indirectDrawBuffer', app.graphicsDevice.indirectDrawBuffer);
compute.setParameter('indirectSlot', indirectSlot);

// Dispatch compute shader to generate draw parameters
device.computeDispatch([compute], 'GenerateIndirectDraw');
```

## API Reference

For detailed API documentation, refer to these PlayCanvas engine classes and methods:

- [`MeshInstance.setIndirect()`](https://api.playcanvas.com/engine/classes/MeshInstance.html#setindirect) - Configure a mesh instance for indirect rendering
- [`GraphicsDevice.getIndirectDrawSlot()`](https://api.playcanvas.com/engine/classes/GraphicsDevice.html#getindirectdrawslot) - Allocate a slot in the indirect draw buffer
- [`GraphicsDevice.indirectDrawBuffer`](https://api.playcanvas.com/engine/classes/GraphicsDevice.html#indirectdrawbuffer) - Access the indirect draw buffer
- [`GraphicsDevice.maxIndirectDrawCount`](https://api.playcanvas.com/engine/classes/GraphicsDevice.html#maxindirectdrawcount) - Control maximum indirect draw calls per frame

## Live Example

See the [Indirect Draw example](https://playcanvas.github.io/#/compute/indirect-draw) for a complete demonstration of indirect drawing with animated instance counts.

--------------------------------------------------------------------------------

## Multiple Render Targets

URL: https://developer.playcanvas.com/user-manual/graphics/advanced-rendering/multiple-render-targets/

The multiple render targets feature allows you to simultaneously render to multiple textures. This manual page explores implementation, configuration, and an example use case of multiple render targets.

For its support on a device, check `pc.GraphicsDevice.supportsMrt`. In general, it is supported on all WebGL2 and WebGPU devices and also on WebGL1 devices that support the `WEBGL_draw_buffers` extension. Note that on WebGL1 devices, the support is very high apart from on Android, where it is very low.

Additionally, you can detect the number of color attachments you can use by checking `pc.GraphicsDevice.maxColorAttachments`. Typically, 8 attachments are supported.

Multiple render targets have the following restrictions:

- All color attachments of a multiple render target must have the same width and height.
- All color attachments are cleared to the same value, specified using `pc.CameraComponent.clearColor`.
- All color attachments use the same write mask and alpha blend mode, as specified using `pc.BlendState`.

## How to use MRT

Create a render target using multiple color textures:

```javascript
const colorBuffers = app.graphicsDevice.supportsMrt ? [texture0, texture1, texture2] : [texture0];
const renderTarget = new pc.RenderTarget({
    name: 'MRT',
    colorBuffers: colorBuffers,
    depth: true
    samples: 2
});
```

Create a camera which will be used to render to MRT:

```javascript
const camera = new pc.Entity('MRTCamera');
camera.addComponent('camera', {
    // set its priority to make it render before the main camera each frame
    priority: -1,

    // this camera renders into MRT
    renderTarget: renderTarget
});
app.root.addChild(camera);

// if MRT is supported, set the camera to use a custom shader pass called MyMRT
if (app.graphicsDevice.supportsMrt) {
    camera.camera.setShaderPass('MyMRT');
}
```

### Standard Materials

When rendering using `StandardMaterial` into Multiple Render Targets (MRT), it is necessary to override the output shader chunk to direct values to additional color buffers. It is important to note that the modification in this example does not affect `gl_FragColor`, which is used for the forward pass output in target 0. If you wish to override it as well, you can output values to `pcFragColor0` as well.

```javascript
materials.forEach((material) => {
    material.chunks.outputPS = `
        #ifdef MYMRT_PASS
            // output world normal to target 1
            pcFragColor1 = vec4(litArgs_worldNormal * 0.5 + 0.5, 1.0);

            // output gloss to target 2
            pcFragColor2 = vec4(vec3(litArgs_gloss) , 1.0);
        #endif
    `;
});
```

### Custom Shaders

When not using `StandardMaterial` for rendering and instead employing a fully custom fragment shader, you can directly output the desired values to `pcFragColor0...pcFragColor7`. If you only need to modify the rendering for a specific camera, utilize the `MYMRT_PASS` define, which corresponds to the shader pass configured for that camera.

--------------------------------------------------------------------------------

## Cameras

URL: https://developer.playcanvas.com/user-manual/graphics/cameras/

Cameras are responsible for rendering a scene to the screen. You need at least one camera in your scene to see anything. When you create a new scene in PlayCanvas, it is automatically populated with a single camera (along with a directional light).

## Creating a Camera

In the Editor's 3D View, an unselected camera is represented with the following icon:

[Image: Camera icon]

To create a new camera, simply create a new entity and add a camera component to it. For convenience, the Editor menu has an item that does this in a single step:

[Image: Camera creation]

## Orthographic vs Perspective Projection

Cameras can have one of two types of projection: orthographic or perspective. Orthographic cameras define a parallel projection and are often used for 2D or isometric games.

[Image: Orthographic camera]

More commonly used is the perspective projection. It more closely mimics how our eyes or cameras work.

[Image: Perspective camera]

## Controlling the Viewport

By default, a camera will render to the full width and height of its render target. However, there are circumstances where you might want to change this behavior. For example, perhaps you are writing a game that has a local multiplayer mode that requires split-screen rendering to show each player's viewpoint.

For 2-player horizontal split screen, you would create two cameras and configure their viewports as follows:

[Image: Horizontal splitscreen]

And for vertical split screen, you would configure the viewports as follows:

[Image: Vertical splitscreen]

--------------------------------------------------------------------------------

## Depth Layer

URL: https://developer.playcanvas.com/user-manual/graphics/cameras/depth-layer/

Some rendering techniques require access to the depth or the color buffer of the scene for a specific camera. The Depth Layer is a special layer, which can be added to the `layers` property of a camera. The [`order`](/user-manual/graphics/layers/#choosing-the-layer-order) of the layers defines at which point during the rendering, the depth or the color buffer is captured. The captured buffers can then be used in the following layers of the camera.

Typically, those buffers are captured after all opaque layers are rendered and can be used in following transparent layers or post-processing.

Additionally, to capture these buffers, the capture needs to be enabled on a CameraComponent from a script:

- [```requestSceneColorMap```](https://api.playcanvas.com/engine/classes/CameraComponent.html#requestscenecolormap) to request a color map
- [```requestSceneDepthMap```](https://api.playcanvas.com/engine/classes/CameraComponent.html#requestscenedepthmap) to request a depth map

## Buffer access

To access one of these buffers in the shader as a texture, these are the uniform names to be used:

- for the color map: `uSceneColorMap`
- for the depth map: `uSceneDepthMap`

## Color Space Handling

When using `uSceneColorMap`, it's important to handle color space correctly. The scene color texture contains either gamma-corrected values (when rendering LDR to a non-sRGB target) or linear values (when using HDR rendering with CameraFrame).

### SCENE_COLORMAP_GAMMA Define

The engine automatically provides a `SCENE_COLORMAP_GAMMA` shader define when the scene color map contains gamma-corrected values that need to be converted to linear space. This typically occurs during LDR rendering when the camera outputs gamma-corrected colors to a standard (non-sRGB) render target.

**GLSL Example:**

```glsl
vec3 sceneColor = texture2DLod(uSceneColorMap, uv, 0.0).rgb;

#ifdef SCENE_COLORMAP_GAMMA
// Convert from gamma to linear space
sceneColor = decodeGamma(sceneColor);
#endif

// Now sceneColor is in linear space for lighting calculations
```

**WGSL Example:**

```wgsl
var sceneColor: vec3f = textureSampleLevel(uSceneColorMap, uSceneColorMapSampler, uv, 0.0).rgb;

#ifdef SCENE_COLORMAP_GAMMA
// Convert from gamma to linear space
sceneColor = decodeGamma3(sceneColor);
#endif

// Now sceneColor is in linear space for lighting calculations
```

The `decodeGamma()` and `decodeGamma3()` functions are built-in utility functions that perform the gamma-to-linear conversion using the standard gamma 2.2 curve.

For more information on linear workflow and color space handling, see the [Linear Workflow](/user-manual/graphics/linear-workflow/) documentation.

## CameraFrame Integration

When using [`CameraFrame`](/user-manual/graphics/posteffects/cameraframe/) for HDR post-processing, you can request depth and color maps through the [`rendering`](https://api.playcanvas.com/engine/classes/CameraFrame.html#rendering) settings:

- **Color Map**: Set `cameraFrame.rendering.sceneColorMap = true` to enable access to `uSceneColorMap` (e.g., for refraction or custom post-processing effects)
- **Depth Map**: Set `cameraFrame.rendering.sceneDepthMap = true` to enable access to `uSceneDepthMap` (e.g., for depth-dependent effects like custom fog or edge detection)

The color map in CameraFrame contains linear HDR values (no gamma correction needed), so the `SCENE_COLORMAP_GAMMA` define will not be set. This ensures consistent color space handling across different rendering configurations.

## Examples

These engine examples demonstrate the rendering of both the depth and the color map, and also custom shaders allowing their use:

- GrabPass demonstrates the use of the color buffer: [`GrabPass`](https://playcanvas.github.io/#/shaders/grab-pass)
- GroundFog demonstrates the use of the depth buffer: [`GroundFog`](https://playcanvas.github.io/#/shaders/ground-fog)
- Dispersion demonstrates refraction and chromatic dispersion using the scene color texture: [`Dispersion`](https://playcanvas.github.io/#/materials/dispersion)

--------------------------------------------------------------------------------

## Scene Picker

URL: https://developer.playcanvas.com/user-manual/graphics/cameras/scene-picker/

The Picker class provides a way to select mesh instances from screen coordinates by clicking or touching the screen. It works by rendering the scene from a camera's viewpoint to an offscreen buffer with unique ID colors, allowing efficient identification of clicked objects. The picker supports both regular meshes and Gaussian Splats, and works across WebGL2 and WebGPU backends.

## Basic Usage

To create a picker, instantiate it with your application, desired resolution, and optionally enable depth picking:

```javascript
const picker = new pc.Picker(app, width, height, depth);
```

The basic workflow involves three steps:

1. **Prepare** - Render the pick buffer by calling [`prepare(camera, scene, layers)`](https://api.playcanvas.com/engine/classes/Picker.html#prepare) once per frame (or only when the camera or scene changes)
2. **Resize** - Adjust picker resolution if needed using [`resize(width, height)`](https://api.playcanvas.com/engine/classes/Picker.html#resize)
3. **Query** - Get picked mesh instances asynchronously using [`getSelectionAsync(x, y, width, height)`](https://api.playcanvas.com/engine/classes/Picker.html#getSelectionAsync)

The picker uses an asynchronous API to read pixel data without blocking the rendering thread, ensuring smooth frame rates even when picking. When you're done with the picker, call [`destroy()`](https://api.playcanvas.com/engine/classes/Picker.html#destroy) to clean up GPU resources.

For complete API documentation, see the [Picker API reference](https://api.playcanvas.com/engine/classes/Picker.html).

## Depth Support

By default, the picker only captures mesh instance IDs. However, you can enable depth picking by passing `true` as the fourth constructor parameter:

```javascript
const picker = new pc.Picker(app, width, height, true);
```

When depth picking is enabled, the picker captures depth values along with mesh IDs. This additional information enables calculating the exact 3D world position of clicked points on object surfaces, which is useful for placing objects, measuring distances, or creating editor tools.

## World Position Picking

When depth picking is enabled, you can use [`getWorldPointAsync(x, y)`](https://api.playcanvas.com/engine/classes/Picker.html#getWorldPointAsync) to get the 3D world position at screen coordinates:

```javascript
picker.getWorldPointAsync(x, y).then((worldPoint) => {
    if (worldPoint) {
        // worldPoint is a Vec3 in world space
        console.log('Clicked at:', worldPoint);
    } else {
        // No object was clicked (background)
        console.log('Clicked on empty space');
    }
});
```

The method returns a promise that resolves to a `Vec3` containing the world position, or `null` if no object was clicked. This works correctly with both perspective and orthographic cameras.

## Performance Considerations

The picker's performance can be optimized in several ways:

**Lower Resolution**: Rendering the pick buffer at a fraction of the screen resolution significantly improves performance. For example, using 0.25x screen resolution:

```javascript
const pickerScale = 0.25;
const picker = new pc.Picker(
    app,
    canvas.width * pickerScale,
    canvas.height * pickerScale,
    true
);
```

The trade-off is reduced precision - very small objects may be missed at lower resolutions.

**Asynchronous Reads**: The picker's async API prevents blocking the main thread while reading pixel data from the GPU, maintaining smooth frame rates.

**Selective Updates**: Call `prepare()` only when needed. If your camera and objects are static, you can reuse the previously rendered pick buffer without calling `prepare()` again.

## Gaussian Splatting Support

The picker fully supports Gaussian Splat instances with the same API as regular meshes. You can pick splat instances by their mesh instance ID and, with depth enabled, determine exact 3D positions on splat surfaces.

This enables interactive applications like placing markers on splats, measuring distances, or selecting individual splat entities in complex scenes. See the [Gaussian Splatting Picking example](https://playcanvas.github.io/#gaussian-splatting/picking) for a complete demonstration.

## Examples

These engine examples demonstrate the picker in action:

- [**Area Picker**](https://playcanvas.github.io/#/graphics/area-picker) - Shows how to pick mesh instances in rectangular screen regions with visual feedback
- [**Gaussian Splatting Picking**](https://playcanvas.github.io/#gaussian-splatting/picking) - Demonstrates picking splat instances and using world position picking to place markers on splat surfaces

--------------------------------------------------------------------------------

## Layers

URL: https://developer.playcanvas.com/user-manual/graphics/layers/

## Layers Overview {#layers-overview}

Layers allow you to customize the render loop for your application. Using layers you can implement some advanced rendering features. For example:

* modify the order in which your meshes are rendered
* set cameras to render only some meshes
* set which lights affect which meshes

A PlayCanvas application is created with a default set of layers which are always present. You can create your own layers and re-order them to suit your particular requirements.

At a fundamental level a layer is list of meshes to render. Each layer is divided into two sub-layers: Opaque and Transparent. When a mesh is added to a layer the layer stores it in one of the two sub-layers, depending on whether the material on the mesh needs to render transparently or not. This is because transparent sub-layers are often sorted differently than opaque sub-layers.

## Rendering Order {#rendering-order}

There are three factors that determine the order in which meshes are rendered.

### Camera Priority {#camera-priority}

Priority of the camera is the main factor that controls the order in which the meshes are rendered. Each camera has a priority assigned to it, and cameras with smaller values for priority are rendered first.

Each camera also has a list of layers set up on it, which controls which layers the camera renders. Their order is described in the next section.

[Image: Camera Layers]

### Layer Composition {#layer-composition}

Next is order of layers in the application. Each application contains a `pc.LayerComposition` object which is available in your application as `this.app.scene.layers`. The layer composition determines the order of all sub-layers. The ordering is based on the sub-layer not on the layer so that you can, for example, render all the opaque sub-layers first, then all the transparent sub-layers afterwards.

**Note**: Putting a model component inside a layer that is rendered after the world layer **will not** make the model render on top of everything in the world layer! The Standard Material used to render models has a property called `depthTest`. When this is true (the default) before each pixel of the model is rendered the GPU will test to see if there is something else in front if this pixel. Even if that pixel was drawn in an earlier layer depth test ensures that only visible pixels are drawn. If you wish to ignore the distance from the camera when rendering a mesh, disable `depthTest` in your material.

### Sort Modes {#sort-modes}

Each sub-layer has a sort mode. Every frame the meshes in a sub-layer are sorted according to its sort mode. This determines the order that the meshes are rendered in when the sub-layer is rendered.

* **Material / Mesh** (`pc.SORTMODE_MATERIALMESH`) - This is the default mode for opaque sub-layers. Mesh instances are sorted to minimize switching between materials and meshes to improve rendering performance.
* **Back-to-front** (`pc.SORTMODE_BACK2FRONT`) - This is the default mode for transparent sub-layers. Mesh instances are sorted back to front. This is the way to properly render many semi-transparent objects on different depth, one is blended on top of another.
* **Front-to-back** (`pc.SORTMODE_FRONT2BACK`) - Mesh instances are sorted front to back. Depending on GPU and the scene, this option may give better performance than `pc.SORTMODE_MATERIALMESH` due to reduced overdraw.
* **Manual** (`pc.SORTMODE_MANUAL`) - This is the default mode for UI or 2D layers. Mesh instances are sorted based on the `MeshInstance.drawOrder` property. The Element Component and Sprite Component should be placed in layers using this sort mode.
* **None** (`pc.SORTMODE_NONE`) - No sorting is applied. Mesh instances are rendered in the same order they were added to a layer.

In addition to these sort modes, the `MeshInstance.drawBucket` property provides an additional, coarser level of sorting of `MeshInstances` within a layer. This integer value, ranging from 0 to 255 (default 127), serves as the primary sort key for mesh rendering. Meshes are sorted in ascending order by `drawBucket` (lower values rendered first), and then further sorted within each bucket according to the layer's selected sort mode. Note that the `drawBucket` setting is only effective when mesh instances are added to a sub-layer with its sort mode set to `pc.SORTMODE_BACK2FRONT`, `pc.SORTMODE_FRONT2BACK`, or `pc.SORTMODE_MATERIALMESH`. This allows you to group meshes into distinct rendering buckets, forcing certain groups to render before or after others, regardless of their material or depth, offering fine-grained control over the overall rendering order within those specific sort modes.

## Default Layers {#default-layers}

PlayCanvas applications are created with a set of default layers. You should leave these layers in place as some engine features will not function correctly if they are not present. The default order is below:

[Image: Default Layers]

1. **World (Opaque)** - Used to render components that are not transparent and most opaque component meshes.
1. **Depth (Opaque)** - Used to capture the color or the depth buffer of the scene, see [Depth Layer](/user-manual/graphics/cameras/depth-layer).
1. **Skybox (Opaque)** - Used to render the skybox. It is rendered after the World (Opaque) to reduce overdraw.
1. **World (Transparent)** - Used to render components that are transparent and other transparent component meshes.
1. **Immediate (Opaque)** - Used to render immediate mode meshes. e.g. `app.renderLine()`.
1. **Immediate (Transparent)** - Used to render immediate mode meshes. e.g. `app.renderLine()`.
1. **UI (Transparent)** - Used to render Element components. All Element components are transparent, so the Opaque sub-layer is not used.

## Using Custom Layers {#using-custom-layers}

The default layers are great for implementing the existing engine features but the real power comes from creating your own layers to customize the order in which your content is rendered.

### Create a layer {#create-a-layer}

Layers are controlled from the **LAYERS** panel in the **Settings** section of the Editor.

[Image: Creating a layer]

In the Layers section, type in the name of the layer that you wish to create and click **Add Layer**. Your new layer will appear in the list of available layers below the button.

### Setting the sort mode {#setting-the-sort-mode}

[Image: Edit a layer]

You can choose the sort mode for each sub-layer in the layer list. Expand your layer and choose the sort mode from the dropdown menu.

### Choosing the layer order {#choosing-the-layer-order}

[Image: Add layer]

Add a sub-layer to the layer composition by selecting **ADD SUBLAYER** and choosing which sub-layer you wish to add. Once your layer is in the Render Order list you can re-arrange the order by dragging each sub-layer up and down.

### Rendering entities in layers {#rendering-entities-in-layers}

Components that render meshes all have a `layers` property which is used to determine which layer and sub-layer the mesh should be added to. These components include: Model, Element, Sprite, Particle System. The Camera and Light components also have a `layers` property to determine which layers they render and light respectively.

[Image: Layer Components]

*Note:* The model is assigned to the Test Layer. In order for it to be rendered, the camera must include Test Layer in its layer list. In order for it to be lit, the light must include Test Layer in its layer list too.

### Recommended setup {#recommended-setup}

Your scene typically contains many entities, which render meshes. It is recommended for each of these to be on exactly one layer. In most cases, these would be on the World layer, but for more control, you can assign them to layers such as Terrain, Buildings, Characters.

A new scene by default contains a single camera, and this is all that is needed in many applications. Additional cameras are useful for cases such as cutting between different cameras in the scene, or when rendering picture in picture or split screen, or when rendering the scene into a texture.

When you add an additional camera, these are the recommended steps:

1. Set the priority of new and existing cameras to control the order in which they render.
2. Set up the layers of the newly created camera to specify which layers it renders. For example you might render a top down map camera and only want Terrain and Building layers in it, but not Characters.
3. If your camera renders into a texture, use a script to assign a render target to the `renderTarget` property of the camera.

--------------------------------------------------------------------------------

## Lighting

URL: https://developer.playcanvas.com/user-manual/graphics/lighting/

Lighting a scene is the process of calculating the color or shading of a pixel render to the screen based on the material properties of the surface and the light sources that are applied to that material.

In PlayCanvas, lighting can be broadly divided up into two basic categories: dynamic lights and lightmaps.

## Dynamic Lights

Lighting calculations that are performed at runtime are classed as dynamic. Every frame the engine calculates the amount of light falling on a surface from the type, position and properties of Light Entities and uses this to color the material.

## Lightmaps

For lights and geometry that does not move, it is often preferable to determine the lighting information in advance. This information is then saved into lightmap textures which are applied to the surface materials. This method has a very low runtime cost at the expense of having static lighting which can not change and pre-computation times.

There are two methods of creating lightmaps:

### External Lightmap Generation

Many 3D creation tools have lightmap generation included or available as an add-on, including 3DS Max, Maya and Blender. These tools generally generate lightmap textures which can be uploaded as regular assets and added to the Lightmap slot in the standard Physical Material.

### PlayCanvas Runtime Lightmap Generation

The PlayCanvas Engine has built in lightmap generation. This can be used to generate lightmaps automatically just before your game runs. With this method you can use the standard light components, make changes and preview your scene directly in the Editor.

--------------------------------------------------------------------------------

## Ambient Occlusion

URL: https://developer.playcanvas.com/user-manual/graphics/lighting/ambient-occlusion/

Ambient Occlusion is a technique to approximate how much light gets onto the surface based on its occlusion by the environment.

In PlayCanvas, ambient lighting is multiplied by the AO map, both diffuse and specular.

[Image: Ambient Occlusion comparison: without/with]  
*Left: without AO; Right: with Global AO*

AO map can be of different scale, e.g. a texture detail AO, showing surface crevices or a world-scale AO with large shadowing effect of different parts of the model. Large-scale AO will give more effect, it usually matches lightmaps on UV1, and is more important.

## Rendering Global AO

Rendering AO can be done using popular 3D modelling and archviz tools. It is practically same as rendering Lightmap that is described in [Lightmapping Section](/user-manual/graphics/lighting/lightmapping/).

With a few small differences where special material is applied on all geometry that will paint it so it looks like AO, and different settings during rendering into texture.

In 3D's Max with VRay it is done by creating a material of **VRayDirt** type and **Ambient Occlusion** mode, where **radius** can be changed to get desirable results.

[Image: 3D's Max: Ambient Occlusion VRay Material]

This material should be applied on all static geometry that have to be in ambient occlusion map. In 3D's Max this can be done either by manually applying it on individual objects or by using VRay Render Settings, using **Override mtl** property. This way individual materials are preserved and it makes life easier.

[Image: 3D's Max VRay Render Settings: Override mtl]

Then we need to get this data out into the texture. In 3D's Max this is done by using [Render To Texture](/user-manual/graphics/lighting/lightmapping#render-to-texture), same as during Lightmapping rendering, except **output** option should be using **VRayCompleteMap**.

AO Texture does not need to store very detailed information and visually does not suffer from texture compression artifacts much. So JPEG is economical and is a suitable format for it.

## Upload to Editor

Simply upload texture in Editor and apply it on Ambient Occlusion slot on materials.

[Image: Editor Ambient Occlusion Map]

## Example

You can [explore example](https://playcanv.as/p/zdkARz26/) that uses global Ambient Occlusion described above and its [project](https://playcanvas.com/project/446587/overview/archviz-example).

[[Image: PlayCanvas Lightmapping]](https://playcanv.as/p/zdkARz26/)  
*The lighting in this scene is implemented using Lightmap and AO textures and Box Projected IBL (reflections)*

--------------------------------------------------------------------------------

## Clustered Lighting

URL: https://developer.playcanvas.com/user-manual/graphics/lighting/clustered-lighting/

:::note

Clustered lighting is enabled by default from PlayCanvas Engine v1.56 onwards. The older lighting system will still be available in the Engine for the short term. However, we will deprecate it in a future minor release.

:::

Lights are a great way to add realism to your application. However, real time lights can also create significant runtime performance cost, especially when you have large numbers of lights that cast shadows.

Part of the solution to reduce performance costs may involve limiting the amount of lights that affect individual meshes. This is often implemented by finding and using lights that are nearby each object. However, there are multiple disadvantages to this strategy:

- Since each object can use different sets of lights, a custom shader must be compiled to handle them.
- Large objects need to be split into smaller objects in order for this strategy to be effective.
- Large numbers of shadow casting lights can cause the shader to run out of the available texture slots used by shadow maps.

To address these issues, PlayCanvas uses the **Clustered Lighting** solution to provide a performant implementation of Omni Lights and Spot Lights. It stores information about the lights in textures and allows the GPU to easily use only the lights that are nearby the shaded fragment. There are multiple advantages to Clustered Lighting:

- Shaders do not need to be recompiled when lights are added or removed from the scene, since the shader can handle multiple lights.
- Large numbers of lights (including Shadows and Cookies) can be used in the scene, since only the lights nearby each pixel are evaluated.

Note that Directional Lights affect all objects so they do not use the Clustered Lighting solution.

## Implementation Overview {#implementation-overview}

The following steps provide a basic overview of the Clustered Lighting implementation:

1. Cull lights by all of the camera's frustums, to evaluate the list of lights visible for a frame.
2. Place a world space 3D grid over the axis aligned bounds of all visible lights.

    [Image: 3D Grid]

3. Each cell in the 3D grid stores the light indices that intersect with it. On the CPU, this information is updated every frame and it is allowed to get a list of the lights that affect any positions. The information is stored in a texture and made available to the GPU.
4. The properties of all visible lights are stored in another texture, so they are also accessible by the GPU.
5. Shadow Maps and Cookie Textures are all rendered into an atlas, instead of being individual textures, so they are all accessible to the shader at the same time.
6. During lighting evaluation in the fragment shader, a fragment world space position is used to access a cell of the 3D grid and evaluate the stored lights.

## Editor Options {#editor-options}

Options for Clustered Lighting can be found in the Editor Settings under 'Rendering'.

[Image: Clustered Lighting Editor UI]

This will allow you to disable Clustered Lighting (if you need to use the previous lighting system) and to [tune performance and features](#tuning-clustered-lighting) mentioned below.

## Tuning Clustered Lighting {#tuning-clustered-lighting}

### Enabling Features {#enabling-features}

The Clustered Lighting shader needs to handle all supported lights, so it must contain the code to handle these features. This can cause the shader to be larger than needed and take longer to compile. To resolve these issues, there is a set of feature options that allow you to disable the features your application doesn't need and speed up the shader compilation:

- **Shadows Enabled** – Enable or disable the support for Shadows
- **Cookies Enabled** – Enable or disable the support for light Cookies
- **Area Lights Enabled** – Enable or disable the support for Area Lights

### Configuring the 3D Grid {#configuring-the-3d-grid}

[Image: 3D Grid Configuration]

The **Cells** property allows you to specify the number of cells along each world axis. This dynamically subdivides the Axis Aligned Bounding Box, that contains all visible lights, into a specified number of cells.

The **Max Lights Per Cell** property allows you to specify the maximum number of lights stored in each individual cell. This represents the maximum number of lights that overlap. Usually, the number of lights must be increased for coarser grid subdivisions, since the light overlap is larger.

### Configuring Atlas {#configuring-atlas}

All of the Shadow Maps and Cookie Textures used by visible lights are stored in an atlas. There is one atlas texture for Shadows and another for Cookies. The atlases can have different resolutions, although internally they use the same subdivision into smaller areas used by individual lights.

**Shadow Atlas Resolution** allows the Shadow atlas size to be configured, while **Cookie Atlas Resolution** allows for the Cookie atlas to be set. The sizes do not need to be a power of 2.

**Atlas Split** controls how the atlas is split into the individual sub-textures used by lights. There are two split strategies:

- **Automatic** – When the array size is specified as 0, the engine automatically splits the atlas as needed, to assign each visible light an equally sized sub-texture. For example, if you have three lights visible in a frame, the atlas will split into 2x2 sub-textures, and three of those four sub-textures will be assigned to the lights.

    [Image: Atlas Split 0]

- **Manual** – Allows the atlas to be split into a fixed number of sub-textures, which can be different sizes. It is set up using an array of numbers, where each number represents a split, both vertically and horizontally. See the next section for an example of manual atlas splits.

### Configuring Manual Atlas Split {#configuring-manual-atlas-split}

To understand how an atlas is manually split, take an array with two numbers: [2, 2]. The first number in the array splits the atlas into 2x2, for a total of four areas. Any following numbers in the array would split these areas again. In this case, the second number in the array splits one of the existing areas into another 2x2 (e.g. four areas), for a combined total of 7 areas.

[Image: Manual Split]

The following image shows how the manual atlas split should be specified.

[Image: Atlas Split 2]

Other Examples:

- [3, 2] – The first number splits the atlas into 3x3 (9 areas). The second number splits one of these areas into 2x2 (4 areas), for a total of 12 areas.
- [4] – The atlas is split into 4x4 (16 areas).

The main advantage of using manual subdivision is the level of detail that can be achieved. You can set up a fixed amount of sub-textures, which are assigned to the lights by order of their screen-space size. This allows lights that are larger on screen to receive a larger area of the atlas, while smaller lights in the distance use a smaller area of the atlas. If there are more lights than the number of available areas, the smallest screen-space lights will not cast any shadows.

### Shadows Type {#shadows-type}

All lights that cast shadows use the same shadow type. This allows you to globally set the shadow softness and related performance impact. The supported options are PCF1, PCF3, and PCF5. For more information, see the [Shadows](/user-manual/graphics/lighting/shadows/#soft-shadows-vs-hard-shadows) page.

## Limitations {#limitations}

Internally, a light index is stored using 8 bits, so the maximum number of visible lights at any frame is 254 (one index is reserved). In the future, there may be an additional option to use 16 bits to store the index and increase the limit.

## Performance Considerations {#performance-considerations}

- **Cell subdivisions** should be as small as possible, since large cell subdivisions lead to larger CPU usage when the grid is filled by the lights each frame. This should be optimized for each scene, depending on its lighting complexity. Optimally, you should have enough cells to limit the overlap of lights and the number of lights in each cell.
- The **Max Lights Per Cell** should be as small as possible, since this limits the size of the texture used to store the 3D grid, which needs to be updated every frame.
- If an application using Clustered Lighting **runs slowly** on older mobile devices, consider globally turning off features like Shadows or Cookies.

## Render Debug Grid {#render-debug-grid}

To help with debugging and tuning performance with Clustered Lighting, you can assign the [Layer](https://api.playcanvas.com/engine/classes/Layer.html) ID to render to the [debugLayer of LightingParams](https://api.playcanvas.com/engine/classes/LightingParams.html#debuglayer). e.g

```javascript
// Assuming being in a script type
this.app.scene.lighting.debugLayer = this.app.scene.layers.getLayerByName("World").id;
```

And to stop rendering, assign `undefined` to the `debugLayer` property:

```javascript
// Assuming being in a script type
this.app.scene.lighting.debugLayer = undefined;
```

--------------------------------------------------------------------------------

## Lightmapping

URL: https://developer.playcanvas.com/user-manual/graphics/lighting/lightmapping/

[[Image: PlayCanvas Lightmapping]](https://playcanv.as/p/zdkARz26/)
*The lighting in this scene is implemented using Lightmap and AO textures and Box Projected IBL (reflections)*

Here is the link to [final scene](https://playcanv.as/p/zdkARz26/) and [project](https://playcanvas.com/project/446587/overview/archviz-example) that uses these techniques to achieve results in image above: External HDR Lightmaps (described in this page below), [Ambient Occlusion](/user-manual/graphics/lighting/ambient-occlusion/) and HDR Cubemap applied using Box Projection using [Image Based Lighting](/user-manual/graphics/physical-rendering/image-based-lighting/) technique to achieve realistic reflections.

## Overview {#overview}

Lightmap generation is the process of pre-calculating lighting information for a static scene and storing it in textures which are then applied on materials. This is an efficient way of lighting a scene if many of the light sources and geometry are static or environmental.

PlayCanvas offers two ways to use lightmaps in your scene: **External lightmap generation** using a 3rd-party tool and [**Runtime Lightmapping**](/user-manual/graphics/lighting/runtime-lightmaps/) that can be generated automatically by the Engine on loading or later while application is running.

This page gives details and best practices on rendering lightmaps from external tools.

## External Lightmap Generation {#external-lightmap-generation}

Many 3D content tools have ways to generate lightmap textures. For example, 3ds Max, Maya, Blender and other tools all have ways to bake lightmaps into textures. The advantages of using an offline tool for lightmap generation is that you can use very sophisticated lighting calculations like Global Illumination for bounce lighting, soft shadows, ambient occlusion, etc. The major disadvantage is that you have to have a complete representation of your scene inside the 3D tool. So if your PlayCanvas scene is made up of lots of instances positioned in the Editor, you need to re-create this inside your lightmapping tool.

Once you have created lightmaps using an external tool you simply upload them as regular texture assets and they can be added to your materials using the lightmap slot in the standard Physical Material.

## Tools {#tools}

In this page we will use 3ds Max with VRay to generate lightmaps, but the same functionality is achievable with any other similar modeling tools.

## Gamma Correction {#gamma-correction}

When rendering Lightmaps or CubeMaps they should be rendered in Linear Space to ensure color curves are not affected by gamma correction twice. The PlayCanvas Engine will apply gamma correction during real-time rendering.

In 3ds Max this option (Enable Gamma/LUT Correction) should be disabled and can be found in Preference Settings (Customize > Preferences):

[Image: 3ds Max > Preferences > Linear Space]

Then make sure Color Mapping is updated. It can be found in Render Settings (F10, or from Render to Texture window). Output should not be clamped, and not post-processed (Mode option), Linear Multiply should be used for linear color space.
Here is a screenshot of what options should be set to what values, click the "Default" button to expand settings to "Expert":

[Image: 3D's Max > Render Settings]

## UV Mapping {#uv-mapping}

In order to apply a lightmap texture on geometry we need to unwrap it first. Here are some practices that will help you to get good lightmap friendly UV's.

### Simple Geometry {#simple-geometry}

A smaller area of geometry is better. Try to minimize the area of triangles and eliminate non-visible triangles. A larger area will reduce lightmap detail, require larger textures and sometimes multiple assets.

[Image: Lightmapping Tips: Simple Geometry]

### Consistent Texel Size {#consistent-texel-size}

Keep texels in UV unstretched and consistent in size with other texels within same geometry. This is to ensure that level of detail in lightmap texture is consistent within the scene. Some variations of texel size could be applied when geometry will be seen from up close or in the far distance as required by artistic and optimization decisions.

[Image: Lightmapping Tips: UV Consistent Texel Size]

### Non-overlapping UV {#non-overlapping-uv}

Triangles in UV should not overlap to ensure each pixel has a unique position in 3D space on geometry so it can store its own illumination information appropriately. UV space for lightmaps is clamped, meaning that UV will be contained between 0.0 and 1.0 and will not tile outside.

[Image: Lightmapping Tips: Non-overlapping UV]

## Other Tips {#other-tips}

To get good lightmap results we need to ensure that rendering is based only on data that is relevant to light propagation and not to the point of view of camera during rendering.

1. **Disable normal maps** on materials - micro surface details are too tiny to be relevant in lightmap textures.
2. Set **Reflection to 0** and **Disable Gloss Maps** on materials - reflection can lead to caustics and complications for renderers, leading to visual artifacts. Generally lightmaps should contain only diffuse lighting and reflectivity should be implemented using some runtime technique.
3. **Very dark materials won't produce good results** as they do not reflect light much and so will not assist Global Illumination.
4. In the Render To Texture window (see below) set **Padding** to larger value.
5. **Light can leak** from behind the geometry, add blocking geometry to prevent light.

[Image: Lightmapping Light Leaking]

## Render To Texture {#render-to-texture}

To get illumination data out of the modeling tool we want to render the light data into a texture. We can specify resolution and format for it.

In 3ds Max this is done using the Render To Texture window. Where Padding needs to be set to larger value; selected 2nd UV Channel; and Output profile depending on your renderer, in screenshot below `VRayRawTotalLightingMap` is used.

[Image: Render To Texture: PlayCanvas Lightmapping]

## Noise {#noise}

Depending on the quality and time of rendering in some situations the illumination data in the output might be not perfect and suffer from noise. This is easily solvable by applying some blur to the image that will not blur the edges in texture but will smoothen plain sections. In Photoshop this is done using Surface Blur filter:

[Image: Lightmapping: Photoshop > Surface Blur]

## Upload to Editor {#upload-to-editor}

At this stage you have your geometry with a second UV channel (UV1) and HDR lightmap textures and it is time to upload them to your PlayCanvas scene and setup the materials. This is done by dragging and dropping the files or using the upload button in the assets panel. After you've uploaded your geometry it will auto-generate materials. For each material that a lightmap is rendered for you need to set the lightmap texture. Simply select all required materials and drag'n'drop or pick lightmap texture for the Lightmap slot.

[Image: PlayCanvas Editor: Material Lightmap Texture Slot]

## Final remarks {#final-remarks}

Gamma correction, tone mapping and exposure - are good settings that you will want to play with to get the desired look and color for your scene.

You can [explore the example](https://playcanv.as/p/zdkARz26/) that uses the techniques described above and also its [project](https://playcanvas.com/project/446587/overview/archviz-example).

--------------------------------------------------------------------------------

## Lights

URL: https://developer.playcanvas.com/user-manual/graphics/lighting/lights/

In the real world, the environment around you is lit from many sources. The Sun, street lights, TV screens and so on. In a PlayCanvas scene, you have the ability to set up a number of different types of light sources and the shape of the light source to approximate the different types of light we find in reality.

Descriptions of each light type and light source shape are below and examples of different combinations of light types are given in the [table](#use-cases) below.

## Light Types {#light-types}

There are three types of light in PlayCanvas:

* Directional lights
* Omni lights
* Spot lights

### Directional Lights {#directional-lights}

The most familiar light source to us is the Sun. Because the Sun is so far from Earth, light that hits the surface of our planet can be approximated as traveling in a single direction. In PlayCanvas, this type of light source is called a Directional light.

When unselected, a directional light is represented by the following icon in the Editor's 3D view:

[Image: Directional light icon]

Click this icon to select the light in the Hierarchy and Inspector panels.

A directional light lights an object like this:

[Image: Directional light]

### Omni Lights {#omni-lights}

Omni lights are light sources that emit light in all directions. An example of this type of light source is a candle and other examples can be seen in the [table](#use-cases) below.

When unselected, an omni light is represented by the following icon in the Editor's 3D view:

[Image: Omni light icon]

Click this icon to select the light in the Hierarchy and Inspector panels.

An omni light lights an object like this:

[Image: Omni light]

### Spot Lights {#spot-lights}

Spot lights, like omni lights, emit light in all directions. However, the light from the spot light is constrained to a cone shape.

When unselected, a spot light is represented by the following icon in the Editor's 3D view:

[Image: Spot light icon]

Click this icon to select the light in the Hierarchy and Inspector panels.

A spot light lights an object like this:

[Image: Spot light]

## Light Shapes {#light-shapes}

There are four light source shapes:

* Punctual
* Rectangle
* Disk
* Sphere

### Punctual {#punctual}

The punctual light source shape is an infinitesimally small point. This is the default light source shape and is a less physically correct, but relatively low cost approximation of a light source. The other light source shapes are more costly to render but will give more correct lighting and specular reflections.

### Rectangle {#rectangle}

The rectangle light source shape is a flat 4 sided shape with a specified width and height.

### Disk {#disk}

The disk light source shape is a round and flat light shape with a specified radius.

### Sphere {#sphere}

The sphere light source shape is ball shaped with a specified radius.

[Image: Shapes]

## Use Cases {#use-cases}

Below is a table of some common use cases each light source shape and light type:

| Shape/Type    | Punctual      | Rectangle               | Disk                  | Sphere              |
| ------------- |---------------| ------------------------| ----------------------| --------------------|
| Directional   | sun           | ❌                      | sun or moon           | sun or moon         |
| Omni          | unshaded bulb | ❌                      | ❌                    | unshaded round bulb |
| Spot          | torch         | tv screen               | shaded bulb           | shaded round bulb   |

❌ = no common use cases - but still can be used for application/game specific lighting effects.

## Performance Considerations {#performance-considerations}

Light sources with Rectangle, Disk and Sphere shapes do cost more to render than Punctual lights, so use Punctual light source shapes if you have relatively small light sources or do not have reflective surfaces where Punctual lights would appear visibly incorrect.

--------------------------------------------------------------------------------

## Runtime Lightmaps

URL: https://developer.playcanvas.com/user-manual/graphics/lighting/runtime-lightmaps/

[Image: Sponza]
*All the lighting in this scene is provided by lightmap textures*

Lightmap generation is the process of pre-calculating lighting information for a static scene and storing it in textures, which are then applied on materials. This is an efficient and realistic way to light a scene if many of the light sources and geometry are static or environmental.

## Runtime Lightmap Generation {#runtime-lightmap-generation}

PlayCanvas offers a convenient solution to generating lightmaps. Using the standard light components in the Editor, you can choose which lights are used to bake lightmaps and which are used to dynamically light the scene at runtime. The lights that you set to bake will be used when the application generates the lightmaps that light the scene.

There are multiple advantages to runtime lightmap generation:

- Lighting is not performed at **runtime**
- It is possible to use hundreds of **static lights** to light your scene
- In most cases, rendering lightmaps at runtime is **faster** than downloading many lightmap textures
- It is possible to mix **static and dynamic lights** in the Editor
- **Rebaking** can be performed at runtime
- Lightmaps are **HDR**
- Both **Color** and **Direction** data can be baked, enabling some specularity on baked surfaces

However, a disadvantage of runtime lightmap generation is that currently we do not support baking global illumination or some other advanced features of specialized baking tools.

:::note

The use of [batching](/user-manual/graphics/advanced-rendering/batching) is not compatible with runtime lightmaps, as each lightmapped object requires its own unique lightmap texture.

:::

## Setting Up Lights for Baking {#setting-up-lights-for-baking}

Each Light Component contains the following settings to enable lightmap baking. By default, new lights are set to Dynamic.

- **Bake Lightmap** – When enabled, the light will bake lightmaps for any lightmapped model that is in range.
- **Bake Direction** – Specifies whether light contributes to light direction information baking. This affects specularity results if the **Color and Direction** Lightmapping Mode is chosen in Scene Setting.

[Image: Light Component Settings]

There are two other options that modify the lights behavior: Affect Dynamic and Affect Lightmapped. These determine which models the light will affect at runtime. If either of these options are enabled, the light will operate at runtime and incur runtime cost.

- **Affect Dynamic** – If enabled, the light will affect any model that is **not lightmapped**.
- **Affect Lightmapped** – If enabled, the light will also affect any model that **is lightmapped**.

Note that a light can't have both **Bake Lightmap** and **Affect Lightmapped** enabled, as this would generate a lightmap for a model while the light adds the same lighting at runtime (i.e. the same work is done twice).

[Image: Light Component Shadow Settings]

Lightmap lights use the same **Shadows** setting as dynamic lights, except the shadow calculations are done once, when generating the lightmaps. This way, it is much cheaper to enable shadows on lightmap lights. For more information, see the [Shadows](/user-manual/graphics/lighting/shadows) page. Note that the Shadow Cascade options are ignored for baking.

### Soft Directional Light {#soft-directional-light}

By default, baked lights cast hard shadows. To improve the visual quality, a soft baked shadow is available for **Directional** lights when the **Bake Direction** option is not enabled. In this case, two additional options are available:

- **Bake Samples** – Specifies the number of samples used to bake the light into the lightmap. It defaults to 1 and has a maximum value of 255. The value affects the baking performance and should be set as low as possible (5-20).
- **Bake Area** – Specifies the penumbra angle in degrees, allowing a soft shadow boundary.

[Image: Soft Directional Light Settings]

The following images show the difference between hard shadows and soft shadows. The Bake Samples is 15 and the Area is 10.

[Image: Hard and Soft Shadow Examples]

## Baking an Environment Light {#baking-an-environment-light}

PlayCanvas supports two types of environment lighting: [Ambient Color](/user-manual/editor/interface/settings/rendering/) and [Skybox](/user-manual/editor/interface/settings/rendering/). By default, these are both applied at runtime.

A limitation of runtime environment light application is the lack of **Ambient Occlusion**. As an alternative, the environment light can be baked into the lightmap, including Ambient Occlusion. This can be configured in the [Lightmapping](/user-manual/editor/interface/settings/lightmapping/) section of the global settings.

If **Ambient Bake** is enabled, the contribution of the environment light will be baked to the lightmaps, including this Ambient Occlusion. Note that the **Samples** setting affects the baking performance and should be set as low as possible (5-20).

[Image: Lightmapping Settings]

The following images show the effect of Ambient Color, with and without the Ambient Occlusion.

[Image: Ambient Color Examples]

## Lightmap Filtering {#lightmap-filtering}

For Soft Directional Light or Environment Light baking, a low number of samples is often used in order to improve the baking performance. This creates some banding artifacts, as you can see in the following image, which uses 15 samples.

[Image: Lightmap with 15 samples]

To improve the quality of lightmaps, a higher number of samples can be used. This results in the best quality possible, as you can see in the following image, which uses 100 samples.

[Image: Lightmap with 100 samples]

As a more performant alternative, the lightmap can be filtered using a smart bilateral blur for acceptable quality with greater performance. This can be seen in the following image, which uses 15 samples and has filtering enabled.

[Image: Lightmap with 15 samples and filtering]

Note that the filtering is done on the final baked lightmaps and can create some visible edges over the seams of unwrapped UVs, since the lightmap is not filtered across the seams. Therefore, filtering may not be suitable for every scene. To minimize the artifacts, you should have a good balance between a strong filter and a large number of samples.

## Setting Up Models for Baking {#setting-up-models-for-baking}

Each **Model** or **Render** component must have lightmapping enabled, in order for it to receive lightmaps. Lightmapping can be enabled in the component's properties, by checking the **Lightmapped** option.

[Image: Model Component Settings]

[Image: Render Component Settings]

The **Cast Lightmap Shadows** option determines if the model casts shadows in the lightmap. You can see the resolution of the lightmap texture generated and there is also an option to apply a multiplier to the area of UV1 to affect its size. Lightmap size multipliers are discussed below.

## Common Light Settings {#common-light-settings}

There are several combinations of light settings that can be used. Each one has a use case and by using lights with different combinations, you can balance high-quality visuals with performance.

| Bake  | Affect Non-Baked | Affect Baked | Description |
|-------|-----------------|--------------|-------------|
| false | true            | false        | This is the default dynamic light. Affects all non-lightmapped models. |
| true  | false           | false        | This light generates lightmaps for lightmapped models and has no cost at runtime. Most static environment lights could use this setting. |
| true  | true            | false        | This light generates lightmaps but also affects non-lightmapped models. It is useful if you have dynamic/moving entities that need to be lit with this light. For example, a prominent environment light that also should affect the player character. |
| false | true            | true         | This light is a dynamic light which will affect both lightmapped and non-lightmapped models. |

## Lightmapping Settings {#lightmapping-settings}

The **Size Multiplier** setting affects all Model and Render Components. PlayCanvas will automatically decide what resolution lightmaps are required for a model. It calculates this value based on the scale and geometry area size of the model. You can influence this calculation by modifying the **Size Multiplier** field in the Model or Render Component's Global Settings.

For example, consider a plane that is 1x1 unit (meter) in size. If the Global Size Multiplier is 16 and the Model Component Multiplier is 2, it will generate a Lightmap Texture size of 32x32 (`1 sq/m * 16 * 2`). You will have 32x32 pixels on one square meter, which is about 3cm a pixel size.

**Max Resolution** sets the maximum resolution limit for the generated lightmaps, in order to conserve memory.

**Mode** allows you to specify what data should be baked (e.g. Diffuse Color or Direction from pixel to light). Direction data is used to simulate simplistic specularity. Only a single direction can be baked, which leads to complexity when multiple lights overlap. Direction baking can be then set on individual lights as well.

[Image: Global Lightmapping Settings]

## Auto-Unwrapping and UV1 Generation {#auto-unwrapping-and-uv1-generation}

Lightmaps are always applied using the second set of **UV coordinates (UV1)** on the model asset. For the best results, we recommend that you add a second UV set from the 3D content tool to your model, before you upload it to PlayCanvas. For more information about lightmap friendly UV's, see the [UV Mapping](/user-manual/graphics/lighting/lightmapping/#uv-mapping) section.

If your model doesn't have a UV1 set, the PlayCanvas Editor can automatically unwrap and generate UV1 co-ordinates for the model.

[Image: Model Component: UV1 Missing]

If your model is missing a UV1 map, you will see a warning in the Model Component when you enable lightmapping.

[Image: Model Asset: Auto Unwrap Pipeline]

To fix the warning, select the model asset and open the **Pipeline** section. Click the **Auto-Unwrap** button and wait for the progress bar to complete. Auto-unwrap will edit the model asset, so if you re-import the model from the source (e.g. upload a new FBX) the precomputed UV1 will be lost. If the uploaded model has no UV1, you will need to auto-unwrap the model again.

The **Padding** option determines the space between sections when unwrapping occurs. If you see light bleeding (i.e. light that shouldn't be in the lightmap), you can increase the padding to reduce bleeding.

--------------------------------------------------------------------------------

## Shadows

URL: https://developer.playcanvas.com/user-manual/graphics/lighting/shadows/

Shadows are a great way to add realism to your games. However, dynamic (realtime) shadows can come with a significant runtime performance cost. For a more performant way of adding static shadows to your scene, see [Lightmaps](/user-manual/graphics/lighting/lightmapping).

[Image: Characters with shadow casting]

The PlayCanvas engine implements a shadowing algorithm called shadow mapping. It is completely cross-platform and so is guaranteed to work on both mobile and the desktop.

## Enabling Shadows {#enabling-shadows}

<img loading="lazy" src="/img/user-manual/graphics/lighting/shadows/light-shadow-options.png" width="480" />

By default, shadow casting is disabled in PlayCanvas. You have to explicitly enable it yourself. Fortunately, enabling shadows is easy. First of all, identify which lights in your scene you want to cast shadows. Select the lights in the Hierarchy to edit their properties in the Inspector panel. Every light has a 'Cast Shadows' option. Simply check this option for the light to generate shadows for shadow casting graphical objects in your scene.

[Image: Model Component]

Now you need to specify which graphical objects in your scene cast and receive shadows. By default, all render and model components cast and receive shadows. To modify these properties, select the entity in the Hierarchy, locate the render or model component in the Inspector and uncheck the 'Cast Shadows' or 'Receive Shadows' option as required.

## Shadow Cascades {#shadow-cascades}

When a directional shadow is used over a large area, it often exhibits aliasing, where a shadow near the camera has a low resolution. Capturing the shadow in a single shadow map requires very high and impractical resolution to improve this.

Shadow cascades help to fix this problem by splitting the camera view frustum along the viewing direction, and a separate shadow map is used for each split. This gives nearby objects one shadow map, and another shadow map captures everything in the distance, and optionally additional shadow maps in between.

Note that the number of shadow cascades has an effect on performance, as each shadow casting mesh might need to be rendered into more than a single shadow map.

The following properties can be used to set up shadow cascades.

### Number of cascades {#number-of-cascades}

Number of cascades represents the number of view frustum subdivisions, and can be 1, 2, 3 or 4. The default value of 1 represents a single shadow map.

A screenshot showing a single shadow cascade.

[Image: One cascade]

A screenshot showing four shadow cascades.

[Image: Four cascades]

### Distribution of cascades {#distribution-of-cascades}

The distribution of subdivision of the camera frustum for individual shadow cascades. A value in the range of 0 to 1 can be specified. A value of 0 represents a linear distribution and a value of 1 represents a logarithmic distribution. Visually, a higher value distributes more shadow map resolution to foreground objects, while a lower value distributes it to more distant objects.

## Tuning Shadows {#tuning-shadows}

The shadow mapping technique used by PlayCanvas has only finite resolution. Therefore, you may need to tune some values to make them look as good as possible. The following properties can be found in the [Light Component](/user-manual/editor/scenes/components/light) UI.

### Shadow Distance {#shadow-distance}

The shadow distance is the distance from the viewpoint beyond which directional light shadows are no longer rendered. The smaller this value, the crisper your shadows will be. The problem is that the viewer will be able to see the shadows suddenly appear as the viewpoint moves around the scene. Therefore, you should balance this value based on how far the player can see into the distance and generally what looks good.

### Shadow Intensity {#shadow-intensity}

The intensity of the shadow, where 1 represents full intensity shadow cast by this light, and 0 represents no shadow.

[Image: Shadow Intensity]

### Shadow Resolution {#shadow-resolution}

Every light casts shadows via a shadow map. This shadow map can have a resolution of 256x256, 512x512, 1024x1024 or 2048x2048 and this value is also set in the light component's interface. The higher the resolution, the crisper the shadows. However, higher resolution shadows are more expensive to render so be sure to balance performance against quality.

### Shadow Bias {#shadow-bias}

Shadow mapping can be prone to rendering artifacts that can look very ugly. If you notice bands of shadow or speckled patches where you do not expect, you should try tuning the shadow bias to resolve the problem.

### Normal Offset Bias {#normal-offset-bias}

'Shadow acne' artifacts are a big problem and the shadow bias can eliminate them quite effectively. Unfortunately, this always introduces some level of 'Peter Panning', the phenomenon where shadows make an object appear to be floating in mid-air.

The Normal Offset Bias solves this problem. In addition to using the depth bias, we can avoid both shadow acne and Peter Panning by making small tweaks to the UV coordinates used in the shadow map look-up. A fragment's position is offset along its geometric normal. This "Normal Offset" technique yields vastly superior results to a constant shadow bias only approach.

## Soft Shadows vs Hard Shadows {#soft-shadows-vs-hard-shadows}

The outline of a shadow is called the penumbra. This is a transition from dark to light which gives shadows a soft edge. Softening shadow edges is the default in PlayCanvas but you can change this setting if you wish to achieve hard edged shadows. See below for a comparison of soft and hard edged shadows:

[Image: Hard vs soft shadows]

Soft shadows are achieved by performing more samples of the shadow map on the GPU. The algorithm used is called Percentage Closest Filtering or PCF for short. This algorithm reads 9 localized samples (a 3 by 3 matrix) from the shadow map instead of just one as is used for hard shadows.

The shadow sampling type is specified per light and so the option can be found in the Light Inspector.

## Performance Considerations {#performance-considerations}

Enabling shadows has performance implications:

* For each shadow casting directional or spot light, the scene must be rendered once into a shadow map every frame. Omni light shadows are far more expensive since the scene is rendered six times per light (the shadow map is stored as a 6-sided cube map). Rendering the scene into shadow maps places load on both the CPU and the GPU.
* Using a greater shadow map resolution will generate crisper shadows but the GPU must fill more shadow map pixels and therefore this may affect frame rate.
* Selecting soft shadows (PCF3x3) for the shadow sample type on a shadow receiving material is more expensive on the GPU versus the hard shadows option.
* If your shadows are from static parts of the environment consider using [lightmaps](/user-manual/graphics/lighting/lightmapping) to bake shadows into textures.

--------------------------------------------------------------------------------

## Linear Workflow

URL: https://developer.playcanvas.com/user-manual/graphics/linear-workflow/

In modern rendering engines, a linear workflow is essential for achieving physically accurate lighting and color representation. This approach ensures that all calculations, from shading to post-processing, occur in a linear color space, preventing errors introduced by gamma-compressed textures or incorrect blending. By working in linear space and applying gamma correction only at the final output stage, we maintain consistency across lighting, textures, and effects, resulting in more realistic and predictable visuals.

In engine v1, linear workflow was limited to `StandardMaterial`, but in engine v2, it is fully integrated across all shaders and rendering stages (including `ShaderMaterial`, UI rendering, particles, and every other element) ensuring consistent, physically accurate color processing throughout.

## Shader Input and Output Handling

A proper linear workflow ensures that all color calculations in the shader occur in a physically correct manner. This requires careful handling of both inputs and outputs to maintain accuracy throughout the rendering pipeline.  

### **Shader Inputs: Ensuring Linear Data**  

Shaders require all input values to be in linear space to avoid incorrect lighting results. This affects both textures and uniform color values:  

- **Textures** that store color data (such as albedo maps) should be marked as **sRGB**. When a texture is sampled, the GPU automatically converts sRGB-encoded values into linear space, ensuring correct color calculations.  
- **Color uniforms** are automatically converted from gamma space to linear space for `StandardMaterial`, particle rendering, and other built-in rendering systems. However, when setting uniforms manually using `Material.setParameter` or `MeshInstance.setParameter`, it is the caller's responsibility to ensure the values are provided in linear space. This is especially critical for `ShaderMaterial`, where all parameters must be explicitly defined using `setParameter`. To assist with this, the `Color` class provides the `Color.linear()` function, which converts gamma-space colors to linear space.  

Once all inputs are in linear space, the shader performs lighting calculations with physically accurate results.  

### **Shader Output: Managing Gamma Correction**  

When writing the final color output, the handling of gamma correction depends on whether the rendering is LDR (Low Dynamic Range) or HDR (High Dynamic Range):  

- **LDR Rendering**: Colors are gamma corrected immediately in the shader before being written to the render target, ensuring they are displayed correctly on standard monitors.  
- **HDR Rendering**: Colors remain in linear space when written to the render target, typically requiring a **floating-point format** (e.g., `RGBA16F` or `RGBA32F`) to preserve precision and avoid banding. Gamma correction is then applied later, usually at the final tone-mapping or post-processing stage, allowing effects such as bloom and color grading to work with high-precision linear HDR colors.  

This structured approach ensures that lighting, blending, and post-processing operate consistently, leading to more realistic and predictable rendering results.

--------------------------------------------------------------------------------

## HDR Rendering

URL: https://developer.playcanvas.com/user-manual/graphics/linear-workflow/hdr-rendering/

High Dynamic Range (HDR) rendering significantly enhances visual realism in computer graphics by capturing and displaying a broader spectrum of light and color. This technique ensures that both the brightest highlights and the deepest shadows retain their details, offering a more lifelike representation of scenes. One notable advantage of HDR rendering is its ability to produce physically based bloom effects, where intense light sources naturally bleed into surrounding areas, mimicking real-world camera and eye behavior. Additionally, HDR rendering facilitates more accurate reflections and refractions, as it allows for light values that exceed the standard displayable range, resulting in visuals that are both striking and true to life.

[Image: HDR]

## Camera Settings

The camera provides two key settings for handling HDR rendering:

- **gammaCorrection**
- **toneMapping**

These settings can be configured based on the rendering mode.

### LDR (Low Dynamic Range)

- **toneMapping**: For LDR rendering, you can select any tone mapping method to achieve the desired visual style. The tone mapping compresses HDR values into displayable LDR values.
- **gammaCorrection**: Set to `GAMMA_SRGB` to indicate that the output should be stored in gamma space, as it represents colors.
  - If the output pixel format is sRGB, gamma correction is handled by the hardware.
  - Otherwise, gamma encoding is applied in shader code.

### HDR (High Dynamic Range)

For HDR rendering, the goal is to preserve HDR color information:

- **toneMapping**: Set to `TONEMAP_LINEAR` to maintain HDR colors.
- **gammaCorrection**: Disable by setting to `GAMMA_NONE`.
- Ensure that a compatible HDR pixel format is used for the render target. This format can be obtained using the `GraphicsDevice.getRenderableHdrFormat()` API.

### HDR Display Output

When rendering in HDR mode, an HDR display output can be enabled by configuring the `Application` with the `displayFormat` parameter set to `DISPLAYFORMAT_HDR`.

- **toneMapping**: If HDR output is supported, set to `TONEMAP_NONE`.
- **gammaCorrection**: Keep set to `GAMMA_SRGB` to ensure low-intensity values remain visually similar to LDR rendering.
- After the device has been created, check if HDR display output is supported using `GraphicsDevice.isHdr()`. Note that for `isHdr()` to return `true`, the browser must be running on a display that supports HDR output.

**Note:** Currently, HDR display output is only supported by WebGPU. On other platforms, `GraphicsDevice.isHdr()` will always return `false`.

## PlayCanvas Engine - CameraFrame Class

The PlayCanvas Engine offers a comprehensive rendering setup through the `CameraFrame` class, which integrates advanced effects such as High Dynamic Range (HDR) rendering, bloom, Screen Space Ambient Occlusion (SSAO), and more. This setup enhances visual fidelity by simulating realistic lighting and post-processing effects.

### Key Features of CameraFrame

- **Bloom**: Simulates the scattering of light to create a glow around bright areas.
- **SSAO**: Enhances depth perception by simulating ambient light occlusion in crevices and corners.
- **Depth of Field (DoF)**: Mimics camera focus effects, blurring objects outside the focal plane.
- **Temporal Anti-Aliasing (TAA)**: Reduces visual artifacts by smoothing jagged edges over time.
- **Vignette**: Darkens the image's corners to draw attention to the center.
- **Color Grading**: Adjusts the color balance for stylistic effects.

### Configuring CameraFrame on a Camera

```javascript
const cameraFrame = new pc.CameraFrame(app, cameraEntity.camera);
cameraFrame.rendering.toneMapping = pc.TONEMAP_NEUTRAL;
cameraFrame.rendering.samples = 4;
cameraFrame.bloom.enabled = true;
cameraFrame.bloom.intensity = 0.01;
cameraFrame.update();
```

For HDR bloom to be effective, the scene should include bright light sources. This is typically achieved using emissive materials with high intensity. For example:

```javascript
material.emissive = pc.Color.YELLOW;
material.emissiveIntensity = 50;
```

For more detailed information, refer to the CameraFrame [API documentation](https://api.playcanvas.com/engine/classes/CameraFrame.html).

## CameraFrame in the Editor

There is a `CameraScript` [available here](https://github.com/playcanvas/engine/blob/main/scripts/esm/camera-frame.mjs) for the PlayCanvas Editor project. This script integrates `CameraFrame` functionality directly into the Editor's Inspector, making it easy to set up and configure cameras with advanced rendering features.

### Instructions on Use

1. Add the `CameraScript` into your project and parse it.
2. Add it to an entity that has the `CameraComponent`.
3. Use the Inspector to configure the rendering settings for the camera, such as tone mapping, bloom, SSAO, and other effects.

This integration streamlines the process of setting up complex camera effects and enhances the overall workflow within the PlayCanvas Editor.

[Image: CameraFrame Script]

## CameraFrame Tips

- HDR bloom requires at least one renderable float format (e.g., RG11B10, RGBA16F, or RGBA32F). If none of these formats are supported by the device, HDR bloom is automatically disabled.
- The `toneMapping` property of `StandardMaterial` is ignored. Tonemapping is applied as a full-screen post-processing pass, so per-mesh tonemapping control is not possible.
- When using `CameraFrame`, two properties control tonemapping:
  - `CameraFrame.rendering.toneMapping` – Controls tonemapping for the 3D scene rendered within the `CameraFrame`.
  - `CameraComponent.toneMapping` – Controls tonemapping applied after the 3D scene including post-processing is rendered. This typically affects UI elements rendered on top.
- When using `CameraFrame`, you may notice differences in the intensity of alpha-blended geometry. This occurs because blending takes place in linear HDR space, which is more physically accurate than blending in gamma space. As a result, you may need to adjust material properties related to alpha blending.

--------------------------------------------------------------------------------

## Textures

URL: https://developer.playcanvas.com/user-manual/graphics/linear-workflow/textures/

### sRGB Texture Handling  

Textures that represent colors, such as Diffuse, Emissive, Specular, and Sheen, are typically stored in sRGB space to maintain color accuracy and reduce banding. When used by the engine, these textures are automatically converted from sRGB to linear space for correct lighting calculations. This conversion is performed by the GPU efficiently at no extra cost, provided that the texture is created using an sRGB format.  

#### **Specifying sRGB Encoding for Textures**  

When loading a texture asset that represents colors in sRGB space, it is important to specify sRGB encoding. The following example demonstrates how to create an asset with sRGB encoding:  

```javascript
new pc.Asset(
    'color',
    'texture',
    { url: 'heart.png' },
    { encoding: 'srgb' }
);
```

#### **Marking sRGB Textures in the Editor**  

When working in the Editor, ensure that the color texture is marked as **sRGB** in the inspector panel. This guarantees that the engine correctly interprets the texture as sRGB and applies the necessary conversion to linear space.

[Image: sRGB]

#### **sRGB Procedural Textures / Render Targets**  

When creating a procedural texture or rendering to a texture that represents color and will be read by a shader, it is important to create it with an **sRGB format** to enable automatic conversion. When rendering to this texture, linear values are automatically converted to gamma space to prevent banding. Later, when the texture is used as a color texture, pixels are automatically converted back to linear space.  

The following example demonstrates how to create an sRGB render target texture:  

```javascript
const texture = new pc.Texture(app.graphicsDevice, {
    name: 'color-texture',
    width: 512,
    height: 512,
    format: pc.PIXELFORMAT_SRGBA8
});
```

--------------------------------------------------------------------------------

## Particles

URL: https://developer.playcanvas.com/user-manual/graphics/particles/

PlayCanvas provides comprehensive support for creating and editing particle systems.

## What is a Particle System?

A particle system is a simulation that manages many independently moving particles. They can be used to approximate a huge number of effects such as rain, snow, smoke, fire and so on.

Note that particles are not physically simulated. They do not interact or collide with each other. They will pass through surfaces in your scene.

## Creating a Particle System

In the Editor's 3D View, an unselected particle system is represented with the following icon:

[Image: Particle system icon]

To create a new particle system, simply create a new entity and add a particle system component to it. For convenience, the Editor menu has an item that does this in a single step:

[Image: Particle system creation]

A newly created particle system with the default settings looks like this:

[Image: Default particle system]

To configure the particle system via the particle system component interface, consult the reference [here](/user-manual/editor/scenes/components/particlesystem).

## Triggering a Particle System in Script

Sometimes, you might want a particle system to play in response to some event or at a particular time. For example, an explosion should play when a missile reaches its target. To do this, ensure that the Autoplay option is disabled for your particle system. Then, attach a script component to your particle system entity. The following two lines will start (or restart) a particle system:

```javascript
this.entity.particlesystem.reset();
this.entity.particlesystem.play();
```

## Soft Particles

Soft particles are particles that are faded out near their intersections with scene geometry. If soft particles are enabled by using [```depthSoftening```](https://api.playcanvas.com/engine/classes/ParticleSystemComponent.html#depthsoftening), the camera which renders the particles needs to have a [Depth Map](/user-manual/graphics/cameras/depth-layer) rendering enabled.

--------------------------------------------------------------------------------

## Physically Based Rendering

URL: https://developer.playcanvas.com/user-manual/graphics/physical-rendering/

[Image: Star-Lord]  
*Star-Lord Model by [Joachim Coppens](https://www.joachimcoppens.com/)*

Physically based rendering (PBR) is a combination of artist workflow, measured physical properties and material shaders that work together to bring order and consistency to graphics rendering. Using the underlying physical principles of how light and surfaces interact we can create predictable visuals which work in all lighting conditions without special cases.

## Fundamental Principles

Below, we'll try and summarize the basic principles behind how physically based shaders calculate the lighting. In the next sections we'll cover in more detail the specifics of how physically based rendering can be used within PlayCanvas.

## Diffuse & Specular

Diffuse and Specular (or reflected) light are the two terms that describe two main types of interaction between light and a material. Specular light refers to light which has bounced off the surface. On a smooth surface this light will reflect all the in same direction and the surface will appear mirror-like. Diffuse light is light that has been absorbed, scattered in the material and re-emerged. This light tends to be uniform in direction unlike specular light. During this absorbing and re-emerging some light wavelengths will be absorbed. The wavelengths that are not absorbed give the material its color. For example, if all blue and green wavelengths are absorbed, the material will appear red. In rendering terms, diffuse color is sometimes known as "albedo" or "base color".

## Energy Conservation

[Image: Energy Conservation]
*Smooth surfaces have small bright patches, rough surfaces have large dim patches*

One of the key features of physically correct rendering is that of Energy Conservation. Derived from the fact that the diffuse light and the reflected light all come from the light hitting the material, the sum of diffuse and reflected light can not be more than the total light hitting the material. In practice this means that if a surface is highly reflective it will show very little diffuse color. And the opposite, if a material has a bright diffuse color, it can not reflect much.

The joy of PBR is that energy conservation is included in the shader, so as an artist you don't have to think about it. It just works!

## Metals & Non-metals

[Image: Metals & Non-metals]

One thing that's new with PBR versus older shading models is thinking about what a material is made of in order to determine its behavior. The main thing we consider here is whether the material is a conductor (usually a metal) or an insulator (a non-metal).

The reason this is important is it determines many factors about how the material responds to light. For example, metals are generally reflective (between 60%-90%) whereas non-metals are not (0%-20%). Secondly, reflections on non-metals are usually white whereas metals will usually reflect the same color as the diffuse.

Because of these differences one of the PBR workflows includes a **metalness** property which makes this stuff simple by defining a material as either a metal, or a non-metal. More on metalness workflow in the following sections.

## Fresnel

Fresnel is a term that you don't really have to know about to work with PBR in PlayCanvas, but it will give you a better view of how materials behave if you do.

All you need to know about Fresnel is it means that the angle at which you are viewing a surface influences how reflective that surface appears. If the surface is almost edge on to your view, it will be almost completely reflective.

## Microsurface

Finally onto microsurface. Generally, 3D artists are familiar with the idea of normal maps. Textures that modify the direction of the surface they are applied to. Microsurface, otherwise known as roughness or glossiness, provides a similar thing, only on a much smaller scale. The microsurface of a material describes how rough or smooth a surface is. Compare glass (high glossiness, low roughness) to sandpaper (high roughness, low glossiness). We're not specifying the exact direction the surface faces, just the general idea of rough or smooth.

Some PBR systems use Roughness, some use Glossiness, they are the same thing. Roughness is the inverse of Glossiness and vice versa. If you want to convert from one to the other, simply invert the texture or value.

[Next: Physical Materials](/user-manual/graphics/physical-rendering/physical-materials)

*More reading is available in the great Marmoset Toolbag's [PBR Theory](https://www.marmoset.co/toolbag/learn/pbr-theory) article.*

--------------------------------------------------------------------------------

## Image Based Lighting

URL: https://developer.playcanvas.com/user-manual/graphics/physical-rendering/image-based-lighting/

To get best results with Physically Based Rendering in PlayCanvas you can use the technique called Image Based Lighting or IBL, it allows you to use pre-rendered image data as source information for ambient and reflection light.

This technique relies on [CubeMap](/user-manual/editor/assets/inspectors/cubemap/) - the environment map that is made of 6 textures (faces) forming a cube to have full surround texture coverage.

## HDR

Image data can be stored in LDR or HDR (High Dynamic Range) color space, which allows you to store more than 0.0 to 1.0 (256 gradations) in single channel. HDR allows to store values above 1.0 (what is considered "white"), with combination of many factors of environment such as gamma correction, tone mapping and exposure  it allows to contain more light details and provide much better control over light quality and desirable results to artists.

[Image: HDR vs LDR CubeMap for Image Based Rendering]

Notice how bright parts in the texture are clamped using LDR.

## Energy Conservation

The concept is derived from the fact that the diffuse light and the reflected light all come from the light hitting the material, the sum of diffuse and reflected light can not be more than the total light hitting the material. In practice this means that if a surface is highly reflective it will show very little diffuse color. And the opposite, if a material has a bright diffuse color, it can not reflect much.

In nature, smoother surfaces have sharper reflections and rougher surfaces have blurrier. The reason for that is basically that rougher surfaces have larger, more prominent microfacets, reflecting light in many directions, while smooth surfaces tend to reflect it mostly in one direction. When light coming from different directions is averaged inside a tiny visible point, the result looks blurry to us, and also less bright, thanks to energy conservation. PlayCanvas simulates this behavior with the glossiness parameter, which works automatically for lights, however, for IBL we must precalculate the correct blurred response in advance. This is what the Prefilter button does.

**Prefilter** button is available on CubeMap asset in the Inspector, it is mandatory to enable IBL on physical materials using a CubeMap.

## Authoring Environment Maps

Environment Maps comes in different projections: equirectangular, CubeMap (face list), azimuthal and many others. WebGL and GPU works with face list - set of 6 textures representing sides of a cube - CubeMap. So environment map should be converted into 6 textures if it comes in any other projection.

In order to convert between projections you can use various tools, one of them is cross-platform open-source CubeMap filtering tool: [cmftStudio](https://github.com/dariomanesku/cmftStudio).

CubeMaps can be CGI rendered or assembled from photography, and there are websites to download/buy HDR environment maps. Some of good sources for experimenting can be: [sIBL Archive](http://www.hdrlabs.com/sibl/archive.html), [No Emotion HDR's](http://noemotionhdrs.net/), [Open Footage](https://www.openfootage.net/?tag=hdri), [Paul Debevec](https://www.pauldebevec.com/Probes/). Environment maps can come in equirectangular projection and convertible by cmftStudio mentioned above.

## Rendering CubeMap

CubeMap is made of 6 faces, each representing square side of a cube, simply put: it can be rendered using square viewport camera, by rotating it in different 90 degrees directions with 90 degrees field of view.

[Image: CubeMap Faces]

You can use popular 3D modelling tools, or photography and 360 Imagery software. They should be rendered in linear gamma space and without color corrections that is described in [Lightmapping Gamma Correction section](/user-manual/graphics/lighting/lightmapping/#gamma-correction).

One of the plugins for 3D Studio Max such as [this](http://www.scriptspot.com/3ds-max/scripts/vray-cubemap-generator-for-unity) can be used to render VRay CubeMap Faces, ready to be uploaded into PlayCanvas Editor.

## Applying IBL

This can be done using two methods:

1. Use CubeMap as Skybox in Scene Settings.
2. Use CubeMap as environment map on the Material directly.

## Box Projection Mapping

This technique changes the projection of environment map which allows to specify box within the space so CubeMap corresponds to its bounds. The most common use is to simulate reflections on surfaces within room scale environment.

[Image: Material CubeMap Box Projection]

## Example

Here is an [example](https://playcanv.as/p/zdkARz26/) and [project](https://playcanvas.com/project/446587/overview/archviz-example) of the scene using CubeMap Box Projection. Notice the reflection on the wooden floor from the windows and the subtle reflection on the ceiling, as well as the reflection of the room on the metal PlayCanvas logo on the wall on the right. This is a dynamic effect and can provide very realistic reflections and control to the artist of how surfaces reflect the room environment.

[[Image: Environment Box Projection Mapping]](https://playcanv.as/p/zdkARz26/)

The lighting in this scene is implemented using Lightmap and AO textures and Box Projected IBL (reflections).

--------------------------------------------------------------------------------

## Physical Materials

URL: https://developer.playcanvas.com/user-manual/graphics/physical-rendering/physical-materials/

To use Physically Based Rendering in PlayCanvas you will need to understand how the Physical Material is configured and what effect altering the various parameters will have.

In this section we'll talk about the most useful properties of the material and you can see in the live demos how altering them affects the appearance of a material.

First a note about Cubemaps and Workflows

## Image Based Lighting

[First up IBL](/user-manual/graphics/physical-rendering/image-based-lighting/), because inevitably you'll jump into the Editor and create Materials and wonder why your materials don't look the like the samples below. **Physical Materials with an HDR Prefiltered CubeMap look great!**

## Metalness and Specular Workflow

[Image: Workflows]

PBR is often split into two different methods of building assets or workflows. The two workflows are equivalent and offer the same results. It is really down to your preference as to which you choose. At PlayCanvas we usually choose the "metalness" flow as we find the metalness map simpler to create and is as it is only a single channel it is usually more efficient.

The **metalness** workflow involves setting a metalness value or creating a metalness map which determines which areas of the material are metal or non-metal. Usually this is a simple binary choice. 1 for metal, 0 for non-metal. It is not often that you have a value between the two.

The **specular** workflow involves setting a specular value or creating a specular map which determines the color and intensity of the reflected light for your material.

There is a good explanation of the differences on the [Marmoset Toolbag blog](https://marmoset.co/posts/pbr-texture-conversion/).

On to materials...

## Material Properties and Maps

### Diffuse

The Diffuse Color is the base color of the material. This is an RGB color value. For clean pure (metal, plastic) substances this can be a constant value but it can also be supplied as a diffuse map texture. Note, you should usually avoid including lighting detail (shadows or highlights) in your diffuse map as this can be applied in other maps.

It can also be known as **albedo** or **base color**.

[Interactive Demo]

You can often find the charts of recorded values for diffuse/albedo values on the internet.

[Image: Metals]

| Material | RGB                                      |
|----------|------------------------------------------|
| Gold     | (1.000, 0.766, 0.336) or [255, 195, 86]  |
| Silver   | (0.972, 0.960, 0.915) or [248, 245, 233] |
| Copper   | (0.955, 0.637, 0.538) or [244, 162, 137] |

### Metalness

The metalness value is part of the **metalness** workflow. Metalness is a single value between 0-1 which determines if a material is metal (1) or non-metal (0).

:::note

The metalness value should almost always be 0 or 1. It is rare that you will need a value somewhere between these two.

:::

You can also supply a metalness map which lets you define specific areas of your material as metal or non-metal.

[Interactive Demo]

### Glossiness

Glossiness is used in both **metalness** and **specular** workflows and it defines how smooth your material surface is. The glossiness will affect how blurry or sharp the reflections on the material are, or how broad or narrow the specular highlights are. Glossiness is provided as a single value between 0-100 or a glossiness map.

[Interactive Demo]

Some PBR systems use **Roughness** instead of Glossiness. The roughness is the inverse of the glossiness. If you need to convert a roughness map to a glossiness map, simply invert it.

Sometimes glossiness and roughness are referred to as the **microsurface** value.

### All together

These three properties **diffuse**, **metalness** and **glossiness** are the core of the physical material system. You can try different combinations in the live demo below.

There are many other additional properties to investigate that can be used to make great materials such as Ambient Occlusion, Emissive, Opacity, Normal and Height maps.

[Interactive Demo]

--------------------------------------------------------------------------------

## Post Effects

URL: https://developer.playcanvas.com/user-manual/graphics/posteffects/

PlayCanvas supports adding post-processing effects to your projects. Post effects modify the final rendered image and provide an easy way to add visual flair to your application.

PlayCanvas offers two approaches to post-processing:

## Modern Post Processing

The modern approach provides advanced, performant post-processing with HDR support and extensibility. The primary method uses the `CameraFrame` class with built-in effects, but you can also create fully custom render passes for complete control.

**Key features include:**

- HDR physically based bloom
- Screen Space Ambient Occlusion (SSAO)
- Depth of Field (DoF)
- Temporal Anti-Aliasing (TAA)
- Color grading and LUT support
- Vignette and fringing effects
- And more...

The system is highly extensible, allowing you to customize the compose shader, add custom render passes, or build a complete custom pipeline.

[Learn more about Modern Post Processing →](cameraframe)

## Legacy Post Effects (Script-Based)

The legacy approach uses script-based post effects that can be attached to camera entities. While older, these effects are still fully supported and functional. They provide a simpler setup for basic post-processing needs.

**Available effects:**

- Bloom
- Brightness-Contrast
- Hue-Saturation
- FXAA
- Sepia
- Vignette

[Learn more about Legacy Post Effects →](legacy)

--------------------------------------------------------------------------------

## Modern Post Processing

URL: https://developer.playcanvas.com/user-manual/graphics/posteffects/cameraframe/

PlayCanvas offers modern post-processing workflows that provide visually advanced and performant implementations. The primary approach uses the [`CameraFrame`](https://api.playcanvas.com/engine/classes/CameraFrame.html) class for HDR post-processing with built-in effects, but you can also create fully custom render passes for complete control.

## Features

The `CameraFrame` enables advanced rendering techniques including:

- **Bloom** - HDR physically based bloom effect that simulates the natural glow of bright light sources
- **SSAO (Screen Space Ambient Occlusion)** - Enhances depth perception by simulating ambient light occlusion
- **Depth of Field (DoF)** - Mimics camera focus effects, blurring objects outside the focal plane
- **Temporal Anti-Aliasing (TAA)** - Reduces visual artifacts by smoothing jagged edges over time
- **Vignette** - Darkens or lightens the image edges to draw attention to the center
- **Color Grading** - Adjusts brightness, contrast, saturation, and color tint for stylistic effects
- **Color LUT** - Apply color lookup tables for advanced color transformations
- **Fringing** - Chromatic aberration effect that simulates color channel separation
- **Tone Mapping** - Controls how HDR colors are mapped to displayable range
- **Sharpness** - Enhances image sharpness to counteract blurriness from TAA or upscaling

## Setup and Usage

For detailed information on setting up and using `CameraFrame`, see the [HDR Rendering](/user-manual/graphics/linear-workflow/hdr-rendering/) guide and the [CameraFrame API documentation](https://api.playcanvas.com/engine/classes/CameraFrame.html).

For Editor users, a ready-to-use script is available. See [CameraFrame in the Editor](/user-manual/graphics/linear-workflow/hdr-rendering/#cameraframe-in-the-editor) for setup instructions.

## Examples

- [HDR with Bloom and LUT](https://playcanvas.vercel.app/#/graphics/hdr) - Demonstrates HDR bloom and color lookup table effects
- [Post-Processing](https://playcanvas.vercel.app/#/graphics/post-processing) - Shows multiple effects including bloom, grading, vignette, fringing, and TAA
- [Ambient Occlusion](https://playcanvas.vercel.app/#/graphics/ambient-occlusion) - Demonstrates SSAO implementation
- [Depth of Field](https://playcanvas.vercel.app/#/graphics/depth-of-field) - Demonstrates depth of field effect
- [Temporal Anti-Aliasing](https://playcanvas.vercel.app/#/graphics/taa) - Demonstrates TAA implementation

## Custom Post Processing

Modern post-processing can be customized and extended in several ways. Choose the approach that best fits your needs:

### [Customizing the Compose Shader](compose-shader)

Extend the `CameraFrame` by adding effects to the final compose pass only. This is the simplest approach when you don't need additional render passes.

**Best for:** Simple screen-space effects, color adjustments, quick prototyping.

### [Extending RenderPassCameraFrame Class](extending-class)

Extend the `CameraFrame` by adding custom render passes. This allows you to integrate additional rendering techniques while leveraging built-in effects.

**Best for:** Multi-pass effects, advanced integrations, processing intermediate results.

### [Custom Render Passes](custom-passes)

Build a complete custom post-processing stack without using `CameraFrame`. This gives you full control over the entire rendering pipeline.

**Best for:** Complete custom pipelines, specialized rendering, maximum flexibility.

--------------------------------------------------------------------------------

## Customizing the Compose Shader

URL: https://developer.playcanvas.com/user-manual/graphics/posteffects/cameraframe/compose-shader/

The simplest way to add custom post-effects is by customizing the final compose pass, where all effects are combined and rendered to the backbuffer. This approach is ideal when you don't need additional render passes and want to modify the final output.

## Overview

You can inject custom shader code by overriding shader chunks before creating the `CameraFrame`. The compose pass provides three empty chunks specifically for customization:

- `composeDeclarationsPS` - Add custom uniform declarations and helper functions
- `composeMainStartPS` - Add code at the start of the main function
- `composeMainEndPS` - Add code at the end of the main function, just before the final output

## Example: Simple Pixelation Effect

Here's a complete example showing how to add a pixelation effect:

```javascript

// Override compose shader chunks before creating CameraFrame
const shaderChunks = pc.ShaderChunks.get(graphicsDevice, pc.SHADERLANGUAGE_GLSL);

shaderChunks.set('composeDeclarationsPS', `
    uniform float pixelSize;
`);

shaderChunks.set('composeMainEndPS', `
    // Apply pixelation effect
    vec2 pixelatedUV = floor(uv0 / pixelSize) * pixelSize;
    color = getLinear(texture2D(sceneTexture, pixelatedUV));
`);

// For WebGPU, also set WGSL chunks
const wgslChunks = pc.ShaderChunks.get(graphicsDevice, pc.SHADERLANGUAGE_WGSL);
wgslChunks.set('composeDeclarationsPS', `
    uniform pixelSize: f32;
`);

wgslChunks.set('composeMainEndPS', `
    let pixelatedUV: vec2f = floor(input.uv0 / uniform.pixelSize) * uniform.pixelSize;
    color = getLinear(textureSample(sceneTexture, sceneTextureSampler, pixelatedUV));
`);

// Now create the CameraFrame
const cameraFrame = new pc.CameraFrame(app, cameraEntity.camera);
cameraFrame.update();

// Set the custom uniform value
app.on('update', () => {
    graphicsDevice.scope.resolve('pixelSize').setValue(0.005);
});
```

## Important Notes

- **Global Application**: Changes to shader chunks are applied globally to all `CameraFrame` instances.
- **WebGPU Support**: Remember to provide both GLSL and WGSL shader chunks for cross-platform compatibility.
- **Timing**: Shader chunks must be set before creating the `CameraFrame` instance.

## Resources

- [Custom Compose Shader Example](https://playcanvas.vercel.app/#/graphics/custom-compose-shader) - Complete working demonstration

## Use Cases

This approach is ideal for:

- Adding simple screen-space effects (vignette, color adjustments, distortion)
- Post-processing that doesn't require additional textures or render passes
- Quick prototyping of visual effects
- Effects that operate on the final composed image

--------------------------------------------------------------------------------

## Custom Render Passes

URL: https://developer.playcanvas.com/user-manual/graphics/posteffects/cameraframe/custom-passes/

The most flexible approach is to implement completely custom render passes that work independently from `CameraFrame`. This gives you full control over the rendering pipeline and allows you to build a custom post-processing stack from scratch.

## Overview

This approach does not use `CameraFrame` at all. Instead, you create your own render passes and assign them directly to the camera's `renderPasses` array. This is ideal when you need complete control or want to implement a custom post-processing pipeline.

## Example: Simple Tint Render Pass

Here's a complete example of a custom render pass that tints the scene:

```javascript

class RenderPassTint extends pc.RenderPassShaderQuad {
    constructor(device, sourceTexture) {
        super(device);
        this.sourceTexture = sourceTexture;
        this.tint = pc.Color.WHITE.clone();
        
        this.shader = this.createShader();
    }
    
    createShader() {
        return pc.ShaderUtils.createShader(this.device, {
            uniqueName: 'TintShader',
            attributes: { aPosition: pc.SEMANTIC_POSITION },
            vertexChunk: 'quadVS',
            
            fragmentGLSL: `
                uniform sampler2D sourceTexture;
                uniform vec3 tint;
                varying vec2 uv0;
                
                void main() {
                    vec4 color = texture2D(sourceTexture, uv0);
                    gl_FragColor = vec4(color.rgb * tint, color.a);
                }
            `,
            
            fragmentWGSL: `
                var sourceTexture: texture_2d<f32>;
                var sourceTextureSampler: sampler;
                uniform tint: vec3f;
                varying uv0: vec2f;
                
                @fragment fn fragmentMain(input: FragmentInput) -> FragmentOutput {
                    var output: FragmentOutput;
                    let color: vec4f = textureSample(sourceTexture, sourceTextureSampler, uv0);
                    output.color = vec4f(color.rgb * uniform.tint, color.a);
                    return output;
                }
            `
        });
    }
    
    execute() {
        this.device.scope.resolve('sourceTexture').setValue(this.sourceTexture);
        this.device.scope.resolve('tint').setValue([this.tint.r, this.tint.g, this.tint.b]);
        super.execute();
    }
}
```

## Setting Up Custom Render Passes

To use custom render passes without `CameraFrame`:

```javascript
// Create your scene render pass (renders the 3D scene)
const scenePass = new pc.RenderPassForward(device, composition, scene, renderer);
scenePass.init(renderTarget);

// Create your custom post-processing pass
const tintPass = new RenderPassTint(device, renderTarget.colorBuffer);
tintPass.init(camera.renderTarget);

// Assign passes to the camera
camera.renderPasses = [scenePass, tintPass];
```

## Multi-Pass Example

Here's an example of chaining multiple custom passes:

```javascript
// Create render targets
const rt1 = new pc.RenderTarget({
    colorBuffer: new pc.Texture(device, {
        width: 1920, height: 1080,
        format: pc.PIXELFORMAT_RGBA8
    })
});

const rt2 = new pc.RenderTarget({
    colorBuffer: new pc.Texture(device, {
        width: 1920, height: 1080,
        format: pc.PIXELFORMAT_RGBA8
    })
});

// Create scene pass
const scenePass = new pc.RenderPassForward(device, composition, scene, renderer);
scenePass.init(rt1);

// Create blur pass (horizontal)
const blurHPass = new RenderPassBlurHorizontal(device, rt1.colorBuffer);
blurHPass.init(rt2);

// Create blur pass (vertical)
const blurVPass = new RenderPassBlurVertical(device, rt2.colorBuffer);
blurVPass.init(camera.renderTarget); // Final output

// Set the pass chain
camera.renderPasses = [scenePass, blurHPass, blurVPass];
```

## Resources

- [Render Pass Example](https://playcanvas.vercel.app/#/graphics/render-pass) - Complete demonstration of custom render passes

## Use Cases

This approach is ideal for:

- **Complete custom pipelines** - When you need total control over the rendering process
- **Integration with external systems** - Integrating third-party rendering or effects libraries
- **Performance optimization** - Building a minimal pipeline tailored to your specific needs
- **Learning and experimentation** - Understanding how rendering pipelines work
- **Specialized rendering** - Non-standard rendering techniques or research implementations

## Important Considerations

- **Manual management** - You're responsible for managing render targets, textures, and memory
- **Memory management** - Ensure proper resource cleanup to avoid memory leaks
- **Cross-platform** - Provide both GLSL and WGSL shaders for WebGL and WebGPU support
- **Resolution handling** - Handle dynamic resolution changes appropriately
- **Layer management** - Properly configure which layers each pass renders

--------------------------------------------------------------------------------

## Extending RenderPassCameraFrame Class

URL: https://developer.playcanvas.com/user-manual/graphics/posteffects/cameraframe/extending-class/

For more advanced customization, you can extend the `RenderPassCameraFrame` class to add custom render passes or modify the rendering pipeline. This approach gives you full control over the pass creation and ordering while still leveraging the built-in CameraFrame effects.

## Overview

By extending `RenderPassCameraFrame`, you can:

- Add custom render passes to the pipeline
- Modify or replace existing passes
- Control the order of pass execution
- Access intermediate textures from the rendering pipeline

## Example: Adding a Custom Render Pass

This example demonstrates how to extend the `RenderPassCameraFrame` class to insert a custom render pass into the rendering pipeline. By overriding the `createPasses()` method, you can add your own processing step that operates on the scene texture. The `collectPasses()` method then controls where in the pipeline your custom pass executes—in this case, right before the final compose pass. This is useful when you need to apply custom effects that require their own render pass, such as edge detection, custom blur effects, or any processing that needs intermediate render targets.

```javascript

class CustomRenderPassCameraFrame extends pc.RenderPassCameraFrame {
    createPasses(options) {
        // Call the base implementation to create standard passes
        super.createPasses(options);
        
        // Add your custom render pass
        this.customPass = new MyCustomRenderPass(this.device, this.sceneTexture);
        
        // You can also modify or replace existing passes here
    }
    
    collectPasses() {
        // Override to control pass ordering
        const passes = super.collectPasses();
        
        // Insert your custom pass at the desired position
        // This example inserts it before the compose pass
        passes.splice(passes.indexOf(this.composePass), 0, this.customPass);
        
        return passes;
    }
}

// Use your custom class by extending CameraFrame
class MyCameraFrame extends pc.CameraFrame {
    createRenderPass() {
        return new CustomRenderPassCameraFrame(this.app, this, this.cameraComponent, this.options);
    }
}

// Create an instance of your custom CameraFrame
const cameraFrame = new MyCameraFrame(app, cameraEntity.camera);
cameraFrame.update();
```

## Use Cases

This approach is suitable for:

- Adding render passes that require their own render targets
- Implementing complex multi-pass effects
- Integrating third-party rendering techniques
- Advanced users who need fine-grained control over the pipeline
- Effects that need to process intermediate results from other passes

--------------------------------------------------------------------------------

## Legacy Post Effects

URL: https://developer.playcanvas.com/user-manual/graphics/posteffects/legacy/

PlayCanvas provides legacy script-based post effects that are still supported and functional. While many of these effects now have newer, more performant replacements in [CameraFrame](/user-manual/graphics/posteffects/cameraframe), they remain available for use.

## Setup

We have implemented post effects as scripts that you can add to an Entity that has a [Camera](/user-manual/editor/scenes/components/camera) component attached. To add post effects to a camera, do the following:

1. Choose one of the effects below and get the script from the GitHub link.

    - [Bloom](/user-manual/graphics/posteffects/legacy/bloom)
    - [Brightness-Contrast](/user-manual/graphics/posteffects/legacy/brightness_contrast)
    - [Hue-Saturation](/user-manual/graphics/posteffects/legacy/hue_saturation)
    - [FXAA](/user-manual/graphics/posteffects/legacy/fxaa)
    - [Sepia](/user-manual/graphics/posteffects/legacy/sepia)
    - [Vignette](/user-manual/graphics/posteffects/legacy/vignette)

2. Add a [Script](/user-manual/editor/scenes/components/script) component to the Entity representing your camera.
3. Assign the desired post effect scripts to the camera entity's Script component. Note that the order in which the post effect scripts are listed in the Script component determine the order in which they are applied.

You can also create your own post effects. You can find some additional ones on [GitHub](https://github.com/playcanvas/engine/tree/main/scripts/posteffects).

--------------------------------------------------------------------------------

## Bloom Effect

URL: https://developer.playcanvas.com/user-manual/graphics/posteffects/legacy/bloom/

[Bloom](https://en.wikipedia.org/wiki/Bloom_(shader_effect)) is a post-processing effect used to reproduce an imaging artifact of real-world cameras. The effect produces fringes (or feathers) of light extending from the borders of bright areas in an image, contributing to the illusion of an extremely bright light overwhelming the camera capturing the scene.

Here is an image without bloom:

[Image: Image without effect]

And the same image with bloom applied:

[Image: Image with effect]

The built-in bloom effect has the following attributes:

* **Bloom Intensity**: The intensity of the effect
* **Bloom Threshold**: Only pixels brighter than this threshold will be processed. Ranges from 0 to 1.
* **Blur Amount**: Controls the amount of blurring.

Find the post-processing effect script on [GitHub](https://github.com/playcanvas/engine/blob/main/scripts/posteffects/posteffect-bloom.js).

--------------------------------------------------------------------------------

## Brightness-Contrast Effect

URL: https://developer.playcanvas.com/user-manual/graphics/posteffects/legacy/brightness_contrast/

The brightness-contrast effect allows you to modify the brightness and contrast of the rendered image.

Here is an image without the effect:

[Image: Image without effect]

And the same image with the effect applied and changes to brightness and contrast:

[Image: Image with effect]

The built-in brightness-contrast effect has the following attributes:

* **Brightness**: The brightness of the image. Ranges from -1 to 1 (-1 is solid black, 0 no change, 1 solid white).
* **Contrast**: The contrast of the image. Ranges from -1 to 1 (-1 is solid gray, 0 no change, 1 maximum contrast).

Find the post-processing effect script on [GitHub](https://github.com/playcanvas/engine/blob/main/scripts/posteffects/posteffect-brightnesscontrast.js).

--------------------------------------------------------------------------------

## FXAA Effect

URL: https://developer.playcanvas.com/user-manual/graphics/posteffects/legacy/fxaa/

Fast Approximate Anti-Aliasing (FXAA) is an anti-aliasing algorithm created by NVIDIA. It provides an easy and fast way to add anti-aliasing to your scene.

Here is an image without the effect:

[Image: Image without effect]

And the same image with the effect applied:

[Image: Image with effect]

Find the post-processing effect script on [GitHub](https://github.com/playcanvas/engine/blob/main/scripts/posteffects/posteffect-fxaa.js).

--------------------------------------------------------------------------------

## Hue-Saturation Effect

URL: https://developer.playcanvas.com/user-manual/graphics/posteffects/legacy/hue_saturation/

The hue-saturation effect allows you to modify the hue and saturation of the rendered image.

Here is an image without the effect:

[Image: Image without effect]

And the same image with the effect applied and changes to hue and saturation:

[Image: Image with effect]

The built-in hue-saturation effect has the following attributes:

* **Hue**: The hue of the image. Ranges from -1 to 1 (-1 is 180 degrees in the negative direction, 0 no change, 1 is 180 degrees in the positive direction).
* **Saturation**: The saturation of the image. Ranges from -1 to 1 (-1 is solid gray, 0 no change, 1 maximum saturation).

Find the post-processing effect script on [GitHub](https://github.com/playcanvas/engine/blob/main/scripts/posteffects/posteffect-huesaturation.js).

--------------------------------------------------------------------------------

## Sepia Effect

URL: https://developer.playcanvas.com/user-manual/graphics/posteffects/legacy/sepia/

The Sepia effect makes the image look like an old photograph.

Here is an image without the effect:

[Image: Image without effect]

And the same image with the effect applied:

[Image: Image with effect]

The built-in sepia effect has the following attributes:

* **Amount**: Controls the intensity of the effect. Ranges from 0 to 1.

Find the post-processing effect script on [GitHub](https://github.com/playcanvas/engine/blob/main/scripts/posteffects/posteffect-sepia.js).

--------------------------------------------------------------------------------

## Vignette Effect

URL: https://developer.playcanvas.com/user-manual/graphics/posteffects/legacy/vignette/

In photography and optics, [vignetting](https://en.wikipedia.org/wiki/Vignetting) is the reduction of an image's brightness or saturation at the periphery compared to the image center. You can use it to draw attention to the center of the frame.

Here is an image without the effect:

[Image: Image without effect]

And the same image with the effect applied:

[Image: Image with effect]

The built-in vignette effect has the following attributes:

* **Offset**: Controls the offset of the effect.
* **Darkness**: Controls the darkness of the effect.

Find the post-processing effect script on [GitHub](https://github.com/playcanvas/engine/blob/main/scripts/posteffects/posteffect-vignette.js).

--------------------------------------------------------------------------------

## Shaders

URL: https://developer.playcanvas.com/user-manual/graphics/shaders/

When you import your 3D models into PlayCanvas, by default, they will use our [Physical Material](/user-manual/graphics/physical-rendering/physical-materials/). This is a versatile material type that can cover a lot of your rendering needs.

However, you will often want to perform special effects or special cases for your materials. To do this you will need to write a custom shader. In this case, you need to use `ShaderMaterial`.

To create an instance of `ShaderMaterial`, these are the steps:

Create a description of your shader:

```javascript
const shaderDesc = {
        uniqueName: 'MyShader',
        vertexGLSL: `
            // write your vertex shader source code in GLSL language
        `,
        fragmentGLSL: `
            // write your fragment shader source code in GLSL language
        `,
        vertexWGSL: `
            // write your vertex shader source code in WGSL language
        `,
        fragmentWGSL: `
            // write your fragment shader source code in WGSL language
        `,
        attributes: {
            aPosition: pc.SEMANTIC_POSITION,
            aUv0: pc.SEMANTIC_TEXCOORD0
        }
    };

```

Then create instances of your material, which you can use for rendering:

```javascript
const material = new pc.ShaderMaterial(shaderDesc);
```

The shader source code can be written in GLSL if you're targeting the WebGL2 or WebGPU platforms, or in WGSL if you're targeting WebGPU only, or both.

:::note

If you write a GLSL shader, it is directly supported by the WebGL2 platform. However, on the WebGPU platform, GLSL shaders require transpilation to WGSL using a WASM transpiler. To avoid this transpilation step and achieve native performance related to shader compilation, and avoid additional download of WASM files, you might want to consider writing an equivalent shader in WGSL for the WebGPU platform, which is supported directly.

:::

## Preprocessor {#preprocessor}

Before the shader is used, a preprocessing step is applied, allowing you to manage shader variations effectively.

This preprocessing step follows a typical C-like preprocessor structure, handling directives such as `#define`, `#if`, `#else`, `#endif`, and similar. This gives you fine-grained control over how the shader code is compiled and customized for different use cases.

You can also use a `#include` directive to include one of the registered shader chunks. For example: `#include "screenDepthPS"`

### Material Shader Defines

Shader defines can be set on a per-material basis, allowing dynamic customization of shader behavior. For example:

```javascript
material.setDefine('USE_TEXTURE', true);
material.setDefine('FIRETYPE', 'RED');
```

This results in the following lines being added to the shader source:

```glsl
#define USE_TEXTURE
#define FIRETYPE RED
```

You can then use these defines within the shader for conditional logic:

```glsl
#if defined(USE_TEXTURE)
// Apply texture-based rendering
#endif

#if FIRETYPE == RED
// Apply red fire effect
#endif
```

This system enables flexible shader variation without requiring multiple shader files, making it easier to customize rendering for different materials.

### RenderPass Defines {#renderpass-defines}

The engine provides some defines automatically, allowing integration with render passes. By default, one of these three defines is provided to allow you to write code specific to different render passes:

```glsl
// Defined for normal forward passes rendering colors
#define FORWARD_PASS

// Defined for shadow rendering passes
// Shader output specifics may depend on the shadow type used
#define SHADOW_PASS

// Defined for the render pass used by the `Picker` class to render mesh instance IDs
#define SHADOW_PICK 
```

If you use a custom render pass, created using [`CameraComponent.setShaderPass`](https://api.playcanvas.com/engine/classes/CameraComponent.html#setshaderpass), a matching define is automatically generated. For example:

```javascript
camera.setRenderPass('custom');
```

This results in the following define being added to the shader:

```glsl
#define CUSTOM_PASS
```

### Shader Includes {#shader-includes}

The engine builds internal shaders out of chunks; small shader functions that are combined to form a final shader. These chunks are also available for use in custom shaders with `ShaderMaterial`, making it easy to integrate engine functionality.

#### Vertex Shader {#vertex-shader}

The engine provides predefined shader includes that handle common transformations, normal calculations, and other essential operations. This allows your custom shader to automatically support skinning, morphing and instancing.

For example:

```glsl
// Includes transformation-related functionality provided by the engine.
// - Automatically declares the `vertex_position` attribute.
// - Handles skinning and morphing if necessary.
// - Adds the following uniforms:
//   - `matrix_viewProjection`
//   - `matrix_model`
//   - `matrix_normal`
// - Provides utility functions:
//   - `getModelMatrix()`
//   - `getLocalPosition()`
#include "transformCoreVS"

// Includes normal-related functionality provided by the engine.
// - Automatically declares the `vertex_normal` attribute.
// - Handles skinning and morphing if necessary.
// - Provides utility functions:
//   - `getNormalMatrix()`
//   - `getLocalNormal()`
#include "normalCoreVS"

void main(void)
{
    // Retrieve the model matrix, accounting for skinning, morphing, or instancing.
    mat4 modelMatrix = getModelMatrix();
    vec3 localPos = getLocalPosition(vertex_position.xyz);
    vec4 worldPos = modelMatrix * vec4(localPos, 1.0);

    // Retrieve the normal matrix and compute the world normal.
    mat3 normalMatrix = getNormalMatrix(modelMatrix);
    vec3 localNormal = getLocalNormal(vertex_normal);
    vec3 worldNormal = normalize(normalMatrix * localNormal);

    // Example: Apply simple wrap-around diffuse lighting using the world normal.
    brightness = (dot(worldNormal, uLightDir) + 1.0) * 0.5;

    // Transform the geometry.
    gl_Position = matrix_viewProjection * worldPos;
}
```

#### Fragment Shader {#fragment-shader}

The engine provides predefined shader chunks you can include for common color processing effects such as gamma correction, tone mapping and fog. These includes ensure that colors are processed correctly according to the rendering settings.

Example Usage

```glsl
#include "gammaPS"       // Adds support for gamma correction of inputs and outputs
#include "tonemappingPS" // Adds support for tone mapping
#include "fogPS"         // Adds support for fog effects

void main(void)
{
    // Evaluate color in linear color space
    vec3 colorLinear = ...;

    // Apply fog if enabled
    vec3 fogged = addFog(colorLinear);

    // Apply tone mapping if enabled
    vec3 toneMapped = toneMap(fogged);

    // Apply gamma correction and output the final color
    gl_FragColor.rgb = gammaCorrectOutput(toneMapped);
    gl_FragColor.a = alpha;
}
```

These functions are automatically configured based on the engine's settings, ensuring that color processing is consistent across different rendering conditions.

:::note

For more complete examples, and also for details on how to implement instancing, refer to the engine examples.

:::

#### Generated Shaders {#generated-shaders}

If you have a need to inspect the generated shaders, you can add this to your script

```javascript
pc.Tracing.set(pc.TRACEID_SHADER_ALLOC, true);
```

And Each created shader will be logged in the browser console, where you can inspect its source code, for example:

[Image: sRGB]

For further information, refer to the [ShaderMaterial API documentation](https://api.playcanvas.com/engine/classes/ShaderMaterial.html).

--------------------------------------------------------------------------------

## Compute Shaders

URL: https://developer.playcanvas.com/user-manual/graphics/shaders/compute-shaders/

Compute shaders are programs that run general-purpose computations on the GPU, independent of the rendering pipeline. Unlike vertex and fragment shaders, compute shaders are not tied to geometry or pixels—they operate on arbitrary data, making them ideal for tasks such as particle simulation, image processing, physics calculations, and procedural content generation.

:::warning

Compute shaders are only supported on the **WebGPU** platform. They are not available when using WebGL.

:::

## Checking for Support

Before using compute shaders, verify that the device supports them:

```javascript
if (device.supportsCompute) {
    // Compute shaders are available
}
```

## Creating a Compute Shader

A compute shader is created using the `Shader` class with WGSL code. The shader definition includes the compute shader source (`cshader`), bind group format, and optionally uniform buffer formats.

### Basic Shader Definition

```javascript
const shader = new pc.Shader(device, {
    name: 'MyComputeShader',
    shaderLanguage: pc.SHADERLANGUAGE_WGSL,
    cshader: `
        @compute @workgroup_size(1, 1, 1)
        fn main(@builtin(global_invocation_id) global_id: vec3u) {
            // Compute shader logic here
        }
    `,
    computeBindGroupFormat: new pc.BindGroupFormat(device, [
        // Resource bindings go here
    ])
});
```

By default, the engine expects the entry point function to be named `main`. You can use `computeEntryPoint` to specify a different function name, which also allows a single shader source to contain multiple entry points:

```javascript
const shaderSource = `
    @compute @workgroup_size(64, 1, 1)
    fn initParticles(@builtin(global_invocation_id) global_id: vec3u) {
        // Initialize particles
    }

    @compute @workgroup_size(64, 1, 1)
    fn updateParticles(@builtin(global_invocation_id) global_id: vec3u) {
        // Update particles
    }
`;

// Create separate shaders from the same source, each using a different entry point
const initShader = new pc.Shader(device, {
    name: 'InitParticles',
    shaderLanguage: pc.SHADERLANGUAGE_WGSL,
    cshader: shaderSource,
    computeEntryPoint: 'initParticles',
    // ...
});

const updateShader = new pc.Shader(device, {
    name: 'UpdateParticles',
    shaderLanguage: pc.SHADERLANGUAGE_WGSL,
    cshader: shaderSource,
    computeEntryPoint: 'updateParticles',
    // ...
});
```

### Bind Group Format

The `computeBindGroupFormat` defines what resources are available to the compute shader. You can bind various types of resources:

#### Storage Buffers

Storage buffers allow read/write access to large amounts of data:

```javascript
// Read-write storage buffer
new pc.BindStorageBufferFormat('particles', pc.SHADERSTAGE_COMPUTE)

// Read-only storage buffer
new pc.BindStorageBufferFormat('spheres', pc.SHADERSTAGE_COMPUTE, true)
```

In WGSL, access storage buffers like this:

```wgsl
@group(0) @binding(0) var<storage, read_write> particles: array<f32>;
@group(0) @binding(1) var<storage, read> spheres: array<vec4f>;
```

#### Storage Textures

Storage textures allow the compute shader to write directly to a texture:

```javascript
new pc.BindStorageTextureFormat('outTexture', pc.PIXELFORMAT_RGBA8, pc.TEXTUREDIMENSION_2D)
```

In WGSL:

```wgsl
@group(0) @binding(0) var outputTexture: texture_storage_2d<rgba8unorm, write>;

// Writing to the texture
textureStore(outputTexture, vec2i(global_id.xy), color);
```

#### Input Textures

Input textures provide read-only texture data. The last parameter controls whether a sampler is included:

```javascript
// Texture without sampler (for textureLoad)
new pc.BindTextureFormat('inputTexture', pc.SHADERSTAGE_COMPUTE, undefined, undefined, false)

// Texture with sampler (for textureSampleLevel)
new pc.BindTextureFormat('inputTexture', pc.SHADERSTAGE_COMPUTE, undefined, undefined, true)
```

In WGSL, when a sampler is included, it uses the texture name with a `_sampler` suffix:

```wgsl
// Without sampler - use textureLoad for direct texel access
@group(0) @binding(0) var inputTexture: texture_2d<f32>;
let color = textureLoad(inputTexture, position, 0);

// With sampler - use textureSampleLevel for filtered sampling
@group(0) @binding(0) var inputTexture: texture_2d<f32>;
@group(0) @binding(1) var inputTexture_sampler: sampler;
let color = textureSampleLevel(inputTexture, inputTexture_sampler, uv, 0.0);
```

:::note

In compute shaders, use `textureSampleLevel` instead of `textureSample` because you must explicitly specify the mip level (LOD).

:::

#### Uniform Buffers

For passing uniform data to compute shaders, first define the uniform buffer format:

```javascript
const uniformBufferFormat = new pc.UniformBufferFormat(device, [
    new pc.UniformFormat('tint', pc.UNIFORMTYPE_VEC4),
    new pc.UniformFormat('time', pc.UNIFORMTYPE_FLOAT),
    new pc.UniformFormat('count', pc.UNIFORMTYPE_UINT)
]);
```

Then include it in the shader definition along with the bind group:

```javascript
const shader = new pc.Shader(device, {
    name: 'ComputeShader',
    shaderLanguage: pc.SHADERLANGUAGE_WGSL,
    cshader: shaderCode,

    // Assign the uniform buffer format
    computeUniformBufferFormats: {
        ub: uniformBufferFormat
    },

    // Include uniform buffer in bind group
    computeBindGroupFormat: new pc.BindGroupFormat(device, [
        new pc.BindUniformBufferFormat('ub', pc.SHADERSTAGE_COMPUTE),
        // ... other bindings
    ])
});
```

In WGSL:

```wgsl
struct ub_compute {
    tint: vec4f,
    time: f32,
    count: u32
}

@group(0) @binding(0) var<uniform> ubCompute: ub_compute;

@compute @workgroup_size(1, 1, 1)
fn main(@builtin(global_invocation_id) global_id: vec3u) {
    let t = ubCompute.time;
    let c = ubCompute.count;
}
```

## Creating a Compute Instance

The `Compute` class represents an executable instance of a compute shader with its associated parameters:

```javascript
const compute = new pc.Compute(device, shader, 'MyComputeInstance');
```

## Setting Parameters

Use `setParameter` to bind resources and set uniform values:

```javascript
// Bind a storage buffer
compute.setParameter('particles', storageBuffer);

// Bind a texture
compute.setParameter('inputTexture', texture);

// Set uniform values
compute.setParameter('time', 1.5);
compute.setParameter('count', 1024);
compute.setParameter('tint', [1.0, 0.5, 0.0, 1.0]);
```

## Creating Storage Buffers

Storage buffers hold data that compute shaders can read from and write to:

```javascript
const storageBuffer = new pc.StorageBuffer(
    device,
    bufferSizeInBytes,
    pc.BUFFERUSAGE_COPY_SRC |  // Enable reading back to CPU
    pc.BUFFERUSAGE_COPY_DST    // Enable writing from CPU
);

// Write initial data
const data = new Float32Array([...]);
storageBuffer.write(0, data);

// Clear the buffer
storageBuffer.clear();
```

## Creating Storage Textures

Storage textures are created with the `storage: true` option:

```javascript
const storageTexture = new pc.Texture(device, {
    name: 'StorageTexture',
    width: 512,
    height: 512,
    format: pc.PIXELFORMAT_RGBA8,
    mipmaps: false,
    minFilter: pc.FILTER_LINEAR,
    magFilter: pc.FILTER_LINEAR,
    storage: true  // Enable as storage texture
});
```

## Dispatching Compute Shaders

To execute a compute shader, first set up the dispatch dimensions, then dispatch:

```javascript
// Set up dispatch dimensions (number of workgroups in X, Y, Z)
compute.setupDispatch(width, height, 1);

// Dispatch the compute shader
device.computeDispatch([compute], 'MyDispatch');
```

Multiple compute shaders can be dispatched together in a single compute pass:

```javascript
compute1.setupDispatch(64, 64);
compute2.setupDispatch(128, 128);
device.computeDispatch([compute1, compute2], 'BatchedDispatch');
```

### Workgroup Size

The total number of invocations is `dispatchSize × workgroupSize`. For example, if you dispatch with `(width, height)` and your shader has `@workgroup_size(1, 1, 1)`, you get `width × height` invocations.

For better performance with large datasets, use larger workgroup sizes:

```wgsl
@compute @workgroup_size(64, 1, 1)
fn main(@builtin(global_invocation_id) global_id: vec3u) {
    // Process element at global_id.x
}
```

Then dispatch accordingly:

```javascript
const numElements = 1024 * 1024;
const workgroupSize = 64;
compute.setupDispatch(numElements / workgroupSize);
```

## Indirect Dispatch

Indirect dispatch allows one compute shader to generate dispatch parameters for another compute shader, enabling fully GPU-driven workloads without CPU readback. This is useful for:

- Variable workload sizes determined on the GPU
- Tile-based processing where tile counts are computed dynamically
- GPU culling followed by processing only visible elements

### Reserving Dispatch Slots

The device provides a built-in buffer for indirect dispatch parameters. Reserve slots each frame:

```javascript
const slot = device.getIndirectDispatchSlot();
```

Each slot holds three 32-bit unsigned integers representing the x, y, and z workgroup counts. The maximum number of slots is controlled by `device.maxIndirectDispatchCount` (default: 256).

### Writing Dispatch Parameters

Pass the indirect buffer to your compute shader so it can write dispatch parameters. In your bind group format:

```javascript
new pc.BindStorageBufferFormat('indirectBuffer', pc.SHADERSTAGE_COMPUTE)
```

In WGSL, define a struct matching the indirect dispatch layout and write the parameters:

```wgsl
struct DispatchIndirectArgs {
    x: u32,
    y: u32,
    z: u32
};

@group(0) @binding(0) var<storage, read_write> indirectBuffer: array<DispatchIndirectArgs>;
@group(0) @binding(1) var<uniform> uniforms: Uniforms; // Contains slot index

@compute @workgroup_size(1)
fn main() {
    // Compute workload size dynamically
    let workloadSize = calculateWorkload();
    
    // Write dispatch parameters to the slot
    indirectBuffer[uniforms.slot].x = workloadSize;
    indirectBuffer[uniforms.slot].y = 1u;
    indirectBuffer[uniforms.slot].z = 1u;
}
```

### Using Indirect Dispatch

Configure the second compute shader to read dispatch parameters from the buffer using `setupIndirectDispatch`:

```javascript
// Reserve a slot for this frame
const slot = device.getIndirectDispatchSlot();

// First pass: compute shader writes dispatch parameters
prepareCompute.setParameter('indirectBuffer', device.indirectDispatchBuffer);
prepareCompute.setParameter('slot', slot);
prepareCompute.setupDispatch(1, 1, 1);
device.computeDispatch([prepareCompute]);

// Second pass: dispatch using parameters from the buffer
processCompute.setupIndirectDispatch(slot);
device.computeDispatch([processCompute]);
```

:::note

When using the device's built-in indirect buffer, `setupIndirectDispatch` must be called each frame because slots are only valid for the current frame.

:::

### Custom Indirect Buffers

For advanced use cases like complex scheduling outside of rendering frames, you can provide your own storage buffer:

```javascript
// Create a custom buffer for indirect dispatch
const customBuffer = new pc.StorageBuffer(device, 3 * 4, pc.BUFFERUSAGE_INDIRECT);

// Use custom buffer for indirect dispatch
compute.setupIndirectDispatch(0, customBuffer);
```

When using a custom buffer, you manage its lifetime and contents—no frame validation is performed.

## Reading Data Back to CPU

To read results from a storage buffer back to the CPU:

```javascript
const resultData = new Float32Array(numElements);
storageBuffer.read(0, undefined, resultData).then((data) => {
    // Process the data
    console.log('First value:', data[0]);
});
```

Note that `read()` returns a Promise because GPU operations are asynchronous. The data will be available after the GPU finishes executing the compute shader, which may be several frames later.

For time-critical reads, you can pass `immediate: true` as the fourth parameter:

```javascript
storageBuffer.read(0, undefined, resultData, true).then((data) => {
    // Data available sooner, but with performance cost
});
```

By default (`immediate: false`), the read is deferred to the next event handling cycle when the GPU command buffer is naturally submitted. With `immediate: true`, the command buffer is submitted immediately and the read executes right away.

:::warning

Using `immediate: true` has a performance impact as it forces an early command buffer submission. Only use it when low-latency reads are essential.

:::

## Preprocessor

Compute shaders support the same [shader preprocessor](/user-manual/graphics/shaders/preprocessor) as vertex and fragment shaders, including `#define`, `#ifdef`, `#if`, `#include`, and more.

### Defines and Includes

Use `cdefines` to pass defines and `cincludes` to provide include content:

```javascript
const shader = new pc.Shader(device, {
    name: 'ComputeShader',
    shaderLanguage: pc.SHADERLANGUAGE_WGSL,
    cshader: `
        #include "myHelper"

        @compute @workgroup_size({WORKGROUP_SIZE}, 1, 1)
        fn main(@builtin(global_invocation_id) global_id: vec3u) {
            var<workgroup> sharedData: array<f32, {WORKGROUP_SIZE}>;
            // ...
        }
    `,
    cdefines: new Map([
        ['{WORKGROUP_SIZE}', '64']
    ]),
    cincludes: pc.ShaderChunks.get(device, pc.SHADERLANGUAGE_WGSL),
    // ...
});
```

The `{WORKGROUP_SIZE}` placeholders are replaced with `64` before compilation. See the [preprocessor documentation](/user-manual/graphics/shaders/preprocessor) for details on regular defines vs injection defines.

## Examples

Explore these live examples demonstrating various compute shader use cases:

- [Histogram](https://playcanvas.github.io/#/compute/histogram) - Compute image histogram using atomic operations
- [Texture Generation](https://playcanvas.github.io/#/compute/texture-gen) - Generate and modify textures with compute shaders
- [Particles](https://playcanvas.github.io/#/compute/particles) - GPU-based particle simulation with collision detection
- [Vertex Update](https://playcanvas.github.io/#/compute/vertex-update) - Modify mesh vertex buffers in real-time
- [Edge Detect](https://playcanvas.github.io/#/compute/edge-detect) - Image processing with edge detection
- [Indirect Draw](https://playcanvas.github.io/#/compute/indirect-draw) - GPU-driven rendering with indirect draw calls
- [Indirect Dispatch](https://playcanvas.github.io/#/compute/indirect-dispatch) - GPU-driven compute dispatch with depth-based tile classification

--------------------------------------------------------------------------------

## GLSL Specifics

URL: https://developer.playcanvas.com/user-manual/graphics/shaders/glsl-specifics/

GLSL shaders used by the PlayCanvas engine must satisfy certain requirements. These requirements allow the engine to correctly integrate shaders, ensuring they receive the necessary resources such as attributes, uniforms, and varyings. Following these rules also allows us to automatically process the shader for slightly different requirements when transpiling to WGSL for use with WebGPU.

The following sections outline key aspects of writing GLSL shaders for PlayCanvas.

:::note

`#version` should not be included in the shader source. PlayCanvas automatically adds the appropriate version directive based on whether WebGL2 or WebGPU is targeted.

:::

### Attributes

Attributes define per-vertex input data, and can only be used in the vertex shader. They must be declared using the following syntax:

```glsl
attribute vec2 aUv0;
```

The attribute names must match the names specified in the `attributes` property when creating the [ShaderMaterial](/user-manual/graphics/shaders/).

:::note

The `in` keyword (introduced in GLSL 3.3+) is not supported.

:::

### Uniforms

Uniforms are used to pass resources from the engine to the shader. They are declared in the standard way for numerical and texture uniforms:

```glsl
uniform vec3 view_position;
```

The engine automatically sets appropriate uniform values when rendering.

:::note

Currently, our uniform system supports only simple types, including `float`, `int`, `uint`, as well as vectors and matrices (e.g., `vec4`, `mat4`). Structs are not supported at this time, so all uniform values must be declared as individual variables of basic types.

:::

### Precision Qualifiers

PlayCanvas automatically sets `highp` precision (when supported by the device) for the following types:

- `float`
- `int` (also applies to `uint`)
- `usampler2D` (unsigned integer textures)
- `isampler2D` (signed integer textures)
- `sampler2DShadow`
- `samplerCubeShadow`
- `sampler2DArray`

The following sampler types do **not** have default precision set:

- `sampler2D`
- `sampler3D`
- `samplerCube`

If you need `highp` precision for these types, you must specify it manually. This is particularly important when sampling from floating-point textures (e.g., `RGBA32F`) where full 32-bit precision is required. Without `highp`, sampled values may lose precision. Note that `highp` is not applied to standard samplers by default due to potential performance impact on mobile GPUs.

To apply precision to all samplers of a type:

```glsl
precision highp sampler2D;

uniform sampler2D hdrTextureA;
uniform sampler2D hdrTextureB;
```

To apply precision to a single uniform:

```glsl
uniform highp sampler2D hdrTexture;
uniform sampler2D ldrTexture;
```

### Varyings

Varyings are used to pass values from the vertex shader to the fragment shader. They must be declared using standard GLSL syntax:

```glsl
varying vec2 uv0;
```

:::note

The `in`/`out` syntax (introduced in GLSL 3.3+) is not supported.

:::

--------------------------------------------------------------------------------

## Shader Chunk Migrations

URL: https://developer.playcanvas.com/user-manual/graphics/shaders/migrations/

## Introduction

The PlayCanvas Engine's material shader chunk system is undergoing substantial changes in order to support a more flexible material system. Please see [this page](https://github.com/playcanvas/engine/issues/4250) for more context.

In order to help users migrate their existing custom shader chunks, this page lists the changes made to chunks and organizes them by engine release (starting v1.51).

## Chunk API Versions

The debug version of the Engine will report any API changes to the runtime console when it detects overridden chunks. For example:

[Image: Console output]

Once an application's chunks have been updated to the latest API they must be flagged as such. For example, after updating a material's custom chunks to the latest engine release (say v2.8), specify this in the chunks object as follows:

```javascript
const materialChunksGLSL = material.getShaderChunks(pc.SHADERLANGUAGE_GLSL);
materialChunksGLSL.set('diffusePS', '...');
material.shaderChunksVersion = '2.8';
```

By doing this you will no longer see warning messages in the console.

## Chunk changes

The following tables break down the chunk changes by Engine release.

### *Engine v2.15*

#### Gaussian Splat Shader Customization

The `gsplatCustomizeVS` shader chunk has been deprecated and replaced with `gsplatModifyVS`. The new chunk provides a more efficient API that uses rotation quaternion and scale vector instead of covariance matrices. See [PR #8246](https://github.com/playcanvas/engine/pull/8246) for details.

| Old (`gsplatCustomizeVS`) | New (`gsplatModifyVS`) |
| --- | --- |
| `modifyCenter(inout vec3 center)` | `modifySplatCenter(inout vec3 center)` |
| `modifyCovariance(originalCenter, modifiedCenter, inout covA, inout covB)` | `modifySplatRotationScale(originalCenter, modifiedCenter, inout rotation, inout scale)` |
| `modifyColor(center, inout color)` | `modifySplatColor(center, inout color)` |

Helper function changes:

| Old | New |
| --- | --- |
| `gsplatApplyUniformScale(covA, covB, scale)` | `scale *= factor` (direct multiplication) |
| `gsplatExtractSize(covA, covB)` | `gsplatGetSizeFromScale(scale)` |
| `gsplatMakeRound(covA, covB, radius)` | `gsplatMakeSpherical(scale, radius)` |

**Migration example (GLSL):**

Before:

```glsl
void modifyCenter(inout vec3 center) {
    center.y += 1.0;
}

void modifyCovariance(vec3 originalCenter, vec3 modifiedCenter, inout vec3 covA, inout vec3 covB) {
    gsplatApplyUniformScale(covA, covB, 2.0);
}

void modifyColor(vec3 center, inout vec4 color) {
    color.rgb *= 0.5;
}
```

After:

```glsl
void modifySplatCenter(inout vec3 center) {
    center.y += 1.0;
}

void modifySplatRotationScale(vec3 originalCenter, vec3 modifiedCenter, inout vec4 rotation, inout vec3 scale) {
    scale *= 2.0;
}

void modifySplatColor(vec3 center, inout vec4 color) {
    color.rgb *= 0.5;
}
```

**JavaScript usage:**

```javascript
// Before
gsplatMaterial.getShaderChunks(shaderLanguage).set('gsplatCustomizeVS', customShader);

// After
gsplatMaterial.getShaderChunks(shaderLanguage).set('gsplatModifyVS', customShader);
```

---

### *Engine v2.6*

#### Internal engine chunks

The following vertex shader chunks were removed and replaced by a single `litMainVS` chunk:

- `endVS`
- `startVS`
- `baseVS`
- `viewNormalVS`
- `baseNineSlicedVS`

`lightmapDirAddPS` chunk has been removed, and its functionality integrated into `lightmapAddPS` chunk.

`TBNderivativePS` and `TBNObjectSpacePS` chunks were removed, and their functionality integrated into `TBNPS` chunk.

`startPS` chunk has been removed, and a replacement larger chunk will be added at a later stage.

`outputAlphaOpaquePS` and `outputAlphaPremulPS` chunks were merged into `outputAlphaPS` chunk.

`cubeMapProjectBoxPS` and `cubeMapProjectNonePS` chunks were merged into `cubeMapProjectPS` chunk.

`envMultiplyPS` and `envConstPS` were merged into `envProcPS` chunk.

`aoSpecOccSimplePS`, `aoSpecOccConstSimplePS`, `aoSpecOccPS` and `aoSpecOccConstPS` chunks were merged into `aoSpecOccPS` chunk.

`shadowSampleCoordPS` chunk has been removed, and its content is now part of `lightFunctionPS` chunk.

The following reflection related chunks had a slight change in how the texture decode function is provided. `$DECODE` is now `{reflectionDecode}` and `$DECODE_CUBEMAP` is now `{reflectionCubemapDecode}`. These chunks were affected:

- `reflectionEnvPS`
- `reflectionEnvHQPS`
- `reflectionCubePS`
- `reflectionSpherePS`

The following ambient lighting related chunks had been removed, and merged into a single `ambientPS` chunk:

- `ambientConstantPS`
- `ambientEnvPS`
- `ambientSHPS`

### *Engine v2.5*

The following chunks were removed and replaced by a single `fogPS` chunk:

- `fogExpPS`
- `fogExp2PS`
- `fogLinearPS`
- `fogNonePS`

The following chunks were removed and replaced by a single `gammaPS` chunk:

- `gamma1_0PS`
- `gamma2_2PS`

### *Engine v1.70*

| Chunk | Changes |
| ---   | ---     |
| `refractionDynamicPS` | <ul><li>Now accepts additional parameter `float dispersion`.</li></ul> |
| `refractionCubePS` | <ul><li>Now accepts additional parameter `float dispersion`.</li></ul> |

### *Engine v1.65*

In 1.62, global variables used to pass the values between the front end back end chunks were grouped into structures LitShaderArguments, IridescenceArgs, ClearcoatArgs and SheenArgs. Those were causing multiple compatibility issues on Android devices, and so in 1.65, these are being converted back to global variables. For example `litShaderArgs.albedo` is now `litArgs_albedo`.

These are the new global variables:

```glsl
// Surface albedo absorbance
vec3 litArgs_albedo;

// Transparency
float litArgs_opacity;

// Emission color
vec3 litArgs_emission;

// Normal direction in world space
vec3 litArgs_worldNormal;

// Ambient occlusion amount, range [0..1]
float litArgs_ao;

// Light map color
vec3 litArgs_lightmap;

// Light map direction
vec3 litArgs_lightmapDir;

// Surface metalness factor, range [0..1]
float litArgs_metalness;

// The f0 specularity factor
vec3 litArgs_specularity;

// Specularity intensity factor, range [0..1]
float litArgs_specularityFactor;

// The microfacet glossiness factor, range [0..1]
float litArgs_gloss;

// Glossiness of the sheen layer, range [0..1]
float litArgs_sheen_gloss;

// The color of the f0 specularity factor for the sheen layer
vec3 litArgs_sheen_specularity;

// Transmission factor (refraction), range [0..1]
float litArgs_transmission;

// Uniform thickness of medium, used by transmission, range [0..inf]
float litArgs_thickness;

// Index of refraction
float litArgs_ior;

// Iridescence effect intensity, range [0..1]
float litArgs_iridescence_intensity;

// Thickness of the iridescent microfilm layer, value is in nanometers, range [0..1000]
float litArgs_iridescence_thickness;

// The normal used for the clearcoat layer
vec3 litArgs_clearcoat_worldNormal;

// Intensity of the clearcoat layer, range [0..1]
float litArgs_clearcoat_specularity;

// Glossiness of clearcoat layer, range [0..1]
float litArgs_clearcoat_gloss;
```

These are the chunk that had their signature changed to accept individual members, instead of the whole structures:

- endPS
- metalnessModulatePS
- outputAlphaPS
- outputAlphaPremulPS
- fresnelSchlickPS
- iridescenceDiffractionPS
- lightmapAddPS
- lightmapDirAddPS
- refractionCubePS
- refractionDynamicPS

### *Engine v1.62*

In PlayCanvas, we have two sets of shader chunks, one set we refer to as the shader frontend, which provide values for the arguments passed to our lighting algorithm, also called the shader backend.

With 1.62, we are creating a clearer distinction between these two, such that the values passed to the backend are well defined and known in advance, not automatically generated. This allows for writing a fully custom shader that can interface with our lighting code just like how our native materials do.

As a result of that, almost all backend chunks have been changed to accommodate for the split. This means that any custom backend shader chunks must move away from using globals to using the arguments passed to them by the lighting backend.

This change also makes some chunks, such as the clearcoat specific ones, redundant, as their functions have become reusable when they're no longer reliant on global values.

#### Changes

This release breaks most lit/frag chunks. Most of these chunks have had their signatures changed to accept the various values they need, instead of relying on globals. With that said, most globals are still set in the shader. An example of this change is:

```glsl
vec3 combineColor() {
    vec3 ret = vec3(0);
    ret = dAlbedo * dDiffuseLight;
    ...
}
```

Is now expressed:

```glsl
vec3 combineColor(vec3 albedo, vec3 sheenSpecularity, float clearcoatSpecularity) {
    vec3 ret = vec3(0);
    ret = albedo * dDiffuseLight;
    ...
}
```

Where we previously had globals, in 1.62 they are packed into structs, these structs are the primary LitShaderArgs which is defined as such:

```glsl
struct LitShaderArguments
{
    // Transparency
    float opacity;

    // Normal direction in world space
    vec3 worldNormal;

    // Surface albedo absorbance
    vec3 albedo;

    // Transmission factor (refraction), range [0..1]
    float transmission;

    // Uniform thickness of medium, used by transmission, range [0..inf]
    float thickness;

    // The f0 specularity factor
    vec3 specularity;

    // The microfacet glossiness factor, range [0..1]
    float gloss;

    // Surface metalness factor, range [0..1]
    float metalness;

    // Specularity intensity factor, range [0..1]
    float specularityFactor;

    // Ambient occlusion amount, range [0..1]
    float ao;

    // Emission color
    vec3 emission;

    // Light map color
    vec3 lightmap;

    // Light map direction
    vec3 lightmapDir;

    // Iridescence extension arguments
    IridescenceArgs iridescence;

    // Clearcoat extension arguments
    ClearcoatArgs clearcoat;

    // Sheen extension arguments
    SheenArgs sheen;
};
```

The last three arguments are our shading extensions. IridescenceArgs is defined as such:

```glsl
struct IridescenceArgs
{
    // Iridescence effect intensity, range [0..1]
    float intensity;

    // Thickness of the iridescent microfilm layer, value is in nanometers, range [0..1000]
    float thickness;
};
```

ClearcoatArgs:

```glsl
struct ClearcoatArgs
{
    // Intensity of the clearcoat layer, range [0..1]
    float specularity;

    // Glossiness of clearcoat layer, range [0..1]
    float gloss;

    // The normal used for the clearcoat layer
    vec3 worldNormal;
};
```

SheenArgs:

```glsl
struct SheenArgs
{
    // Glossiness of the sheen layer, range [0..1]
    float gloss;

    // The color of the f0 specularity factor for the sheen layer
    vec3 specularity;
};
```

| Chunk | Changes |
| --- | --- |
| `ambient(Constant/Env/SH)` | <ul><li>Accepts a vec3 for the world normal instead of using `dNormalW`</li></ul> |
| `aoDiffuseOcc` | <ul><li>Accepts a float value for the AO, instead of using `dAO`</li></ul> |
| `aoSpec(Occ/OccConst/OccConstSimple/OccSimple)` | <ul><li>Accepts float gloss, float ao, a vec3 world normal and a vec3 view direction instead of using `dGlossiness`, `dAo`, `dNormalW` and `dViewDirW`</li></ul> |
| `combine` | <ul><li>Accepts vec3 for albedo, sheen specularity and a float for clearcoat specularity instead of using `dAlbedo`, `sSpecularity` and `ccSpecularity`</li></ul> |
| `clusteredLight` | <ul><li>Reliance on globals have been reduced to only `dLightPosW`, `dLightDirW`, `dLightDirNormW` and `dShadowCoord` which is initialized per light</li></ul> |
| `clusteredLightShadow` | <ul><li>For omni lights, generates a local variable instead of relying on `dShadowCoord`. For spot lights, accepts the shadow coordinate instead of using `dShadowCoord` as before</li></ul> |
| `combine` | <ul><li>Accepts vec3 albedo, vec3 sheen specularity and float clearcoat specularity instead of using `dAlbedo`, `sSpecularity` and `ccSpecularity`</li></ul> |
| `end` | <ul><li>Passes albedo, sheen specularity and clearcoat specularity to combine using `litShaderArgs`, uses `litShaderArgs.emission` instead of relying on `dEmission`</li></ul> |
| `fallOff(InvSquared/Linear)` | <ul><li>Accepts a float light radius and a vec3 light direction instead of using `dLightDirW`</li></ul> |
| `fresnelSchlick` | <ul><li>Accepts gloss and `IridescenceArgs` instead of relying on `dGlossiness`, `dIridescenceFresnel` and `dIridescence`</li></ul> |
| `iridescenceDiffraction` | <ul><li>Accepts a float as iridescence thickness instead of using `dIridescenceThickness`</li></ul> |
| `lightDiffuseLambert` | <ul><li>Accepts vec3 world normal, a vec3 view direction, a vec3 light direction and a vec3 normalized light direction instead of using `dNormalW`, `dViewDirW`, `dLightDirW` and `dLightDirNormW`</li></ul> |
| `lightSheen` | <ul><li>Accepts a vec3 half vector, a vec3 world normal, a vec3 view direction, a vec3 normalized light direction and a float gloss instead of relying on `dNormalW`, `dViewDirW`, `dLightDirNormW` and `dGlossiness`</li></ul> |
| `lightSpecular(AnisoGGX/Blinn/Phong)` | <ul><li>Accepts a vec3 half vector for the reflection, a vec3 reflection direction (used by Phong only), a vec3 world normal, a vec3 view dir, a float gloss value and a 3x3 matrix for the TBN, instead of relying on `dReflDirW`, `dNormalW`, `dViewDirW`, `dGlossiness/ccGlossiness` and `dTBN`</li></ul> |
| `lightmap(DirAdd/Add)` | <ul><li>Accepts a vec3 lightmap value, a vec3 lightmap direction, a vec3 world normal, a vec3 view direction, float gloss, vec3 specularity, a read-write vec3 normalized light direction, a vec3 geometric normal and IridescenceArgs instead of relying on `dLightMap`, `dLightmapDir`, `dNormalW`, `dViewDirW`, `dGlossiness`, `dVertexNormalW` and `dSpecularity`</li></ul> |
| `ltc` | <ul><li>No longer uses `dViewDirW`, `dNormalW`, `dGlossiness`, `dSpecularity`, `ccGlossiness`, `ccSpecularity` and `dLightDirW`, but instead relies on their values being passed as arguments</li></ul> |
| `metalnessModulate` | <ul><li>Accepts a `LitShaderArguments` struct which is updated by the chunk. Removes the reliance on `dSpecularity`, `dMetalness` and `dAlbedo`</li></ul> |
| `output(Alpha/AlphaPremul)` | <ul><li>Uses `litShaderArgs.opacity` instead of `dAlpha`</li></ul> |
| `reflDir(Aniso)` | <ul><li>Accepts a vec3 world normal, a vec3 view direction, a float value for gloss and 3x3 matrix for the TBN, instead of using `dGlossiness`, `dViewDirW`, `dNormalW` and `dTBN`</li></ul> |
| `reflection(CC/Cube/Env/EnvHQ/Sphere/SphereLow)` | <ul><li>Accepts a vec3 reflection direction and a float gloss value instead of using `dReflDirW`/`ccReflDirW` and `dGlossiness`</li></ul> |
| `reflectionSheen` | <ul><li>Accepts a vec3 world normal, a vec3 view direction and a float gloss value instead of using `dNormalW`, `dViewDirW` and `sGlossiness`</li></ul> |
| `refraction(Cube/Dynamic)` | <ul><li>Accepts a vec3 world normal, float thickness, float gloss, vec3 specularity, vec3 albedo, float transmission and `IridescenceArgs` instead of using `dNormalW`, `dAlbedo`, `dTransmission`, `dThickness`, `dGlossiness`, `dSpecularity` and passes the iridescence arguments to the fresnel function</li></ul> |
| `shadow(Common/Coord/CoordPerspZBuffer` | <ul><li>Accepts a permutation of a vec3 light direction, a vec3 light position, a vec3 normalized light direction and a vec3 geometric normal instead of using `dLightDirW`, `dLightPosW`, `dLightDirNormW` and `dVertexNormalW` and instead accepts them as arguments. The permutation depends on the requirements for the different shadow coordinate functions</li></ul> |
| `shadow(EVSM/EVSMn/Standard/StandardGL2/VSM8)` | <ul><li>Accepts a vec3 shadow sample coordinate instead of using `dShadowCoord`</li></ul> |
| `spot` | <ul><li>Accepts a vec3 normalized light direction instead of using `dLightDirNormW`</li></ul> |
| `TBN(-/ObjectSpace/derivative/fast)` | <ul><li>Accepts a vec3 tangent, binormal and normal instead of using `dTangentW`, `dBinormalW` and `dNormalW`</li></ul> |

---

### Engine v1.60

| Chunk | Changes |
| ---   | ---     |
| `clearCoatGlossPS` | <ul><li>Renamed uniform `material_clearCoatGlossiness` to `material_clearCoatGloss`.</li></ul> |
| `glossPS`          | <ul><li>Renamed uniform `material_glossiness` to `material_gloss`.</li></ul> |
| `sheenGlossPS`     | <ul><li>Renamed uniform `material_sheenGlossiness` to `material_sheenGloss`.</li></ul> |

---

### Engine v1.57

In 1.57, almost all front-end chunks have been changed to minimize the amount of samplers used by the shader. This is an optional feature, however it's recommended to follow the same coding style to reduce the amount of samplers used by the shader. The following chunks are affected by it:

| Chunk  |
| --- |
| `aoPS` |
| `clearCoatPS` |
| `clearCoatGlossPS` |
| `clearCoatNormalPS` |
| `diffusePS` |
| `diffuseDetailMapPS` |
| `emissivePS` |
| `metalnessPS` |
| `normalMapPS` |
| `normalDetailMapPS` |
| `opacityPS` |
| `parallaxPS` |
| `sheenPS` |
| `sheenGlossPS` |
| `specularPS` |
| `specularityFactorPS` |
| `thicknessPS` |
| `transmissionPS` |

This is also supported in custom front-end chunks, given that your chunk piggybacks on the pre-existing material samplers. To support this method in your chunks, what you'd need to do is:

- Remove the sampler uniform declaration from the chunk
- Replace the sampler name with the `$SAMPLER` macro

For example:

```glsl
uniform sampler2D texture_aoMap;
void getAO() {
    dAo = 1.0;

    #ifdef MAPTEXTURE
    dAo *= texture2DBias(texture_aoMap, $UV, textureBias).$CH;
    #endif

    #ifdef MAPVERTEX
    dAo *= saturate(vVertexColor.$VC);
    #endif
}
```

Would be converted to:

```glsl
void getAO() {
    dAo = 1.0;

    #ifdef MAPTEXTURE
    dAo *= texture2DBias($SAMPLER, $UV, textureBias).$CH;
    #endif

    #ifdef MAPVERTEX
    dAo *= saturate(vVertexColor.$VC);
    #endif
}
```

This allows the engine to automatically pick the sampler uniform to use, thus potentially reducing the total number of samplers. But note, this is only supported for front-end chunks.

---

### Engine v1.56

| Chunk | Changes |
| ---   | ---     |
| `combineXXXX` | <ul><li>all combine chunks except for `combinePS` have been deleted.</li><li>instead, combinePS is controlled with a handful of preprocessor defines.</li></ul> |
| `refractionPS` | <ul><li>split into two new chunks, `refractionCubePS` and `refractionDynamicPS`.</li></ul> |
| `refractionCubePS` | <ul><li>the old `refractionPS` is identical to this one, uses a cube map for refractions.</li></ul> |
| `refractionDynamicPS` | <ul><li>new chunk which supports dynamic refractions by using the grab pass, needs `requestSceneColorMap(true);` to be set on the camera to work.</li></ul> |
| `sheenPS` | <ul><li>new chunk to provide sheen (fabric) color.</li></ul> |
| `sheenGlossPS` | <ul><li>new chunk to provide sheen (fabric) glossiness.</li></ul> |
| `reflectionEnvHQPS` | <ul><li>new chunk to provide a high quality specular environment map for reflections and refractions.</li></ul> |
| `thicknessPS` | <ul><li>new chunk to provide thickness which modifies attenuation color for transmissive (transparent/refractive) materials.</li></ul> |
| `bakeDirLmEndPs` | <ul><li>moved to `chunks-lightmapper.js`.</li></ul> |
| `bakeLmEndPS` | <ul><li>moved to `chunks-lightmapper.js`.</li></ul> |

---

### Engine v1.55

| Chunk | Changes |
| --- | --- |
| `clearCoatNormalPS` | <ul><li>refrain from generating world CC reflection, now done on the backend instead</li><li>normalize final world space normal</li></ul> |
| `clusteredLightPS` | <ul><li>remove dead code.</li><li>the `CLUSTER_XXX` macros have been renamed to `LIT_XXX`.</li><li>each light calculates fresnel</li></ul> |
| `combinePS` | <ul><li>new chunk to replace all the other combine chunks.</li></ul> |
| `combineXXXX` | <ul><li>combine chunk variations have been made deprecated and replaced with a single chunk.</li></ul> |
| `diffusePS` | <ul><li>fix gamma handling relative to albedo detail</li></ul> |
| `diffuseDetailMapPS` | <ul><li>gamma correct detail map before combining with base albedo</li></ul> |
| `endPS` | <ul><li>combine emissive with `dEmissive` instead of a call to `getEmission()`</li><li>`CLEARCOAT` macro is now `LIT_CLEARCOAT`.</li></ul> |
| `emissivePS` | <ul><li>set `dEmission` global instead of returning the value in order to bring it in line with the other frontend components</li></ul> |
| `fresnelSchlickPS` | <ul><li>fresnel effect now reacts to index of refraction.</li><li>no longer changes specularity global, but returns value to be used per-light and for the environment</li></ul> |
| `lightmapSingleVert.js` | <ul><li>removed (unused)</li></ul> |
| `lightmapDirPS`, `lightmapSinglePS`| <ul><li>renamed the lightmap function to `getLightMap()` instead of `addLightMap()`</li><li>changed the implementation to write `dLightmap` and `dLightmapDir` global instead of updating `dDiffuseLight` and `dSpecularLight` directly</li><li>backend now handles combining lightmap in `lightmapAddPS` and `lightmapDirAddPS`</li></ul> |
| `lightmapAddPS`, `lightmapDirAddPS` | <ul><li>new chunks for adding the lightmap values passed in from the backend</li><li>`CLEARCOAT` macro replaced with `LIT_CLEARCOAT`.</li></ul> |
| `lightSpecularAnisoGGXPS` | <ul><li>`CLEARCOAT` define replaced with `LIT_CLEARCOAT`</li></ul> |
| `lightSpecularBlinnPS`, `lightSpecularPhongPS` | <ul><li>added clear coat `#define`, removed call to `antiAliasGlossiness()`</li></ul> |
| `ltcPS` | <ul><li>`CLEARCOAT` macro replaced with `LIT_CLEARCOAT`.</li></ul> |
| `normalMapFastPS` | <ul><li>removed</li></ul> |
| `normalMapPS` | <ul><li>added `MAPTEXTURE` #define like the other chunks</li><li>normalize final normal</li><li>when normal texture isn't defined, calculate normal from geometry normal instead</li></ul> |
| `normalDetailMapPS` | <ul><li>remove two (mostly) unnecessary calls to `normalize` - final normal is normalized instead</li></ul> |
| `normalVertexPS` | <ul><li>removed chunk, moved functionality to `normalMapPS` frontend chunk</li></ul> |
| `metalnessPS` | <ul><li>now controls metalness in front end and is not exclusive of `specularPS`</li></ul> |
| `metalnessModulatePS` | <ul><li>new chunk to control how specular color is modulated based on specular color and albedo with regards to metalness</li></ul> |
| `reflectionCC` | <ul><li>`CLEARCOAT` define replaced with `LIT_CLEARCOAT`.</li></ul> |
| `specularAaNonePS`, `specularAaToksvigPS`, `specularAaToksvigFastPS` | <ul><li>removed</li></ul> |
| `startPS` | <ul><li>removed global declarations, generate them on demand instead</li><li>`CLEARCOAT` macro replaced with `LIT_CLEARCOAT`.</li></ul> |
| `specularPS` | <ul><li>only provides specular color, metalness modulation is now done in backend.</li></ul> |
| `specularityFactorPS` | <ul><li>new chunk to control specular intensity for metalness workflow.</li></ul> |

--------------------------------------------------------------------------------

## Shader Preprocessor

URL: https://developer.playcanvas.com/user-manual/graphics/shaders/preprocessor/

Before shaders are compiled, PlayCanvas applies a C-style preprocessor to the source code. This allows you to manage shader variations, conditionally include code, and inject values. The preprocessor works with GLSL, WGSL, and compute shaders.

## Preprocessor Directives

The following directives are supported:

### Define and Undefine

```glsl
#define FEATURE_ENABLED
#define MAX_LIGHTS 4
#define MULTIPLIER 2.5

#undef FEATURE_ENABLED
```

Defines can be simple flags (no value) or have associated values.

### Conditional Compilation

```glsl
#ifdef FEATURE_ENABLED
    // Code included only if FEATURE_ENABLED is defined
#endif

#ifndef FEATURE_ENABLED
    // Code included only if FEATURE_ENABLED is NOT defined
#endif

#if defined(FEATURE_A) && defined(FEATURE_B)
    // Code included if both FEATURE_A and FEATURE_B are defined
#endif

#if MAX_LIGHTS > 2
    // Code included if MAX_LIGHTS is greater than 2
#endif
```

### If-Else-Elif Chains

```glsl
#if QUALITY == 0
    // Low quality path
#elif QUALITY == 1
    // Medium quality path
#else
    // High quality path
#endif
```

### Logical Operators

The preprocessor supports `&&` (AND), `||` (OR), and `!` (NOT) operators:

```glsl
#if defined(FEATURE_A) && !defined(FEATURE_B)
    // FEATURE_A is defined but FEATURE_B is not
#endif

#if defined(FEATURE_A) || defined(FEATURE_B)
    // At least one of FEATURE_A or FEATURE_B is defined
#endif
```

### Comparison Operators

Supported operators: `==`, `!=`, `<`, `<=`, `>`, `>=`

```glsl
#if MAX_LIGHTS >= 4
    // 4 or more lights supported
#endif

#if QUALITY != 0
    // Not low quality
#endif
```

### Include Directive

The `#include` directive inserts content from registered shader chunks:

```glsl
#include "chunkName"
```

For example, to include engine-provided chunks:

```glsl
#include "gammaPS"
#include "tonemappingPS"
```

#### Registering Custom Shader Chunks

The recommended way to add custom includes is by registering them with `ShaderChunks`. This allows you to provide both GLSL and WGSL versions, and the engine automatically uses the appropriate one:

```javascript
// Get the shader chunks for each language
const chunksGLSL = pc.ShaderChunks.get(device, pc.SHADERLANGUAGE_GLSL);
const chunksWGSL = pc.ShaderChunks.get(device, pc.SHADERLANGUAGE_WGSL);

// Register your custom chunk in both languages
chunksGLSL.set('myUtilsPS', `
    float myHelper(float x) {
        return x * 2.0;
    }
`);

chunksWGSL.set('myUtilsPS', `
    fn myHelper(x: f32) -> f32 {
        return x * 2.0;
    }
`);
```

Once registered, use the chunk in your shaders with `#include`:

```glsl
#include "myUtilsPS"

void main() {
    float result = myHelper(0.5);
}
```

#### Looped Includes

You can include a chunk multiple times with a loop counter:

```glsl
#define LIGHT_COUNT 4
#include "lightPS, LIGHT_COUNT"
```

This includes `lightPS` four times, with `{i}` in the chunk replaced by `0`, `1`, `2`, `3`.

## Injection Defines vs Regular Defines

The preprocessor supports two types of defines, distinguished by their syntax:

### Regular Defines

Regular defines work with preprocessor directives like `#ifdef` and `#if`:

```glsl
#define FEATURE_ENABLED
#define MAX_LIGHTS 4

#ifdef FEATURE_ENABLED
    // This code is included
#endif

#if MAX_LIGHTS > 2
    // This code is included
#endif
```

The GLSL language natively supports using defines in array sizes and similar contexts:

```glsl
#define SAMPLE_COUNT 8
float samples[SAMPLE_COUNT];
```

However, WGSL does not support this—use injection defines with the `{NAME}` syntax instead.

### Injection Defines (Curly Brace Syntax)

Injection defines use curly braces `{NAME}` and perform direct string replacement throughout the shader source (excluding preprocessor directive lines):

```glsl
#define {WORKGROUP_SIZE} 64

@compute @workgroup_size({WORKGROUP_SIZE}, 1, 1)
fn main() {
    var<workgroup> data: array<f32, {WORKGROUP_SIZE}>;
}
```

After preprocessing, this becomes:

```glsl
@compute @workgroup_size(64, 1, 1)
fn main() {
    var<workgroup> data: array<f32, 64>;
}
```

Injection defines are particularly useful for:

- WGSL workgroup sizes (which must be compile-time constants)
- Values that need to appear in non-preprocessor contexts
- Parameterizing shader code that doesn't support `#if` substitution

## Supplying Defines to Shaders

### ShaderMaterial Defines

For `ShaderMaterial`, use `setDefine()`:

```javascript
material.setDefine('USE_TEXTURE', true);
material.setDefine('MAX_LIGHTS', '4');
```

### Shader Definition Defines

When creating shaders programmatically (rather than using `ShaderMaterial`), you can supply defines.

#### Vertex and Fragment Shaders

Use `ShaderUtils.createShader()` to create vertex/fragment shaders with defines:

```javascript
const shader = pc.ShaderUtils.createShader(device, {
    uniqueName: 'MyShader',
    vertexGLSL: vertexCodeGLSL,
    vertexWGSL: vertexCodeWGSL,
    fragmentGLSL: fragmentCodeGLSL,
    fragmentWGSL: fragmentCodeWGSL,
    vertexDefines: definesMap,
    fragmentDefines: definesMap
});
```

#### Compute Shaders

Compute shaders are created directly using the `Shader` class:

```javascript
const shader = new pc.Shader(device, {
    name: 'MyComputeShader',
    shaderLanguage: pc.SHADERLANGUAGE_WGSL,
    cshader: computeCode,
    cincludes: includesMap,  // Custom includes for compute shader
    cdefines: definesMap     // Defines for compute shader
});
```

### Includes Map

The includes map provides content for `#include` directives:

```javascript
const includesMap = new Map([
    ['myChunk', 'float helper() { return 1.0; }'],
    ['anotherChunk', '// More shader code...']
]);
```

You can also use engine-provided chunks:

```javascript
cincludes: pc.ShaderChunks.get(device, pc.SHADERLANGUAGE_WGSL)
```

### Defines Map

The defines map uses the key as the define name (including curly braces for injection defines):

```javascript
// Regular defines (for #ifdef, #if)
const definesMap = new Map([
    ['FEATURE_ENABLED', ''],      // Flag define (no value)
    ['MAX_LIGHTS', '4']           // Value define
]);

// Injection defines (for direct replacement)
const definesMap = new Map([
    ['{WORKGROUP_SIZE}', '64'],
    ['{TILE_SIZE}', '16']
]);
```

## Best Practices

1. **Use regular defines** for conditional compilation with `#ifdef` and `#if`
2. **Use injection defines** `{NAME}` when you need direct string replacement in non-preprocessor contexts
3. **Prefer engine chunks** when available to ensure compatibility across platforms

--------------------------------------------------------------------------------

## WGSL Specifics

URL: https://developer.playcanvas.com/user-manual/graphics/shaders/wgsl-specifics/

WGSL shaders used by the PlayCanvas engine must satisfy certain requirements. These requirements allow the engine to correctly integrate shaders, ensuring they receive the necessary resources such as attributes, uniforms, and varyings.

The following sections outline key aspects of writing WGSL shaders for PlayCanvas.

### Simplified Shader Interface Syntax

In standard WGSL (WebGPU Shading Language), declaring uniforms, attributes, and varyings requires explicitly specifying a `@group` and `@binding` index for each resource. This can be verbose and error-prone, especially for common patterns.

To improve usability and streamline shader development, we adopt a simplified syntax similar to GLSL. In this model, you do not need to specify `@group` or `@binding` attributes manually—these are automatically assigned and managed by the engine based on a predefined layout.

#### Example Comparison

Standard WGSL:

```wgsl
struct Uniforms {
    uTime: f32,
};

struct FragmentInput {
    @location(0) uv0: vec2f,
    @builtin(position) position: vec4f
};

@group(0) @binding(0) var<uniform> ub: Uniforms;

@fragment fn fragmentMain(FragmentInput) -> @location(0) vec4f {
    // body
}
```

In contrast, the simplified syntax avoids a lot of the boilerplate.

```wgsl
uniform uTime: f32;
varying uv0: vec2f;

@fragment fn fragmentMain(input: FragmentInput) -> FragmentOutput {
    // body
}
```

### Attributes

Attributes define per-vertex input data, and can only be used in the vertex shader. They must be declared using the following syntax:

```wgsl
attribute aUv0: vec2f;
```

Internally, a `VertexInput` struct is automatically created and populated with all the attributes. Attributes can be accessed from the structure passed to the main function, but also in the global scope.

```wgsl
attribute aUv0: vec2f;

@vertex fn vertexMain(input: VertexInput) -> VertexOutput {

    // access it using input passed to the main function
    var myUv1 = input.aUv0;

    // but also as a global variable (particularly useful inside other functions)
    var myUv2 = aUv0;
}
```

As part of the `VertexInput` structure, and also in the global scope, these built-in attributes are automatically available:

```wgsl
vertexIndex: @builtin(vertex_index)
instanceIndex: @builtin(instance_index)
```

The attribute names must match the names specified in the `attributes` property when creating the [ShaderMaterial](/user-manual/graphics/shaders/).

### Uniforms

Uniforms are used to pass *numerical resources* from the engine to the shader.

Uniforms are declared using this simplified syntax:

```wgsl
uniform view_position: vec3f;
uniform tints: array<vec3f, 4>;
uniform weights: array<f32, 8>;
```

Internally, uniforms are automatically placed in uniform buffers, and in the shader code are accessed using a `uniform.` prefix:

```wgsl
var pos = uniform.view_position;
var color = uniform.tints[2];

// f32 and vec2<> types used in an array are due to alignment requirements wrapped
// in an aligned structure, and the value is available as its `element` property.
// struct WrappedF32 { @size(16) element: f32 }
var weight = uniform.weights[3].element;
```

The engine automatically sets appropriate uniform values when rendering.

:::note

Currently, our uniform system supports only simple types, including `f32`, `i32`, `u32`, as well as vectors and matrices (e.g., `vec4f`, `mat4x4f`). Structs are not supported at this time, so all uniform values must be declared as individual variables of basic types.

:::

### Texture Resources

Texture resources are using simplified WGSL syntax, where specifying a `@group` and `@binding` index for each resource has to be omitted.

#### Sampling Textures

In WGSL, textures and samplers are treated as separate objects, unlike in GLSL, where those are combined.

When you want to sample a texture (i.e. retrieve filtered texel values), you must provide a texture object *directly followed* by a sampler.

```wgsl
// 2d texture with a sampler declaration
var diffuseMap: texture_2d<f32>;
var diffuseMapSampler: sampler;

// texture sampling
var texel = textureSample(diffuseMap, diffuseMapSampler, coords);
```

#### Fetching Textures

If you only need to read raw texel data (i.e., without filtering, mipmapping, or addressing modes), you can use `textureLoad` instead of `textureSample`. This is called non-filtered access, or simply texel fetching.

In such cases, no sampler is required or allowed. For example:

```wgsl
// cubemap texture without a sampler
var noSamplerMap: texture_cube<f32>;

// fetching the texel
let texel = textureLoad(noSamplerMap, coords, mipLevel);
```

#### Unfilterable Textures

WebGPU supports unfilterable float textures, which are typically used for specialized purposes such as sampling from depth textures, where filtering is not allowed. However, WGSL does not provide a distinct sample type in the syntax for declaring these unfilterable float textures. To address this limitation and enable proper bind group auto-generation based on shader declarations, we introduce a new sample type called `uff` (unfilterable-float).

Using `uff`, you can explicitly declare an unfilterable-float texture in the shader like this:

```wgsl
// declaration
var colorMap: texture_2d<uff>;

// sampling
let data: vec4f = textureLoad(colorMap, uv, 0);
```

This extension allows the engine to correctly interpret the texture’s sampling capabilities and bind it accordingly.

:::note

Support for `texture_external` is not available yet, and will be added in the future.

:::

### Storage Buffers

Storage buffers are GPU-accessible memory resources that allow shaders to read and write arbitrary data with random access. In WGSL, they are declared using `var<storage>` and are ideal for working with large or structured datasets such as particle systems, compute data, or dynamic geometry. Unlike uniforms, storage buffers support both read and write access (with appropriate access control).

Example of using storage buffer in Vertex Shader:

```wgsl
struct Particle {
    position: vec3f,
    velocity: vec3f,
}

// particle storage buffer in read-only mode
var<storage, read> particles: array<Particle>;
```

### Varyings

Varyings are used to pass values from the vertex shader to the fragment shader. Declare them in both vertex and fragment shader using this simplified syntax:

```wgsl
varying texCoord: vec2f;
```

Internally, those are parsed, and stored in `VertexOutput` structure in the vertex shader, as well as in `FragmentInput` structure in the fragment shader.

#### Vertex Shader

As part of the `VertexOutput` structure these built-in variables are automatically available:

```wgsl
position: @builtin(position)
```

Example:

```wgsl
varying texCoord: vec2f;

@vertex fn vertexMain(input: VertexInput) -> VertexOutput {
    var output: VertexOutput;
    output.position = uniform.matrix_viewProjection * pos;
    output.texCoord = vec2f(0.0, 1.0);
    return output;
}
```

#### Fragment Shader

As part of the `FragmentInput` structure these built-in variables are automatically available:

```wgsl
position: @builtin(position)            // interpolated fragment position
frontFacing: @builtin(front_facing)     // front-facing
sampleIndex: @builtin(sample_index)     // sample index for MSAA
primitiveIndex: @builtin(primitive_index) // primitive index (when supported)
```

These built-ins are also available in the global scope using these names:

```wgsl
pcPosition
pcFrontFacing
pcSampleIndex
pcPrimitiveIndex  // when supported
```

:::note

The `primitiveIndex` / `pcPrimitiveIndex` built-in is only available when `device.supportsPrimitiveIndex` is true. This feature is WebGPU-only (not available on WebGL2). When the feature is supported, the engine automatically adds the required `enable primitive_index;` WGSL directive and the shader define `CAPS_PRIMITIVE_INDEX` is available for conditional compilation.

:::

Example:

```wgsl
varying texCoord: vec2f;

@fragment
fn fragmentMain(input: FragmentInput) -> FragmentOutput {
    var output: FragmentOutput;
    output.color = vec4f(1.0);
    return output;
}
```

### Fragment Shader Outputs

The fragment shader is responsible for producing one or more color outputs, which are written to the render targets (color attachments) of the framebuffer.

The engine automatically provides a `FragmentOutput` structure, which includes a predefined set of vec4f fields: `color`, `color1`, `color2` and so on, covering all possible color attachments, up to the limit defined by `GraphicsDevice.maxColorAttachments`.

As part of the `FragmentOutput` structure these built-in variables are automatically available:

```wgsl
fragDepth: @builtin(frag_depth)
```

Example:

```wgsl
@fragment fn fragmentMain(input: FragmentInput) -> FragmentOutput {
    var output: FragmentOutput;
    output.color = vec4f(1.0);
    output.color1 = vec4f(0.5);
    output.fragDepth = 0.2;
    return output;
}
```

:::note

Support for rendering to integer textures (output format other than `vec4f`) is not available yet, and will be added in the future.

:::

--------------------------------------------------------------------------------

## Optimization

URL: https://developer.playcanvas.com/user-manual/optimization/

Optimization is a critical part of developing a PlayCanvas application. It can mean the difference between a great user experience and a terrible one.

:::tip

Don't wait until a project is near completion before you consider optimization. Be thinking about it from the start. It may meaningfully impact how you design your application.

:::

Let's begin by establishing the key goals for optimization and highlight why each goal is important:

| Goal | Why it matters |
| ---- | -------------- |
| ⏱️ Minimize load time | Your users have limited patience. If your app does not load quickly, they may give up waiting and go elsewhere. |
| 🎞️ Maximize frame rate | A high (and stable) frame rate makes for pleasing visuals and low latency response to user input. |
| 🔋 Minimize CPU and GPU load | Just because your app maintains 60 frames per second does not mean your work is done. Reducing processor load preserves battery power and keeps devices running cool. |
| 🧠 Minimize memory utilization | Browsers allocate a limited pool of memory to applications. Once this pool is exhausted, the tab will crash and reload. Your users will be upset! |

--------------------------------------------------------------------------------

## GPU Profiling

URL: https://developer.playcanvas.com/user-manual/optimization/gpu-profiling/

This section explains how to use native GPU profilers to capture and analyze WebGL or WebGPU frames, enabling debugging and performance profiling of GPU operations.

This is particularly challenging on the Web platform, as web applications typically run within a sandboxed environment, which inherently limits compatibility and integration with native GPU profilers. This page outlines tested options available on certain platforms.

## WebGPU applications on macOS with Apple Silicon

* Clone WebKit:

  ```bash
  git clone https://github.com/WebKit/WebKit.git WebKit
  ```

* Build MiniBrowser (this takes around 30 minutes):

  ```bash
  cd WebKit
  Tools/Scripts/build-webkit -cmakeargs="-DENABLE_WEBGPU_BY_DEFAULT=1" --release
  ```

* Start the MiniBrowser, specify url to your web application:

  ```bash
  __XPC_METAL_CAPTURE_ENABLED=1 Tools/Scripts/run-minibrowser --release --url https://playcanvas.github.io/
  ```

* Configure the number of frames to capture from a separate command-line interface window. This defaults to 1.

  ```bash
  notifyutil -s com.apple.WebKit.WebGPU.CaptureFrame 2
  ```

* At the appropriate time, capture the frame(s):

  ```bash
  notifyutil -p com.apple.WebKit.WebGPU.CaptureFrame
  ```

  This generates a capture file, and the command-line window in which you started the MiniBrowser logs a path to it. For example:

  ```bash
  Success starting GPU frame capture at path file:///var/folders/m3/cnrw6k214hxd0hq1rf7cy3w40000gn/T/com.apple.WebKit.GPU+org.webkit.MiniBrowser/8C9372EF-1254-4FC5-8CA9-730FB
  ```

* Double-click this file to open it in Xcode, then click the Replay button in the dialog that appears. This enables you to inspect frame draw calls, analyze resources, debug shaders, and gather performance metrics.

  [Image: Xcode]

## WebGL applications on macOS with Apple Silicon

The steps above only enable capturing for WebGPU-based applications. To capture a WebGL application, you can embed a small WebGPU application on the same page and capture typically 2–3 frames. This process captures both the WebGPU application and the WebGL application since they both utilize the Metal API under the hood.

For PlayCanvas applications, this process can be simplified by using the provided script. Simply attach it to any single entity in your scene:

https://github.com/playcanvas/engine/blob/main/scripts/utils/mac-gpu-profiling.js

## WebGL and WebGPU applications on Windows

Please read this article on how to use Microsoft's PIX to capture GPU frames using the Chrome browser: https://toji.dev/webgpu-profiling/pix

Alternatively, read this article on how to use RenderDoc to capture GPU frames: https://edw.is/renderdoc-webgl/

If you want to use AMD's Radeon GPU Profiler or Nvidia's Nsight with Chrome, read [this article](https://frguthmann.github.io/posts/profiling_webgpu).

## WebGL applications on Meta Quest

Please read [this article](https://developers.meta.com/horizon/downloads/package/renderdoc-oculus/) on how to use Meta's fork of RenderDoc to capture rendering on Meta's Quest devices.

--------------------------------------------------------------------------------

## General Guidelines

URL: https://developer.playcanvas.com/user-manual/optimization/guidelines/

Here are some tips and hints on how to achieve good performance in your PlayCanvas app.

## JavaScript

* Calling 'new' to allocate a JavaScript object (particularly vectors, matrices and quaternions) represents a dynamic allocation and can be expensive. Therefore you should, where possible, preallocate objects in a script's initialize function and reuse them in the update function. It also leads to Garbage Collection which can cause periodical freezes.

## Graphics - CPU

* In PlayCanvas, a mesh instance is a draw call (a command to draw an individual graphical primitive). Each draw call requires some effort on the CPU to dispatch to WebGL. Therefore, keeping the number of draw calls low is advisable, particularly on mobile. You can see a list of the draw calls for a particular Model by selecting the model asset and viewing it in the Inspector. 100-200 draw calls is a rough target for low end mobile devices. High end desktop machines on the other hand can process thousands every frame and still maintain 60fps.
* Use [Batching](/user-manual/graphics/advanced-rendering/batching) to reduce draw calls. By creating Batch Groups in your Project and assigning them to Render, Model and Element components, the engine will try to merge them in as few mesh instances as possible, reducing draw calls and increasing performance.
* Try to keep the number of shaders generated by your app as low as possible. Shaders have to be compiled and linked on demand and this operation is expensive, causing delay in app startup and glitches in frame rate. If material A has an emissive map but material B doesn't, two shaders will be generated. If you set a black emissive map on material B, the materials can share the same shader. Reducing the number of materials in your scene should also reduce the number of generated shaders.
* For skinned meshes, the engine generates precise bounding box, required by the camera frustum culling, each frame. This operation has a cost for each bone, and executes even when the character is completely outside of the view frustum. To avoid this cost, consider setting up a custom AABB for the character, which is a property of the [`Render`](https://api.playcanvas.com/engine/classes/RenderComponent.html#customaabb) or [`Model`](https://api.playcanvas.com/engine/classes/ModelComponent.html#customaabb) component.
* Only enable frustum culling on a camera component if, on balance, it is likely to save more performance than it costs to calculate visibility. If you are rendering a scene where all mesh instances are always visible, disable this option.

## Graphics - GPU

* Be careful when enabling 'Use Device Pixel Ratio' in your project settings. This will cause your PlayCanvas app to utilize the native resolution of a device reducing pixelation but can result in many more pixels being filled, which can cause a significant drop in frame rate. This can be adjusted at runtime after assessing the user's device capabilities. Read more at [Adjusting Device Pixel Ratio](/user-manual/optimization/runtime-devicepixelratio).
* Be mindful of the number of dynamic lights in your scene. Keep them to a minimum.
* As the value for texture anisotropy increases, visuals improve but performance decreases. Be careful to balance visuals against performance.
* Look for opportunities to pack multiple textures into single images. For example, a grayscale opacity map can be stored in the alpha channel of a diffuse map. Or a grayscale gloss map can be stored in the alpha channel of a specular map. This results in lower VRAM usage.
* Post effects can be expensive so think carefully before you enable them. They can cost a lot in terms of pixel fill.
* Enabling backface culling on a material will be cheaper than disabling it. Generally speaking, backface culling reduces the number of pixels that the GPU has to fill. This is the default setting for newly created materials.

## Graphics - CPU and GPU

* For applications where there is little visual change over a period such as product configurators, there is a special property to reduce CPU and GPU usage. [`pc.Application#autoRender`](https://api.playcanvas.com/engine/classes/AppBase.html#autorender) can be set to `false` so that frames are rendered on demand via [`pc.Application#renderNextFrame`](https://api.playcanvas.com/engine/classes/AppBase.html#rendernextframe) when there is a visual change such as the user moving the camera or adding a part to the product.
* Enabling shadow casting on dynamic lights is expensive. Omni light shadows are particularly expensive. For each omni light that casts shadow, the scene must be rendered 6 times into a shadow map.
* Keep the number of blended mesh instances in your scene to a minimum. Blended meshes are deferred until all opaque mesh instances have been dispatched and are then submitted in back to front camera depth order. This results in pixels being filled multiple times and can result in a lot of render state changes since blended meshes cannot be sorted by material.

## Physics

* Collision meshes do not need to be the same level of detail as the renderable mesh. It is recommended that you set a lower resolution mesh for collision.
* Keep the number of dynamic rigid bodies in your scene to a minimum, particularly on mobile.

--------------------------------------------------------------------------------

## Optimizing Load Time

URL: https://developer.playcanvas.com/user-manual/optimization/load-time/

Optimizing isn't just related to improving frame rate. Fast load times are also critical. The faster your app loads, the more likely your users will stick around to experience it. Aim to have your app load in less than 5 seconds to prevent users from churning.

Here are some tips to achieve super-fast load times:

* In general, AVIF images produce smaller files than WebP, JPG, or PNG for the same image quality. It also supports an alpha channel like WebP and PNG. However [not all browsers currently support AVIF](https://caniuse.com/avif) so use it where it makes sense for your project. If you can't use AVIF, [WebP has much wider support](https://caniuse.com/webp) and produces smaller files than JPEG or PNG with similar quality, but we encourage you to test with different formats.
* Look for opportunities to downsample certain texture images. For example, a 2048x2048 texture that is used on a small graphical object may look almost exactly the same at 1024x1024 or even 512x512.
* Don't preload assets which can be loaded asynchronously. For example, it may not be necessary for your game music to play immediately at game start, so consider unchecking the Preload option for that asset in the Inspector panel.
* If you have a prefiltered cubemap and are not displaying the top-level mipmap for the skybox, you can uncheck preload for all the 6 face images.
* If you are not instantiating Templates at runtime, uncheck preload on the asset as they aren't needed. (See '[When do I need to load Template Assets?](/user-manual/editor/templates/#when-do-i-need-to-load-template-assets)' for more information).
* Ensure that imported models only have the vertex attributes that you need. For example, if your model has a second set of UVs but doesn't use them or if it has all-white vertex colors, go back to the modeling application and delete those attributes.
* Use the Networking panel in Chrome Dev Tools (or the equivalent in other browsers) to sort loaded assets by size and look for anything that stands out. Look for assets that are not used and could be deleted. Look for assets that are essentially duplicates and delete them.
* Using PlayCanvas' built-in physics engine incurs an additional download cost of 379KB. If you are using the physics engine to solve very simple problems, consider rolling an alternative solution that doesn't incur the download penalty.
* If you self-host your PlayCanvas app, be sure to configure your web server to serve files with GZIP compression. In particular, JSON and JS files.

## Loading sequence best practices

Beyond the above guidelines, it's possible to retain users by spacing the loading in multiple stages while giving the users something new to interact with or watch.

Using [Virtual Voodoo](https://playcanv.as/p/tRUfwVg1/) as an example, we can show the 'typical' sequence that most applications will use for browser experiences.

The game has 3 phases:

1. Preloader
2. Title Screen and Character Customization
3. Main Game

[Image: Virtual Voodoo Phases]

The Preloader phase loads the assets that are needed for the first PlayCanvas scene which is the Title Screen and Character Customization. This would include assets for the UI, character model and assets.

When the Title Screen is active, the game starts background loading the assets that are needed for the Main Game. During the transition to the Title Screen and possible interaction with the character customization, by the time the user presses the start button, the assets for the Main Game may have already finished loading.

However, if the user presses the start button before the assets have finished loading, a progress bar will appear on the button instead. Once it reaches 100%, the game will automatically transition to the Main Game.

[Image: Virtual Voodoo Assets Not Ready]

With the assets being loaded in phases and giving something new for the user to interact with and/or look at periodically, the user stays engaged despite a long loading time.

### Further improvements

Some developers will go as far as to reduce the Preloader phase to only load the bare minimum and add an 'in-application' loading screen that allows the developer to populate with application related assets and text, use animation, etc. This engages the user as they are seeing something that is directly related to the application.

If the game allows, using common placeholders while the more detailed assets are loading can get the user interacting with the application sooner.

An example below is using a silhouette of a character as the placeholder until it has fully loaded. The silhouette placeholder is small in file size so it can be part of a preload sequence and also can be reused for other characters in the application.

[Image: Lazy Load Character]

--------------------------------------------------------------------------------

## MiniStats

URL: https://developer.playcanvas.com/user-manual/optimization/mini-stats/

MiniStats is a lightweight graphical overlay that displays real-time performance metrics for your PlayCanvas application. It provides essential statistics including draw call count, frame time, CPU load, and GPU load, helping you identify performance bottlenecks during development.

## Enabling MiniStats

Editor users can enable the MiniStats panel via the Launch button menu:

<img loading="lazy" alt="Launch Menu" width="600" src="/img/user-manual/optimization/mini-stats/launch-menu-mini-stats.png" />

## Display Sizes

Clicking on the MiniStats overlay cycles through three display sizes: small (compact numeric values), medium, and large. The medium and large sizes add graphical timelines and detailed sub-timing breakdowns for both CPU and GPU.

<img loading="lazy" alt="Mini Stats" width="411" src="/img/user-manual/optimization/mini-stats/mini-stats.gif" />

## Basic Statistics

The following metrics are always displayed:

| Metric | Description |
|--------|-------------|
| **DrawCalls** | The number of draw calls dispatched each frame. Each draw call has overhead on both CPU and GPU, so minimizing this number improves performance. |
| **Frame** | Total time in milliseconds for the browser to process each frame. Target 16.67ms for 60 FPS or 33.33ms for 30 FPS. |
| **GPU** | Time in milliseconds for the GPU to render each frame. See [GPU Timing Requirements](#gpu-timing-requirements) below. |
| **CPU** | Time in milliseconds for CPU-side frame processing, split into update (red) and render (green) portions. |

## Detailed Timing Mode

When using medium or large display sizes, MiniStats shows additional timing breakdowns for both CPU and GPU that help identify specific performance bottlenecks.

### CPU Sub-Timings

In detailed mode, the CPU graph expands to show individual timing components:

| Stat | Description |
|------|-------------|
| **scriptUpdate** | Time spent executing script `update` methods |
| **scriptPostUpdate** | Time spent executing script `postUpdate` methods |
| **render** | CPU time spent preparing rendering commands and managing GPU resources |
| **physics** | Time spent in physics simulation |
| **anim** | Time spent updating the animation system |
| **gsplatSort** | Time spent sorting Gaussian splats for rendering. This runs in a Web Worker thread and is non-blocking, so it does not impact main thread performance. |

:::note
Some CPU stats only appear once they have non-zero values. For example, `physics` only appears if your scene uses physics simulation.
:::

### GPU Pass Timings

In detailed mode, individual GPU render pass and compute pass timings are displayed, showing how long each stage takes. Passes with the same name are aggregated into a single timing value. Common passes include:

- **Forward** - Main scene rendering
- **Downsample** - Post-processing downsampling stages
- **Upsample** - Post-processing upsampling stages
- **Compose** - Final frame composition
- **Compute** passes - GPU compute shader dispatches

:::important[WebGPU Only]
Detailed GPU pass timing is only available when using the **WebGPU** graphics backend.

**WebGL2 does not support detailed GPU profiling** - only the overall GPU frame time is shown. This limitation exists because WebGL's timer query extension only supports measuring elapsed time for the entire frame. Browser security restrictions (Spectre mitigations) prevent the fine-grained timestamp queries needed for per-pass measurements.
:::

## GPU Timing Requirements

GPU timing requires specific browser/API support:

| Backend | Requirement |
|---------|-------------|
| **WebGL 2** | The [`EXT_disjoint_timer_query_webgl2`](https://web3dsurvey.com/webgl2/extensions/EXT_disjoint_timer_query_webgl2) extension must be supported. Check [WebGL Report](https://webglreport.com/?v=2) to verify browser support. |
| **WebGPU** | The [`timestamp-query`](https://web3dsurvey.com/webgpu/features/timestamp-query) adapter feature must be available. This is enabled automatically when supported. |

## Using MiniStats Outside of the Editor

While the MiniStats panel is incorporated into the Editor's Launch page, you can also use it independently. To add MiniStats to your application:

```javascript
const miniStats = new pc.MiniStats(app);
```

You can customize the display with options:

```javascript
const miniStats = new pc.MiniStats(app, {
    startSizeIndex: 1,  // Start with medium size
    cpu: {
        enabled: true,
        watermark: 33   // Show 33ms budget line
    },
    gpu: {
        enabled: true,
        watermark: 33
    }
});
```

For the complete API, refer to the [MiniStats API reference](https://api.playcanvas.com/engine/classes/MiniStats.html). See the [MiniStats example](https://playcanvas.github.io/#/misc/mini-stats) for a demonstration of customization options.

## See It In Action

Visit the [Engine Examples Browser](https://playcanvas.github.io/) to see MiniStats in action. Try clicking on the overlay to cycle through the different display sizes and observe the detailed timing breakdowns.

--------------------------------------------------------------------------------

## Optimize Scene Format

URL: https://developer.playcanvas.com/user-manual/optimization/optimizing-scene-format/

This is a publish option that can reduce the size of the scene files to approximately 30-50% after being gzipped on the server.

To enable, on the publish screen, tick 'Optimize Scene Format' and publish.

--------------------------------------------------------------------------------

## Profiler

URL: https://developer.playcanvas.com/user-manual/optimization/profiler/

PlayCanvas provides a real-time profiler to assist in diagnosing performance problems.

[Image: Profiler]

The Profiler is a panel that overlays your app, displaying lots of useful timing information and performance stats. So whenever you’re wondering why your app isn’t hitting 60 frames per second, simply launch the Profiler and you should be able to figure out exactly what the problem is.

To launch the Profiler, tick the Profiler checkbox from the sub-menu of the Launch button:

[Image: Profiler Launch]

There is also a hot-key to toggle the Profiler: CTRL (CMD) + ALT + T.

## Profiler Overview

[Image: Profiler Stats]

The left-hand panel of the Profiler displays statistics related to the currently rendered scene. It displays frame rate, the number of cameras enabled (you will normally want this to be 1), the number of shaders, materials, triangles and so on. Also, frame time is broken down into update (the time to run all component updates), physics (simulation time) and render time (the time to pass all of the graphics commands to WebGL). At a glance, you can quickly see where there might be problems.

[Image: Profiler Timeline]

The right-hand panel is the Profiler Timeline. It displays a number of key events in your app’s life from launch:

* **dom** (DOM interactive): event when the browser finishes parsing html document, and is able to render first frame of a page to a screen.
* **preload**: event when PlayCanvas initiates preloading of all assets that are required before the app can start.
* **start**: event when PlayCanvas begins the main application loop and rendering begins.

Green bars represent individual asynchronous asset loads. Orange bars are blocking shader compilations.

--------------------------------------------------------------------------------

## Device Pixel Ratio

URL: https://developer.playcanvas.com/user-manual/optimization/runtime-devicepixelratio/

Device pixel ratio is the ratio between the physical pixels on the hardware screen and the logical pixels (related to the physical size of the screen, also known as CSS resolution).

Enabling Device Pixel Ratio on the Project settings will render the application at the native resolution of the screen which will make it look very crisp. However, that comes at a performance cost as now there are more pixels to fill and render each frame.

[Image: Project setting]

Below is an example of the Model Viewer Starter Kit with device pixel ratio enabled and disabled. Click on the thumbnail to see the full size.

[Image: Device Pixel Ratio]

This can be problematic on devices such as low or mid-tier mobile devices where they have high resolution screens but low graphics capability. This would lead to low frame rates if device pixel ratio is enabled due to fill rate limitations of the hardware.

Ideally, we want the best of both worlds where users on high-tier devices will render at the highest quality but users on lower-tier devices will reduce the ratio to maintain a playable frame rate.

The Device pixel ratio can be changed at runtime via the property [`pc.GraphicsDevice#maxPixelRatio`](https://api.playcanvas.com/engine/classes/GraphicsDevice.html#maxpixelratio):

```javascript
const device = pc.Application.getApplication().graphicsDevice;
if (highTierDevice) {
    // Use the default device pixel ratio of the device
    device.maxPixelRatio = window.devicePixelRatio;
} else {
    // Use the CSS resolution device pixel ratio
    device.maxPixelRatio = 1;
}
```

The challenge is working out the performance capabilities of the device and this can be done in a couple of ways:

* Using some form of benchmark on the start of the application and observing the frame rate
* Querying the WebGL renderer data to get the name of the GPU and checking against a known list performance tiers

To get information about the GPU, use the property `pc.GraphicsDevice#unmaskedRenderer`. This will contain a string with the information or an empty string if the browser does not support the property.

The string will have something similar to the following:

```none
ANGLE (NVIDIA GeForce GTX 1050 Direct3D11 vs_5_0 ps_5_0)
```

Benchmarks for different GPU cards can be found on [Video Card Benchmark](https://www.videocardbenchmark.net/GPU_mega_page.html) and [Notebook Check Smartphone and Tablet list](https://www.notebookcheck.net/Smartphone-Graphics-Cards-Benchmark-List.149363.0.html) to help gauge each GPU's capability. However, given the sheer number of GPU cards available, this can be extremely difficult to assess the device capabilities.

An example for mobile can be found below (correct at time of writing Thu 30 Jul 2020):

```javascript
function isLowQualityGPU() {
    const renderer = pc.Application.getApplication().graphicsDevice.unmaskedRenderer;

    // Only check the GPU if we are on mobile
    if (renderer && pc.platform.mobile) {
        // low level GPU's
        if(renderer.search(/Adreno\D*3/) !== -1 ||
           renderer.search(/Adreno\D*4/) !== -1 ||
           renderer.search(/Adreno\D*505/) !== -1 ||
           renderer.search(/Adreno\D*506/) !== -1 ||
           renderer.search(/Mali\D*4/) !== -1 ||
           renderer.search(/Mali\D*5/) !== -1 ||
           renderer.search(/Mali\D*6/) !== -1 ||
           renderer.search(/Mali\D*T7/) !== -1 ||
           renderer.search(/Mali\D*T82/) !== -1 ||
           renderer.search(/Mali\D*T83/) !== -1)
        {
            return true;
        }
    }

    return false;
};
```

We also recommend to have an option in the application for the user to be able to switch between quality levels. This allows them to choose the level that they are comfortable with and also be able to lower the quality in favor of using lower device resources and extending battery life.

--------------------------------------------------------------------------------

## Texture Compression

URL: https://developer.playcanvas.com/user-manual/optimization/texture-compression/

Texture data is stored in a device's video memory (or VRAM). It is important to ensure that your application does not exhaust VRAM as this can cause undesirable things like browser tab crashes.

The Editor has the ability to apply lossy compression schemes to your textures to dramatically reduce the amount of VRAM used using Basis.

[Basis](https://github.com/BinomialLLC/basis_universal) is a 'super-compressed' texture format. It's a platform independent lossy block compression format that can be transcoded to the natively supported hardware compression format at runtime. Supported transcode formats are ASTC, DXT, ETC2, ETC, PVR and ATC (selected in that order where available).

Consider this texture asset:

<img loading="lazy" src="/img/user-manual/assets/textures/texture-compression/brick.jpg" alt="Brick Texture" width="256" height="256" />

It's a 512x512 JPG that is 202KB in size. However, JPG is a compressed format and when passed to the graphics engine, it is expanded to an uncompressed RGB8 format that occupies 1.05MB of VRAM (including mipmap levels).

Enabling texture compression achieves the following results:

<img loading="lazy" src="/img/user-manual/assets/textures/texture-compression/compression-results.png" alt="Basis Compression results" width="400" />

The compression has achieved a 6 times reduction in VRAM usage. Furthermore, in this case, compression has also reduced download size from 202KB to as little as 46KB using the Default quality setting and ETC Mode.

Below is a side by side comparison of the brick texture on Mac with Chrome:

<a href="/img/user-manual/assets/textures/texture-compression/basis-vs-no-compression-brick.png" target="_blank"><img loading="lazy" src="/img/user-manual/assets/textures/texture-compression/basis-vs-no-compression-brick-thumb.jpg" alt="Brick texture compression comparison" /></a>

Here is another example of the PlayCanvas cube [with Basis (ETC mode)](https://playcanv.as/p/j8rsh3eO/) and [without](https://playcanv.as/p/nAW3WkW8/) on Mac with Chrome:

<a href="/img/user-manual/assets/textures/texture-compression/basis-vs-no-compression-cube.png" target="_blank"><img loading="lazy" src="/img/user-manual/assets/textures/texture-compression/basis-vs-no-compression-cube-thumb.jpg" alt="PlayCanvas cube compression comparison" /></a>

## Using Basis Texture Compression {#using-basis-texture-compression}

Once the texture has been imported into the Editor, select it and scroll down in the inspector to find the Compression section.

1. Tick BASIS.
2. Click on Import Basis to add the WASM module for the Basis runtime to the project (this only needs to be done once).
3. Change mode from 'ETC (smaller size, lower quality)' to 'ASTC (larger size, higher quality)' if you need to reduce compression artifacts on this texture.
4. Tick Normals if compressing a normal map.
5. Change the quality setting to balance file size vs quality. Lower quality results in smaller file sizes.
6. Click on Compress Basis.

<img loading="lazy" src="/img/user-manual/assets/textures/texture-compression/enable-basis-texture-compression.gif" alt="Enabling Basis Texture Compression" width="400" />

The Basis WASM module will add 253KB of extra gzipped data to the preload download size but that should be offset by the texture size savings compared to using the legacy texture compression format files ([see below](#legacy-texture-compression)).

To remove Basis compression from a texture:

1. Untick BASIS.
2. Click on Compress Basis.

<img loading="lazy" src="/img/user-manual/assets/textures/texture-compression/disable-basis-texture-compression.gif" alt="Disabling Basis Texture Compression" width="400" />

If you would no longer want to use Basis, remove Basis compression from all textures and delete the Basis folder from the project.

<img loading="lazy" src="/img/user-manual/assets/textures/texture-compression/delete-basis-library.png" alt="Delete Basis Module" width="400" />

## Basis Limitations {#basis-limitations}

There are some limitations of Basis texture compression in PlayCanvas.

1. The PVR format only supports textures that have dimensions that are both square (same width and height) and power of two (e.g. 256, 512, 1024 and so on). Older iOS devices (with an A6 SoC or lower like the iPhone 5 and 5C) and older iOS versions (13.7 and lower) only support PVR. A Basis texture that is non-square or non-power of two cannot be transcoded to PVR format but will instead use a 16-bit 565 pixel format. It will still display correctly, although may occupy more VRAM.
2. The maximum texture dimensions supported for Basis compression are 4096x4096. Textures larger than this would take an inordinate amount of time to compress so this is disabled.

## Legacy Texture Compression {#legacy-texture-compression}

We strongly recommend using Basis compression where possible as it requires a single texture file to cover all platforms and it is also a much smaller file compared to the legacy formats. Our tests show Basis to be ~50% smaller with minimal difference in quality.

The Legacy Texture schemes are:

- DXT: Typically supported by desktop devices.
- PVR: Typically supported by iOS devices.
- ETC: Typically supported by Android devices.

To use the Legacy Texture Compression options, select the texture and scroll down in the inspector to find the Compression section.

1. Tick LEGACY.
2. Tick all the formats you wish to use.
3. Click on Compress Legacy.

<img loading="lazy" src="/img/user-manual/assets/textures/texture-compression/enable-legacy-texture-compression.gif" alt="Enabling Legacy Texture Compression" width="400" />

To remove one or several formats:

1. Untick all the formats you wish to remove.
2. Click on Compress Legacy.

<img loading="lazy" src="/img/user-manual/assets/textures/texture-compression/disable-legacy-texture-compression.gif" alt="Disabling Legacy Texture Compression" width="400" />

## Migrating from Legacy to Basis Texture Compression {#migrating-from-legacy-to-basis-texture-compression}

If you have a project that is already using the Legacy Texture Compression formats and wish to use Basis, do the following:

1. Remove all the legacy texture formats.
2. Enable and use Basis.

<img loading="lazy" src="/img/user-manual/assets/textures/texture-compression/migrate-legacy-to-basis.gif" alt="Migrate from Legacy to Basis" width="400" />

--------------------------------------------------------------------------------

## Troubleshooting Performance

URL: https://developer.playcanvas.com/user-manual/optimization/troubleshooting-performance/

Here are some tips to help you uncover common performance problems.

## PlayCanvas and other WebGL applications run very slowly in Google Chrome

Chrome contains a blacklist of GPUs (graphics processors) and graphics card drivers which it will block from running WebGL content. In these cases Chrome will revert to using a software renderer to render WebGL content. This will be significantly slower than using the GPU. It is possible that upgrading graphics card drivers will improve performance.

You can view the blacklisted cards [here](https://www.khronos.org/webgl/wiki/BlacklistsAndWhitelists).

To confirm if you are running using the software renderer, visit [WebGL Report](https://webglreport.com/). If Chrome is using the software renderer you will see the row for **Unmasked Renderer** reports **SwiftShader**.

## PlayCanvas and other WebGL applications do not load in Firefox

Firefox contains a blacklist of GPUs (graphics processors) and graphics card drivers which it will block from running WebGL content. In these cases Firefox will not allow the application to create a WebGL context and the PlayCanvas application cannot start. It is possible that upgrading your graphics card drivers will enable the application to run.

You can view the blacklisted cards [here](https://www.khronos.org/webgl/wiki/BlacklistsAndWhitelists).

## PlayCanvas and other WebGL applications do not perform as well as expected on Windows with Nvidia graphics cards

Many Windows PCs contain two graphics cards, an "Integrated" graphics card which is a lower powered GPU built onto the motherboard of the computer and an additional graphics card. Nvidia cards come with a control panel installed that lets the PC owner choose which graphics card is used to run which application. By default many PCs are set up to use the lower powered integrated graphics card to run web browsers like Google Chrome. In order to get the best performance out of WebGL applications it is important to ensure that the web browser is accelerated using the more powerful graphics card.

--------------------------------------------------------------------------------

## PCUI

URL: https://developer.playcanvas.com/user-manual/pcui/

[Image: PCUI splash]

PCUI stands for **P**lay**C**anvas **U**ser **I**nterface. It is the front-end framework on which all PlayCanvas tools are built:

* [PlayCanvas Editor](https://github.com/playcanvas/editor)
* [SuperSplat](https://superspl.at/editor)
* [Model Viewer](https://playcanvas.com/viewer)
* [Texture Tool](https://playcanvas.com/texture-tool)

PCUI is open source and available on [GitHub](https://github.com/playcanvas/pcui). As such, you can use it in your own projects. This guide shows you how!

--------------------------------------------------------------------------------

## Data Binding

URL: https://developer.playcanvas.com/user-manual/pcui/data-binding/

The PCUI library offers a data binding layer that can be used to synchronize data across multiple components. It offers two-way binding to a given observer object, so updates made in a component are reflected in the observer's data and distributed out to all other subscribed components.

--------------------------------------------------------------------------------

## Two Way Binding

URL: https://developer.playcanvas.com/user-manual/pcui/data-binding/two-way-binding/

Observers can also be bound bi-directionally, in which case an element can both send and receive updates through its observer. The following example shows a two way binding between two text inputs, where either input can update the value of the other. It's been written in React to showcase binding with React components.

[Interactive Demo]

### How To

First import the components, binding classes and PCUI styles.

```javascript

```

Then create a new observer for an object which contains a text string.

```javascript
const observer = new Observer({
    text: 'Hello World'
});
```

Create two text inputs, which can both send and receive updates through the linked observer. This style of binding is defined through the use of the `BindingTwoWay` object which is passed as a property.

```javascript
const link = { observer, path: 'text' };
const TextInput1 = () => 
const TextInput2 = () => 
```

--------------------------------------------------------------------------------

## Using Observers

URL: https://developer.playcanvas.com/user-manual/pcui/data-binding/using-observers/

A simple use case is shown below.

[Interactive Demo]

In this example, the created label will start with `Hello World` as its text value. When a user enters a value into the text input, the label will be updated with the new value.

### How To

First import the components, binding classes and PCUI styles.

```javascript

```

Create a new observer for an object which contains a text string.

```javascript
const observer = new Observer({
    text: 'Hello World'
});
```

Create a label which will listen to updates from the observer.

```javascript
const label = new Label({
    binding: new BindingObserversToElement()
});
```

Link the observer to the label, telling it to use the text variable as its value.

```javascript
label.link(observer, 'text');
```

Create a text input which will send updates to the observer.

```javascript
const textInput = new TextInput({
    binding: new BindingElementToObservers()
});
```

Link the observer to the label, telling it to set the text variable on change.

```javascript
textInput.link(observer, 'text');
```

--------------------------------------------------------------------------------

## Examples

URL: https://developer.playcanvas.com/user-manual/pcui/examples/

Here are some simple examples demonstrating the various PCUI elements:

[Interactive Demo]

The source code for these examples can be found in the [PCUI GitHub repository](https://github.com/playcanvas/pcui/tree/main/examples).

--------------------------------------------------------------------------------

## History

URL: https://developer.playcanvas.com/user-manual/pcui/examples/history/

In this example you can interact with the input slider to update the progress bar. Any actions you make can be undone / redone using the history buttons.

[Interactive Demo]

### Code

```jsx

const observer = new Observer({ progress: 0 });
const history = new History();

const HistoryExample = (props) => {
    const [ canUndo, setCanUndo ] = useState(false);
    const [ canRedo, setCanRedo ] = useState(false);
    const [ historyLabel, setHistoryLabel ] = useState('');
    history.on('canUndo', setCanUndo);
    history.on('canRedo', setCanRedo);
    history.on('add', (name) => setHistoryLabel(`add action: ${name}`));
    history.on('undo', (name) => setHistoryLabel(`undo action: ${name}`));
    history.on('redo', (name) => setHistoryLabel(`redo action: ${name}`));
    const linkProgress = { observer, path: 'progress' };
    return (
        
            
            
        </Container>
    );
};

ReactDOM.render(, document.body);
```

--------------------------------------------------------------------------------

## Todo List

URL: https://developer.playcanvas.com/user-manual/pcui/examples/todo-list/

The todo list below allows you to add items to the list, toggle their 'done' status and filter items by that status.

[Interactive Demo]

### Code

```jsx

const observer = new Observer({ input: '', items: {} });

    const [ items, setItems ] = useState({});
    const [ listFilter, setListFilter ] = useState(0);
    observer.on('items:set', setItems);
    const addItem = (value) => {
        const items = observer.get('items');
        if (value === '') return;
        items[Date.now()] = { done: false, text: value };
        observer.set('input', '');
        observer.set('items', items);
    };
    const removeItem = (key) => {
        const items = observer.get('items');
        delete items[key];
        observer.set('items', items);
    };
    const toggleItem = (key) => {
        const items = observer.get('items');
        items[key].done = !items[key].done;
        observer.set('items', items);
    };
    const textInputLink = { observer, path: 'input' };
    return (
        
                    ];
                })}
            </Container>
        </Container>
    );
};

ReactDOM.render(, document.body);
```

--------------------------------------------------------------------------------

## Getting Started

URL: https://developer.playcanvas.com/user-manual/pcui/getting-started/

Before you begin, make sure you have [Node.js](https://nodejs.org/) 18 or later installed.

## Installing from NPM

PCUI is available as a package on [NPM](https://www.npmjs.com/package/@playcanvas/pcui). You can install it as follows:

```bash
npm install @playcanvas/pcui --save-dev
```

This will include the entire PCUI library in your project. The various parts of the library will be available to import from that package at the following locations:

- Observers: `@playcanvas/observer`
- ES Module Components: `@playcanvas/pcui`
- React Components: `@playcanvas/pcui/react`

You can import the ES Module components into your own `.js` files and use them as follows:

```javascript

const button = new Button({
    text: 'Click Me'
});

document.body.appendChild(button.dom);
```

This will result in your first component being appended to your document body!

[Interactive Demo]

## API Reference

The [API reference](https://api.playcanvas.com/pcui/) is a list of all of PCUI's class components and their properties. It is automatically generated from the source code.

--------------------------------------------------------------------------------

## PCUI Graph

URL: https://developer.playcanvas.com/user-manual/pcui/pcui-graph/

[Image: PCUI Graph Banner]

PCUI Graph is a graph visualization framework designed to help you build applications that can create and view node-based graphs in the browser. It was built as an extension of the PCUI library but now lives in its own repository.

--------------------------------------------------------------------------------

## Context Menus

URL: https://developer.playcanvas.com/user-manual/pcui/pcui-graph/context-menus/

It is possible to create context menus on your graph which display when right clicking various graph items. There are three types of context menus; background, node and edge. You can define a set of actions which will display in each of these menus and each action item in the menu will fire an action event when selected.

The background context menu appears when you right click on any blank space in the canvas. This context menu is used to add new nodes to the graph. It can be created by adding a `contextMenuItems` array to the options object passed to the graph constructor:

```javascript
const graph = new Graph(schema, {
    contextMenuItems: [
        {
            {
                text: 'Add a hello node',
                action: GRAPH.GRAPH_ACTIONS.ADD_NODE,
                nodeType: NODE_KEYS.HELLO,
                attributes: {
                    name: 'New hello'
                    'Editable boolean': true
                }
            },
            {
                text: 'Add a world node',
                action: GRAPH.GRAPH_ACTIONS.ADD_NODE,
                nodeType: NODE_KEYS.WORLD,
                attributes: {
                    name: 'New world'
                    'Editable boolean': true
                }
            }
        }
    ]
})
```

The text property defines the display text of the context menu item. The action property tells the graph that this context menu item should fire an `ADD_NODE` action when it is selected. The other properties define the type of node that will be created when this item is selected. The node type references one of the node keys defined in the graphs schema. The attributes object defines the initial values of any editable attributes that exist in that node's schema. The name attribute will also show up in the header for the node.

Context menus can also be added to nodes and edges by including contextMenu properties in their schemas as follows:

```javascript
const schema = {
    edges: {
        0: {
            contextMenuItems: [
                {
                    text: 'Delete edge', // name of the context menu item
                    action: Graph.GRAPH_ACTIONS.DELETE_EDGE // action to carry out when item is selected
                }
            ]
        }
    }
};
```

Currently, node context menus support two actions:

```javascript
Graph.GRAPH_ACTIONS.DELETE_NODE // Delete the node associated with this context menu.
Graph.GRAPH_ACTIONS.ADD_EDGE // Add an edge that starts from the node associated with this context menu, selecting another node will complete the edge connection. Selecting the background canvas will cancel adding an edge.
```

While edges support their own deletion by adding this action in their context menu:

```javascript
Graph.GRAPH_ACTIONS.DELETE_EDGE // Delete the edge associated with this context menu.
```

--------------------------------------------------------------------------------

## Events

URL: https://developer.playcanvas.com/user-manual/pcui/pcui-graph/events/

After creating a graph, you can register a callback for various events. This is achieved using the graph's [on function](https://api.playcanvas.com/pcui-graph/classes/Graph.html#on). The following events are supported:

```javascript

const schema = { ... };
const graph = new Graph(schema);

/*
 * @event
 * @param {object} args.node - The node that was added
 */
graph.on(Graph.GRAPH_ACTIONS.ADD_NODE, ({ node }) => { ... });

/*
 * @event
 * @param {object} args.node - The node that was deleted
 * @param {object} args.edgeData - The edges contained in the graph
 * @param {object} args.edges - The edges that were removed when deleting this node
 */
graph.on(Graph.GRAPH_ACTIONS.DELETE_NODE, ({ node, edgeData, edges }) => { ... });

/*
 * @event
 * @param {object} args.node - The node that was selected
 * @param {object} args.prevItem - The previously selected item, either a node or an edge
 */
graph.on(Graph.GRAPH_ACTIONS.SELECT_NODE, ({ node, prevItem }) => { ... });

/*
 * @event
 * @param {object} args.node - The node that was updated
 * @param {object} args.nodeId - The node id of the node that was updated
 */
graph.on(Graph.GRAPH_ACTIONS.UPDATE_NODE_POSITION, ({ node, nodeId }) => { ... });

/*
 * @event
 * @param {object} args.node - The node that was updated
 * @param {object} args.attribute - The name of the attribute that was updated
 * @param {object} args.attributeKey - The key of the attribute that was updated
 */
graph.on(Graph.GRAPH_ACTIONS.UPDATE_NODE_ATTRIBUTE, ({ node, attribute, attributeKey }) => { ... });

/*
 * @event
 * @param {object} args.edge - The edge that was updated
 * @param {object} args.edgeId - The id of the edge that was updated
 */
graph.on(Graph.GRAPH_ACTIONS.ADD_EDGE, ({ edge, edgeId }) => { ... });

/*
 * @event
 * @param {object} args.edge - The edge that was updated
 * @param {object} args.edgeId - The id of the edge that was updated
 */
graph.on(Graph.GRAPH_ACTIONS.DELETE_EDGE, ({ edge, edgeId }) => { ... });

/*
 * @event
 * @param {object} args.edge - The edge that was selected
 * @param {object} args.prevItem - The previously selected item, either a node or an edge
 */
graph.on(Graph.GRAPH_ACTIONS.SELECT_EDGE, ({ edge, prevItem }) => { ... });

/*
 * @event
 * @param {object} args.prevItem - The previously selected item, either a node or an edge
 */
graph.on(Graph.GRAPH_ACTIONS.DESELECT_ITEM, ({ prevItem }) => { ... });

/*
 * @event
 * @param {number} args.pos.x - The x position of the viewport in relation to the graph
 * @param {number} args.pos.y - The y position of the viewport in relation to the graph
 */
graph.on(Graph.GRAPH_ACTIONS.UPDATE_TRANSLATE, ({ pos }) => { ... });

/*
 * @event
 * param {number} args.scale - The current scale of the graph
 */
graph.on(Graph.GRAPH_ACTIONS.UPDATE_SCALE, ({ scale }) => { ... });
```

--------------------------------------------------------------------------------

## Getting Started

URL: https://developer.playcanvas.com/user-manual/pcui/pcui-graph/getting-started/

## Installing from NPM

PCUI Graph is available as a package on [NPM](https://www.npmjs.com/package/@playcanvas/pcui-graph). You can install it as follows:

```bash
npm install @playcanvas/pcui-graph --save-dev
```

:::note

It is assumed you already have the `@playcanvas/pcui` package installed in your project.

:::

## Importing the Graph Component

You can import the `Graph` component as follows:

```javascript

```

## Creating a Graph

Options can be passed to the `Graph` constructor as a JSON object which change the default behavior of the graph. You can do so as follows:

```javascript
const graph = new Graph(schema, {
    readOnly: true,
    initialData: { ... }
});
```

You can see a full list of options [here](https://api.playcanvas.com/pcui-graph/classes/Graph.html#constructor).

## Examples

Examples of PCUI Graph usage can be found in this [Storybook](https://playcanvas.github.io/pcui-graph/storybook/).

--------------------------------------------------------------------------------

## Schema

URL: https://developer.playcanvas.com/user-manual/pcui/pcui-graph/schema/

The schema object is used to define what type of graph you will be initializing. More specifically, it defines which nodes your graph can contain and how those nodes can be connected together with edges.

It should contain a set of nodes and edges which can be created in the graph. Each node and edge that is defined will need a unique number key which is used to reference that particular part of the schema. In the above example the single edge type defined references the two nodes contained in the schema when defining which node types it can connect. When creating large schemas, it can be useful to define these keys before creating the schema, so they can be easily referenced:

```javascript
const NODE_KEYS = {
    HELLO: 0,
    WORLD: 1
};
const EDGE_KEYS = {
    HELLO_TO_WORLD: 0
};

const schema = {
    nodes: {
        [NODE_KEYS.HELLO]: {
            name: 'Hello',
            fill: 'red'
        },
        [NODE_KEYS.WORLD]: {
            name: 'World',
            fill: 'green'
        }
    },
    edges: {
        [EDGE_KEYS.HELLO_TO_WORLD]: {
            from: [NODE_KEYS.HELLO], // this edge can connect nodes of type NODE_KEYS.HELLO
            to: [NODE_KEYS.WORLD] // to nodes of type NODE_KEYS.WORLD,
            stroke: 'blue'
        }
    }
};
```

The schemas above are used to create directed graphs, as they define edges which contain `from` and `to` attributes. These attributes tell an edge which nodes they can connect, creating a directed edge from one node to another.

When creating visual programming graphs, nodes are not connected directly. Instead, they contain input and output ports which can be connected together. This will need to be expressed in the schema you create. To achieve this, you can add `inPorts` and `outPorts` attributes to your nodes in the schema. These will define a set of ports which will be created on a given node, specifying which edges can connect those ports.

The schema defined above can be reworked to support port connections as follows:

```javascript
const NODE_KEYS = {
    HELLO: 0,
    WORLD: 1
};
const EDGE_KEYS = {
    HELLO_TO_WORLD: 0
};

const schema = {
    nodes: {
        [NODE_KEYS.HELLO]: {
            name: 'Hello',
            fill: 'red',
            outPorts: [
                {
                    name: 'output',
                    type: EDGE_KEYS.HELLO_TO_WORLD
                }
            ]
        },
        [NODE_KEYS.WORLD]: {
            name: 'World',
            fill: 'green',
            inPorts: [
                {
                    name: 'input',
                    type: EDGE_KEYS.HELLO_TO_WORLD
                }
            ]
        }
    },
    edges: {
        [EDGE_KEYS.HELLO_TO_WORLD]: {
            stroke: 'blue'
        }
    }
};
```

You can see that created ports have a type which defines the edge type each port accepts. Only input and output ports of the same type can be connected together in the graph. Ports also contain a name which will appear next to the port in the graph.

Nodes can also contain editable attributes, which will show up as input fields within them. These attributes can be set in a node as follows:

```javascript
const schema = {
    nodes: {
        0: {
            name: 'Foobar',
            attributes: [
                {
                    name: 'Editable boolean',
                    type: 'BOOLEAN_INPUT'
                },
                {
                    name: 'Editable text',
                    type: 'TEXT_INPUT'
                },
                {
                    name: 'Editable number',
                    type: 'NUMERIC_INPUT'
                },
                {
                    name: 'Editable 2D vector',
                    type: 'VEC2_INPUT'
                },
                {
                    name: 'Editable 3D vector',
                    type: 'VEC3_INPUT'
                },
                {
                    name: 'Editable 4D vector',
                    type: 'VEC4_INPUT'
                }
            ]
        }
    }
};
```

Editable attributes for a given node type must have unique names as they are stored in the graph data in a dictionary. When a node with an editable attribute is created, it can be accessed via the graph data as follows:

```javascript
const selectedItemId = graph.selectedItem.id;
const currentBooleanValue = graph.data.nodes[selectedItemId].attributes['Editable boolean'].value;
```

--------------------------------------------------------------------------------

## Styling

URL: https://developer.playcanvas.com/user-manual/pcui/pcui-graph/styling/

You can style your graph by overriding its default style properties. This can be achieved by modifying the defaultStyles passed in as part of an options object to the graph constructor.

```javascript
const graph = new Graph(schema, {
    defaultStyles: {
        background: {
            color: 'black'
        }
    }
})
```

The `defaultStyles` object contains styling options for the graphs background as well as node and edge styles. A full list of these overridable properties can be seen [here](https://github.com/playcanvas/pcui-graph/blob/main/src/constants.js).

If you'd like to update the styling of particular node/edge types, you can override any of the node or edge properties given in the `defaultStyles` object by defining them in the schema for a particular node or edge as follows:

```javascript
const schema = {
    nodes: {
        0: {
            name: 'standard node'
        },
        1: {
            name: 'red node'
            fill: 'red' // Update the background color of any nodes of this type to red
        },
    }
};

const graph = new Graph(schema, {
    defaultStyles: {
        node: {
            fill: 'grey' // All other node types will have a grey background
        }
    }
})
```

--------------------------------------------------------------------------------

## React

URL: https://developer.playcanvas.com/user-manual/pcui/react/

PCUI components can be used directly in React applications. Import the components from the React package and use them in your `.jsx` files as follows:

```jsx

ReactDOM.render(
    ,
    document.body
);
```

This example renders a basic text input component. You can see it in action below:

[Interactive Demo]

For more complex implementations, check out the [examples](../examples) section.

## Storybook

The [PCUI Storybook](https://playcanvas.github.io/pcui/storybook/) provides an interactive showcase of all available components. You can:

- Explore each component's properties and behavior
- Test different configurations in real-time
- View component documentation
- Copy example code

--------------------------------------------------------------------------------

## Physics

URL: https://developer.playcanvas.com/user-manual/physics/

Most video games you have ever played will have some form of physics. The player expects objects to fall under the influence of gravity. For objects to collide instead of pass through each other. For a sound to play if two objects collide. And so on.

A physics engine attempts to reproduce our understanding of the natural world in an artificial game world. It attempts to realistically animate objects in an expected and predictable way.

[Image: Physics Constraints]

PlayCanvas provides a very powerful physics engine that can be used to achieve a great many effects. This section introduces the concepts of rigid bodies, collision, forces, impulses, raycasting and more.

--------------------------------------------------------------------------------

## Alternatives to ammo.js

URL: https://developer.playcanvas.com/user-manual/physics/ammo-alternatives/

ammo.js is perhaps the most popular and well known JavaScript physics engine. It is highly versatile and can generate high fidelity simulations. But it has quite high performance and memory requirements. Therefore, you should investigate whether it is indeed the best choice for your application. For example, if you are making a 2D game, a 2D physics engine might be more appropriate.

As it happens, there are several alternatives to ammo.js:

| Physics Engine                                     | JS       | WASM     | 2D       | 3D       | PlayCanvas Integration                                |
| -------------------------------------------------- | -------- | -------- | -------- | -------- | ----------------------------------------------------- |
| [box2d.js](https://github.com/kripken/box2d.js)    | &#x2713; | &#x2713; | &#x2713; |          |                                                       |
| [Matter.js](https://github.com/liabru/matter-js)   | &#x2713; |          | &#x2713; |          |                                                       |
| [p2.js](https://github.com/schteppe/p2.js)         | &#x2713; |          | &#x2713; |          | [Yes](https://github.com/playcanvas/playcanvas-p2.js) |
| [cannon.js](https://github.com/schteppe/cannon.js) | &#x2713; |          |          | &#x2713; |                                                       |
| [Oimo.js](https://github.com/lo-th/Oimo.js)        | &#x2713; |          |          | &#x2713; |                                                       |

While there is currently only one existing PlayCanvas integration for the p2.js engine, it should be straightforward to create additional integrations for the other engines listed using a similar approach.

In December 2018, Nvidia open sourced the [PhysX](https://github.com/NVIDIAGameWorks/PhysX) physics engine. While there is no JS/WASM port of PhysX yet, it is perhaps the most competitive physics runtime compared to Bullet/ammo.js. When a web port becomes available, it will be added to the table above.

--------------------------------------------------------------------------------

## Calling the ammo.js API

URL: https://developer.playcanvas.com/user-manual/physics/calling-ammo/

The PlayCanvas integration with ammo.js does not expose the full capability of the ammo.js API. However, it is possible to call the ammo.js API directly from your PlayCanvas scripts.

PlayCanvas currently uses [this build](https://github.com/kripken/ammo.js/commit/dcab07bf0e7f2b4b64c01dc45da846344c8f50be) of ammo.js. The API exposed by this build can be found [here](https://github.com/kripken/ammo.js/blob/dcab07bf0e7f2b4b64c01dc45da846344c8f50be/ammo.idl). Although there is no official documentation for ammo.js, you can refer to the [Bullet Physics User Guide](https://github.com/bulletphysics/bullet3/blob/master/docs/Bullet_User_Manual.pdf) to learn more.

## Joint Constraints

There are currently no PlayCanvas components which implement physics constraints (sometimes known as physics joints). However, it is easy to leverage the ammo.js API to create scripts that implement constraints.

Here is the script for a point-to-point constraint (essentially a ball and socket joint):

You can find a project that implements all of the constraint types from ammo.js [here](https://playcanvas.com/project/618829/overview/physics-constraints).

## Continuous Collision Detection

Sometimes, you might find that fast moving rigid bodies in your simulations pass through one another. To overcome this, ammo.js provides a concept called Continuous Collision Detection (or CCD for short). This enables additional checks for collisions by sweeping a sphere volume between the previous and current positions of a rigid body and looking for intersections with the volumes of other bodies.

You can enable CCD for any PlayCanvas rigid body using the following script:

You can find a project that implements CCD [here](https://playcanvas.com/project/447023/overview/physics-with-ccd).

These are just two examples of using the ammo.js API directly. You can also use it to implement additional things like:

- Compound collision shapes
- Soft body simulation
- Cloth simulation
- Vehicles

--------------------------------------------------------------------------------

## Compound Shapes

URL: https://developer.playcanvas.com/user-manual/physics/compound-shapes/

Compound shapes are custom collision shapes created out of multiple primitive shapes ([full list of shapes here](/user-manual/physics/physics-basics/#rigid-bodies)). This allows you to have more complex collision shapes without using a custom mesh model.

The main advantage is that you are able to have dynamic rigidbody collisions between compound shapes (shown below) which is not possible with mesh collision types.

[Interactive Demo]

[PlayCanvas project link](https://playcanvas.com/project/688146/overview/compound-physics-shapes)

The shape of a compound physics object is defined by the children's collision shapes as shown below.

[Image: Compound shapes setup]

[Image: Compound shapes chair]

The Chair entity (parent) has the [collision component](/user-manual/editor/scenes/components/collision/) with type set to 'Compound'.

The children entities will form the shape of the physics object with collision components and the type set to a primitive shape and positioned relative to the parent.

The parent is also the center of mass of the physics object and it is recommended to have it within the bounds of the shape of the object (usually the center). If not, the object may show odd behavior when forces and torque are applied such as rotating around an invisible pivot in the world.

--------------------------------------------------------------------------------

## Forces and Impulses

URL: https://developer.playcanvas.com/user-manual/physics/forces-and-impulses/

Dynamic rigid bodies move in response to forces and impulses. A force is applied to a body over a period of time whereas an impulse is a force that is applied in an instant.

To apply a force or an impulse to a rigid body, you must use the [pc.RigidBodyComponent scripting API](https://api.playcanvas.com/engine/classes/RigidBodyComponent.html).

Let's consider a couple of examples. If you want to push a heavy weight across the floor, you would apply a force over an amount of time:

```javascript
MyScript.prototype.update = function(dt) {
    // While the right arrow key is pressed, apply a force to the right
    if (app.keyboard.isPressed(pc.KEY_RIGHT)) {
        this.entity.rigidbody.applyForce(10, 0, 0);
    }
};
```

If you want to fire a cannonball from a cannon, you would apply a single impulse:

```javascript
MyScript.prototype.update = function(dt) {
    // If the space bar was pressed, apply an impulse up and to the right
    if (app.keyboard.wasPressed(pc.KEY_SPACE)) {
        this.entity.rigidbody.applyImpulse(10, 10, 0);
    }
};
```

--------------------------------------------------------------------------------

## Physics Basics

URL: https://developer.playcanvas.com/user-manual/physics/physics-basics/

PlayCanvas incorporates a very powerful physics engine called [ammo.js](https://github.com/kripken/ammo.js). This is a browser port of the open source C++ Bullet physics engine.

PlayCanvas provides the [rigidbody](/user-manual/editor/scenes/components/rigidbody/) and [collision](/user-manual/editor/scenes/components/collision/) components to set up physics simulations.

## Enabling Physics {#enabling-physics}

By default, a new PlayCanvas project does not include ammo.js modules. This is because ammo.js weighs in at several hundred kilobytes and your app should not have to load this library if it is not needed.

You can import ammo.js modules into your project using the import button on the Scene Settings panel:

[Image: Physics Settings]

This will import the default build of ammo.js provided by PlayCanvas. However please note it is possible to compile your own version of ammo.js and add it to your project instead. For more information see [this page](/user-manual/editor/assets/inspectors/wasm/).

For details on migrating legacy projects to the latest ammo.js see [this page](/user-manual/physics/physics-migration/).

## Gravity {#gravity}

In the same Settings panel, you can set global gravity of the physics simulation. Gravity is a constant force applied to all rigid bodies in your scene. By default, this is set to -9.81 in the world's negative Y axis (straight down, in other words). This default approximates Earth gravity. But you may want to increase or decrease this value. For example, for a game set in space, you will probably want to set gravity to zero.

## Units of Measurement {#units-of-measurement}

By default, the PlayCanvas physics engine interprets 1 unit as 1 meter. Therefore, for objects to fall at a rate that appears to be physically accurate, you should ensure that your scenes size objects appropriately.

For example, if your game features a character that is 1.8m tall, he should be 1.8 units high in the Editor's 3D view.

## Rigid Bodies {#rigid-bodies}

You can make any entity in your scene participate in the physics simulation. Just add a rigidbody component and a collision component. The rigidbody component specifies a type:

- Static - A physical object that never moves
- Dynamic - A physical object that will move in response to an applied force
- Kinematic - A physical object that can only be positioned explicitly via the API

It also specifies physical properties like mass, friction and restitution (essentially a measure of 'bounciness').

The collision component specifies the physical shape of the body. Note that a rigid body's physical shape does not have to match its graphical shape. It is typical for physical representations of objects to be much simpler than the graphical. The available collision component types are:

- Box
- Sphere
- Capsule
- Cylinder
- Mesh
- Cone
- [Compound](/user-manual/physics/compound-shapes/)

## Creating a Static Ground {#creating-a-static-ground}

Most of the time, you will want to create some kind of static physical environment. For example, a race track or a football pitch. The simplest example is a flat plane. PlayCanvas doesn't expose a plane-type collision primitive but it does provide a box primitive. Here is how to configure a 1 unit high 10x10 box that is a static rigid body:

[Image: Static Ground]

You could also set the collision component type to Mesh and assign a model asset if you want something more complex.

## Creating Dynamic Bodies {#creating-dynamic-bodies}

Physics is all about movement so things get interesting when we create dynamic rigid bodies. Let's create a dynamic 1x1x1 box:

[Image: Dynamic Box]

The box has been rotated so that when it collides with the static ground, it will bounce in an interesting way:

[Image: Falling Box]

## Creating Kinematic Bodies {#creating-kinematic-bodies}

Sometimes, it can be useful to be able to explicitly control the motion of physical objects in your scene and have these objects exert an irresistible force on other physical objects. For example, imagine a moving platform that can carry the player across a level. To achieve this, you can set a rigid body's type to Kinematic. Let's create a kinematic box:

[Image: Kinematic Box]

The responsibility for animating kinematic bodies is on you, the developer. You will notice that the kinematic box shown above also has a script component with a script called movement.js assigned:

This script simply animates the box along the world x-axis using a sine function. You move kinematic bodies using the standard transformation functions on the entity like `setPosition`, `setRotation` and `setEulerAngles`. Now when we run the scene, the dynamic box falls on the kinematic box and is carried along on top of it:

[Image: Kinematic Box]

## Teleporting Dynamic Bodies {#teleporting-dynamic-bodies}

Although you can use the standard entity transformation function with kinematic bodies, this is not allowed for dynamic bodies. When creating a dynamic rigid body, you pass the responsibility for setting the position and orientation of that entity to the physics engine. This means that if you try to update the position or orientation of an entity in a script using the pc.Entity API, the functions will not have an effect. Instead, you must call the teleport function on the rigid body component which explicitly notifies the physics engine you want to momentarily update a rigid body's position and/or orientation.

--------------------------------------------------------------------------------

## Updating ammo.js

URL: https://developer.playcanvas.com/user-manual/physics/physics-migration/

### Introduction

PlayCanvas has added support for the latest version of ammo.js.

The new version has a number of benefits:

- more of the underlying Bullet API has been exposed
- support for ammo.js wasm module has been added
- the wasm version is smaller and runs faster

### Existing projects

Projects created before the introduction of wasm modules continue by default to use the legacy version of ammo.js. It is up to project owners to migrate their physics system to the latest version of ammo.js.

To check if your project is using the legacy version of ammo.js, navigate to the Scene Settings panel:

[Image: Physics Legacy Settings]

If 'Enable Physics' does not appear, then the project is new and doesn't have the option of using the legacy built-in version of ammo.js.

Otherwise, if 'Enable Physics' is checked the legacy version of ammo.js is silently being added to your project at build time.

### Migration

In order to use the latest version of ammo.js, disable 'Enable Physics' and click 'Import Ammo'. This imports the latest version of ammo.js provided by PlayCanvas into the Assets Panel.

If updating ammo.js causes issues with your project you may need to revert back to the old version. Do this by deleting (or disabling) the project's ammo.js modules and re-enabling the physics checkbox.

### Note

The project should either include legacy physics (using the 'Enable Physics' checkbox) or it should have the ammo.js modules included in the project directly, but it should not include both at the same time!

--------------------------------------------------------------------------------

## Ray Casting

URL: https://developer.playcanvas.com/user-manual/physics/ray-casting/

The PlayCanvas physics engine allows you to perform ray casts. A ray cast is a query that determines if a straight line between two arbitrary 3D points intersects with a rigid body.

One application of ray casting is picking, where the user can touch/click the screen and select an entity. Here is a script which performs a ray cast from the camera position into the scene through the screen touch/click position and returns the closest selected rigid body-enabled entity:

You can find a project that uses ray casting for entity selection [here](https://playcanvas.com/project/410547/overview/entity-picking-using-physics).

Ray casting has other applications too. An entity can probe the environment by firing ray casts. For example, to determine if an entity is on the ground, it can fire a ray cast directly downwards some distance and check if it intersects with the environment.

--------------------------------------------------------------------------------

## Trigger Volumes

URL: https://developer.playcanvas.com/user-manual/physics/trigger-volumes/

Trigger volumes are static collision shapes which can fire events whenever a rigid body enters or leaves their volume. They can be useful to determine when a goal has been scored in a football match or when a race car has reached the finish line.

To create a trigger volume, add a [collision component](/user-manual/editor/scenes/components/collision) to an entity and configure its shape. Do not add a rigidbody component to your trigger volume entity.

[Image: Trigger Volume]

To check whether a volume has been entered or exited by a rigid body based entity, you need a simple script:

--------------------------------------------------------------------------------

## Press Pack

URL: https://developer.playcanvas.com/user-manual/press-pack/

We offer a press pack for people to use when promoting or advertising PlayCanvas within their materials.

The pack includes logos and promotional images in the common aspect ratios. There is also a video that can be used where animation is needed.

[Download ZIP (44.5MB)](pathname:///downloads/playcanvas-press-pack.zip)
Last updated: Mon 15 Aug 2022

--------------------------------------------------------------------------------

## PlayCanvas React

URL: https://developer.playcanvas.com/user-manual/react/

[@playcanvas/react](https://github.com/playcanvas/react) brings PlayCanvas to the React ecosystem. It provides a declarative, component-based approach to building interactive 3D applications using familiar React patterns and a powerful ECS architecture. It's a thin wrapper around PlayCanvas - a batteries-included library for building interactive 3D content in React.

<div className='flex py-1 font-bold mb-2'>
  <a href="./getting-started">
    <div className='tablet'>
      <span>Getting Started →</span>
    </div>
  </a>  
  <a href="https://github.com/playcanvas/react" className='foreground underline' target="_blank">View on GitHub ↗</a>
</div>

## How does it Work?

PlayCanvas has a powerful ECS architecture that aligns perfectly with React's declarative nature. You create Entities, add Components to them, and load assets. That's it. No more worrying about the underlying engine, just build your scene.

Here's how you load and render a 3D model and create an orbit camera.

```tsx

  // Load an asset
  const { asset: model } = useModel('/assets/lambo.glb');

  // If the asset is not loaded, return null
  if (!model) return null;

  return <>    
    {/* create and position a camera entity */}
    

    {/* render the asset */}
    
  </>
}
```

## Full Engine Power, React Simplicity

PlayCanvas React gives you access to the **entire** PlayCanvas Engine - no third-party libraries required. If it's possible in PlayCanvas, it's possible in PlayCanvas React.

On top of that, PlayCanvas React adds a set of **React-first features** to make development faster and more ergonomic:

- **Asset Loading** – React-style asset loading with Suspense. Fine-grained control over when and how assets are fetched.
- **Pointer Events** – Familiar event bubbling for clicks, drags and gestures. Attach listeners to any entity and handle events naturally.
- **Physics** – Lazy-loaded physics engine that integrates cleanly with React's lifecycle.
- **Hooks** – Convenient hooks for engine events, materials, asset state and more.

## Getting Started

Follow our [Getting Started](./getting-started/) guide which will walk you through your first PlayCanvas React project.

Alternatively, you can spin up a new project in seconds with our [StackBlitz template](https://playcanvas-react.vercel.app/new)

## Examples

But less talking, more doing. Here are some examples of what you can build with PlayCanvas React:

<div style={{ display: 'grid', gridTemplateColumns: 'repeat(auto-fit, minmax(250px, 1fr))', gap: '20px', marginBottom: '40px' }}>
  <a href="https://playcanvas-react.vercel.app/examples/splats">
    <img
      src="/img/user-manual/react/splats.jpg"
      alt="Gaussian Splats Example"
      style={{ width: '100%', height: '200px', objectFit: 'cover', borderRadius: '8px', transition: 'transform 0.2s', cursor: 'pointer' }}
    />
  </a>
  <a href="https://playcanvas-react.vercel.app/examples/model-viewer">
    <img
      src="/img/user-manual/react/render.png"
      alt="3D Model Viewer Example"
      style={{ width: '100%', height: '200px', objectFit: 'cover', borderRadius: '8px', transition: 'transform 0.2s', cursor: 'pointer' }}
    />
  </a>
  <a href="https://playcanvas-react.vercel.app/examples/physics">
    <img
      src="/img/user-manual/react/physics.png"
      alt="Physics Example"
      style={{ width: '100%', height: '200px', objectFit: 'cover', borderRadius: '8px', transition: 'transform 0.2s', cursor: 'pointer' }}
    />
  </a>
  <a href="https://stackblitz.com/edit/pc-react-tick-tock?file=src%2FScene.tsx">
    <img
      src="/img/user-manual/react/clock.png"
      alt="Clock Example"
      style={{ width: '100%', height: '200px', objectFit: 'cover', borderRadius: '8px', transition: 'transform 0.2s', cursor: 'pointer' }}
    />
  </a>
</div>

## Learn More

- [Getting Started Guide](./getting-started/) - Set up your first project
- [Building a Scene](./building-a-scene) - Learn the fundamentals of scene composition
- [Features Showcase](https://playcanvas-react.vercel.app/docs#features) - Explore all capabilities
- [Interactive Playground](https://playcanvas-react.vercel.app/examples) - Learn by doing
- [API Reference](https://playcanvas-react.vercel.app/docs/api) - Complete component documentation

Explore the [official documentation](https://playcanvas-react.vercel.app/docs) or browse the [GitHub repository](https://github.com/playcanvas/react) for more.

--------------------------------------------------------------------------------

## `<Align/>`

URL: https://developer.playcanvas.com/user-manual/react/api/align/

# ``

The `` component is a simple container that positions an entity relative to its parent. This can be useful when using 3D assets that are offset from the origin. Sometimes, you just want to sit the asset on the ground, or align it with the camera.

The `` component accepts the following props: `top`, `bottom`, `left`, `right`, `front`, `back`.

## Usage

This example will align the asset to the bottom right of the parent entity.

```jsx copy

  return (
    
  )
}
```

## Properties

```tsx asTypedoc

type $ = React.ComponentProps<typeof Align>;

```

--------------------------------------------------------------------------------

## `<Anim/>`

URL: https://developer.playcanvas.com/user-manual/react/api/anim/

# ``

The `Anim` component enables animation playback for an [Entity](../entity). It allows you to play animation clips from animation assets and control their playback.

## Usage

```jsx copy

const AnimatingDinosaur = (props) => {
  const { asset } = useModel('/dinosaur.glb')

  if (!asset) return null

  return (
    
  )
}
```

Learn more about the [Anim Component](https://api.playcanvas.com/engine/classes/AnimComponent.html) in the PlayCanvas documentation.

## Properties

```tsx asTypedoc

type $ = React.ComponentProps<typeof Anim>;

```

--------------------------------------------------------------------------------

## `<Application/>`

URL: https://developer.playcanvas.com/user-manual/react/api/application/

# ``

The `` is the root your PlayCanvas React application. It initializes the engine and provides a rendering context. An Application maps to a single canvas in your app.

You can set the fill mode and resolution mode to control how the canvas fills the window, and other properties that control the graphics device.

```jsx copy

  return (
    
  )
}
```

## Properties

```tsx asTypedoc

type $ = React.ComponentProps<typeof Application>;

```

--------------------------------------------------------------------------------

## `<Camera/>`

URL: https://developer.playcanvas.com/user-manual/react/api/camera/

# ``

Without a camera in your scene, nothing will be rendered. The `` component enables an [``](../entity) to render a scene from its perspective. Set the entity's position and rotation to control the camera's viewpoint.

## Usage

Attach a `` to an [``](../entity) and it will render the scene from the entity's position and rotation.

```tsx

```

In this example, we're switching between 2 different cameras in the scene, each with its own properties.

Learn more about the [Camera Component](https://api.playcanvas.com/engine/classes/CameraComponent.html) in the PlayCanvas documentation.

## Properties

```tsx asTypedoc

type $ = React.ComponentProps<typeof Camera>;

```

--------------------------------------------------------------------------------

## `<Collision/>`

URL: https://developer.playcanvas.com/user-manual/react/api/collision/

# ``

The `` component attaches a PlayCanvas [Collision Component](https://api.playcanvas.com/engine/classes/CollisionComponent.html) to an [Entity](../entity).

It allows an Entity to participate in collision detection with other entities that have collision components. This is useful for physics simulations, trigger zones, and other gameplay mechanics that require detecting when objects intersect.

:::note
Enable physics by installing `sync-ammo` and enabling physics on the Application using ``.
:::

## Usage

You attach a Collision component to an Entity in the same way you would attach a Render component. To work with physics, you should also attach a `Rigidbody` component to the same `Entity`, and probably a `Render` component too.

```jsx copy

  return (
    
  )
}
```

Learn more about [Collision Components](https://api.playcanvas.com/engine/classes/CollisionComponent.html) in the PlayCanvas documentation. Also see the [Rigidbody](./rigidbody.mdx) component for more information on how to use collision components with physics.

## Properties

```tsx asTypedoc

type $ = React.ComponentProps<typeof Collision>;

```

--------------------------------------------------------------------------------

## `<Entity/>`

URL: https://developer.playcanvas.com/user-manual/react/api/entity/

# ``

The `` component is the fundamental building block of a React application. It represents a node in the scene graph hierarchy and can contain other entities as children, as well as components that define its behavior and appearance.

```jsx copy

  return (
    
    </Entity>
  )
}
```

An `` is a transform node, which has position, rotation and scale. On its own, it has no behavior. To allow it to render something, react to physics or behave like a camera you must add behavior using components.

:::tip

An `` on its own has no behavior. You can add Components to an `` to give it behavior. Components are the building blocks of behavior in PlayCanvas.

:::

## Properties

```tsx asTypedoc

type $ = React.ComponentProps<typeof Entity>;

```

--------------------------------------------------------------------------------

## `<Environment/>`

URL: https://developer.playcanvas.com/user-manual/react/api/environment/

# ``

The `` component is used to configure the environment lighting and skybox for a scene. It provides a simple way to set up global lighting, and other effects that affect the entire scene.

With an `` component in your scene, you can:

- Set the background texture for the scene
- Provides Image-Based Lighting (IBL) for realistic environmental reflections
- Controls sky dome properties, rotation, scale, and positioning
- Manages exposure, luminance, and other global lighting parameters

## Usage

Place the `` component within an [``](../application) and add it to the scene. You'll need to load a HDR or environment atlas and provide it to the component.

:::tip

**You only need one `` component for the entire scene**. Using multiple `` components within the same `` will trigger a warning.

:::

Simply load a HDR and set it as the skybox. You can find great HDR images on [HDRI Haven](https://hdrihaven.com/).

```jsx copy
const EnvironmentExample = () => {
  const { asset: skybox } = useTexture('/env.hdr')

  return (
      
      {/* The rest of your scene... */}
  )
}
```

The component will affect the entire scene, so you should only use it once. You can specify global lighting parameters like exposure, luminance, and other global lighting parameters. See the [Properties](#properties) section for more details.

You can also use the `` component to create a dome effect. This is useful for creating a more realistic environment with a ground plane. You can specify the scale, position, rotation, and other properties of the dome.

## Properties

```tsx asTypedoc

type $ = React.ComponentProps<typeof Environment>;

```

--------------------------------------------------------------------------------

## `<Gltf/>`

URL: https://developer.playcanvas.com/user-manual/react/api/gltf/

# ``

The `` component loads and instantiates a GLB scene so you can work with its hierarchy from React. It renders the model by default, builds a cache of every entity in the file, and coordinates `<Modify.*>` rules so you can declaratively adjust the imported content.

Learn the bigger picture in the [Modifying GLB Models guide](../../guide/modifying-glb-models).

## Usage

Load a GLB with [`useModel`](../hooks/use-asset) and pass the asset into ``. The component handles instantiation for you:

```tsx copy

  const { asset } = useModel('/assets/statue.glb');
  if (!asset) return null;

  return (
    
  );
}
```

### Live example

This demo adds a control to change the render style of the model. It targets all nodes with a render component and changes the render style reactively.

### Data-only instantiation

Set `render={false}` when you only need the scene hierarchy—for example when generating colliders or running queries. `` still instantiates the GLB as long as you include at least one `<Modify.Node>` child.

```tsx copy

```

## Properties

```tsx asTypedoc

type $ = React.ComponentProps<typeof Gltf>;

```

--------------------------------------------------------------------------------

## `<GSplat/>`

URL: https://developer.playcanvas.com/user-manual/react/api/gsplat/

# ``

The `<GSplat>` component is used to render [Gaussian Splats](https://api.playcanvas.com/engine/classes/GSplatComponent.html). Gaussian Splats are a new way of capturing and rendering high quality 3D content. They capture the shape and lighting of a scene in a way that produces incredibly high quality results.

The `useSplat` hooks accepts ply or sog files or any other files that can be loaded by the engine.

## Usage

You can load a Gaussian Splat asset with the [`useSplat`](../../api/hooks/use-asset#usesplat) hook and then use the `<GSplat>` component to render it.

```tsx
const { asset } = useSplat('/assets/toy-cat.sog');

```

Learn more about the [GSplat Component](https://api.playcanvas.com/engine/classes/GSplatComponent.html) in the PlayCanvas documentation.

## Properties

```tsx asTypedoc

type $ = React.ComponentProps<typeof GSplat>;

```

--------------------------------------------------------------------------------

## Hooks

URL: https://developer.playcanvas.com/user-manual/react/api/hooks/

PlayCanvas React provides a comprehensive set of React hooks that make it easy to integrate PlayCanvas functionality into your React applications. These hooks handle the complex lifecycle management and provide a clean, React-friendly API.

## Available Hooks

### Core Hooks

- **[useApp](./use-app)** - Access the PlayCanvas Application instance
- **[useParent](./use-parent)** - Get the parent Entity from context

### Asset Loading Hooks

- **[useAsset](./use-asset)** - Load any type of PlayCanvas asset
- **[useModel](./use-asset#usemodel)** - Load 3D models (GLB/GLTF)
- **[useTexture](./use-asset#usetexture)** - Load texture assets
- **[useSplat](./use-asset#usesplat)** - Load Gaussian splat assets
- **[useEnvAtlas](./use-asset#useenvatlas)** - Load environment atlas textures
- **[useFont](./use-asset#usefont)** - Load font assets

### Event and Animation Hooks

- **[useAppEvent](./use-app-event)** - Subscribe to PlayCanvas application events

### Material and Physics Hooks

- **[useMaterial](./use-material)** - Create and manage materials
- **[usePhysics](./use-physics)** - Access physics context and state

## Usage Pattern

Most hooks follow a consistent pattern:

```jsx

function MyComponent() {
  const app = useApp()
  const { asset, loading, error } = useAsset('model.glb', 'container')
  
  if (loading) return <div>Loading...</div>
  if (error) return <div>Error: {error}</div>
  
  return 
}
```

## Context Requirements

Many hooks require specific React contexts to be available:

- **useApp** requires an `<Application>` component in the component tree
- **useParent** requires an `<Entity>` component in the component tree
- **usePhysics** requires physics to be enabled on the Application

## TypeScript Support

All hooks are fully typed with TypeScript and provide excellent IntelliSense support. The hooks automatically infer types from the PlayCanvas engine types.

```tsx
// Fully typed with PlayCanvas types
const app = useApp() // Application
const entity = useParent() // Entity
const { asset } = useAsset('texture.jpg', 'texture') // Asset | null
```

## Best Practices

1. **Always check loading states** when using asset hooks
2. **Handle errors gracefully** for better user experience
3. **Use the appropriate hook** for your specific use case
4. **Clean up subscriptions** are handled automatically by the hooks
5. **Follow React rules** - hooks should only be called at the top level of components

--------------------------------------------------------------------------------

## `useAppEvent`

URL: https://developer.playcanvas.com/user-manual/react/api/hooks/use-app-event/

# `useAppEvent`

The `useAppEvent` hook allows you to subscribe to PlayCanvas application events with proper TypeScript typing and automatic cleanup. It supports both built-in events and custom events.

## Basic Usage

```jsx copy

function MyComponent() {
  useAppEvent('update', (dt) => {
    console.log('Frame time:', dt)
  })
  
  useAppEvent('prerender', () => {
    console.log('Pre-rendering')
  })
  
  return null
}
```

## Parameters

- **event** (`TEventName`) - The event name to subscribe to
- **callback** (`TEventMap[TEventName]`) - The callback function to execute when the event fires

## Built-in Events

### Update Event

```jsx copy

function UpdateHandler() {
  useAppEvent('update', (dt) => {
    // dt is the delta time since the last frame
    console.log('Delta time:', dt)
    
    // Your update logic here
    // This runs every frame
  })
  
  return null
}
```

### Render Events

```jsx copy

function RenderHandler() {
  useAppEvent('prerender', () => {
    console.log('About to render frame')
    // Pre-render logic
  })
  
  useAppEvent('postrender', () => {
    console.log('Finished rendering frame')
    // Post-render logic
  })
  
  return null
}
```

## Custom Events

You can define custom event types with TypeScript:

```jsx copy

// Define your custom event map
interface MyEventMap {
  levelComplete: (level: number, score: number) => void
  playerDeath: (position: [number, number, number]) => void
  itemCollected: (itemId: string, quantity: number) => void
}

function GameEventHandler() {
  useAppEvent<MyEventMap>('levelComplete', (level, score) => {
    console.log(`Level ${level} completed with score ${score}`)
  })
  
  useAppEvent<MyEventMap>('playerDeath', (position) => {
    console.log('Player died at position:', position)
  })
  
  useAppEvent<MyEventMap>('itemCollected', (itemId, quantity) => {
    console.log(`Collected ${quantity} of item ${itemId}`)
  })
  
  return null
}
```

## Event Firing

To fire custom events, you need access to the app instance:

```jsx copy

function EventFirer() {
  const app = useApp()
  
  useAppEvent('update', (dt) => {
    // Fire custom events
    app.fire('levelComplete', 1, 1000)
    app.fire('playerDeath', [0, 0, 0])
    app.fire('itemCollected', 'coin', 1)
  })
  
  return null
}
```

## Input Handling

```jsx copy

interface InputEvents {
  keyDown: (key: string) => void
  keyUp: (key: string) => void
  mouseClick: (x: number, y: number) => void
  mouseMove: (x: number, y: number) => void
}

function InputHandler() {
  useAppEvent<InputEvents>('keyDown', (key) => {
    console.log('Key pressed:', key)
  })
  
  useAppEvent<InputEvents>('keyUp', (key) => {
    console.log('Key released:', key)
  })
  
  useAppEvent<InputEvents>('mouseClick', (x, y) => {
    console.log('Mouse clicked at:', x, y)
  })
  
  useAppEvent<InputEvents>('mouseMove', (x, y) => {
    console.log('Mouse moved to:', x, y)
  })
  
  return null
}
```

## Related

- [useApp](../use-app) - Access the Application instance
- [Application Events](https://api.playcanvas.com/engine/classes/Application.html#events) - PlayCanvas application events documentation

--------------------------------------------------------------------------------

## `useApp`

URL: https://developer.playcanvas.com/user-manual/react/api/hooks/use-app/

# `useApp`

The `useApp` hook provides access to the PlayCanvas [Application](https://api.playcanvas.com/engine/classes/Application.html) instance from within your React components. It's a convenient way to get information about the app state, fire events, and more.

```jsx copy title="./my-component.jsx" copy

function MyComponent() {
  const app = useApp()
  
  // Access application properties
  console.log('Scene:', app.scene)
  console.log('Camera:', app.scene.camera)
  
  return null
}
```

A reference to the app gives you access to the scene, camera, systems, etc.

## Application context

The hook only relies on the `` component for context, so without one it will throw an error. Call it from inside a component that is a child of an ``.

```jsx title="🚫 No 
  )
}
```

```jsx copy title="✅ Correct 
```

## Properties

```tsx asTypedoc

type $ = {
  /**
   * The PlayCanvas Application instance
   */
  app: Application
}

```

## Related

- [Application Component](../../application) - The component that provides the app context
- [useParent](../use-parent) - Access the parent Entity
- [useAppEvent](../use-app-event) - Subscribe to application events

--------------------------------------------------------------------------------

## Asset Hooks

URL: https://developer.playcanvas.com/user-manual/react/api/hooks/use-asset/

# Asset Hooks

PlayCanvas React provides hooks for loading and managing different assets. These hooks simplify the process of loading assets and allow you to handle loading and error states.

The general signature of every hook looks something like the following.

```tsx
const { asset, loading, error } = useModel(src, props);
```

Where `src` is the URL of the asset and `props` are additional properties to pass to the asset loader.

```tsx copy filename="render-glb.tsx"

    const { asset, loading, error } = useModel('model.glb');

    if (loading) return ;
    if (error) return ;
    if (!asset) return null;

    return ;
}
```

## Loading Progress

The `useAsset` hook supports loading progress via the `subscribe` callback. This is useful if you want to show a loading indicator or update a progress bar. Not all asset types return a progress value during load, so this is not guaranteed to be available for all asset types.

```tsx copy filename="render-splat.tsx"

  const [progress, setProgress] = useState(0);
  const { asset, loading, error, subscribe } = useSplat('splat.ply');

  // Subscribe to loading progress
  useEffect(() => {
    const unsubscribe = subscribe(({ progress }) => setProgress(progress));
    return () => unsubscribe();
  }, [subscribe]);

  if (loading) return ;
  if (error) return ;
  if (!asset) return null;

  return ();
}
```

### All Hooks

The following hooks are available:

- [`useModel`](#usemodel) - Load a 3D model asset
- [`useSplat`](#usesplat) - Load a Gaussian Splat asset
- [`useTexture`](#usetexture) - Load a texture asset
- [`useEnvAtlas`](#useenvatlas) - Load an environment atlas texture
- [`useAsset`](#useasset) - Generic hook for loading any type of asset

## API

This is the full response of an asset hook.

```tsx asTypedoc

````

## useModel

A specialized hook for loading 3D model assets (GLB/GLTF). Pass the source URL of the model file and any additional properties to pass to the asset loader and use the resulting asset in the `` component.

```tsx copy filename="render-model.tsx"

  const { asset, loading, error } = useModel('model.glb');

  if (loading) return ;
  if (error) return ;
  if (!asset) return null;

  return ;
}
```

### Draco Decoding

The `useModel` hook also supports Draco decoding out of the box with zero configuration. @playcanvas/react will use the latest version of the Draco decoder ([1.5.7](https://github.com/google/draco?tab=readme-ov-file#version-157-release)) and lazy load it from the Google CDN.

Alternatively if you want to self-host the library you can manually configure the decoder using `dracoInitialize`.

```tsx copy filename="render-draco.tsx"

dracoInitialize({
  jsUrl: '/draco_decoder.js',
  wasmUrl: '/draco_decoder.wasm',
  lazyInit: true
});
```

## useSplat

A specialized hook for loading Gaussian Splat assets. Pass the source URL of the splat file and any additional properties to pass to the asset loader and use the resulting asset in the [``](../../gsplat) component.

```tsx copy filename="render-splat.tsx"

  const { asset, loading, error } = useSplat('splat.ply');
  if (loading) return ;
  if (error) return ;
  if (!asset) return null;

  return ();
}
```

See the [``](../../gsplat) component for more information.

## useTexture

A specialized hook for loading texture assets. Pass the source URL of the texture file and any additional properties to pass to the asset loader and use the resulting asset in any component that accepts a texture.

```tsx copy filename="render-texture.tsx"

  const { asset, loading, error } = useTexture('texture.jpg');
  const material = useMaterial({ map: asset.resource });

  if (loading) return ;
  if (error) return ;
  if (!asset) return null;

  return ;
}
```

See the [useMaterial](../use-material) hook and [Render](../../render) component for more information.

## useEnvAtlas

A specialized hook for loading environment atlas textures. Pass the source URL of the texture file and any additional properties to pass to the asset loader and use the resulting asset in the [``](../../environment) component.

```tsx copy filename="render-env-atlas.tsx"

  const { asset, loading, error } = useEnvAtlas('env.jpg');

  if (loading) return ;
  if (error) return ;
  if (!asset) return null;

  return ;
}
```

See the [Environment](../../environment) component for more information.

## useFont

Text in PlayCanvas is rendered using Signed-Distance-Fields (SDFs), so you'll need to convert your ttf fonts into the appropriate format before loading them.

If you're using Vite or Rollup there's an official plugin to convert ttf files for you at build time. Check out the [@playcanvas/rollup](https://www.npmjs.com/package/@playcanvas/plugin) for more information.

You can import a ttf using the `?sdf` query parameter which will automatically convert it into an SDF texture at build time.

```tsx copy filename="render-font.tsx"

// Use the @playcanvas/rollup plugin to convert the ttf file into an SDF texture at build time.

  const { asset, loading, error } = useFont(inconsolata);

  if (loading) return ;
  if (error) return ;
  if (!asset) return null;

  return asset;
}
```

## useAsset

This is a generic hook for loading any type of asset. You can use it to load any asset type that PlayCanvas supports which is sometimes useful if you need to load an asset dynamically.

```tsx copy filename="use-dynamic-asset.tsx"

const assetTypeToLoadType = {
    'ply': useSplat,
    'jpg': useTexture,
    'png': useTexture,
    'glb': useModel,
    'gltf': useModel,
}

  const mimeType = src.split('.').pop();
  const loadType = assetTypeToLoadType[mimeType];

  if (!loadType) {
    throw new Error(`Unsupported asset type: ${mimeType}`);
  }

  return useAsset(src, loadType, props);
}
```

--------------------------------------------------------------------------------

## `useMaterial`

URL: https://developer.playcanvas.com/user-manual/react/api/hooks/use-material/

# `useMaterial`

The `useMaterial` hook allows you to create and manage a PlayCanvas [StandardMaterial](https://api.playcanvas.com/engine/classes/StandardMaterial.html) instance. Create a material with the hook and apply it to a [``](../../render) component.

## Usage

You can create a material with the hook and apply it to a [``](../../render) component and update material properties dynamically.

## Parameters

The hooks accepts an object with the following properties that closely match those of the [`StandardMaterial`](https://api.playcanvas.com/engine/classes/StandardMaterial.html) class.

<details>
  <summary><b>Parameters</b></summary>
  <p>
  ```tsx asTypedoc
  import { useMaterial } from '@playcanvas/react/hooks'
  type $ = Parameters<typeof useMaterial>[0];
  export default $;
  ```
  </p>
</details>

## Returns

The hooks will return a [`StandardMaterial`](https://api.playcanvas.com/engine/classes/StandardMaterial.html) instance which can be applied to a [`Render`](../../render) component.

```tsx asTypedoc

type $ = {
  /**
   * The PlayCanvas StandardMaterial instance
   */
  material: StandardMaterial
}

```

## Further examples

### Advanced Properties

The `StandardMaterial` accepts many different properties which can be found [here](https://api.playcanvas.com/engine/classes/StandardMaterial.html).

```jsx copy

function AdvancedMaterial() {
  const material = useMaterial({
    diffuse: 'purple',
    opacity: 0.9,
    metalness: 0.3,
    roughness: 0.4,
    emissive: 'yellow',
    emissiveIntensity: 0.2,
    specular: 'white',
    shininess: 50,
    reflectivity: 0.8,
    clearCoat: 0.5,
    clearCoatRoughness: 0.1
  })
  
  return 
}
```

### Material with Textures

You can use textures with the material by loading them with the [`useTexture`](../use-asset#usetexture) hook.

```jsx copy

function TexturedMaterialExample() {
  const { asset: diffuseMap } = useTexture('diffuse.jpg')
  const { asset: normalMap } = useTexture('normal.jpg')
  const { asset: roughnessMap } = useTexture('roughness.jpg')
  
  const material = useMaterial({
    diffuseMap: diffuseMap?.resource,
    normalMap: normalMap?.resource,
    roughnessMap: roughnessMap?.resource,
    diffuse: 'white',
    metalness: 0.5,
    roughness: 0.5
  })
  
  return 
}
```

### Material Sharing

You can easily share materials between multiple render components:

```jsx copy

function SharedMaterial() {
  const material = useMaterial({
    diffuse: 'orange',
    metalness: 0.7,
    roughness: 0.3
  })
  
  return (
    <div>
      
      
      
    </div>
  )
}
```

## Related

- [useTexture](../use-asset#usetexture) - Load texture assets
- [Render Component](../../render) - Use materials with render components
- [StandardMaterial API](https://api.playcanvas.com/engine/classes/StandardMaterial.html) - PlayCanvas material documentation

--------------------------------------------------------------------------------

## `useParent`

URL: https://developer.playcanvas.com/user-manual/react/api/hooks/use-parent/

# `useParent`

The `useParent` hook provides access to the parent [Entity](https://api.playcanvas.com/engine/classes/Entity.html) from within your React components. It's essential for components that need to interact with their parent entity.

## Basic Usage

```jsx copy

function MyComponent() {
  const parent = useParent()
  
  // Access parent entity properties
  console.log('Parent name:', parent.name)
  console.log('Parent position:', parent.position)
  
  return null
}
```

## Returns

```tsx asTypedoc

type $ = {
  /**
   * The PlayCanvas Entity instance
   */
  parent: Entity
}

```

## Requirements

This hook must be used within an `

// ❌ Incorrect usage
 {/* useParent will throw an error */}
```

## Examples

### Accessing Entity Properties

```jsx copy

function EntityInfo() {
  const parent = useParent()
  
  return (
    <div>
      <p>Entity Name: {parent.name}</p>
      <p>Position: {parent.position.toString()}</p>
      <p>Rotation: {parent.rotation.toString()}</p>
      <p>Scale: {parent.scale.toString()}</p>
    </div>
  )
}
```

## Related

- [Entity Component](../../entity) - The component that provides the entity context
- [useApp](../use-app) - Access the Application instance
- [Entity API](https://api.playcanvas.com/engine/classes/Entity.html) - PlayCanvas Entity documentation

--------------------------------------------------------------------------------

## `usePhysics`

URL: https://developer.playcanvas.com/user-manual/react/api/hooks/use-physics/

# `usePhysics`

The `usePhysics` hook provides information on the physics engine state. It allows you to check if physics is enabled, loaded, and to handle any physics-related errors.

PlayCanvas React lazily loads and instantiates the physics engine allowing you to render content ahead of time. The `usePhysics` provides a reactive mechanism to check the physics engine state.

You can read more about the physics system in the [Physics guide](../../../guide/physics) or the [Rigidbody](../../rigidbody) and [Collision](../../collision) components.

## Usage

### Conditional Rendering

```jsx copy

function PhysicsInfo() {
  const { isPhysicsEnabled, isPhysicsLoaded, physicsError } = usePhysics()

  if (!isPhysicsLoaded) return null
  
  return ()
}
```

### Safe Physics Component Creation

```jsx copy

function SafePhysicsEntity() {
  const { isPhysicsEnabled, isPhysicsLoaded } = usePhysics()
  
  return (
    
  )
}
```

### Physics with Fallback

```jsx copy

function PhysicsWithFallback() {
  const { isPhysicsEnabled, isPhysicsLoaded } = usePhysics()
  
  if (isPhysicsEnabled && isPhysicsLoaded) {
    return (
      
    )
  }
  
  // Fallback without physics
  return (
    
  )
}
```

## Properties

```tsx asTypedoc

type $ = ReturnType<typeof usePhysics>

```

## Related

- [useApp](../use-app) - Access the Application instance
- [Collision Component](../../collision) - Create collision components
- [Rigidbody Component](../../rigidbody) - Create rigidbody components
- [Application Component](../../application) - Enable physics with usePhysics prop

--------------------------------------------------------------------------------

## `<Light/>`

URL: https://developer.playcanvas.com/user-manual/react/api/light/

# ``

The `` component allows an Entity to emit light. Any [``](../render) component will be lit by the light. When attached to an [``](../entity), the light can be positioned and oriented using the entity's transform. Lights can be directional, omni or spot.

## Usage

Simply attach a `` to an [``](../entity) and it will emit light from the entity's position and rotation.

```tsx

```

Learn more about [Lights](https://api.playcanvas.com/engine/classes/LightComponent.html) in the PlayCanvas documentation.

## Properties

```tsx asTypedoc

type $ = React.ComponentProps<typeof Light>;

```

--------------------------------------------------------------------------------

## `<Modify.*>`

URL: https://developer.playcanvas.com/user-manual/react/api/modify/

# `<Modify.*>`

`<Modify>` provides declarative rules for editing the entities and components inside a GLB. Place `<Modify.Node>` elements inside a [``](../gltf) to match part of the imported hierarchy, then use the helper subcomponents to tweak what you find.

The system lets you:

- Match entities using filesystem-like paths or predicate functions.
- Merge new props into existing light, render, and camera components.
- Remove components or replace an entity’s children entirely.
- Add brand-new child entities inline with your rule.

The [Modifying GLB Models guide](../../guide/modifying-glb-models) walks through practical scenarios using these primitives.

## `<Modify.Node>`

`<Modify.Node>` defines where a rule applies. Use the `path` prop to target entities by name, component filter, wildcards (`*` and `**`), or even a predicate function.

```tsx copy

```

Set `clearChildren` to start from a clean slate before adding your own children:

```tsx copy
<Modify.Node path="Accessories" clearChildren>
  
</Modify.Node>
```

Every non-`Modify.*` element you nest inside `<Modify.Node>` is cloned and appended as a child of each match.

### Path helpers

- `Body.FrontBumper` — exact match.
- `Body.*` — direct children.
- `Body.**` — any depth below `Body`.
- `**[render]` — any entity with a render component.
- `(entity) => entity.name.startsWith('Wheel')` — predicate function.

## Component modifiers

Within a node rule you can mutate existing components using the modifier helpers. Each helper supports direct values, updater functions, and a `remove` flag.

```tsx copy
<Modify.Node path="**[light]">
  
</Modify.Node>

<Modify.Node path="**[render]">
  <Modify.Render
    castShadows={(value = false) => !value}
    receiveShadows
  />
</Modify.Node>

<Modify.Node path="Cabin.Camera">
  
</Modify.Node>
```

All matching entities are evaluated, but if multiple rules address the same component the most specific path wins.

### Functional updates

Pass a function to merge props based on the current value:

```tsx copy
<Modify.Light intensity={(current = 1) => current * 1.5} />
```

Use this pattern when you want to adjust values relative to whatever ships in the GLB.

## Live example

The interactive demo below combines `` and `<Modify.Node>` to control rendering behavior. Toggle the checkboxes to see component rules take effect. Try disabling `Render visuals` to confirm that rules still run in data-only mode.

## Properties

### ``

```tsx asTypedoc

type $ = React.ComponentProps<typeof Modify.Node>;

```

### ``

```tsx asTypedoc

type $ = React.ComponentProps<typeof Modify.Light>;

```

### ``

```tsx asTypedoc

type $ = React.ComponentProps<typeof Modify.Render>;

```

### ``

```tsx asTypedoc

type $ = React.ComponentProps<typeof Modify.Camera>;

```

--------------------------------------------------------------------------------

## `<Render/>`

URL: https://developer.playcanvas.com/user-manual/react/api/render/

# ``

The `` component allows an [``](../entity) to display a 3D asset or primitive shape. Adding the `` component to an [``](../entity) will add a `RenderComponent` to the entity. The `type` prop allows you to select the primitive shape to render.

## Rendering an Asset

To render a 3D model loaded from a URL, you can use the asset type and pass the asset directly to the `asset` prop. You can learn more about loading assets in the [loading assets guide](../../guide/loading-assets).

```jsx copy

const RenderModel = () => {
  const { asset } = useModel('model.glb')

  if (!asset) return null

  return 
}
```

Learn more about the [Render Component](https://api.playcanvas.com/engine/classes/RenderComponent.html) in the PlayCanvas documentation.

## Properties

```tsx asTypedoc

type $ = React.ComponentProps<typeof Render>;

```

--------------------------------------------------------------------------------

## `<Rigidbody/>`

URL: https://developer.playcanvas.com/user-manual/react/api/rigidbody/

# ``

The `` component gives an [``](../entity) physical properties such as mass, velocity, and friction and allows it to interact with the global physics simulation. An `` with a `` will respond to gravity, forces, torques and other physics behaviors.

You can learn more about how the physics system works in PlayCanvas in the [Physics docs](/user-manual/physics/) and for React specific details, see the guide on [React Physics](/user-manual/react/guide/physics).

## Getting started

:::tip

Using physics in PlayCanvas requires ammo.js and `` to be enabled. Run `npm i sync-ammo` from within your app.

:::

To use the `` component, you need to install the `sync-ammo` dependency and you need to add the [``](../application) prop set in your root [``](../application) component. This will enable the physics system and allow your entities to interact with the physics simulation.

Check out the guide on [React Physics](/user-manual/react/guide/physics) for more information.

## Usage

Add a `` component to an [``](../entity). You'll also need to add a [``](../collision) component to the entity to define the shape of the rigid body.

```jsx

```

Learn more about the [RigidBody Component](https://api.playcanvas.com/engine/classes/RigidBodyComponent.html) in the PlayCanvas documentation.

## Properties

```tsx asTypedoc

type $ = React.ComponentProps<typeof RigidBody>;

```

--------------------------------------------------------------------------------

## `<Script/>`

URL: https://developer.playcanvas.com/user-manual/react/api/script/

# ``

The `` component provides a simple imperative way to add behaviors to an [``](../entity).

This is useful for things like [physics](../../guide/physics), animation, and [interactivity](../../guide/interactivity) where, for performance reasons, you want to bypass updating React props. It allows you to hook into the engine's update loop outside of React, which is useful for performance-critical code. You can also leverage existing Scripts from PlayCanvas Editor Projects.

You can find more information about [Scripts](/user-manual/scripting/) in the Engine documentation.

## Usage

The `Script` component takes a class that extends the PlayCanvas [Script](https://api.playcanvas.com/engine/classes/Script.html) class and attaches it to the `Entity`. Any additional props are passed to the Script class directly and can be used as properties on the class.

```tsx

return 
```

The following examples adds a `SpinMe` script to the entity that rotates the entity every frame.

You can use any existing Script or create your own, or any from the engine repository for [existing Scripts](https://github.com/playcanvas/engine/tree/main/scripts/esm).

:::tip
The PlayCanvas engine contains a number of useful scripts that you can use in your projects. **Find them [here](https://github.com/playcanvas/engine/tree/main/scripts/esm)**.
:::

You can learn more about the anatomy of a ESM Script in the [Scripting documentation](/user-manual/scripting/esm-scripts).

## Properties

```tsx asTypedoc

type SubclassOf<T> = new (...args: any[]) => T;
type $ = {
  /**
   * The Script class to attach to the entity
   */
  script: SubclassOf<PcScript>;
  /**
   * Additional props that will be passed to the script instance as attributes
   */
  [key: string]: unknown;
}

```

--------------------------------------------------------------------------------

## Building a Scene

URL: https://developer.playcanvas.com/user-manual/react/building-a-scene/

This guide will walk you through the process of setting up a new `@playcanvas/react` project and creating a simple 3D scene. By the end of this guide, you'll have a basic understanding of the core concepts of PlayCanvas React.

At the end you'll have something like the following.

<div style={{ position: 'relative' }}>
    
    <div style={{ textAlign: 'center', position: 'absolute', bottom: 0, right: 0, margin: '1rem 2rem', fontSize: '0.9rem' }}>An interactive 3D scene built with <b>@playcanvas/react</b>.</div>
</div>

:::info

This guide assumes you have some familiarity with React. If you're new to React, it's worth taking a look at the [React documentation](https://react.dev/learn) to get up to speed.

:::

We'll build the scene step by step, and you'll see the code and live examples at each step.

## Your first scene

If you've not already done so, **follow the [Installation Guide](../getting-started/installation)**, and
create a new PlayCanvas React project.

```bash
npm create playcanvas@latest my-first-3d-app -t react-ts
```

Before we get to anything more complex, let's create a simple example that displays a box. Any PlayCanvas scene starts with an **Application**, a **Scene** and a **Camera** so we can see something.

In your `/src/App.jsx` file, you can delete everything and replace it with the following code.

When you run this or click the Demo tab, you'll see a square in the center of the screen.

### What's happening here?

We've created a PlayCanvas `Application` and a separate `Scene` that contains an `Entity` with a `Render` component. The `Application` is the root of every PlayCanvas application. It sets up an HTML `canvas` element and defines a rendering context.

We've abstracted out a `Scene` that contains everything we want to render. It's not strictly necessary, but it's a clean and logical way to organize your code. Inside the `Scene`, we've added an `Entity` component with a `Render` component. The `Entity` component is simply a container. It can be positioned, rotated and scaled, but on its own, it doesn't do anything. The `Render` component gives the `Entity` the ability to render 3D assets.

:::tip

PlayCanvas uses an **ECS architecture**. You add **'components'** to an `Entity` to give it functionality.

:::

Finally we've added a `Camera` component to the `Entity`. When you attach a `Camera` component to an `Entity`, it will render the scene from that entity's position down the entity's negative Z axis. We've set the camera's position to `[0, 0, 5]` so that it's a bit away from the box. If you play with those values, you'll see how it changes the rendered scene.

## Adding lights

This is a great start, but we don't have any lights in the scene. You can add lighting to the scene by either adding an `Environment` or `Light` component. There are various types of lights in PlayCanvas, so check the [docs](/user-manual/graphics/lighting/lights) for a broader overview. For now, we'll use environmental lighting (which provides a more naturally lit scene) and a simple directional light which we'll use later to help ground the scene.

If you run this or click the Demo tab, you'll see the box with a bit of lighting. There are a few new concepts here, so let's break them down.

### Loading assets

We've loaded an environment atlas using the `useEnvAtlas` hook. This is a hook that allows us to load an environment atlas from a URL which we pass to the `Environment` component. The library provides a number of powerful hooks for loading different types of assets that let you manage loading states and errors.

[Learn more](../guide/loading-assets) about loading assets.

### Adding components

- `Environment` - This component adds environmental lighting to the scene.
- `Light` - This component adds a light to the scene.
- `Render` - This component renders the box.

We do this by loading an environment atlas and passing it to the `Environment` component, and adding a `Light` component.

## Adding interactivity

Ok, so we have some lighting. Let's add some interactivity by adding a `Script` component so you can rotate the cube with the mouse. We'll use the `CameraControls` script to allow the camera to be rotated with the mouse. We've also added a `Camera` component to the `Entity` to set the clear color and field of view.

There are a number of ways to add interactivity to your scene. You can attach event listeners directly to entities, or use the `Script` component to add custom behavior.

### Using the `Script` component

In the above example, we're using the `Script` component together with the `CameraControls` script. A `Script` component is a handy way to attach custom behavior to an `Entity`. It hooks into the engine's update loop and allows you to run code on every frame outside the React render loop. This is great for interactivity where you want to respond to user input for example, or do animations.

:::tip

Using a `<Script>` component is more performant than re-rendering the React tree. However, a Script can override Entity props, so it's important to be aware of this.

:::

Now when you run it, you should be able to rotate the cube with the mouse. You can learn more in the [Scripting section](/user-manual/scripting/).

## Adding 3D models

Ok, so we have a camera and a rotating box. Let's make it more interesting by adding a 3D model to the scene. You can use any 3D model format that PlayCanvas supports. [Fab](https://www.fab.com/) is a great resource for free 3D models.

You should now see the Lamborghini model in the scene. We've removed the box primitive by switching the `Render` to render assets. The asset is loaded using the `useModel` hook which is passed directly to the `Render` component. While the asset loads, we're returning `null` so nothing is rendered until the asset is loaded. You could optionally use a loading spinner here instead. Check out the [loading assets guide](../guide/loading-assets) to learn more about loading assets.

## Staging the scene

We've now got a basic 3D scene with a camera, a rotating model, and some environment lighting. Let's add a ground plane and some shadows to stage the scene a little better. We can use the `ShadowCatcher` and `Grid` components to help with this.

## Live example

We now have a complete 3D scene with a camera, a rotating model, and some environment lighting. You can now start adding your own assets and creating your own scenes. Check out the [loading assets guide](../guide/loading-assets) to learn how to load your own assets and add interactivity with scripts.

```jsx title="src/App.jsx" live noInline
const Scene = () => {
  // load the assets
  const { asset: envAtlas } = useEnvAtlas('/assets/environment.png');
  const { asset: lambo } = useModel('/assets/lambo.glb');

  // if the assets haven't loaded, don't render anything
  if (!lambo || !envAtlas) return null;

  return 
    
  </Entity>
}

const App = () => {
  return 
}

render()
```

## Your next steps

Now you've got a basic understanding of the core concepts of PlayCanvas React, you're ready to start building your own scenes. Check out the [loading assets guide](../guide/loading-assets) to learn how to load your own assets and add interactivity with scripts.

You can also check out the [guide](../guide/) to learn about the different concepts and techniques you can use to build your scenes.

--------------------------------------------------------------------------------

## Getting Started

URL: https://developer.playcanvas.com/user-manual/react/getting-started/

Welcome to PlayCanvas React! This section will guide you through setting up your first 3D React application and teach you the fundamental concepts you need to build interactive 3D experiences.

## What You'll Learn

In this getting started guide, you'll learn how to:

- [Install and set up](./installation) PlayCanvas React in your project
- [Create your first 3D scene](../building-a-scene) with cameras, entities, and 3D models
- [Load and manage 3D assets](../guide/loading-assets) efficiently in your application

## Prerequisites

Before you begin, make sure you have:

- **Node.js** (version 18 or higher) installed on your system
- Basic familiarity with **React** - if you're new to React, check out the [React documentation](https://react.dev/learn)
- A text editor or IDE (we recommend [Visual Studio Code](https://code.visualstudio.com/))

## Quick Start

The fastest way to get started is with our scaffolding tool:

```bash
npm create playcanvas@latest -t react-ts
```

This creates a new project with everything set up and ready to go. You can also try our [online playground](https://playcanvas-react.vercel.app/new) to experiment without installing anything locally.

## Next Steps

Ready to dive in? Start with the [installation guide](./installation) to set up your development environment, then follow the [building a scene](../building-a-scene) tutorial to build your first 3D application.

For more advanced topics and examples, explore the complete [PlayCanvas React documentation](https://playcanvas-react.vercel.app/docs) and [example gallery](https://playcanvas-react.vercel.app/examples).

--------------------------------------------------------------------------------

## Installation

URL: https://developer.playcanvas.com/user-manual/react/getting-started/installation/

The recommended way to get started is to use the official PlayCanvas scaffolding tool, and follow the prompts.

```bash
npm create playcanvas@latest -t react-ts
```

This creates a new project with everything set up and ready to go. If you've followed the prompts, you'll have a new PlayCanvas React project running in your browser.

We recommend following [this guide](../../building-a-scene) to start building your first project.

:::note

We’re working on adding more React templates. Currently, only **TypeScript** is available.

:::

## Manual Installation

If you have an existing project, just install `@playcanvas/react` using your preferred package manager.

Once installed, you can start using it in your project. As the next step, we recommend following the [building a scene guide](../../building-a-scene) to create your first project.

### Starter Templates

Alternatively you can grab one of the [templates directly](https://github.com/playcanvas/create-playcanvas/tree/main/templates) or quickly spin a project up from our [StackBlitz template](https://stackblitz.com/edit/playcanvas-react-template?file=src%2FScene.tsx).

--------------------------------------------------------------------------------

## Concepts

URL: https://developer.playcanvas.com/user-manual/react/guide/

PlayCanvas React is a thin wrapper around the PlayCanvas engine. It's a simple, declarative interface over the engine API which surfaces all the features of the engine plus a few extras to make it easier to use. Anything you can do with the engine, you can do with PlayCanvas React.

## Application

The `Application` is the root component of your PlayCanvas React application. It initializes the engine and provides a rendering context. If you have a single canvas in your app, it will map to an `Application` component.

The `` has a number of properties for configuring the scene. You can set the fill mode and resolution mode to control how the canvas fills the window, and other properties that control the graphics device.

```jsx

```

Learn more about the `Application` component in the [API documentation](https://playcanvas-react.vercel.app/docs/api/application).

## The Basics

The engine itself uses an **Entity Component System** - it's a common architecture used by many game engines that separates out behavior from data. ECS maps incredibly well to the declarative nature of React. It allows you to structure your scene in a way that is easy to understand and maintain.

Let's take a simple example. To make a camera in PlayCanvas React, you need a `Camera` component and an `Entity`. You add the Camera to the Entity. The Entity has position and rotation allowing you to move the camera in the scene, and the Camera component provides the necessary functionality to render things. Let's take a look at how to do this in both React and JavaScript.

## Components

You can attach multiple components to an Entity, each providing a different functionality. There are a wide range of components available, and any component you can use in the engine, you can use in PlayCanvas React.

- **Lights** (Directional, Point, Spot)
- **Cameras** (Perspective, Orthographic)
- **Render** (Sphere, Box, Cylinder, Cone, Plane, etc.)
- **RigidBody** (RigidBody, Collision)
- **Collision** (Collision)
- **Anim** (Animation, Skinning)
- **Gsplat** (Gsplat)
- **Screen** (Screen)
- **RigidBody** (RigidBody)
- **Anim** (Animation, Skinning)

Now that you've seen how a basic PlayCanvas React app is structured, let's look at some of the most commonly used components.

## Cameras

To view our scene, we need a camera. In PlayCanvas React, we use `Entity` components as containers and attach component behaviors like `Camera`:

```jsx

  return (
    
  )
}
```

We've added a camera entity positioned 5 units down the positive Z axis. By default, a camera looks down the negative Z axis so our camera is now looking at the origin. The rendered scene shows a solid grey color (the clear color of the camera).

## Lights

Let's add a directional light to illuminate our scene:

```jsx

  return (
    
  )
}
```

The light is rotated to shine at an angle, which will create more interesting shading on our objects.

## Primitives

Now let's add a sphere to our scene using the `Render` component:

```jsx

  return (
    
  )
}
```

You should now see a white sphere in the center of your screen!

## Next Steps

Now you have the basics, it's time to learn about [Interactivity](./interactivity), how to [manage assets](./loading-assets) and how to use [Physics](./physics) in your project. With those basics, you're ready to build your first project!

--------------------------------------------------------------------------------

## Interactivity

URL: https://developer.playcanvas.com/user-manual/react/guide/interactivity/

Once you have a scene, you'll want to make it interactive. You can do this in a number of ways depending on your use case.

### Pointer Events

You can attach pointer events to an `Entity` to respond to user interaction. It's a simple way to handle user input in a reactive way.

:::tip

Pointer events bubble up. You can attach events to a parent `Entity` and events will propagate up through the React tree.

:::

Let's make a sphere that changes color when you click it.

Entities support the following pointer events which closely match the behavior of the DOM Pointer Events API.

- `onPointerUp`
- `onPointerDown`
- `onPointerOver`
- `onPointerOut`
- `onClick`

### The `Script` component

The `Script` component is a handy way to add custom behavior to an `Entity`. It's a convenient way to attach any ESM PlayCanvas [Script](https://api.playcanvas.com/engine/classes/Script.html) to an `Entity`. This is useful for interactivity or animations or anything that needs to happen outside the React render loop.

You assign an ESM PlayCanvas [Script](https://api.playcanvas.com/engine/classes/Script.html) to the `script` prop of a `<Script>` component.

PlayCanvas Scripts are ESM modules that extend the `Script` class. You can use any existing Script or create your own, or any from the engine repository for [existing Scripts](https://github.com/playcanvas/engine/tree/main/scripts/esm).

:::tip

If you need to animate something every frame, using a `<Script>` component is generally more performant than re-rendering the React tree, however a Script can override Entity props.

:::

You can read more about [Scripts in the docs](/user-manual/scripting/).

### Application events

A more React-like way to handle interactivity is to use the `useAppEvent` hook. This hook allows you to subscribe to PlayCanvas application events with proper TypeScript support. It supports both built-in events and custom events with their specific callback signatures.

The following example hooks into the `update` event using the `useAppEvent` and rotates the capsule.

There are various application events that you can hook into, each with a specific callback signature. It's worth checking the [Engine Application API](https://api.playcanvas.com/engine/classes/Application.html) to see what events are available.

You can also fire your own events using the `app.fire` method.

```jsx

const MyComponent = () => {
  useAppEvent('myEvent', (message) => {
    console.log('My event fired.', message);
  });
}

app.fire('myEvent', 'Hello, world!');
```

## Next steps

Now that you've learned about interactivity, you can learn about [Physics](../physics) and [Materials](../materials).

--------------------------------------------------------------------------------

## Loading Assets

URL: https://developer.playcanvas.com/user-manual/react/guide/loading-assets/

Whether its textures, materials or 3D models, assets are a key part of any 3D app. And generally speaking, they take the most time to load. PlayCanvas React provides a set of specialized hooks for loading different types of assets, as well as a utility function for loading assets. This helps you get up to speed quickly but with granular control of how and when assets are loaded.

## Basic Usage

The simplest way to load an asset is to use one of the specialized hooks. There are special hooks for [models](../../api/hooks/use-asset#usemodel), [gaussian splats](../../api/hooks/use-asset#usesplat), [textures](../../api/hooks/use-asset#usetexture), [environment map](../../api/hooks/use-asset#useenvatlas), [fonts](../../api/hooks/use-asset#usefont) and more. They all have a similar shape and return a PlayCanvas [`Asset`](https://api.playcanvas.com/engine/classes/Asset.html).

For GLB models, use the [`<Gltf>`](../modifying-glb-models) component:

```tsx

const { asset } = useModel('model.glb');
return ;
```

The `<Gltf>` component is the recommended way to load and render GLB models. It gives you access to the model's internal structure, making it easy to modify components, add animations, or attach physics. For simple cases without modifications, you can also use [``](../../api/render), but `<Gltf>` provides more flexibility.

Learn more about [modifying GLB models](../modifying-glb-models) to add animations, remove lights, or attach physics components.

## Preloading

The asset hooks also return additional loading info and error states, so you can fallback to a preloader while loading or display an error message if the asset fails to load.

```tsx title="model-viewer.tsx"

  const { asset, loading, error } = useModel('model.glb');

  // If the asset is still loading, show a loading spinner
  if (loading) return ;

  // If there is an error, show an error message
  if (error) return ;

  // If the asset is loaded, render it
  return ;
}
```

## Loading with Props

Some assets accept additional properties to customize how they are loaded. You can pass these properties to the hook as a second argument.

```tsx
// Load a texture with specific settings
const { asset } = useTexture('texture.jpg', {
    mipmaps: true,
    anisotropy: 16,
    type: 'rgba'
});
```

## Asset hooks

There are different hooks for loading different types of assets. You can create more advanced hooks by wrapping the `useAsset` hook.

- [`useModel`](../../api/hooks/use-asset#usemodel) for loading 3D GLTF/GLB models
- [`useTexture`](../../api/hooks/use-asset#usetexture) for loading textures
- [`useSplat`](../../api/hooks/use-asset#usesplat) for loading Gaussian Splats
- [`useEnvAtlas`](../../api/hooks/use-asset#useenvatlas) for loading environment atlases
- [`useAsset`](../../api/hooks/use-asset#useasset) for loading any type of asset
- [`useFont`](../../api/hooks/use-asset#usefont) for loading fonts

### Asset Caching

Assets are cached by default to avoid reloading the same file multiple times. This means you're not duplicating on memory, but you'll need to ensure assets are correctly unloaded when they're no longer needed.

```tsx title="unloading-model-viewer.tsx"

  const { asset, loading, error } = useModel('model.glb');

  useEffect(() => {
    return () => asset?.unload();
  }, [asset]);

  if (!asset) return null;

  return ;
}
```

:::warning
**Unloading an asset will remove it globally.** This will affect other components that are using the same asset.
:::

### Custom Loading States

You can use placeholders or custom loaders while assets load by checking the loading state of an asset. This gives you granular control.

```tsx title="loading-spinner.tsx"

// A component that displays a model with a custom loading state

  const { asset: plane, loading: planeLoading } = useModel('plane.glb');
  const { asset: car, loading: carLoading } = useModel('car.glb');
    
  if (planeLoading || carLoading) return ;

  return <>
    
    
  </>
}
```

### Progressive Loading

The loading hooks also provide a simple mechanic to progressively load assets, so you can prioritize rendering quickly following up with high quality content later.

```tsx title="progressive-loading.tsx"

// A component that displays a model with a custom loading state

  const { asset: low } = useModel('./low-quality-model.sog'); // load the low quality asset
  const { asset: high } = useModel(low && './high-quality-model.sog'); // load the high quality asset, when the low quality is loaded
  
  if (!low && !high ) return null;

  return 
}
```

### Data Fetching Libraries

If you need more advanced caching or loading strategies, you can integrate with libraries like **[React Query](https://tanstack.com/query/latest)** or **[SWR](https://swr.vercel.app/)** or any other Promise based library using the `fetchAsset` utility.

```tsx title="model-with-query.tsx"

function useQueryModel(src: string) {
  const query = useQuery({
    queryKey: ['asset', src],
    // 'container' is the type of asset we're loading (e.g., model, texture, etc.)
    queryFn: () => fetchAsset({ app, url: src, type: 'container' })
  });

  return query;
}

  const { data: asset, isLoading } = useQueryModel('model.glb');

  if (isLoading) return ;
  return ;
}
```

See the [React Query documentation](https://tanstack.com/query/latest) and [SWR documentation](https://swr.vercel.app/) for more information on how to use it.

### Suspense Integration

React Query and SWR have built-in support for Suspense, which allows you to handle loading states in a more declarative way.

```tsx

function useSuspendedQueryModel(src: string) {
  const query = useQuery({
    queryKey: ['asset', src],
    queryFn: () => fetchAsset({ app, url: src, type: 'container' }),
    suspense: true
  });

  return query;
}

  const { data: asset } = useSuspendedQueryModel('model.glb');
  return ;
}
```

You can read more about Suspense in the [React documentation](https://react.dev/reference/react/Suspense), as well as the [React Query documentation](https://tanstack.com/query/v4/docs/framework/react/guides/suspense) and [SWR documentation](https://swr.vercel.app/docs/suspense).

--------------------------------------------------------------------------------

## Materials

URL: https://developer.playcanvas.com/user-manual/react/guide/materials/

Materials are a fundamental part of 3D graphics. They define the appearance and properties of 3D objects when lit.

You work with materials in React using the [`useMaterial`](../../api/hooks/use-material) hook which returns a [`StandardMaterial`](https://api.playcanvas.com/engine/classes/StandardMaterial.html) instance. You can then apply it to a [``](../../api/render) component.

```jsx copy filename="use-material.jsx"

  const material = useMaterial({ diffuse: 'red' });
  return (
    
  )
}
```

In the example below, we're using the `useMaterial` hook to create a material and applying it to a `Render` component. We also add a `onClick` event to the `Entity` to change the material properties when the cube is clicked.

### Basic Properties

The hook accepts an object of properties that closely match those of the [`StandardMaterial`](https://api.playcanvas.com/engine/classes/StandardMaterial.html) class. It will return a [`StandardMaterial`](https://api.playcanvas.com/engine/classes/StandardMaterial.html) instance which can be applied to a [``](../../api/render) component.

```jsx copy

function BasicMaterialExample() {
  const material = useMaterial({
    diffuse: 'blue',       // Base color
    opacity: 0.7,          // Transparency (0-1)
    metalness: 0.8,        // Metallic property (0-1)
    roughness: 0.2,        // Surface roughness (0-1)
    emissive: 'green',     // Emissive color
    emissiveIntensity: 0.5 // Emissive strength
  })
  
  return 
}
```

### Textures

You can also use textures with the material by loading them with the [`useTexture`](../../api/hooks/use-asset#usetexture) hook. Try switching between different texture sets and adjusting the material properties:

```jsx copy

function TexturedMaterialExample() {
  const { asset: diffuseMap } = useTexture('diffuse.jpg')
  const material = useMaterial({ diffuseMap })

  return 
}
```

## Related

- [useMaterial](../../api/hooks/use-material)
- [useTexture](../../api/hooks/use-asset#usetexture)
- [Light](../../api/light)
- [Render](../../api/render)

--------------------------------------------------------------------------------

## Modifying GLB Models

URL: https://developer.playcanvas.com/user-manual/react/guide/modifying-glb-models/

3D models are usually provided as standalone assets. They can feel like black boxes, which makes even small tweaks awkward—you have to reopen a DCC tool, make the change, export, and repeat whenever the model updates.

There are plenty of scenarios where you need to inspect or adjust a model in code. Maybe the source GLB includes lights you don't want. Maybe you’re building a car configurator and need to drive the headlights’ emissive intensity from React state. Or perhaps you want every mesh in the model to get physics components.

## The `
```

`<Modify.Node>` accepts a `path` prop that identifies which nodes to modify. In this example we’re targeting the `"Hand"` node beneath `"Arm"` beneath `"Body"`.

## Finding the Right Nodes

The path prop accepts a glob-like pattern for selecting nodes. This means you can select multiple components at once. For example if you want to select every `"Hand"` under the `"Body"` node in the model you can use a wildcard pattern:

```tsx
{/* Load the model */}

```

You can also select nodes by their component type. For example if you want to select all nodes with a `"Light"` component you can use the `"[light]"` pattern:

```tsx
{/* Load the model */}

```

To modify nodes in your GLB, you need to find them using path patterns. Here are the most common patterns:

- **Exact path**: `"Body.Arm.Hand"` - Selects a specific node path
- **Single wildcard**: `"Body.*"` - Selects all direct children of Body
- **Multi-level wildcard**: `"Body.**"` - Selects all descendants of Body at any depth
- **All nodes**: `"**"` - Selects every node in the hierarchy
- **Component filter**: `"**[light]"` - Selects all nodes with a light component
- **Combined**: `"Head.*[light]"` - Selects all direct children of Head that have a light component

You can also use a predicate function to match nodes:

```tsx
<Modify.Node path={(entity) => entity.name.includes('Weapon')}>
  
</Modify.Node>
```

## Adding Components & Entities

You can add new components or entities to specific nodes in your GLB. This is useful for attaching animations, adding physics, or inserting new elements into the hierarchy.

### Adding Physics

You can add collision and rigidbody components to specific meshes in your GLB:

```tsx

```

This adds physics components to the mesh found at the path `"Body.Mesh"`. The mesh will now participate in physics simulations.

### Adding Animations

If you need to attach an animation component to your model's skeleton, find the skeleton root node and add the `<Anim>` component:

```tsx

```

The path `"Root"` selects the root node of your GLB. You may need to adjust this based on your model's structure - common names are `"Root"`, `"Scene"`, or the name of your skeleton root entity.

### Adding a New Entity

You can also add entirely new entities as children of nodes in your GLB:

```tsx

```

## Modifying Existing Components

You can modify properties of existing components without removing them. This merges your changes with the existing component properties.

### Removing Components

Your GLB might include lights or other components you don't want. You can remove them easily by selecting the nodes and using the `remove` prop.

```tsx

```

The `path="**[light]"` pattern finds all nodes with a light component anywhere in the hierarchy. The `remove` prop removes the component from those nodes.

### Changing Light Properties

```tsx

```

This finds the node named `"Headlight"` and updates its light color and intensity, while keeping all other light properties unchanged.

### Updating Render Properties

```tsx

```

The `[render]` filter selects nodes that have a render component. This enables shadow casting and receiving for the body mesh.

### Functional Updates

You can also use functions to update properties based on their current values:

```tsx

```

This doubles the intensity of all lights in the GLB, regardless of their current values.

## Finding Entities with useEntity

Sometimes you need to find entities within the GLB hierarchy to work with them programmatically. The `useEntity` hook lets you search for entities relative to the current parent context. This is especially useful when you're adding components as children of `<Modify.Node>` and need to find other entities in the same subtree.

```tsx

function HandGlow() {
  // Find the 'Hand' entity relative to the current parent
  const handEntity = useEntity('Hand');
  
  if (!handEntity) return null;
  
  // Add a glow effect to the hand
  return (
    
  );
}

```

In this example, `useEntity('Hand')` searches for a child named `"Hand"` relative to the `"Arm"` node. The hook returns the entity if found, or `null` if not found.

### Using Path Patterns

`useEntity` supports the same path patterns as `<Modify.Node>`:

```tsx
function FindMultipleLights() {
  // Find all direct children with lights
  const lights = useEntity('*[light]');
  
  if (!lights || !Array.isArray(lights)) return null;
  
  return (
    <>
      {lights.map((light, index) => (
        
      ))}
    </>
  );
}
```

When using wildcards, `useEntity` returns an array of matching entities. When using an exact path, it returns a single entity.

### Using Predicate Functions

You can also use a function to find entities based on custom logic:

```tsx
function FindWeapons() {
  // Find all entities whose name includes "Weapon"
  const weapons = useEntity((entity, metadata) => 
    entity.name.includes('Weapon')
  );
  
  if (!weapons) return null;
  
  const weaponArray = Array.isArray(weapons) ? weapons : [weapons];
  
  return (
    <>
      {weaponArray.map((weapon, index) => (
        
      ))}
    </>
  );
}
```

The predicate function receives the entity and its metadata, allowing you to match based on any criteria - component presence, name patterns, or custom properties.

Note: `useEntity` only works inside components that are children of `
```

If multiple rules try to modify the same component on the same node, the most specific path wins. This lets you set general rules and override them with specific ones.

## Related

- [Loading Assets](../loading-assets/) - Learn how to load GLB assets
- [Anim Component](../../api/anim) - Animation component reference
- [Collision Component](../../api/collision) - Physics collision reference

--------------------------------------------------------------------------------

## Physics

URL: https://developer.playcanvas.com/user-manual/react/guide/physics/

PlayCanvas ships with a robust [physics system](/user-manual/physics/). This enables realistic simulations of objects in 3D space, including gravity, collisions, forces and constraints. The physics system is powered by the [ammo.js](https://github.com/kripken/ammo.js) physics engine.

## Getting started

PlayCanvas uses ammo.js as the physics engine. ammo.js is a WASM library and in React it's lazy loaded. This means you're only shipping ammo.js when you need it, helping you keep your bundle size down. It also means you can render something before ammo.js has loaded, or maybe you only need physics as a result of some user interaction. In any case, lazy loading ammo.js means you can get your content rendering fast.

To use the physics system, you'll need to install the `sync-ammo` dependency.

```bash
npm install sync-ammo
```

You'll also need to add the `` prop to your root `` component.

:::tip

PlayCanvas React will only load ammo.js when you have the `` prop set.

:::

```jsx

```

## Usage

You allow entities to participate in physics simulation by adding a `RigidBody` and `Collision` component to them. In the example below, we're creating a static ground, a falling sphere and a falling box.

### Physics Properties

You can customize physics behavior by changing the various properties of the [``](../../api/rigidbody) and [``](../../api/collision) components. Check out the [Physics documentation](/user-manual/physics/) for more information.

```jsx

```

```jsx

```

## The `usePhysics` Hook

Because the physics engine is lazily instantiated you'll want to handle this in a reactive way. This is where the `usePhysics` hook comes in.

```jsx

const PhysicsStatus = () => {
  const { isPhysicsEnabled, isPhysicsLoaded, physicsError } = usePhysics();
  // ...
};
```

The `usePhysics` hook provides information about the physics engine state. You can read more about the [`usePhysics`](../../api/hooks/use-physics) hook in the API reference.

For more detailed information about PlayCanvas physics, see the [PlayCanvas physics](/user-manual/physics/) docs.

--------------------------------------------------------------------------------

## XR

URL: https://developer.playcanvas.com/user-manual/react/guide/xr/

PlayCanvas React makes it easy to add Virtual Reality (VR) and Augmented Reality (AR) support to your applications using the WebXR API.

## Basic Setup

To enable XR support in your React application, you'll need:

1. **XR Scripts** - Import the XR controller and navigation scripts from the PlayCanvas engine package
2. **Camera Setup** - Configure your camera entity with XR scripts attached
3. **XR Controls** - Add UI buttons to enter and exit XR sessions (WebXR requires user interaction)
4. **Secure Context** - Serve your app over HTTPS (or `localhost` during development)

## XR Scripts

The PlayCanvas engine provides two essential XR scripts:

- **`XrControllers`** - Automatically downloads and renders XR controller models for detected controllers (including hand tracking)
- **`XrNavigation`** - Implements teleportation-based navigation using point-and-select actions

Import them from the PlayCanvas scripts package:

```tsx

```

## Camera Configuration

For XR to work properly, you need a camera entity with the XR scripts attached. The recommended structure is:

```tsx

  
  
</Entity>
```

The camera is positioned at approximately eye-level (1.6 meters) and the scripts are attached to the parent entity.

## Starting XR Sessions

WebXR requires a user gesture to start a session. Use the `app.xr` API to start and manage XR sessions:

```tsx
const app = useApp()

const startVR = () => {
  const camera = app.root.findComponent('camera')
  if (camera) {
    app.xr.start(camera, 'immersive-vr', 'local-floor')
  }
}

const startAR = () => {
  const camera = app.root.findComponent('camera')
  if (camera) {
    app.xr.start(camera, 'immersive-ar', 'local-floor')
  }
}
```

## Complete Example

Here's a complete example with XR support, including buttons to enter AR/VR mode and a simple scene with cubes you can navigate around in XR:

:::tip

- Press **Escape** to exit an active XR session
- XR availability depends on your device and browser support
- Use a VR headset or AR-capable mobile device to test the full experience
- During development, Chrome and Edge support WebXR emulation via DevTools

:::

## Checking XR Availability

You can check if AR or VR is available on the current device:

```tsx
const app = useApp()
const arAvailable = app.xr.isAvailable('immersive-ar')
const vrAvailable = app.xr.isAvailable('immersive-vr')
```

## XR Events

Listen to XR session events to update your UI:

```tsx
useEffect(() => {
  const onStart = () => console.log('XR session started')
  const onEnd = () => console.log('XR session ended')
  
  app.xr.on('start', onStart)
  app.xr.on('end', onEnd)
  
  return () => {
    app.xr.off('start', onStart)
    app.xr.off('end', onEnd)
  }
}, [app])
```

## Next Steps

The PlayCanvas Engine has comprehensive XR support with many advanced features. For more information, check out:

- [XR User Manual](/user-manual/xr) - Comprehensive XR documentation
- [WebXR API Reference](https://api.playcanvas.com/engine/classes/XrManager.html) - Full API documentation
- [XR Scripts Source](https://github.com/playcanvas/engine/tree/main/scripts/esm) - View the XR scripts source code

--------------------------------------------------------------------------------

## Scripting

URL: https://developer.playcanvas.com/user-manual/scripting/

Scripts are the heart of interactivity in PlayCanvas. They're reusable pieces of code that you attach to Entities to define behaviors, handle user input, manage game logic, and bring your projects to life.

:::tip Using the Editor?
If you're using the PlayCanvas Editor, check out the [Editor Scripting](/user-manual/editor/scripting/) section to learn about managing scripts, the code editor, VS Code integration, and hot reloading.
:::

## Two Scripting Systems

PlayCanvas supports two scripting approaches:

- **ESM Scripts** (`.mjs` files) — Modern ES Module-based scripts using class syntax. **Recommended for new projects.**
- **Classic Scripts** (`.js` files) — The original PlayCanvas scripting system using prototype-based syntax.

Both systems can coexist in the same project, allowing you to migrate gradually or use whichever approach fits your needs.

## Quick Example

Here's a simple script that rotates an entity:

## In This Section

- [Getting Started](./getting-started.md) — Basic script structure and syntax.
- [ESM Scripts](./esm-scripts.md) — Modern scripting with ES Modules.
- [Script Lifecycle](./script-lifecycle.md) — When and how script methods are called.
- [Application Lifecycle](./application-lifecycle.md) — Understanding app initialization and frame updates.
- [Script Attributes](./script-attributes/index.md) — Exposing configurable properties.
- [Engine API](./engine-api.md) — Key classes and patterns.
- [Events](./events.md) — Communication between scripts.
- [Debugging](./debugging/index.md) — Tools and techniques for troubleshooting.
- [Migration Guide](./migration-guide.md) — Upgrading from classic to ESM scripts.

--------------------------------------------------------------------------------

## Application Lifecycle

URL: https://developer.playcanvas.com/user-manual/scripting/application-lifecycle/

Below is a diagram of the high level flow of a PlayCanvas application that is created with the PlayCanvas Editor.

[Image: Application lifecycle]

--------------------------------------------------------------------------------

## Debugging

URL: https://developer.playcanvas.com/user-manual/scripting/debugging/

Debugging is essential for identifying and fixing issues in your PlayCanvas scripts. When your code doesn't behave as expected, these tools and techniques will help you quickly find and resolve problems.

## Debugging Techniques

### Console Logging

The quickest way to understand what your code is doing. Add `console.log()` statements to track execution flow and inspect variable values.

**[Learn Console Logging →](./console-logging.md)**

### Browser Developer Tools

Use breakpoints, step-through debugging, and performance profiling to deeply inspect your running code.

**[Learn Browser Dev Tools →](./browser-dev-tools.md)**

--------------------------------------------------------------------------------

## Browser Dev Tools

URL: https://developer.playcanvas.com/user-manual/scripting/debugging/browser-dev-tools/

In order to create scripts for PlayCanvas, it is vital that you know how to access and use your browser's development tools.

Chrome, Firefox and other browsers have Developer Tools integrated directly into the browser. You can usually access them by pressing F12 on Windows or ALT-CMD-I on Mac. Or use menu of a browser to access developer tools.

### Debugging Scripts

In order to debug your scripts, select the Sources tab in Developer Tools (Chrome). Open the 'navigator' by clicking the icon in the top left corner of the Sources pane. You should see something similar to what is shown below:

[Image: Debugger]

In Firefox it looks like this:

[Image: Firefox]

The navigator lists all of the scripts currently running in the active tab, including any PlayCanvas scripts that you have written. Find one of your scripts in the navigator and select it to open the source code. You are now able to set breakpoints and debug.

Each browser has detailed instructions on how to debug javascript. You should read through these documents: [Chrome](https://developer.chrome.com/docs/devtools/javascript/), [Firefox](https://firefox-source-docs.mozilla.org/devtools-user/debugger/index.html), [Safari](https://developer.apple.com/safari/tools/), [Edge / Internet Explorer](https://docs.microsoft.com/en-us/microsoft-edge/devtools-guide-chromium/javascript/reference).

:::note

Note that when a running app is paused at a breakpoint in the debugger, other browser windows/tabs used to launch that app (containing the PlayCanvas Code Editor or Editor etc.) might also be paused.

:::

### Debugging on Mobile Devices

On Android, it is possible to connect to the mobile Chrome browser via the desktop Chrome browser devtools and USB cable. [Google Developer documentation](https://developer.chrome.com/docs/devtools/remote-debugging/) has the detailed steps for the setup.

iOS debugging requires access to a Mac and [Apple has outlined the steps](https://webkit.org/web-inspector/enabling-web-inspector/) needed to enable developer options to start debugging.

In the situation where either it's not possible to connect to the web view directly from Chrome or Safari (e.g. a web view in another app) or don't have access to a Mac, the following libraries and services can help and are simple to add to projects:

* [RemoteJS](https://remotejs.com/) - Allows developers to view the console output remotely in a desktop browser and also execute JS in the console which makes it very powerful. The console output can be delayed or slow though.
* [vConsole](https://github.com/Tencent/vConsole) or [Eruda](https://github.com/liriliri/eruda) - Adds a widget DOM object to the page that can be expanded to show console output, network requests, the page elements and more.

--------------------------------------------------------------------------------

## Console Logging

URL: https://developer.playcanvas.com/user-manual/scripting/debugging/console-logging/

One of the most fundamental and indispensable tools for debugging your PlayCanvas scripts is the Browser Developer Console. Every modern web browser includes a suite of developer tools, and the console is your window into logging information, viewing errors, and understanding what your code is doing behind the scenes.

## What is the Console?

The console is a command-line interface within your browser's developer tools that:

* **Displays Log Messages:** You can print messages, variable values, and object states directly from your JavaScript code.
* **Shows Errors and Warnings:** JavaScript errors, engine warnings, and explicit error messages you log will appear here, often with stack traces to help pinpoint the source.
* **Allows Interaction:** You can execute JavaScript commands directly in the console to inspect the state of your running application (more advanced).

## Opening the Console

How you open the developer console depends on your browser, but here are common methods:

* **Keyboard Shortcut:** Press F12 on Windows/Linux or Option + Command + J (⌥ + ⌘ + J) on macOS (Chrome/Edge/Firefox). Safari uses Option + Command + C (⌥ + ⌘ + C), but you might need to enable the Develop menu first (Preferences > Advanced > Show Develop menu in menu bar).
* **Right-Click Menu:** Right-click anywhere on your running PlayCanvas application page and select "Inspect" or "Inspect Element". Then, navigate to the "Console" tab in the tools that appear.
* **Browser Menu:** Look for "Developer Tools", "Web Developer", or similar options in your browser's main menu (e.g., Chrome: More Tools > Developer Tools).

## Logging Messages from Your Scripts

The primary way to send information to the console is using the global console object available in JavaScript. You'll typically use these methods within your PlayCanvas script functions (`initialize`, `update`, event handlers, etc.).

### `console.log()`

This is the most common method, used for general informational output.

* **Purpose:** Log status messages, variable values, check if a function is being called.

**Example:**

### `console.warn()`

Used to indicate potential problems or situations that aren't errors but might be unexpected.

* **Purpose:** Highlight non-critical issues, deprecated usage warnings, or suspicious values.
* **Appearance:** Messages typically appear with a yellow background or icon in the console.

**Example:**

```javascript
// Inside some function
if (speed < 0) {
    console.warn('Warning: Speed is negative (' + speed + '). Is this intentional?');
}
```

### `console.error()`

Used for logging actual errors that prevent code from working correctly. PlayCanvas engine errors also use this.

* **Purpose:** Report errors found in your logic, failed operations, or critical failures.
* **Appearance:** Messages typically appear with a red background or icon and often include a stack trace (the sequence of function calls leading to the error).

**Example:**

### Other Methods

* `console.info()`: Similar to `console.log()`, sometimes styled differently (e.g., with an 'i' icon).
* `console.debug()`: Often hidden by default in console settings, useful for verbose debugging messages you don't always want to see.

### Logging Different Data Types

You can log more than just strings:

* **Variables:** `console.log('Player health:', this.health);`
* **Objects and Arrays:** `console.log('My Entity:', this.entity);`, `console.log('Children:', this.entity.children);`
  * Most browser consoles allow you to interactively inspect logged objects and arrays, expanding them to see their properties and values. This is incredibly useful for examining complex data like Entities, Components, or Materials.

## Where Does the Output Go?

Whether you launch your application using the Launch button in the PlayCanvas Editor or run a published build, the console output always appears in your browser's developer console, not within the Editor interface itself. Keep the developer tools open while testing!

## Tips for Effective Logging

* **Be Specific:** Instead of `console.log('here');`, log what is happening or the value you're interested in: `console.log('Player jumped!', this.entity.getPosition());`.
* **Provide Context:** Especially with multiple scripts or entities, include identifying information: `console.log(this.entity.name + ': Firing weapon.');`
* **Log Key Values:** Output variables, function arguments, and return values at critical points to trace data flow.
* **Use Different Levels:** Use `log`, `warn`, and `error` appropriately to make important messages stand out.
* **Conditional Logging:** Use `if` statements to log only when specific conditions occur, reducing console spam.
* **Clean Up Your Logs:** Remove temporary or excessive `console.log` statements before considering your code finished, or use `console.debug` for logs you might want later but not normally see.

## Conclusion

The browser console is your first line of defense when debugging. Mastering `console.log` and its variations is a fundamental skill that will save you countless hours trying to figure out why your scripts aren't behaving as expected. Get comfortable opening it, logging information, and interpreting the output!

--------------------------------------------------------------------------------

## Calling the Engine API

URL: https://developer.playcanvas.com/user-manual/scripting/engine-api/

When writing PlayCanvas scripts, you're working with the [PlayCanvas Engine API](https://api.playcanvas.com/engine/). This page covers the essential classes and patterns you'll use most often in your scripts.

## Key Classes for Script Writers

### Your Script Context

Every script has access to these core objects:

```javascript
this.app        // The main application (AppBase)
this.entity     // The entity this script is attached to
```

:::important

`this.app` and `this.entity` are only valid within methods defined on your Script instance (`initialize`, `update`, etc.). [Learn more](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/this) about JavaScript's `this` keyword.

:::

### Essential Classes

**[`AppBase`](https://api.playcanvas.com/engine/classes/AppBase.html)** - Your application

```javascript
// Common app operations
this.app.fire('game:start');
const player = this.app.root.findByName('Player');
const texture = this.app.assets.find('logo', 'texture');
```

**[`Entity`](https://api.playcanvas.com/engine/classes/Entity.html)** - Objects in your scene

```javascript
// Common entity operations
this.entity.setPosition(0, 5, 0);
this.entity.rotate(0, 90, 0);
const child = this.entity.findByName('Weapon');
```

**[`Component`](https://api.playcanvas.com/engine/classes/Component.html)** - Add functionality to entities

```javascript
// Accessing components
const camera = this.entity.camera;
const light = this.entity.light;
const rigidbody = this.entity.rigidbody;
const sound = this.entity.sound;
```

### Math Classes

Import these for calculations and transformations:

```javascript

const position = new Vec3(0, 5, 0);
const rotation = new Quat();
const red = new Color(1, 0, 0);
```

## Common Script Patterns

### Finding Entities

```javascript
// By name (searches entire hierarchy)
const player = this.app.root.findByName('Player');

// By tag (returns array)
const enemies = this.app.root.findByTag('enemy');

// Relative to current entity
const weapon = this.entity.findByPath('Arms/RightHand/Weapon');
```

### Working with Assets

```javascript
// Find and load assets
const sound = this.app.assets.find('explosion', 'audio');
sound.ready(() => {
    this.entity.sound.play('explosion');
});
this.app.assets.load(sound);
```

### Events and Communication

```javascript
// Fire application events
this.app.fire('player:died', this.entity);

// Listen for events
this.app.on('game:start', this.onGameStart, this);
```

## Learning More

* **[Full Engine API Reference](https://api.playcanvas.com/engine/)** - Complete documentation
* **[Engine Guide](../engine/index.md)** - In-depth guide to the PlayCanvas Engine runtime
* **[Script Lifecycle](./script-lifecycle.md)** - When your script methods are called
* **[Events](./events.md)** - Script communication patterns

:::tip

**IDE Support:** Use the [VS Code Extension](/user-manual/editor/scripting/vscode-extension.md) for autocomplete and inline documentation while writing scripts.

:::

--------------------------------------------------------------------------------

## ESM Scripts

URL: https://developer.playcanvas.com/user-manual/scripting/esm-scripts/

ESM Scripts use modern ES Module syntax and provide the recommended way to write PlayCanvas scripts. They offer better code organization, static imports, improved bundling, and a more familiar development experience for modern JavaScript developers.

## Why Choose ESM Scripts?

* **Modern JavaScript:** Use ES6+ features like classes, arrow functions, and destructuring
* **Better Tooling:** Enhanced IDE support with better autocomplete and error detection
* **Modular Code:** Import and export functionality between scripts
* **Improved Performance:** Static imports enable better bundling and dead code elimination
* **Future-Proof:** Built on web standards that continue to evolve

## Creating ESM Scripts

ESM scripts must have the `.mjs` file extension:

1. In the Asset Panel, create a new script
2. Name it with `.mjs` extension (e.g., `PlayerController.mjs`)
3. The editor will provide ESM boilerplate code

```javascript

    static scriptName = 'playerController';

    initialize() {
        // Setup code here
    }

    update(dt) {
        // Frame update code here
    }
}
```

:::tip

**Multiple Scripts Per File:** A single `.mjs` file can export multiple script classes, but each must be exported to be available in the editor.

:::

## Module Imports and Exports

Share code between scripts using standard ES Module syntax:

```javascript
// config.mjs - Shared configuration

    playerSpeed: 5,
    jumpHeight: 10,
    gravity: -9.8
};

    return Math.min(Math.max(value, min), max);
}
```

```javascript
// PlayerController.mjs - Using shared code

    static scriptName = 'playerController';

    update(dt) {
        const speed = GAME_SETTINGS.playerSpeed;
        // Use clamp function...
    }
}

--------------------------------------------------------------------------------

## Events

URL: https://developer.playcanvas.com/user-manual/scripting/events/

Events are a useful way of communicating between scripts in order to respond to things that happen without checking every frame.

Many PlayCanvas object types (such as script instances) have event handling support built-in, inherited from the Engine's [`EventHandler`](https://api.playcanvas.com/engine/classes/EventHandler.html) class. Event handling objects have the following methods:

* `on()` - registers an event listener.
* `once()` - registers an event listener that unregisters itself after the first time it is called.
* `off()` - unregisters an event listener.
* `fire()` - sends an event.
* `hasEvent()` - queries whether an object is listening on a particular event.

## Using Events

Trigger an event using `fire()`. In this example, the player script fires a `move` event every frame with the `x` and `y` values passed as arguments.

Listen for events by using `on()` and `off()`. In this example, the display script listens for the `move` event on the player and prints out the x and y values.

## Application Events

There is a very convenient and powerful method of using events to communicate between entities that we call "Application Events". As you can see in the example above, listening for events on specific entities incurs some setup cost. For instance, the listener must have a reference to the specific entity that is firing the event. This works in some cases, but for a more general case we find that it is more appropriate to use the main application (`this.app`) as a central hub for firing events. This means you don't have to keep references to entities around in order to use the events.

This works by firing and listening to all events on `this.app`. By convention, we use namespaces in event names in order to signal event scope and prevent clashes. For example, the `player:move` event is fired on the application instead of firing the `move` event on the player.

Let's try the same example using application events.

Firing the `player:move` event:

As you can see, this reduces the amount of set up and makes for cleaner code.

--------------------------------------------------------------------------------

## Getting Started

URL: https://developer.playcanvas.com/user-manual/scripting/getting-started/

This guide covers the basic structure and concepts you need to understand when writing PlayCanvas scripts, whether you're using the modern ESM approach or the classic system.

## What is a Script?

A script is a piece of JavaScript code that defines behavior for an Entity in your scene. Scripts are:

* **Reusable** - The same script can be attached to multiple entities
* **Configurable** - Use attributes to customize behavior per entity
* **Event-driven** - Respond to lifecycle events and user interactions

## Basic Script Structure

Every PlayCanvas script follows a similar pattern, regardless of which system you use:

## Core Concepts

### Script Lifecycle

Scripts have several methods that are called automatically at different times:

* `initialize()` - Called once when the script starts
* `update(dt)` - Called every frame with delta time
* `postUpdate(dt)` - Called after all updates complete
* Event handlers for `enable`, `disable`, `destroy`

Learn more about the [Script Lifecycle](./script-lifecycle.md).

### Attributes

Attributes let you expose script properties to the editor, making scripts configurable without code changes:

Learn more about [Script Attributes](./script-attributes/index.md).

### Accessing the Entity

Every script has access to the entity it's attached to via `this.entity`:

```javascript
// Get the entity's position
const position = this.entity.getPosition();

// Find child entities
const child = this.entity.findByName('ChildName');

// Access components
const camera = this.entity.camera;
const rigidbody = this.entity.rigidbody;
```

## Next Steps

* **Learn ESM Scripts:** If you're starting fresh, check out [ESM Scripts](./esm-scripts.md) for the modern approach
* **Understand Lifecycle:** Read about [Script Lifecycle](./script-lifecycle.md) to understand when your code runs
* **Add Interactivity:** Explore [Events](./events.md) to make scripts communicate with each other

:::tip

**Which system should I use?**

For new projects, we recommend **ESM Scripts** as they offer better tooling, cleaner syntax, and modern JavaScript features. Classic scripts are still fully supported for existing projects.

:::

--------------------------------------------------------------------------------

## Migration Guide

URL: https://developer.playcanvas.com/user-manual/scripting/migration-guide/

ESM Scripts replace the older Classic Scripting system as the recommended way to develop PlayCanvas applications. Whilst classic scripts will continue to work in existing projects and will be supported for the foreseeable future, we recommend using the newer ESM format for your projects.

## Gradual Migration

Using ESM Scripts within your project is entirely optional and allows you to gradually migrate your projects over to the newer ESM based format in your own time, without affecting existing projects.

:::tip

**Projects can contain both ESM Scripts and Classic Scripts**

You do not need to update all your scripts together. We recommend gradually migrating scripts and iteratively testing

:::

## Codemod

In order to migrate Classic Scripts to the ESM format, we've provided a [codemod](https://codemod.com/registry/playcanvas-esm-scripts) that will automatically update your code.

You can find the codemods in our [github repository](https://github.com/playcanvas/codemods) and you can run the codemod using the following command:

```bash
npx codemod playcanvas-esm-scripts
```

## Known differences

In general, ESM Scripts provide a more expressive and flexible way of creating projects. Whilst we have attempted to keep the migration process as seamless as possible, there are some notable differences that you should bear in mind.

### Module Scope

**ESM Scripts have module scope, Classic Scripts have global scope**. This means modules cannot implicitly access variables defined in other files. Often this is used as a way to define global settings or configuration. The config has a higher loading order than the script, and so the `SPEED` is accessible globally.

<details>
<summary>**See code example**</summary>

```javascript
// config.js
var SPEED = 10;

// script.js
// ❌ This will not work. `SPEED` is scoped to config.js
console.log(SPEED)
```

This is a *hidden dependency* which breaks if the loading order changes. Instead, use `import/export` syntax to explicitly define the dependency.

```javascript
// config.mjs

// script.mjs

// ✅ Works!
console.log(SPEED); 
```

</details>

You can learn more about the other difference between [ES Modules and standard scripts here](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules#other_differences_between_modules_and_standard_scripts)

### Loading Order

:::note

**ESM Scripts do not have a loading order.**

:::

The loading order of scripts was introduced as a way to organize dependencies between scripts and guarantee certain code would be executed before others. With ES modules, these relationships can be explicitly defined through `import/export` syntax. As such, ESM Scripts do not have a loading order and they should not be relied upon to load in a certain way. Instead we encourage you to use `import/export` statements to set up dependencies.

### The New `Script` Class

With ESM Scripts, the new `Script` base class replaces the existing `ScriptType` class as the default base class. The `Script` class represents the minimal set of features necessary, but omits a couple of features present in the original `ScriptType` class.

It's worth noting that although `Script` is now the default base class, it's still possible to use `ScriptType` as the base class (internally `ScriptType` extends `Script`), however we do not recommend doing this for ESM Scripts, due to some of the reasons listed below.

#### Attribute Events

:::note

ESM Scripts do not fire Attribute Events.

:::

The `Script` class does not support attributes events in the format of `attr:[name]`. The reason behind removing this is that, internally, the engine would override class members, and in practice this would create difficult-to-debug scenarios as it's not fully compatible with [ES6 class syntax](https://github.com/playcanvas/engine/issues/6316).

Instead, you can define your own events around class attribute members using something like the following;

<details>
<summary>**See code example**</summary>

```javascript
const watch = (target, prop) => {
    const privateProp = `#{prop}`;
    target[privateProp] = target[prop];

    Object.defineProperty(target, prop, {
        set(value) {
            if (target[privateProp] !== value) {
                target.fire(`changed:${prop}`, value);
                target[privateProp] = value;
            }
        },
        get() {
            return this[privateProp];
        }
    });
}

    static scriptName = 'rotate';

    /** attribute */
    speed = 10;

    initialize() {
        watch(this, 'speed');

        this.on('changed:speed', console.log)
    }
}

```

</details>

This also means you can have events for any class members too, not only script attributes.

#### Attribute Copying

:::note

**ESM Script Attributes are not copied, they are passed by reference.**

:::

**Attributes are no longer copied, they are passed by reference.** The reasons this was changed was also due to a [bug in `ScriptType`](https://github.com/playcanvas/engine/issues/6316) that was incompatible with ES6 classes.

Instead, if you do need to copy values, we recommend you do it manually and explicitly via getters and setters. Whilst this is more verbose, it's clear and explicit.

<details>
<summary>**See code example**</summary>

```javascript

    static scriptName = 'rotate';

    _speed = new Vec3();

    set speed(value) {
        this._speed.copy(value)
    }

    get speed() {
        return this._speed;
    }
}
```

</details>

--------------------------------------------------------------------------------

## Script Attributes

URL: https://developer.playcanvas.com/user-manual/scripting/script-attributes/

Script Attributes are a powerful feature in PlayCanvas that define the public, configurable interface of your scripts. They allow you to expose specific parameters that can be easily tweaked, either programmatically when instantiating or configuring scripts in code, or visually within the [PlayCanvas Editor](../../editor/index.md). This means you can write a script once, and then easily adjust its behavior and properties for different instances or by different team members.

## Why Use Script Attributes?

* **Clear Public Interface:** Attributes formally define which parts of a script are intended to be customized, improving code clarity and maintainability.
* **Editor Integration (Optional):** When using the PlayCanvas Editor, attributes appear as editable fields in the [Inspector panel](../../editor/interface/inspector.md). This provides a user-friendly interface for artists, designers, or other developers to configure scripts without needing to delve into code.
* **Programmatic Configuration:** When creating or managing Entities and Script Components via code, you can directly set the initial values for these attributes, allowing for dynamic and flexible setups.
* **Reusability:** Create generic scripts (e.g., a "Movement" script) and customize their properties (like speed, direction, target) for various Entities, whether through the Editor or in your code.
* **Collaboration:** Enable team members, including those not primarily focused on coding, to modify gameplay elements, character stats, and visual properties.
* **Rapid Iteration:** Quickly test different configurations and values, either by adjusting them in the Editor or by modifying initialization parameters in your code.

## How They Work

When you declare an attribute in your script, you are essentially defining a property that can be initialized and modified.

* **In Code:** You can set the values of these attributes when you add a script to a Script Component or at runtime via script instance properties.
* **In the Editor:** The PlayCanvas Editor parses your script file and creates corresponding UI controls (like number fields, checkboxes, color pickers, asset pickers, etc.) in the Inspector. These controls allow you to set the values for the attributes on each specific instance of your script.

For example, you could expose a `speed` attribute in a rotation script. In the Editor, this would appear as a number field. Programmatically, you could set `this.speed = 5;` in an `initialize` method or `entity.script.myScript.speed = 5;` when setting up an entity. This allows you to set different rotation speeds for different spinning objects, all using the same underlying script logic, configured either visually or through code.

## Two Systems: ESM and Classic

PlayCanvas has two systems for defining script attributes, corresponding to the two types of scripting methodologies:

1. **[ESM Script Attributes](./esm.md):** Used with modern ES Module (`.mjs`) scripts. Attributes are typically declared using JSDoc comments above class member variables. This is the recommended approach for new projects.
2. **[Classic Script Attributes](./classic.md):** Used with the older "Classic" script (`.js`) files. Attributes are declared using a specific `MyScript.attributes.add(...)` API.

While the underlying goal is the same—to define a configurable interface—the syntax and some capabilities differ between the two. Click the links above to learn the specifics for each system.

Understanding and utilizing Script Attributes is key to building flexible, maintainable, and collaboratively-friendly projects in PlayCanvas, whether you are leveraging the visual tools of the Editor or constructing your scenes entirely through code.

--------------------------------------------------------------------------------

## Classic Reference

URL: https://developer.playcanvas.com/user-manual/scripting/script-attributes/classic/

:::note

This page documents Script Attributes for the deprecated **Classic Scripts** system.

For **ESM Script Attributes**, click [here](./esm.md).

:::

Script Attributes are a powerful feature that lets you expose values from your script files so that they appear in the PlayCanvas Editor. This means you can write code once, and then tweak values on different instances of an Entity to give them different properties. This is perfect for exposing properties for artists, designers or other non-programmer team members so that they are able to adjust and modify values without writing code.

## Declaring Script Attributes

Script Attributes are declared at the top of your script file using this format:

```javascript
var MyScript = pc.createScript('myScript');

MyScript.attributes.add('speed', {
    type: 'number',
    default: 80
});
```

In this example, we're declaring a property called `speed` which is a `number` and has a default value of `80`.

If you need an array of attributes, set `array: true` like so:

```javascript
var MyScript = pc.createScript('myScript');

MyScript.attributes.add('names', {
    type: 'string',
    array: true
});
```

## Getting Attributes into the Editor

[Image: Script Attributes]

Once you've declared your attributes, the Editor needs to parse the code in order to expose the script attributes. If attributes have been changed, you need to manually refresh them by clicking the parse button.

[Image: Parse Button]

## Accessing Attributes in Your Code

When you declare an attribute in your script it will be available as a member variable on your script instance. For example, the `speed` property declared above is available as `this.speed`.

```javascript
MyScript.prototype.update = function (dt) {
    this.entity.translate(this.speed * dt, 0, 0);
}
```

## Responding to Attribute Changes

When you modify an attribute in the Editor, the changes are sent to any copies of the application launched from the Editor. This means you can live edit your attributes without reloading your application. If you need to apply special behavior when an attribute changes, use the `attr` and `attr:[name]` events to respond to changes:

```javascript
MyScript.prototype.initialize = function () {
    // fires only for `speed` attribute
    this.on('attr:speed', function (value, prev) {
        // new value for speed
    });

    // fires for all attribute changes
    this.on('attr', function(name, value, prev) {
        // new attribute value
    });
}
```

## Attribute Types

When you declare an attribute you also declare the type of the attribute. This allows the Editor to show the relevant controls for you to edit the attribute. Most types are self-explanatory, for example, 'boolean', 'number' or 'string'. But some require some further explanation in the below examples. See the [full attribute reference](https://api.playcanvas.com/classes/Engine.ScriptAttributes.html) for more details.

### Entity Attribute

```javascript
MyScript.attributes.add('target', { type: 'entity' })
```

The Entity type lets you reference another entity in your hierarchy. A great way to link two entities together.

### Asset Attribute

```javascript
MyScript.attributes.add('textures', { type: 'asset', assetType: 'texture', array: true });
```

The Asset attribute lets you reference a project asset in your script. The asset attribute also supports the `assetType` property which limits the attribute to assets of a particular type, e.g. 'texture', 'material', 'model'.

The runtime type of an Asset attribute is `pc.Asset`. You can reference the resource of an Asset attribute at runtime like so:

```javascript
MyScript.attributes.add('texture', {type: 'asset', assetType: 'texture'});

MyScript.prototype.initialize = function () {
    console.log('This is the texture asset', this.texture);
    console.log('This is the texture resource', this.texture.resource);
};
```

### Color Attribute

```javascript
MyScript.attributes.add('color', { type: 'rgba' });
```

The color attribute shows a color picker when exposed in the Editor. There are two options `rgb` and `rgba` depending on whether you wish to expose the alpha channel as well.

### Curve Attribute

```javascript
MyScript.attributes.add('wave', { type: 'curve' }); // one curve
MyScript.attributes.add('wave', { type: 'curve', curves: [ 'x', 'y', 'z' ] }); // three curves: x, y, z
MyScript.attributes.add('wave', { type: 'curve', color: 'r' }); // one curve for red channel
MyScript.attributes.add('wave', { type: 'curve', color: 'rgba' }); // four curves for full color including alpha
```

The curve attribute is used to express a value that changes over a time period. All curves are defined over the period 0.0 - 1.0. You can define multiple curves. For example, if you wish to have a 3D position from a curve, define three curves for x, y, z using the `curves` property. There is also a special curve editor for modifying colors using the `color` property.

### Enumeration Attribute

The Enumeration attribute allows you to choose one of the available options:

```javascript
MyScript.attributes.add('value', {
    type: 'number',
    enum: [
        { 'valueOne': 1 },
        { 'valueTwo': 2 },
        { 'valueThree': 3 }
    ]
});
```

Use the enum property to declare the list of possible values for your enumeration. Property is an array of objects where each object is an option where `key` is a title of an option and `value` is a value for attribute. This property can be used for various attribute types, e.g. `number`, `string`.

### JSON Attribute

The JSON attribute allows you to create nested attributes of the other attribute types. For every JSON attribute you must specify a schema to describe its properties. The schema contains other regular script attribute definitions like above. For example:

```javascript
MyScript.attributes.add('gameConfig', {
    type: 'json',
    schema: [{
        name: 'numEnemies',
        type: 'number',
        default: 10
    }, {
        name: 'enemyModels',
        type: 'asset',
        assetType: 'model',
        array: true
    }, {
        name: 'godMode',
        type: 'boolean',
        default: false
    }]
});
```

You can also declare arrays of JSON attributes so that you can create arrays of editable objects. Just add `array: true` when defining the JSON attribute like you do for other attribute types.

Here's an example of accessing the above attributes in a script:

```javascript
MyScript.prototype.update = function (dt) {
    if (this.gameConfig.godMode) {
        for (var i = 0; i < this.gameConfig.numEnemies; i++) {
            // ...
        }
    }
};
```

:::note

We currently do not support defining JSON attributes as children of other JSON attributes. You can only go 1 level deep when defining a JSON attribute.

:::

--------------------------------------------------------------------------------

## ESM Reference

URL: https://developer.playcanvas.com/user-manual/scripting/script-attributes/esm/

:::note

This page documents Script Attributes for the recommended **ESM Scripts** system.

For **Classic Script Attributes**, click [here](./classic.md).

:::

## What are Attributes?

Attributes are a powerful feature that allow you to expose specific parameters to the Editor.

This means you can write code once, then tweak values on different instances to give them different properties. Artists, designers or other non-programmer team members can then adjust and modify them without writing code.

Let's start with a simple rotate script example.

```javascript

    static scriptName = 'rotator';

    /**
     * You can now set the `speed` property dynamically in the Editor.
     *
     * @attribute
     */
    speed = 2;

    update(dt){
        this.entity.rotateLocal(0, this.speed * dt, 0);
    }
}
```

In this example the script simply rotates the entity according to its speed, but what value is speed?

The `@attribute` tag above the `speed` member promotes it to an attribute. When attached to an entity, the Editor creates controls that allow you to dynamically set the value of `speed` at runtime for each entity it's attached to.

What this means in practice is that you can expose various members of a script to the Editor and create controls to edit their values at runtime.

[Image: Attribute]

Because `speed` is simply a class member, you can access it as you would any other member.

```javascript
update(dt) {
    this.entity.rotateLocal(0, this.speed * dt, 0);
}
```

## Attributes in the Editor

[Image: Script Attributes]

Once you've declared your attributes, the Editor needs to parse the code in order to expose the script attributes. If attributes have been changed, you need to manually refresh the attributes by clicking the parse button.

[Image: Parse Button]

When you expose an attribute to the Editor, you can also surface additional information that helps provide context and present more specific controls. This can help create a better user experience for your scripts.

### Attribute Descriptions

The first sentence of an `@attribute` comment block is used as a description in the Editor. This is a useful way to surface contextual information on what the attribute is and how it behaves.

```javascript
/**
 * Sets the speed of the Y rotation in degrees.
 *
 * @attribute
 */
speed = 2;
```

In the Editor, this is available as a tooltip.

[Image: Attribute Description]

### Attribute Titles

By default, attribute names are displayed in the Editor using the property name. You can override this with a custom display name using the `@title` tag:

```javascript
/**
 * @attribute
 * @title Rotation Speed
 */
speed = 2;
```

This will display "Rotation Speed" in the Editor instead of "speed".

## Responding to Attribute Changes

In ESM scripts, you can use JavaScript getters and setters to execute custom logic whenever an attribute value changes. This is useful for updating visuals, triggering events, or performing validation when a value is modified.

```javascript

    static scriptName = 'lamp';

    _brightness = 1;

    /**
     * @attribute
     */
    get brightness() {
        return this._brightness;
    }

    set brightness(value) {
        this._brightness = value;
        // Update the light intensity whenever brightness changes
        this.entity.light.intensity = value;
    }
}
```

In this example, when the `brightness` attribute is modified in the Editor (or at runtime), the setter automatically updates the light's intensity.

:::note

The `@attribute` JSDoc block only needs to be declared once, before the getter/setter pair.

:::

## Attribute Types

When you expose a script member as an attribute, the Editor will show a control that's relevant to the type of attribute. If the attribute is a number, it shows a numerical input; if it's a boolean, a checkbox.

An attribute can be a `number`, `string`, `boolean`, `Vec2`, `Vec3`, `Vec4`, `Color`, `Curve`, `Asset` or `Entity`.

### The @type Tag

In some situations you won't actually know an attribute's initial value ahead of time. For example, if you want to define an asset attribute on a script, you won't necessarily have an initial value. In these situations, where a value isn't known ahead of time, but its type is, you can use the JSDoc `@type` tag.

```javascript
/**
 * @attribute
 * @type {Asset}
 */
myTexture;
```

:::warning

An attribute must either be initialized with a value `speed = 10`, or have a JSDoc type `@type {number}`. If neither are present, the attribute will be ignored.

:::

### Number Type

A number attribute displays a numerical input in the Editor:

```javascript
/** @attribute */
speed = 10;
```

You can also define a sensible range of values using the `@range` tag:

```javascript
/** 
 * @attribute
 * @range [0, 10]
 */
speed = 10;
```

This tells the Editor that the value should be within 0 - 10.

The `@range` tag supports two formats:

- `@range [min, max]` - Constrains the value between minimum and maximum. The Editor displays a slider.
- `@range [min]` - Constrains the value to a minimum only (no upper limit). The Editor displays a numeric input without a slider.

For example, to create an attribute that must be non-negative but has no upper limit:

```javascript
/** 
 * @attribute
 * @range [0]
 */
positiveValue = 1;
```

[Image: Attribute Constraint]

There are additional numerical constraints that help the Editor limit the set of possible values:

```javascript
/** 
 * @attribute
 * @range [0, 10]
 * @precision 0.1
 * @step 0.05
 */
speed = 10;
```

### String Type

A string attribute displays a text input in the Editor:

```javascript
/** @attribute */
name = 'Player';
```

You can also use the `@placeholder` tag to display hint text when the field is empty:

```javascript
/**
 * @attribute
 * @placeholder Enter player name
 */
name = '';
```

### Boolean Type

A boolean attribute displays a checkbox in the Editor:

```javascript
/** @attribute */
enabled = true;
```

### Vector Attribute

```javascript
/** @attribute */
position = new Vec3();
```

:::important

You must import `Vec2`/`Vec3`/`Vec4` from `playcanvas` for your attribute to parse correctly.

:::

The vector attribute can be 2, 3, or 4 dimensions. The Editor will show a numerical input for each component, allowing you to set each one independently.

[Image: Attribute Vector]

### Color Attribute

```javascript
/** @attribute */
color = new Color();
```

:::important

You must import `Color` from `playcanvas` for your attribute to parse correctly.

:::

The color attribute shows a color picker when exposed in the Editor.

### Curve Attribute

```javascript
/**
 * @attribute
 * @type {Curve}
 * @color rgba
 */
wave;
```

:::important

You must import `Curve` from `playcanvas` for your attribute to parse correctly.

:::

The curve attribute is used to express a value that changes over a time period. All curves are defined over the period 0.0 - 1.0. You can define multiple curves. For example, if you wish to have a 3D position from a curve, define three curves for x, y, z using the `curves` property. There is also a special curve editor for modifying colors using the `color` property.

### Asset Attribute

The Asset attribute lets you reference a project asset in your script.

The `@resource` tag limits the asset picker in the Editor to only accept assets of a particular type. Valid values are: `animation`, `animstategraph`, `audio`, `binary`, `container`, `css`, `cubemap`, `font`, `gsplat`, `html`, `json`, `material`, `model`, `render`, `script`, `shader`, `sprite`, `template`, `text`, `texture`, `textureAtlas`, `wasm`.

The runtime type of an Asset attribute is `Asset`. You can reference the resource of an Asset attribute at runtime like so:

```javascript
/**
 * @attribute
 * @type {Asset}
 * @resource texture
 */
texture;

initialize() {
    console.log('This is the texture asset', this.texture);
    console.log('This is the texture resource', this.texture.resource);
}
```

:::important

You must import `Asset` from `playcanvas` for your attribute to parse correctly.

:::

### Entity Attribute

The Entity type lets you reference another entity in your hierarchy, providing a great way to link two entities together.

```javascript
/**
 * @attribute
 * @type {Entity}
 */
target;
```

:::important

You must import `Entity` from `playcanvas` for your attribute to parse correctly.

:::

## Attribute Arrays

In some cases you may want to expose a list of grouped attributes together. Let's say you have a script that generates a gradient, but rather than having a start and end point, you want to allow users to set an arbitrary amount of 'color stops' on the gradient. In this case you can use an array qualifier in a `@type` tag.

```javascript
/**
 * @attribute
 * @type {Color[]}
 */
gradientStops;
```

The `Color[]` declaration uses the [JSDoc type tag](https://jsdoc.app/tags-type) to declare that `gradientStops` is an array of `Color` values. The Editor will interpret it this way, creating a controller that allows you to set multiple `Color` values in a list.

[Image: Attribute Array]

In your initialize or update loop, you can iterate over `gradientStops` as an array:

```javascript
initialize() {
    this.gradientStops.forEach(color => {
        console.log('This is a Color class', color);
    });
}
```

### Default Array Size

You can use the `@size` tag to set the initial size of an array attribute:

```javascript
/**
 * @attribute
 * @type {Vec3[]}
 * @size 4
 */
waypoints;
```

This sets the array to 4 elements by default. The size can still be changed in the Editor.

## Enumerations

Sometimes you may want to constrain an attribute to a set of possible values. In this situation you can use the `@enum` tag. This uses an enumeration as a value for the attribute, making the Editor display a combo box constrained to the list of possible values:

```javascript
/** @enum {number} */
const Lights = {
    ON: 1,
    OFF: 0,
    UNKNOWN: 0.5
};

class MyScript extends Script {
    static scriptName = 'myScript';

    /**
     * @attribute
     * @type {Lights}
     */
    ambient = Lights.OFF;
}
```

This uses the `Lights` object as an enumeration of possible values. The `@type {Lights}` indicates that `ambient` should only have a value listed in `Lights`. At author-time the Editor will generate a drop-down control using the Lights enumeration keys as labels (ON/OFF/UNKNOWN) and setting the corresponding value on `ambient`. An enumerator's values can only be numbers, strings, or booleans.

[Image: Attribute Enumerations]

### Literal Union Types

For simpler cases, you can use literal union types as an inline alternative to defining a separate enumeration object:

```javascript
/**
 * @attribute
 * @type {'low' | 'medium' | 'high'}
 */
quality = 'medium';

/**
 * @attribute
 * @type {1 | 2 | 3 | 4}
 */
level = 1;
```

This creates a dropdown in the Editor with the specified values as options. Literal unions can contain strings, numbers, or booleans, but all values must be of the same type.

## Conditional Attributes

Every attribute in your script creates a corresponding UI control in the Editor. In some cases, you may want to hide or disable certain controls based on the values of other attributes.

Let’s walk through an example:

```javascript

    static scriptName = 'delorean';

    /**
     * @attribute
     */
    power = false;

    /** 
     * @attribute
     */
    speed = 10;
}
```

This will create a checkbox for power and a slider for speed. But what if we want to prevent users from adjusting the speed unless power is turned on?

We can achieve this by using the `@enabledif` tag:

```javascript

    static scriptName = 'delorean';

    /**
     * @attribute
     */
    power = false;

    /** 
     * @attribute
     * @enabledif {power}
     */
    speed = 10;
}
```

Now, the speed slider will only be enabled when power is `true`.

### Expression-Based Conditions

You can also use more expressive conditions. If the condition evaluates to a [truthy](https://developer.mozilla.org/en-US/docs/Glossary/Truthy) value, the control is enabled.

```javascript

    static scriptName = 'delorean';

    /**
     * @attribute
     */
    power = false;

    /** 
     * @attribute
     * @enabledif {power}
     */
    speed = 10;

    /**
     * @attribute
     * @visibleif {speed > 88.8}
     */
    enableFluxCapacitor = true;
}
```

In this case:

- The `speed` slider is only enabled if power is on.
- The `enableFluxCapacitor` checkbox is only *visible* when `speed` is greater than `88.8`.

This allows for rich, dynamic Editor interfaces based on script state.

#### Example in Action

<video autoPlay muted loop controls src='/video/conditional-attribtues.mp4' style={{width: '100%', height: 'auto'}} />

## Grouping Attributes

In some situations you may want to logically group attributes together. For example, let's say you have a `GameLogic` Script with an enemy with speed and power. Rather than declare the attributes individually, it makes sense to group them together under one `enemy` attribute. You can do this with **Attribute Groups**.

Attribute groups are essentially objects that contain sub-attributes:

```javascript
class GameLogic extends Script {
    static scriptName = 'gameLogic';

    /** 
     * `power` and `speed` are exposed as sub-attributes.
     *
     * @attribute 
     */
    enemy = { power: 10, speed: 3 };

    initialize() {
        console.log(this.enemy.speed); // 3
        console.log(this.enemy.power); // 10
    }
}
```

This defines `enemy` as an Attribute Group. The Editor will expose the enemy attribute with nested controllable power and speed sub-attributes. It provides a more flexible way to logically group attributes together.

:::tip
Attribute Groups allow you to logically group together related attributes into an object-based structure.
:::

There are different ways you can declare Attribute Groups. You can use Inline Attribute Groups or TypeDef Groups.

### Inline Group

A simple inline way of declaring attribute groups:

```javascript
class GameLogic extends Script {
    static scriptName = 'gameLogic';

    /** @attribute */
    enemy = { power: 10, speed: 3 };
}
```

### Typedef Groups

This is a more modular way of declaring Attribute Groups. Whilst it is more verbose than using the inline version, the typedef version is more modular and can be used across multiple scripts and attributes.

```javascript
/**
 * @typedef {Object} Enemy
 * @prop {number} speed - The enemy's speed
 * @prop {number} power - The enemy's power
 */

class GameLogic extends Script {
    static scriptName = 'gameLogic';

    /** 
     * @attribute 
     * @type {Enemy}
     */
    enemy;
}
```

### Interface Attributes

If you want to group attributes together and set individual constraints on its members, you can use an Interface Attribute. This provides a more flexible way of grouping attributes.

```javascript
/** @interface */
class Enemy {
    /**
     * @range [0, 11]
     */
    power = 10;

    speed = 3;
}

class GameLogic extends Script {
    static scriptName = 'gameLogic';

    /**
     * @attribute 
     * @type {Enemy}
     */
    enemy;
}
```

In the above example we've created a new `Enemy` Interface with a power member constrained within *0 - 11* range. We've also declared that the `GameLogic` Script has an attribute `enemy` which is of type `Enemy`.

:::tip
An *Interface Attribute* allows you to both logically group attributes together and set constraints on individual sub-attributes. It also allows you to modularize your code.
:::

#### Rules of Interface Attributes

There are a number of requirements to use Interface Attributes.

- An Interface Attribute must have an `/** @interface */` block comment before a class declaration
- A Script Attribute must use an Interface Attribute using the `@type {InterfaceAttribute}` tag
- All public members of an Interface Attribute are available to the Editor and will be used. You do not need to use the `@attribute` tag on each member.
- You cannot have nested Interface Attributes.

### Interface Attribute Arrays

Interface attributes can be used as arrays, just like plain attributes. This means that your `GameLogic` script can use an array of enemies, each with their own controllable power and speed properties.

```javascript
class GameLogic extends Script {
    static scriptName = 'gameLogic';

    /**
     * @attribute
     * @type {Enemy[]}
     */
    enemies;

    update(dt) {
        this.enemies.forEach(({ power, speed }) => {
            this.updateEnemy(power, speed);
        })
    }
}
```

This creates an array of Enemy controls in the Editor, each with its own numerical controls for the sub-attributes.

[Image: Attribute Complex Arrays]

--------------------------------------------------------------------------------

## Script Lifecycle

URL: https://developer.playcanvas.com/user-manual/scripting/script-lifecycle/

Every script instance you attach to an Entity in PlayCanvas goes through a well-defined lifecycle. Understanding this lifecycle is crucial because it dictates when your code runs and how it can interact with the rest of your application. PlayCanvas provides specific functions, called lifecycle methods, that you can define in your script. The engine will automatically call these methods at the appropriate times.

Think of it like the stages in an actor's performance: preparing backstage (`initialize`), performing on stage (`update`), and taking a final bow (`destroy` event).

```mermaid
graph TD
    A["<code>initialize()</code>"] --> B["<code>postInitialize()</code>"]
    B --> C["<code>update(dt)</code>"]
    C --> D["<code>postUpdate(dt)</code>"]
    D --> E["<code>destroy</code> handler"]
    %% Loop back to update
    D --> C

    subgraph Script Lifecycle Sequence
        A
        B
        C
        D
        E
    end
```

:::note[Execution Order]

It's important to note that if an Entity has multiple scripts attached via its Script Component, the lifecycle methods (`initialize`, `postInitialize`, `update`, `postUpdate`) for those scripts will be called in the order they appear in the component's script list for that particular Entity. This order applies consistently frame-to-frame.

:::

## Lifecycle Methods {#lifecycle-methods}

Let's break down each of the key lifecycle methods.

### `initialize()`

**When it's called:**

* Once per script instance.
* After the script instance is created and its Entity is enabled.
* After all its Script Attributes have been parsed and assigned their initial values (either defaults or values set in the Editor).
* Crucially, it's called after the application has loaded and the entity hierarchy is constructed, but before the first `update` loop or frame is rendered.
* If an entity or script is disabled when the application starts, the `initialize` method will be called the first time the entity and script are both enabled.

**Purpose:**

* This is your script's primary setup or "constructor-like" phase.
* Ideal for:
  * Subscribing to script [lifecycle events](#lifecycle-events).
  * Registering [DOM event](https://developer.mozilla.org/en-US/docs/Web/Events) handlers.
  * Creating any objects the script needs to manage internally.
  * Caching references to other Entities in the scene hierarchy.

:::warning[Constructor vs initialize]

Avoid using the `constructor` for startup logic — use `initialize()` instead. Execution order of `constructor`s is not guaranteed.

:::

:::info[Cloning Entities]

When an entity is cloned using the `entity.clone()` method, the `initialize` method on its scripts is **not** called immediately. It will only be called when the cloned entity is subsequently added to the scene hierarchy (e.g., using `this.app.root.addChild(clonedEntity)`), provided both the cloned entity and the script instance itself are enabled at that time.

:::

**Example:**

### `postInitialize()`

**When it's called:**

* Once per script instance.
* Called after the `initialize()` method of all script instances on all enabled Entities in the scene has completed.

**Purpose:**

* Useful for setup logic that depends on other scripts or Entities having already completed their own `initialize()` phase.
* Helps avoid race conditions where one script tries to access another script's properties before that other script has set them up.

**Example:**

### `update(dt)`

**When it's called:**

* Every frame, if the script instance, its Entity, and the Entity's ancestors are all enabled.

**Parameter:**

* dt (delta time): A number representing the time in seconds that has passed since the last frame. This is crucial for frame-rate independent logic.

**Purpose:**

* This is the heart of your script's runtime behavior.
* Used for:
  * Handling continuous input.
  * Updating positions, rotations, and scales for movement or animation.
  * Checking game conditions (e.g., collisions, win/loss states).
  * Any logic that needs to be performed repeatedly over time.

:::important

Keep update as efficient as possible, as it runs very frequently. Avoid heavy computations or allocations here if they can be done elsewhere (e.g., in initialize).

:::

**Example:**

### `postUpdate(dt)`

**When it's called:**

* Every frame, if the script instance and its Entity are enabled.
* Called after the `update()` method of all script instances has completed for the current frame.

**Parameter:**

* dt (delta time): Same as in update().

**Purpose:**

* Useful for logic that needs to run after all primary updates have occurred.
* Common use case: A camera script that follows a player. The player's update moves the player, and the camera's `postUpdate` then adjusts the camera's position to follow the player's new location smoothly.

**Example:**

## Lifecycle Events {#lifecycle-events}

Beyond the primary lifecycle methods (`initialize`, `postInitialize`, `update`, `postUpdate`), script instances also emit specific events at key moments in their lifecycle. You can subscribe to these events to execute custom logic when these state changes occur. This is particularly useful for managing resources, toggling behaviors, or performing final cleanup.

The three main lifecycle events are `enable`, `disable`, and `destroy`.

### `enable` Event {#enable-event}

**When it's fired:**

* When a script instance becomes enabled. This can happen in several ways:
  * When the script is first initialized, if both the script component and its Entity start in an enabled state.
  * When `this.enabled` is set from false to true programmatically.
  * When the script's parent Entity (or an ancestor Entity) becomes enabled, and the script itself was already marked as enabled.

**Purpose:**

* To perform actions when a script becomes active after being inactive.
* Ideal for:
  * Re-enabling behaviors that were paused (e.g., resuming animations, re-registering event listeners that were removed on disable).
  * Updating visual states to reflect an active status.

**Subscribing:**

```javascript
// Typically inside initialize()...
this.on('enable', () => {
    console.log('script enabled');
});
```

:::tip

If a script starts in an enabled state, the `enable` event fires during the initialization phase. If you need to ensure certain setup from `onEnable` also runs if the script starts enabled, you can call the handler directly in `initialize` after subscribing, guarded by an `if (this.enabled)` check.

:::

### `disable` Event {#disable-event}

**When it's fired:**

* When a script instance becomes disabled. This can occur when:
  * `this.enabled` is set from `true` to `false` programmatically.
  * The script's parent Entity (or an ancestor Entity) becomes disabled.
  * Before the `destroy` event is fired (as a script is implicitly disabled before destruction).

**Purpose:**

* To perform actions when a script becomes inactive.
* Ideal for:
  * Pausing behaviors (e.g., stopping animations, unregistering event listeners that are only relevant when active).
  * Releasing temporary resources that are only needed when enabled.
  * Updating visual states to reflect an inactive status.

**Subscribing:**

```javascript
// Typically inside initialize()...
this.on('disable', () => {
    console.log('script disabled');
});
```

### `state` Event {#state-event}

**When it's fired:**

* Whenever a script instance's effective running state changes from enabled to disabled, or from disabled to enabled. This can happen due to:
  * The `this.enabled` property of the script instance being changed programmatically.
  * The `enabled` state of the parent Script Component changing.
  * The `enabled` state of the script's parent Entity (or an ancestor Entity) changing.

**Purpose:**

* Provides a single callback to react to any change in the script's active status.
* Useful when you need to perform an action regardless of whether the script just became enabled or disabled, often based on the new state itself.
* Can sometimes simplify logic compared to handling [`enable`](#enable-event) and [`disable`](#disable-event) separately, if the required action is similar in both cases but depends on the resulting state.

**Parameter:**

* enabled (boolean): The new state of the script instance (`true` if it just became enabled, `false` if it just became disabled).

**Subscribing:**

```javascript
// Typically inside initialize()...
this.on('state', (enabled) => {
    console.log(`script ${enabled ? 'enabled' : 'disabled'}`);
});
```

### `destroy` Event {#destroy-event}

**When it's fired:**

* When the script instance is about to be destroyed. This happens when:
  * Its parent Entity is destroyed.
  * The Script Component containing this script instance is removed from the Entity.
  * The script instance itself is explicitly destroyed (e.g., `this.destroy()`, though less common for direct calls).

**Purpose:**

* This is your script's final cleanup phase. It's crucial for preventing memory leaks and ensuring a clean shutdown of the script's functionality.
* Essential for:
  * Unsubscribing from all events the script subscribed to (e.g., `this.app.off(...)`, `someEntity.off(...)`, `this.off(...)` for its own events).
  * Releasing any external resources or DOM elements the script might have created or holds references to.
  * Nullifying references to other objects to help the garbage collector.

**Subscribing:**

```javascript
// Typically inside initialize()...
this.once('destroy', () => {
    console.log('script destroyed');
});
```

:::tip[on vs once]

It's common to use `this.once('destroy', ...)` because the `destroy` handler only needs to run once.

:::

:::important[unregister event handlers]

If your script has used `on` or `once` to register any event handlers, remember to use `off` for those handlers in the `destroy` handler. Otherwise, the garbage collector may not be able to free up memory used by your script.

:::

--------------------------------------------------------------------------------

## User Interface

URL: https://developer.playcanvas.com/user-manual/user-interface/

User Interfaces present a unique challenge for graphical applications. There are several options for building User Interfaces in PlayCanvas.

## Screen and Element Components (Recommended)

[Image: Intro]

PlayCanvas implements two components which can form the building blocks of a user interface system that runs directly inside your WebGL canvas. The [Screen Component](/user-manual/editor/scenes/components/screen) is the user interface container, and the [Element Component](/user-manual/editor/scenes/components/element) is used to add user interface elements. The primary advantage is that your user interface exists in the same context as the rest of your game. This allows interactions between the application and the user interface.

## HTML and CSS

Web browsers have spent years building effective and optimized systems for rendering complex interfaces to users. For some use cases using the HTML, CSS and the browser DOM are a good fit for your user interface.

The primary downside of using the DOM is performance. The DOM is not designed to be run in a high framerate, real time setting. Page reflows and garbage collection can cause stutters in your application. If you're aiming for a consistent 60fps in your application this is not the best option.

---

The rest of this user guide will focus on the Screen and Element component system and using them to build user interfaces in PlayCanvas.

--------------------------------------------------------------------------------

## Elements

URL: https://developer.playcanvas.com/user-manual/user-interface/elements/

Elements are the individual pieces that make up a user interface screen. A user interface element is added by attaching an Element component to an entity and adding that entity as a child or descendent of the Screen entity. An element that is part of a Screen differs from a regular entity in the way that its transform is calculated and hence the way that it is positioned on screen.

In addition to the local position, rotation and scale which are used to calculate an entity's position relative to its parent, Element components also use the anchor, pivot and margin properties to determine, where the local position should be measured against, where the center point of the Element rectangle should be and the distance between the edges of the Element rectangle and the anchors.

These new properties give you plenty of control to layout your user interfaces, including aligning them with other Elements or positioning them with fixed distances to Elements.

## Element Positioning {#element-positioning}

[Image: Element Guide]

## Element Resizing {#element-resizing}

To resize your Element activate the Resize gizmo or press '4':

[Image: Resize Gizmo Toolbar]

Then drag the corners around to adjust the size of the Element:

[Image: Resize Gizmo Viewport]

## Pivot {#pivot}

The pivot property of an Element determines at which point the position, rotation and scale are calculated. The pivot is defined by two numbers between 0 and 1 which determine the pivot's position in the X and Y direction along the total width and height of the element. For example, `[0,0]` sets the pivot to the bottom left of the element, `[1,1]` sets the pivot to the top right of the element.

[Image: Pivot]

## Anchor {#anchor}

The anchor property determines where the point or points on the parent that the Element's position is calculated from. The anchor value is specified by two points `[minX, minY]` and `[maxX, maxY]`. In the Editor this is displayed as 4 numbers: [minX, minY, maxX, maxY]

[Image: Anchor]

For example, setting the anchor to `[0,0,0,0]` will anchor the element to the bottom left of its parent. The position of the element will set the offset from the bottom left of the parent

## Split Anchors {#split-anchors}

Sometimes it is useful to anchor different edges of an Element to different places. For example, if you wish to make an Element that stretches to fill the screen whatever the resolution. You can do this by splitting the anchor's min and max values.

[Image: Split Anchor]

In this image the Anchor is set to `[0,0,1,1]` so we are anchoring the edges of the element to the edges of the parent. Each edge has a margin of 50 pixels so the Element is fixed to fill the parent with 50 pixels from edge.

## Margin {#margin}

The margin property is only available when the anchor value is split in one axis. The margin sets the number of Screen component pixels from the anchor that the edge of the element will be. Shortcuts to the margin values are available in scripts on the Element component as the properties `left`, `right`, `top` and `bottom`.

## Loose Elements {#loose-elements}

Whilst the primary use-case of Elements is to be part of a User Interface Screen Component. It is valid to have an Element component which is not part of a screen. For example, a single in-world piece of text.

Sizes and positioning for Elements that do not have a Screen behaves slightly differently. Whereas for a screen a 32x32 Element is 32 pixels a Screen-less Element will be 32m by 32m. Adjust your sizes accordingly.

--------------------------------------------------------------------------------

## Group Elements

URL: https://developer.playcanvas.com/user-manual/user-interface/group-elements/

A group Element is an empty Element component that can be used as a parent for other elements. You can set the size and position of this Element and any child Elements can be positioned relative to the group.

--------------------------------------------------------------------------------

## Image Elements

URL: https://developer.playcanvas.com/user-manual/user-interface/image-elements/

The image Element is used to display a texture or a solid color. Image Elements can be used as the basis for other more complex user interface components such as buttons.

## Tinting

Each image Element has a color property. If no texture is applied, the image element will simply be a colored rectangle. If a texture is applied, this color will be used to tint the texture. This lets you create color variations using only a single grayscale texture.

[Image: Tinting]

## Transparency

As with the color property, the opacity property can be used to set the transparency level of an Element.

[Image: Transparent]

## Advanced Materials

If you can't achieve the results you are looking for using the texture, color and opacity properties. You can assign your own material to an image element using the material property. For correct rendering you should _disable Depth Write_ on any material added to an image Element.

:::note

Lighting will not function as expected for Screen Space elements. You will probably want to disable lighting and shadows for any material that is used in screen space.

:::

## Masks and Masking

Image Elements can be used to mask or hide elements that are descendants. To enable this feature set the `mask` property of an image Element to true.

If there is no texture asset assigned to an image Element used for masking the mask will be a rectangle defined by the width and height. If the image Element has a texture assigned the alpha-channel of the texture is used as the mask. Note, an image mask is 1-bit, i.e. there is no fading out of the mask using the alpha-channel of the texture.

--------------------------------------------------------------------------------

## Input

URL: https://developer.playcanvas.com/user-manual/user-interface/input/

The user can interact with [Element](/user-manual/editor/scenes/components/element/) components, by enabling the `useInput` field on the Element component:

[Image: Use Input]

Also in order for that to work there must be an initialized instance of `pc.ElementInput` for `pc.Application#elementInput`. This is created automatically for you if you are using the Editor. If you are using the Engine only make sure to create an instance *before* the other input devices like `pc.Mouse` or `pc.TouchDevice` like so:

```javascript
const app = new pc.Application(canvas, {
    elementInput: new pc.ElementInput(canvas),
    mouse: new pc.Mouse(canvas),
    touch: !!('ontouchstart' in window) ? new pc.TouchDevice(canvas) : null,
    keyboard: new pc.Keyboard(window),
    gamepads: new pc.GamePads(),
    ...
});
```

## Input Events

When you enable input on an Element component the following events will be fired:

### mousedown

Fired when the mouse is pressed while the mouse cursor is on the component.

### mouseup

Fired when the mouse is released while the mouse cursor is on the component.

### mouseenter

Fired when the mouse cursor enters the component.

### mouseleave

Fired when the mouse cursor leaves the component.

### mousemove

Fired when the mouse cursor is moved on the component.

### mousewheel

Fired when the mouse wheel is scrolled on the component.

### click

Fired when the mouse is pressed and released on the component or when a touch starts and ends on the component.

### touchstart

Fired when a touch starts on the component.

### touchend

Fired when a touch ends on the component.

### touchmove

Fired when a touch moves after it started touching the component.

### touchcancel

Fired when a touch is cancelled on the component.

## Event Handling

To handle an input event you can listen for it on the Element component:

```javascript
this.entity.element.on('click', function (event) {
    console.log('The element ' + event.element.entity.name + ' was clicked.');
}, this);
```

## Event bubbling

When an input event is fired on an Element component it bubbles up to its parent Elements unless you call `event.stopPropagation()`. For example:

```javascript
this.entity.element.on('click', function (event) {
    // stop bubbling
    event.stopPropagation();

    console.log('The element ' + event.element.entity.name + ' was clicked.');
}, this);
```

Calling `stopPropagation` will also stop the event from being handled by the other input devices like `pc.Mouse` or `pc.TouchDevice`. So if for example you are handling mouse input using `app.mouse.wasPressed`, you can call `stopPropagation` on the `mousedown` event to prevent `app.mouse.wasPressed` from returning true. For example:

## Mouse and Touch event conflict on Google Chrome

Google Chrome simulates mouse events also on touch devices. By doing so it could cause some unexpected behavior. For example if you hide a button right after the click event, another UI element that lays behind it could also receive an unwanted click event.

To prevent this behavior you can call the ```preventDefault()``` method of the native event object on the ```pc.EVENT_TOUCHEND``` event:

Here is small script to include once in your scene:

--------------------------------------------------------------------------------

## Layout Groups

URL: https://developer.playcanvas.com/user-manual/user-interface/layout-groups/

The Layout Group Component is used to automatically set the position and size of child Elements. A Layout Group Component can be used to align child elements into vertical or horizontal columns or a grid. A Layout Group Component applies the layout rules to all its direct children, you can override the Layout Group rules on a single child using a Layout Child Component.

The Layout Group Component can be used to generate common layouts, for example, a [grid](/user-manual/user-interface/layout-groups#grid), a fixed width [vertical column](/user-manual/user-interface/layout-groups#vertical-leaderboard), or [horizontal row](/user-manual/user-interface/layout-groups#horizontal-buttons).

## Creating a Layout Group {#creating-a-layout-group}

Add a Layout Group by adding the LayoutGroup Component to an existing Element Entity.

[Image: Create Layout Group]

## Layout Group Properties {#layout-group-properties}

### Orientation {#orientation}

Set the `Orientation` to Horizontal to organize your layout from left-to-right or right-to-left. Or Vertical to organize your layout top-to-bottom or bottom-to-top.

### Reverse {#reverse}

ReverseX and ReverseY properties are used to set the direction the layout group is built out in. The default is left-to-right and bottom-to-top.

### Alignment {#alignment}

Alignment is used to align the child elements to the edges of the Layout Group. `[0,0]` aligns to the bottom left, `[1,1]` aligns to the top right.

### Padding {#padding}

Padding adds a space to the inside of the Layout Group before positioning any children.

### Spacing {#spacing}

Spacing determines the gap between each child.

### Fitting {#fitting}

The Width Fitting and Height Fitting properties determine how a child element's width or height will be adjusted by the Layout Group.

A value of **None** will apply no fitting.

A value of **Stretch** will stretch the children to fill the width or height of the container using the following procedure:

- Sum the fitWidthProportion/fitHeightProportion values of each child and normalize so that all values sum to 1.
- Apply the natural width/height for each child.
- If there is space remaining in the container, distribute it to each child based on the normalized fitWidthProportion/fitHeightProportion values, but do not exceed the maxWidth/maxHeight of each child.

A value of **Shrink** will shrink the children to fit the container using the following procedure:

- Sum the fitWidthProportion/fitHeightProportion values of each child and normalize so that all values sum to 1.
- Apply the natural width/height for each child.
- If the new total width/height of all children exceeds the available space of the container, reduce each child's width/height proportionally based on the normalized fitWidthProportion/fitHeightProportion values, but do not exceed the minWidth/minHeight of each child.

A value of **Both** will apply both **Stretch** and **Shrink**.

### Wrap {#wrap}

The wrap property causes children that are outside of the width (for vertical groups) or height (for horizontal groups) to be moved to a new row or column. Using the wrap property you can create grid-based layouts.

## Layout Children {#layout-children}

A Layout Group applies its rules to all of its direct children. If you want to override these rules for a specific child you can do that by adding a LayoutChild Component to that child.

[Image: Layout Child]

In this example, the horizontal layout is using the **Stretch** width fitting to stretch each button to fit evenly into the container. The center button has a Layout Child Component with a `maxWidth` value set to 64, so it will not be stretched.

[Image: Layout Child Setup]

## Example Layouts {#example-layouts}

### Vertical Leaderboard {#vertical-leaderboard}

[Image: Leaderboard]

This Leaderboard is setup as vertical column aligned to the top center. We're using the Width Fitting property to stretch each item to be the full width. And using the Padding and Spacing properties to leave clear gaps between the cells.

[Image: Leaderboard Setup]

### Horizontal Buttons {#horizontal-buttons}

[Image: Horizontal Buttons]

This row of buttons is laid out using a Horizontal Layout Group with some Spacing and Padding settings to make the buttons fit the correct width. No width or height fitting is used.

[Image: Horizontal Setup]

### Grid {#grid}

[Image: Grid]

This row of buttons is laid out using a Layout Group with the Wrap setting enabled to set up a grid. The Group Element the Layout Group is attached to is set to the correct width (button width + spacing) to force the wrap to generate a two column grid. The Horizontal layout means that the grid fills in rows not columns and the Alignment property is set to `[0.5, 1]` which means that any "loose" elements (a row with only one element) will be centered in the X axis.

[Image: Setup]

--------------------------------------------------------------------------------

## Localization

URL: https://developer.playcanvas.com/user-manual/user-interface/localization/

This page describes how to localize your Text Elements to different languages.

## Localization Files

For each language you want to support you will need to add a JSON asset that contains the translated phrases for that language. PlayCanvas supports a specific format for the JSON asset. Open the Editor Settings and under LOCALIZATION click CREATE NEW ASSET to generate a new JSON asset in the expected format.

The JSON asset looks like so:

```json
{
    "header": {
        "version": 1
    },
    "data": [
        {
            "info": {
                "locale": "en-US"
            },
            "messages": {
                "key": "Single key translation",
                "key plural": [
                    "One key translation",
                    "Translation for {number} keys"
                ]
            }
        }
    ]
}
```

You can specify a different locale in the `info` part of the JSON file. The `messages` section contains key - value pairs for each localized phrase. The key is the identifier for that phrase and the text is the translated text for that key.

PlayCanvas also supports plural forms for each locale. To specify plural forms for each phrase you need to pass an array of strings for each plural form instead of a single string. Each language has different plural forms which you can find [here](https://www.unicode.org/cldr/charts/latest/supplemental/language_plural_rules.html). Each array element corresponds to a plural form for that language. For example for English:

```json
"key plural": [
    "One item", // plural form ONE
    "Not one" // plural form OTHER
]
```

For Arabic:

```json
"key plural": [
    "Zero items", // plural form ZERO
    "One item", // plural form ONE
    "Two items", // plural form TWO
    "Few items", // plural form FEW
    "Many items", // plural form MANY
    "Rest" // plural form OTHER
]
```

Refer to the language tables [here](https://www.unicode.org/cldr/charts/latest/supplemental/language_plural_rules.html) to determine the rules for each language.

After you have created your localization JSON assets you need to add them to the Editor Settings under LOCALIZATION.

## Localizing Text Elements

Enable the `Localized` checkbox for a Text Element in order to use the Localization Files to translate its text. The text you enter in the `Key` field of the Text Element should match the key in the localization file.

To test your localization you can change the `Locale` field under the Editor Settings. That should update your Editor viewport to that locale and also this will update the locale used when you launch your application. This field is not used when you publish or download a build.

## Localizing Numbers

Different locales will have different rules on how numbers should be formatted. For example, English (UK and US) would format `1000000` as `1,000,000` and Dutch would format with a decimal instead `1.000.000`.

JavaScript provides a built in function to do this formatting based on the locale code, [`Number.protoype.toLocaleString()`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number/toLocaleString).

An example of usage:

```javascript
const numberOfItems = 1000;
const currentLocale = this.app.i18n.locale;
const localeNumberString = numberOfItems.toLocaleString(currentLocale);

console.log(localeNumberString);
// expected output assuming currentLocale is en-US: "1,000"
```

## Localized Fonts

Often you will find that different languages might require different fonts to be used. In order to define a different font for a specific language select the primary font Asset you are using for your Text Element and towards the bottom of the Asset Attributes you will find the Localization section for that font Asset. Type the desired locale and assign a new font Asset for that locale.

At runtime when the application switches to a different locale it will load the font Asset you defined for that locale.

[Image: Localized Fonts Inspector]

## Language Notes

There are some languages that require specific workflows or considerations that are listed below.

### Thai

For word wrapping to work correctly in UI Text Elements with Thai text, [zero width characters (Unicode U+200B)](https://en.wikipedia.org/wiki/Zero-width_space) need be added between words by the translators.

The Thai language has no spaces between words and the same run of glyphs can be split into different combinations of words depending on the context of the sentence.

Being able to split Thai text correctly computationally is still an [unsolved problem](http://www.thai-language.com/ref/breaking-words) and usually done via a dictionary based approach which can be expensive to do at runtime.

The [thai-language.com site also has a separate tool](http://www.thai-language.com/?nav=zwsp) to add the zero width characters between words using a dictionary based approach if you have existing text.

### Right to left Languages

Right to left languages will need extra scripts for support that can be found in this [example project](/tutorials/right-to-left-language-support/).

In the example project, there is a folder called 'Rtl Support' that needs to be [copied and pasted](/user-manual/editor/assets/asset-panel/#copy-and-paste-between-projects) into your project.

In the folder, there is Script Type called 'RtlElement' which should be added to any Entity with a text element component that would be used to show right to left text.

## Engine

To retrieve the text from a key in script, use the APIs:

- [pc.I18n#getText](https://api.playcanvas.com/engine/classes/I18n.html#gettext) To retrieve a non-plural or first text string in a plural list
- [pc.I18n#getPluralText](https://api.playcanvas.com/engine/classes/I18n.html#getpluraltext) To retrieve a plural text string based on the number

For the complete engine API reference for localization see [this page](https://api.playcanvas.com/engine/classes/I18n.html).

--------------------------------------------------------------------------------

## Safe Area

URL: https://developer.playcanvas.com/user-manual/user-interface/safe-area/

With the trend of mobile devices having full device screens, a notch or cut out in the display is used to make room for the ear piece speaker and front facing camera (see below for the iPhone X).

<img loading="lazy" src="/img/user-manual/user-interface/safe-area/iphone-notch.png" />

(Image Original: Rafael Fernandez, Modified version:PlayCanvas, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0), via Wikimedia Commons)

Developers will need to be mindful of any essential information that is needed for the user which could be hidden by the notch during development.

For example, the screenshot below looks fine on desktop in devtools mobile view.

<img loading="lazy" src="/img/user-manual/user-interface/safe-area/desktop-view.png" width="500" />

However, when opened on a mobile device such as the iPhone X, the 'Left' text is rendered under the notch and the 'Bottom' text is rendered under the navigation bar.

<img loading="lazy" src="/img/user-manual/user-interface/safe-area/mobile-view-render-under-notch.png" width="500" />

## Safe Area

To help developers, browsers on these devices do support [environment variables](https://developer.mozilla.org/en-US/docs/Web/CSS/env()) in CSS to return values for positioning elements within an area that is occupied by the notch or navigation bar. This is known as a 'safe area'.

We have a [project with a reusable script](https://playcanvas.com/project/828118/overview/mobile-ui-safe-areas) that takes those CSS values and applies them to an UI Group Element entity via resizing the margins.

<img loading="lazy" src="/img/user-manual/user-interface/safe-area/mobile-view-safe-area.png" width="500" />

The UI setup in the project has an Entity with a full screen Group Element named 'Safe Area'. This has the script 'mobileSafeArea' attached which contains the logic for fitting the Element within the safe area of the device.

<img loading="lazy" src="/img/user-manual/user-interface/safe-area/hierarchy-layout.png" width="420" />

<img loading="lazy" src="/img/user-manual/user-interface/safe-area/safe-area-entity-setup.png" width="420" />

Any essential UI Elements can be placed as a child of the Safe Area Entity to be anchored relative to it.

<img loading="lazy" src="/img/user-manual/user-interface/safe-area/hierarchy-essential-elements.png" width="420" />

To help with development, a debug setting can be enabled to simulate a safe area to preview what a UI layout would look like without needing a device.

<img loading="lazy" src="/img/user-manual/user-interface/safe-area/debug-config.png" width="600" />

The debug config can be edited with live updates in the launch tab too.

<img loading="lazy" src="/img/user-manual/user-interface/safe-area/debug-config-runtime.gif" width="500" />

--------------------------------------------------------------------------------

## Screens

URL: https://developer.playcanvas.com/user-manual/user-interface/screens/

The [Screen Component](/user-manual/editor/scenes/components/screen) defines the area that the User Interface is built inside. Its primary responsibility is to determine how the contents of the Screen (child Entities with Element components) are rendered.

## Screen space or world space

The primary choice in render mode for a screen is whether it is rendered in screen space or world space.

[Image: Screen space]

A screen space Screen Component does not follow the normal transform hierarchy. Instead it is rendered as an overlay the camera. This is useful for creating 2D interfaces, HUDs or other game interfaces. There is an option to create a screen space screen in the menu by adding a *2D Screen*.

[Image: World Space]

If the screen space option is disabled, the Screen is instead rendered using the regular transform hierarchy. Elements are still positioned relative to the screens co-ordinate system, but the screen appears in the 3D world. There is an option to create a world space screen in the menu by adding a *3D Screen*.

## Resolutions and scaling

As PlayCanvas applications can be viewed on many devices and at many resolutions. For screen space components it's important to think about how your user interface will be viewed on different screen sizes.

The simplest way to render the elements is with no scaling.

[Image: iPad No Scaling]

In this case, the user interface works well on an iPad resolution. However, what happens when we view this same scene on an iPhone.

[Image: iPhone No Scaling]

Because of the smaller resolution of the phone, the user interface runs off the sides of the screen and is unusable.

The Screen component features a Scaling Mode property which is available for screen space components. If this is set to *Blend*, then the screen scales the interface to fit the resolution of the screen that it is displayed on. Here is the same interface using blended scaling on the iPad:

[Image: iPad Scaling]

and on the iPhone

[Image: iPhone Scaling]

You can see that the user interface now works at both resolutions.

### How is the scale factor calculated?

We use the *Reference Resolution* property of the Screen component as the "ideal" resolution. If your application is displayed at a larger resolution than the reference, then it is scaled up. If it is displayed at a smaller resolution than the reference it is scaled down. Because applications may need to respond differently to horizontal or vertical changes we also expose the Scale Blend property which determines which axis to use. Scale Blend of 0 only uses the horizontal resolution, Scale Blend of 1 only uses the vertical resolution, Scale Blend of 0.5 uses both in equal quantities.

In these examples you can see how changing the Scale Blend property affects how the interface responds to rotating from Portrait to Landscape

[Image: Scale Blend 0]
With a value of 0 the increase in horizontal resolution makes the button too large

[Image: Scale Blend 1]
With a value of 1 the decrease in vertical resolution makes the button too small

[Image: Scale Blend 0.5]
With a value of 0.5 the two changes are balanced and the button stays the same size

--------------------------------------------------------------------------------

## Text Elements

URL: https://developer.playcanvas.com/user-manual/user-interface/text-elements/

The Text Element is used to display a string of text using a [font asset](/user-manual/editor/assets/inspectors/font).

## Text

The Text Element contains a string field to enter the text that will be displayed. Use `Shift+Enter` to enter a new line character in the string field.

:::tip

Text elements are rendered to the screen using a single quad for each character in the string. When you change the text property, we re-generate the mesh for the element. There is a performance implication for this, though there should be no problems changing text content for a reasonable number of Elements every frame.

:::

### Text Markup

Text elements support a simple markup syntax that allows you to apply different colors to specific parts of the text. Consider this example:

```none
[color="#ff0000"]Red[/color], [color="#00ff00"]green[/color] and [color="#0000ff"]blue[/color].
```

Assuming the base color of the text element is white, this will render as follows:

[Image: Text Markup]

:::tip

You must proactively enable support for the markup syntax on a text element. You can do this via the API:

```javascript
entity.element.enableMarkup = true;
```

Or by enabling it in the Editor:

[Image: Enable Markup]

:::

## Localization

You can check the 'Localized' checkbox to localize the text of the Text Element. If this is enabled then, instead of the text, you specify the localization key for the Text Element which will be used to get the localized text from the localization assets.

Read more about localization [here](/user-manual/user-interface/localization).

## Auto-size

By default a Text Element is set to automatically adjust its width and height to match the text string. You can disable this and specify the height and width of the element directly in the Editor panel.

[Image: Auto Size]

:::note

The height of the character is determined by the largest character present in the font. It is the same for every character so as to avoid the string position changing depending on the contents of the string.

:::

## Alignment

Text Elements have an additional tool to help with positioning which is the alignment. You will be used to how this property works from tools like Word Processes. In this case, rather than presets we expose a variable that can be altered. The alignment consists of two values `[X, Y]` each between 0 and 1. `[0,0]` is bottom left alignment, `[0.5,0.5]` is centered and `[1,1]` is top right.

[Image: Top Left]

[Image: Centered]

[Image: Bottom Right]

## Font Size & Line Height

The font size property sets the rendered size of the font in Screen Component pixels. The line height sets the distance in Screen Component pixels to move down when the text contains a new line character.

Equal Font Size and Line Height is the default:

[Image: Font Size Line Equal]

Increase Line Height to increase line spacing:

[Image: Font Size Line Spaced]

## Spacing

The spacing property increases the distance between characters in a string. Fonts define the ideal distance to move the cursor forward for each character. The spacing property is a multiplier to this distance.

[Image: Spacing]

## Tinting

The Color property allows you to tint the string to the color of your choice.

[Image: Tinted]

## Transparency

The Opacity property allows you to set the transparency of the string

[Image: Transparent]

--------------------------------------------------------------------------------

## User Interface Basics

URL: https://developer.playcanvas.com/user-manual/user-interface/user-interface-basics/

User Interfaces are built from two elements in PlayCanvas. A [Screen Component](/user-manual/user-interface/screens) describes the area that contains all the user interfaces elements and then multiple entities with [Element Components](/user-manual/user-interface/elements) are attached below the Screen Entity in the hierarchy. The Screen defines the area of the User Interface and how it is rendered (in 2D space or in the 3D world). The Elements make up the images and text components of the interface.

## Layout and Positioning

One big difference between regular Entities and Entities that use an Element Component as part of a Screen is the way in which they are positioned with respect to their parents. Element Components have two properties that alter they way that their final position is calculated. The `anchor` property determines how the child is positioned relative to its parent and the `pivot` property determines where the center point of the element is. Read more on the [Element](/user-manual/user-interface/elements) page.

## Draw Order

The graphical portions of the user interface, Image and Text Elements, are drawn in the order that they appear in the hierarchy, i.e. the first child is drawn first, its child is drawn next. A child that is drawn later will appear on top of one that is drawn earlier.

To change the draw order you simply re-order the entities in the Editor hierarchy. You can re-order elements programmatically by calling `entity.reparent(...)`. Though, note, that this forces the draw order to be recalculated for the entire Screen component.

## Element 9-slicing

9-slicing (or 9-patch) is a graphical technique for creating scalable user-interface elements from bitmap graphics. You can setup 9-slicing using the Texture Atlas and Sprite Assets. [Read more](/user-manual/2D/slicing).

## Input

There is an additional way of handling input for UI Elements. An instance of the `pc.ElementInput` is provided on the Application object, usually accessible as `this.app.elementInput`. This allows you to listen for input directly on the element components, e.g. `this.entity.element.on('click', ...)`. Read more on the [Input](/user-manual/user-interface/input) page.

## Localization

PlayCanvas has a built-in localization system which supports localized Text Elements. Read more [here](/user-manual/user-interface/localization).

--------------------------------------------------------------------------------

## PlayCanvas Web Components

URL: https://developer.playcanvas.com/user-manual/web-components/

PlayCanvas Web Components are a powerful set of custom HTML elements that make it incredibly easy to embed interactive 3D content directly into your web pages. Built on modern web standards, these components bridge the gap between traditional web development and 3D graphics, allowing you to create immersive experiences with simple HTML markup.

## What Are Web Components?

[Web Components](https://developer.mozilla.org/en-US/docs/Web/API/Web_components) are reusable custom HTML elements that encapsulate complex functionality behind a simple, declarative interface. PlayCanvas Web Components wrap the full power of the [PlayCanvas Engine](../engine/index.md) in easy-to-use HTML tags, making 3D development accessible to web developers of all skill levels.

```html

<pc-app>
  <pc-scene>
    <pc-entity name="camera" position="0 0 3">
      <pc-camera></pc-camera>
    </pc-entity>
    <pc-entity name="light" rotation="45 45 0">
      <pc-light></pc-light>
    </pc-entity>
    <pc-entity name="ball">
      <pc-render type="sphere"></pc-render>
    </pc-entity>
  </pc-scene>
</pc-app>
```

## Why Use PlayCanvas Web Components?

### 🚀 Zero JavaScript Required

Create interactive 3D scenes using only HTML markup - no complex JavaScript setup or engine initialization needed.

### 🔧 Highly Customizable

Full access to PlayCanvas Engine features through intuitive HTML attributes.

### ⚡ Performance Optimized

Leverages the same high-performance PlayCanvas Engine used by thousands of web applications.

## Perfect For

- **Content creators** who want to add 3D elements to websites without learning complex 3D programming
- **Web developers** looking to integrate 3D graphics into existing HTML/CSS workflows  
- **Educators** teaching 3D concepts through familiar web technologies
- **Rapid prototyping** of 3D ideas and concepts
- **Marketing teams** creating interactive product showcases and demos

## Key Features

- **Declarative 3D scenes** defined entirely in HTML
- **Component-based architecture** mirroring the PlayCanvas Editor
- **Real-time updates** through reactive attribute binding
- **WebGL and WebGPU support** with automatic fallbacks
- **Mobile-optimized** touch controls and responsive layouts

## Browser Support

PlayCanvas Web Components work in all modern browsers that support:

- WebGL 2.0 and/or WebGPU
- ES6 Modules
- Custom Elements v1

## Open Source & MIT Licensed

The Web Components are completely open source and available on [GitHub](https://github.com/playcanvas/web-components) under the MIT license. This means you can use them freely in both personal and commercial projects, modify them to suit your needs, and contribute back to the community.

## Ready to Get Started?

Jump right in with our [Getting Started Guide](getting-started.md) or explore the complete [Tag Reference](./tags/index.md) to see everything that's possible with PlayCanvas Web Components.

--------------------------------------------------------------------------------

## Building a Scene

URL: https://developer.playcanvas.com/user-manual/web-components/building-a-scene/

Let's build a simple 3D scene step by step using PlayCanvas Web Components. We'll create a basic scene with a lit sphere.

## Starting Point

First, let's add the basic structure of our application to our HTML `body` using the [`<pc-app>`](../tags/pc-app) and [`<pc-scene>`](../tags/pc-scene) elements.

```html
<pc-app>
    <pc-scene>
    </pc-scene>
</pc-app>
```

This creates an empty 3D scene. However, we can't see anything rendered yet. We need a camera and some content.

:::note

All `pc-` elements must be closed properly. Self-closing tags (e.g. `<pc-camera />`) are not supported.

:::

## Adding a Camera

To view our scene, we need a camera which we can add to our scene using the [`<pc-entity>`](../tags/pc-entity) and [`<pc-camera>`](../tags/pc-camera) elements.

```html {3-5}
<pc-app>
    <pc-scene>
        <pc-entity name="camera" position="0 0 5">
            <pc-camera></pc-camera>
        </pc-entity>
    </pc-scene>
</pc-app>
```

We've added a camera entity positioned 5 units down the positive Z axis. By default, a camera looks down the negative Z axis so our camera is now looking at the origin. At this point, the rendered scene is a solid grey color (the default clear color of a camera).

## Adding a Light

Let's add a directional light to illuminate our scene using the [`<pc-light>`](../tags/pc-light) element.

```html {6-8}
<pc-app>
    <pc-scene>
        <pc-entity name="camera" position="0 0 5">
            <pc-camera></pc-camera>
        </pc-entity>
        <pc-entity name="light" rotation="45 45 0">
            <pc-light type="directional"></pc-light>
        </pc-entity>
    </pc-scene>
</pc-app>
```

The light is rotated to shine at an angle, which will create more interesting shading on our objects.

## Adding an Object

Now let's add a sphere shape to our scene using the [`<pc-render>`](../tags/pc-render) element.

```html {9-11}
<pc-app>
    <pc-scene>
        <pc-entity name="camera" position="0 0 5">
            <pc-camera></pc-camera>
        </pc-entity>
        <pc-entity name="light" rotation="45 45 0">
            <pc-light type="directional"></pc-light>
        </pc-entity>
        <pc-entity name="sphere">
            <pc-render type="sphere"></pc-render>
        </pc-entity>
    </pc-scene>
</pc-app>
```

You should now see a white sphere in the center of your screen!

--------------------------------------------------------------------------------

## Getting Started

URL: https://developer.playcanvas.com/user-manual/web-components/getting-started/

Before you begin, make sure you have [Node.js](https://nodejs.org/) 18 or later installed.

## Installing from NPM

PlayCanvas Web Components is available as a package on [NPM](https://www.npmjs.com/package/@playcanvas/web-components).
You can install it (and the PlayCanvas Engine) as follows:

```bash
npm install playcanvas @playcanvas/web-components --save-dev
```

Next, in your HTML file, you will need an import map because the Web Components need to be able to find the PlayCanvas Engine (which is an external dependency):

```html
<script type="importmap">
    {
        "imports": {
            "playcanvas": "/node_modules/playcanvas/build/playcanvas.mjs"
        }
    }
</script>
```

You can then import the Web Components as follows:

```html
<script type="module" src="/node_modules/@playcanvas/web-components/dist/pwc.mjs"></script>
```

You can now incorporate any of the PlayCanvas Web Components elements into your HTML!

## Using a CDN

Instead of loading the library from a local package, you can instead opt to load it from a CDN (such as jsDelivr). In this case, you would update the import map:

```html
<script type="importmap">
    {
        "imports": {
            "playcanvas": "https://cdn.jsdelivr.net/npm/playcanvas@latest/build/playcanvas.mjs"
        }
    }
</script>
```

And the components would now be imported as follows:

```html
<script type="module" src="https://cdn.jsdelivr.net/npm/@playcanvas/web-components@latest/dist/pwc.mjs"></script>
```

:::note[Versioning]

The snippets above use `@latest` for convenience. For production deployments, pin to a specific version to ensure deterministic builds (for example: `playcanvas@2.x.y` and `@playcanvas/web-components@x.y.z`). See the release notes for the latest stable versions: [PlayCanvas Engine releases](https://github.com/playcanvas/engine/releases) and [Web Components releases](https://github.com/playcanvas/web-components/releases).

:::

## Boilerplate HTML

Let's see how this looks in a minimal boilerplate HTML file:

```html
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
        <title>My PlayCanvas Web Components App</title>
        <script type="importmap">
            {
                "imports": {
                    "playcanvas": "https://cdn.jsdelivr.net/npm/playcanvas@latest/build/playcanvas.mjs"
                }
            }
        </script>
        <script type="module" src="https://cdn.jsdelivr.net/npm/@playcanvas/web-components@latest/dist/pwc.mjs"></script>
        <style>
            body {
                margin: 0;
                overflow: hidden;
            }
        </style>
    </head>
    <body>
        
    </body>
</html>
```

You are now ready to start using the PlayCanvas Web Components to build a 3D scene!

--------------------------------------------------------------------------------

## Adding Behavior with Scripts

URL: https://developer.playcanvas.com/user-manual/web-components/scripting/

Scripts add custom behaviors to entities in your PlayCanvas Web Components app.

Let's consider a simple script that rotates an entity over time:

```javascript title="rotate-script.mjs"

    static scriptName = 'rotateScript';

    update(dt) {
        // Rotate the entity 90 degrees per second around the world-space Y axis
        this.entity.rotate(0, dt * 90, 0);
    }
}
```

## Loading Scripts

Scripts are loaded via the [`<pc-asset>`](../tags/pc-asset) tag:

```html
<pc-asset src="path/to/rotate-script.mjs"></pc-asset>
```

Then attach it to an entity using [`<pc-scripts>`](../tags/pc-scripts) and [`<pc-script>`](../tags/pc-script):

```html
<pc-entity name="spinning cube">
    <pc-render type="box"></pc-render>
    <pc-scripts>
        <pc-script name="rotateScript"></pc-script>
    </pc-scripts>
</pc-entity>
```

:::important

The `name` attribute of `<pc-script>` must match the value of the `scriptName` property of the script.

:::

## Passing Data to Scripts with Attributes

Our rotate script is currently hardcoded to rotate at 90 degrees per second. But what if we want to rotate at a different speed? And what if we want to rotate multiple entities at different speeds? This is where script attributes come in!

Let's update our script to accept a rotation speed as an attribute:

```javascript title="rotate-script.mjs" {6-10,14}

    static scriptName = 'rotateScript';

    /**
     * The speed of the rotation in degrees per second
     * @attribute
     */
    speed = 90;

    update(dt) {
        // Rotate the entity `speed` degrees per second around the world-space Y axis
        this.entity.rotate(0, dt * this.speed, 0);
    }
}
```

We can now pass configuration to our script using the `attributes` attribute:

```html {4-6}
<pc-entity name="fast spinning cube">
    <pc-render type="box"></pc-render>
    <pc-scripts>
        <pc-script name="rotateScript" attributes='{
            "speed": 180
        }'></pc-script>
    </pc-scripts>
</pc-entity>
```

:::important

The `attributes` attribute takes a JSON string. Because JSON requires properties to be enclosed in double quotes, you should enclose the JSON string in single quotes.

:::

### PlayCanvas-Specific Types for Script Attributes

In addition to standard JavaScript types, you can configure script attributes using special PlayCanvas data types. When passing these values, you must supply them as strings formatted with a prefix followed by the required data. This ensures that the engine correctly interprets the attribute values.

The expected format for each type is as follows:

| PlayCanvas Data Type | Format Example                           | Description |
| -------------------- | ---------------------------------------- | ----------- |
| **Asset**            | `asset:your-asset-id`                    | References a `<pc-asset>`. Concatenate `asset:` with the asset's `id` attribute. |
| **Entity**           | `entity:your-entity-id`                  | References a `<pc-entity>`. Concatenate `entity:` with the entity's `id` attribute. |
| **Color**            | `color:255,200,100` or `color:255,200,100,255` | Specifies a color. Provide three comma-separated values (RGB) or four values (RGBA) prefixed by `color:`. |
| **Vec2**             | `vec2:10,20`                             | Defines a two-dimensional vector. Concatenate `vec2:` with two comma-separated numbers. |
| **Vec3**             | `vec3:10,20,30`                          | Defines a three-dimensional vector. Concatenate `vec3:` with three comma-separated numbers. |
| **Vec4**             | `vec4:10,20,30,40`                       | Defines a four-dimensional vector. Concatenate `vec4:` with four comma-separated numbers. |

Example Usage in HTML:

```html
<pc-script name="myScript" attributes='{
    "speed": 180,
    "targetColor": "color:255,100,50,255",
    "velocity": "vec3:5,0,0"
}'></pc-script>
```

[Read more](/user-manual/scripting/script-attributes) about Script Attributes.

## Using Ready Made Scripts from the Engine

Before you set about writing your own scripts, check to see whether the functionality you need is already available in the PlayCanvas Engine. The Engine ships with a library of useful scripts that you can use in your app. You can find them on [GitHub](https://github.com/playcanvas/engine/tree/main/scripts/esm) and they are used heavily in the [Web Component Examples](https://playcanvas.github.io/web-components/examples/).

--------------------------------------------------------------------------------

## Tag Reference

URL: https://developer.playcanvas.com/user-manual/web-components/tags/

Here is a complete list of the tags that are available in PlayCanvas Web Components.

| Tag | Description |
| --- | --- |
| [`<pc-app>`](pc-app) | Defines the root element of your application. |
| [`<pc-asset>`](pc-asset) | Defines an asset to be loaded by your application. |
| [`<pc-camera>`](pc-camera) | Defines a camera that is used to render the scene. |
| [`<pc-collision>`](pc-collision) | Defines a collision component used by triggers and rigid bodies. |
| [`<pc-element>`](pc-element) | Defines a text, image or group user interface element. |
| [`<pc-entity>`](pc-entity) | Defines an entity. |
| [`<pc-light>`](pc-light) | Defines a light component. |
| [`<pc-listener>`](pc-listener) | Defines a listener component. |
| [`<pc-module>`](pc-module) | Defines a WebAssembly module. |
| [`<pc-particles>`](pc-particles) | Defines a particle system component. |
| [`<pc-render>`](pc-render) | Defines a render component. |
| [`<pc-rigidbody>`](pc-rigidbody) | Defines a rigidbody component. |
| [`<pc-scene>`](pc-scene) | Defines a scene. |
| [`<pc-script>`](pc-script) | Defines a single script assigned to a script component. |
| [`<pc-scripts>`](pc-scripts) | Defines a script component. |
| [`<pc-sky>`](pc-sky) | Defines an image-based skybox. |
| [`<pc-screen>`](pc-screen) | Defines a screen component that can render element components. |
| [`<pc-sound>`](pc-sound) | Defines a single sound assigned to a sound component. |
| [`<pc-sounds>`](pc-sounds) | Defines a sound component. |
| [`<pc-splat>`](pc-splat) | Defines a splat component that renders 3D Gaussian Splats. |

--------------------------------------------------------------------------------

## <pc-app>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-app/

The `<pc-app>` tag is the root element for your PlayCanvas application. It is used to initialize the PlayCanvas application and provide a container for your scene.

:::note[Usage]

* It must be a descendant of the document's `body` element.

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `alpha` | Boolean | `"true"` | Whether the application allocates an alpha channel in the frame buffer |
| `antialias` | Boolean | `"true"` | Whether the application uses anti-aliasing |
| `backend` | Enum | `"webgl2"` | Graphics engine backend: `"webgpu"` \| `"webgl2"` \| `"null"` |
| `depth` | Boolean | `"true"` | Whether the application allocates a depth buffer |
| `high-resolution` | Boolean | `"true"` | Whether to render using physical resolution or CSS resolution |
| `stencil` | Boolean | `"true"` | Whether the application allocates a stencil buffer |

</div>

## Example

<CodePenEmbed id="JoPvXjO" title="<pc-app> example" />

## JavaScript Interface

You can programmatically create and manipulate `<pc-app>` elements using the [AppElement API](https://api.playcanvas.com/web-components/classes/AppElement.html).

--------------------------------------------------------------------------------

## <pc-asset>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-asset/

The `<pc-asset>` tag is used to define an asset.

:::note[Usage]

* It must be a direct child of [`<pc-app>`](../pc-app).

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `id` | String | - | Unique identifier used by other tags to reference this asset |
| `lazy` | Flag | - | Whether to defer loading until first referenced or explicitly requested |
| `src` | String | - | Path to the asset file |
| `type` | Enum | *inferred* | Asset type: `"audio"` \| `"binary"` \| `"css"` \| `"container"` \| `"gsplat"` \| `"html"` \| `"json"` \| `"script"` \| `"shader"` \| `"text"` \| `"texture"` |

</div>

## Example

```html
<pc-app>
    
    <pc-asset src="assets/scripts/animate.mjs"></pc-asset>
    
    <pc-asset src="assets/models/car.glb" id="car"></pc-asset>
</pc-app>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-asset>` elements using the [AssetElement API](https://api.playcanvas.com/web-components/classes/AssetElement.html).

--------------------------------------------------------------------------------

## <pc-camera>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-camera/

The `<pc-camera>` tag is used to define a camera component.

:::note[Usage]

* It must be a direct child of a [`<pc-entity>`](../pc-entity).

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `clear-color` | Color | `"0.75 0.75 0.75 1"` | Background color as space-separated RGBA values, hex code, or [named color](https://github.com/playcanvas/web-components/blob/main/src/colors.ts) |
| `clear-color-buffer` | Boolean | `"true"` | Controls whether the camera clears the color buffer |
| `clear-depth-buffer` | Boolean | `"true"` | Controls whether the camera clears the depth buffer |
| `clear-stencil-buffer` | Boolean | `"true"` | Controls whether the camera clears the stencil buffer |
| `cull-faces` | Boolean | `"true"` | Controls whether the camera culls faces |
| `enabled` | Boolean | `"true"` | Enabled state of the component |
| `far-clip` | Number | `"1000"` | The far clipping plane distance |
| `flip-faces` | Boolean | `"false"` | Controls whether the camera flips faces |
| `fov` | Number | `"45"` | Field of view in degrees |
| `frustum-culling` | Boolean | `"true"` | Controls whether the camera uses frustum culling |
| `gamma` | Enum | `"srgb"` | Color space: `"linear"` \| `"srgb"` |
| `horizontal-fov` | Flag | - | Whether to use horizontal field of view instead of vertical |
| `near-clip` | Number | `"0.1"` | The near clipping plane distance |
| `orthographic` | Flag | - | Whether to use orthographic projection instead of perspective |
| `ortho-height` | Number | `"10"` | Height of the orthographic projection |
| `priority` | Number | `"0"` | Rendering priority of the camera |
| `rect` | Vector4 | `"0 0 1 1"` | Viewport rectangle as "X Y Width Height" values |
| `scissor-rect` | Vector4 | `"0 0 1 1"` | Scissor rectangle as "X Y Width Height" values |
| `tonemap` | Enum | `"none"` | Tone mapping: `"none"` \| `"aces"` \| `"aces2"` \| `"filmic"` \| `"hejl"` \| `"linear"` \| `"neutral"` |

</div>

## Example

```html
<pc-entity>
    <pc-camera clear-color="yellow"></pc-camera>
</pc-entity>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-camera>` elements using the [CameraComponentElement API](https://api.playcanvas.com/web-components/classes/CameraComponentElement.html).

--------------------------------------------------------------------------------

## <pc-collision>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-collision/

The `<pc-collision>` tag is used to define a collision component.

:::note[Usage]

* It must be a direct child of a [`<pc-entity>`](../pc-entity).

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `axis` | Number | `"1"` | Axis for cylinder/capsule shapes (0=X, 1=Y, 2=Z) |
| `convex-hull` | Flag | - | Whether to use a convex hull for mesh collision |
| `enabled` | Boolean | `"true"` | Enabled state of the component |
| `half-extents` | Vector3 | `"0.5 0.5 0.5"` | Half-extents for box collision as "X Y Z" values |
| `height` | Number | `"2"` | Height for cylinder/capsule collision shapes |
| `radius` | Number | `"0.5"` | Radius for sphere/cylinder/capsule collision shapes |
| `type` | Enum | `"box"` | Collision shape: `"box"` \| `"capsule"` \| `"cone"` \| `"cylinder"` \| `"sphere"` |

</div>

## Example

```html

<pc-entity>
    <pc-render type="box"></pc-render>
    <pc-collision></pc-collision>
    <pc-rigidbody></pc-rigidbody>
</pc-entity>

<pc-entity>
    <pc-render type="sphere"></pc-render>
    <pc-collision type="sphere"></pc-collision>
    <pc-rigidbody type="dynamic"></pc-rigidbody>
</pc-entity>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-collision>` elements using the [CollisionComponentElement API](https://api.playcanvas.com/web-components/classes/CollisionComponentElement.html).

--------------------------------------------------------------------------------

## <pc-element>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-element/

The `<pc-element>` tag is used to define an element component.

:::note[Usage]

* It must be a direct child of a [`<pc-entity>`](../pc-entity).

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `anchor` | Vector4 | `"0 0 0 1"` | Sets the element's anchor as `left bottom right top` relative to its parent. Each value ranges from 0 to 1. `[0,0,0,0]` anchors to the bottom-left; `[1,1,1,1]` anchors to the top-right. If left≠right or bottom≠top (a split anchor), the element resizes to cover that area, e.g. `[0,0,1,1]` fills the parent. |
| `asset` | String | - | Font asset ID (must reference a `font` type asset) |
| `auto-width` | Flag | - | Whether to automatically adjust width to fit content |
| `color` | Color | `"1 1 1 1"` | Color as space-separated RGBA values, hex code, or [named color](https://github.com/playcanvas/web-components/blob/main/src/colors.ts) |
| `enabled` | Boolean | `"true"` | Enabled state of the component |
| `enable-markup` | Flag | - | Enables markup processing for styled text. Supports tags like `[color="#ff0000"]text[/color]` for colored text. |
| `font-size` | Number | `"16"` | Font size in pixels |
| `line-height` | Number | `"1.2"` | Line height multiplier |
| `pivot` | Vector2 | `"0.5 0.5"` | Pivot point as "X Y" values |
| `text` | String | - | Text content to display |
| `type` | Enum | `"group"` | Element type: `"group"` \| `"image"` \| `"text"` |
| `width` | Number | `"0"` | Width in pixels (0 for auto-sizing) |
| `wrap-lines` | Flag | - | Whether to wrap text lines |

</div>

## Example

```html
<pc-entity>
    <pc-element type="text" asset="arial" text="Hello, World!"></pc-element>
 </pc-entity>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-element>` elements using the [ElementComponentElement API](https://api.playcanvas.com/web-components/classes/ElementComponentElement.html).

--------------------------------------------------------------------------------

## <pc-entity>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-entity/

The `<pc-entity>` tag is used to define an entity.

:::note[Usage]

* It must be a direct child of [`<pc-scene>`](../pc-scene) or another `<pc-entity>`.
* It can have 0..n [`<pc-entity>`](../pc-entity) children.
* It can optionally have one of each component type as children: [`<pc-camera>`](../pc-camera), [`<pc-collision>`](../pc-collision), [`<pc-element>`](../pc-element), [`<pc-light>`](../pc-light), [`<pc-listener>`](../pc-listener), [`<pc-particles>`](../pc-particles), [`<pc-render>`](../pc-render), [`<pc-rigidbody>`](../pc-rigidbody), [`<pc-screen>`](../pc-screen), [`<pc-scripts>`](../pc-scripts), [`<pc-sounds>`](../pc-sounds), [`<pc-splat>`](../pc-splat).

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `enabled` | Boolean | `"true"` | Enabled state of the entity |
| `name` | String | - | Name identifier for the entity |
| `position` | Vector3 | `"0 0 0"` | Local-space position as "X Y Z" values |
| `rotation` | Vector3 | `"0 0 0"` | Local-space rotation as "X Y Z" Euler angles in degrees |
| `scale` | Vector3 | `"1 1 1"` | Local-space scale as "X Y Z" values |
| `tags` | String | - | Space-separated list of tags |

</div>

## Events

Listen to these events using [`addEventListener()`](https://developer.mozilla.org/en-US/docs/Web/API/EventTarget/addEventListener) or by assigning an event listener to the `oneventname` property of this interface.

| Event | Description |
| --- | --- |
| `pointerdown` | Fired when a pointer is pressed down on the entity. |
| `pointerenter` | Fired when a pointer enters the entity. |
| `pointerleave` | Fired when a pointer leaves the entity. |
| `pointermove` | Fired when a pointer is moved over the entity. |
| `pointerup` | Fired when a pointer is released from the entity. |

## Example

```html
<pc-entity name="MyEntity" position="1 2 3" rotation="45 0 0" scale="2 2 2" tags="tag1 tag2">
    
</pc-entity>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-entity>` elements using the [EntityElement API](https://api.playcanvas.com/web-components/classes/EntityElement.html).

--------------------------------------------------------------------------------

## <pc-light>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-light/

The `<pc-light>` tag is used to define a light component.

:::note[Usage]

* It must be a direct child of a [`<pc-entity>`](../pc-entity).

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `cast-shadows` | Flag | - | Whether the light casts shadows |
| `color` | Color | `"1 1 1"` | Light color as space-separated RGB values, hex code, or [named color](https://github.com/playcanvas/web-components/blob/main/src/colors.ts) |
| `enabled` | Boolean | `"true"` | Enabled state of the component |
| `inner-cone-angle` | Number | `"40"` | Inner cone angle in degrees (for spot lights) |
| `intensity` | Number | `"1"` | Light intensity multiplier |
| `normal-offset-bias` | Number | `"0.05"` | Normal offset bias for shadow rendering |
| `outer-cone-angle` | Number | `"45"` | Outer cone angle in degrees (for spot lights) |
| `range` | Number | `"10"` | Light range distance |
| `shadow-bias` | Number | `"0.2"` | Shadow depth bias |
| `shadow-distance` | Number | `"16"` | Maximum shadow rendering distance |
| `shadow-resolution` | Number | `"1024"` | Shadow map resolution |
| `shadow-type` | Enum | `"pcf3-32f"` | Shadow filtering: `"pcf1-16f"` \| `"pcf1-32f"` \| `"pcf3-16f"` \| `"pcf3-32f"` \| `"pcf5-16f"` \| `"pcf5-32f"` \| `"vsm-16f"` \| `"vsm-32f"` \| `"pcss-32f"` |
| `type` | Enum | `"directional"` | Light type: `"directional"` \| `"omni"` \| `"spot"` |
| `vsm-bias` | Number | `"0.01"` | Variance shadow map bias |

</div>

## Example

```html
<pc-entity>
    <pc-light type="directional" intensity="10" color="red" cast-shadows></pc-light>
</pc-entity>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-light>` elements using the [LightComponentElement API](https://api.playcanvas.com/web-components/classes/LightComponentElement.html).

--------------------------------------------------------------------------------

## <pc-listener>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-listener/

The `<pc-listener>` tag is used to define a listener component.

:::note[Usage]

* It must be a direct child of a [`<pc-entity>`](../pc-entity).

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `enabled` | Boolean | `"true"` | Enabled state of the component |

</div>

## Example

```html
<pc-entity>
    <pc-listener></pc-listener>
</pc-entity>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-listener>` elements using the [ListenerComponentElement API](https://api.playcanvas.com/web-components/classes/ListenerComponentElement.html).

--------------------------------------------------------------------------------

## <pc-model>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-model/

The `<pc-model>` tag is used to define an entity that instantiates a 3D model from a GLB file.

:::note[Usage]

* It must be a direct child of a [`<pc-scene>`](../pc-scene) or a [`<pc-entity>`](../pc-entity).

:::

## Attributes

All attributes of [`<pc-entity>`](../pc-entity) are also available.

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `asset` | String | - | Container asset ID (must reference a `container` type asset) |

</div>

## Example

```html
<pc-asset src="assets/car.glb" id="car"></pc-asset>
<pc-scene>
    <pc-model asset="car"></pc-model>
</pc-scene>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-model>` elements using the [ModelElement API](https://api.playcanvas.com/web-components/classes/ModelElement.html).

--------------------------------------------------------------------------------

## <pc-module>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-module/

The `<pc-module>` tag is used to load a WebAssembly module.

:::note[Usage]

* It must be a direct child of [`<pc-app>`](../pc-app).

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `fallback` | String | - | Path to fallback (asm.js) code when WebAssembly is not supported |
| `glue` | String | - | Path to the JavaScript glue code for the module |
| `name` | String | - | Module name used to reference it in scripts |
| `wasm` | String | - | Path to the WebAssembly (.wasm) file |

</div>

## Example

```html
<pc-app>
    
    <pc-module name="ammo" glue="ammo.wasm.js" wasm="ammo.wasm.wasm" fallback="ammo.js"></pc-module>
</pc-app>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-module>` elements using the [ModuleElement API](https://api.playcanvas.com/web-components/classes/ModuleElement.html).

--------------------------------------------------------------------------------

## <pc-particles>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-particles/

The `<pc-particles>` tag is used to define a particle system.

:::note[Usage]

* It must be a direct child of a [`<pc-entity>`](../pc-entity).

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `asset` | String | - | JSON asset ID defining the particle system configuration |
| `enabled` | Boolean | `"true"` | Enabled state of the component |

</div>

## Example

First define the particle system in JSON format:

```json title="snow.json"
{
    "numParticles": 100,
    "lifetime": 10,
    "rate": 0.1,
    "colorMapAsset": "snowflake",
    "emitterExtents": [ 15, 0, 10 ],
    "startAngle": 360,
    "startAngle2": -360,
    "alphaGraph": {
        "keys": [ 0, 0, 0.5, 0.5, 0.9, 0.9, 1, 0 ]
    },
    "rotationSpeedGraph": {
        "keys": [ 0, 100 ]
    },
    "rotationSpeedGraph2": {
        "keys": [ 0, -100 ]
    },
    "scaleGraph": {
        "keys": [ 0, 0.1 ]
    },
    "velocityGraph": {
        "keys": [
            [ 0, 0 ],
            [ 0, -0.7 ],
            [ 0, 0 ]
        ]
    },
    "velocityGraph2": {
        "keys": [
            [ 0, 0 ],
            [ 0, -0.4 ],
            [ 0, 0 ]
        ]
    }
}
```

Then add the particle system to your scene in HTML:

```html
<pc-app>
    <pc-asset src="assets/snowflake.png" id="snowflake"></pc-asset>
    <pc-asset src="assets/snow.json" id="snow"></pc-asset>
    <pc-scene>
        <pc-entity position="0 0 8">
            <pc-camera></pc-camera>
        </pc-entity>
        <pc-entity position="0 5 0">
            <pc-particles asset="snow"></pc-particles>
        </pc-entity>
    </pc-scene>
</pc-app>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-particles>` elements using the [ParticleSystemComponentElement API](https://api.playcanvas.com/web-components/classes/ParticleSystemComponentElement.html).

--------------------------------------------------------------------------------

## <pc-render>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-render/

The `<pc-render>` tag is used to define a render component that renders a 3D primitive.

:::note[Usage]

* It must be a direct child of a [`<pc-entity>`](../pc-entity).

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `cast-shadows` | Flag | - | Whether the component casts shadows |
| `enabled` | Boolean | `"true"` | Enabled state of the component |
| `receive-shadows` | Flag | - | Whether the component receives shadows |
| `type` | Enum | - | Primitive shape: `"box"` \| `"capsule"` \| `"cone"` \| `"cylinder"` \| `"plane"` \| `"sphere"` |

</div>

## Example

<CodePenEmbed id="NPKMrLy" title="<pc-render> example" />

## JavaScript Interface

You can programmatically create and manipulate `<pc-render>` elements using the [RenderComponentElement API](https://api.playcanvas.com/web-components/classes/RenderComponentElement.html).

--------------------------------------------------------------------------------

## <pc-rigidbody>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-rigidbody/

The `<pc-rigidbody>` tag is used to define a rigidbody component.

:::note[Usage]

* It must be a direct child of a [`<pc-entity>`](../pc-entity).
* It must be a sibling of a [`<pc-collision>`](../pc-collision) component.
* The ammo.js WebAssembly module must be loaded via a [`<pc-module>`](../pc-module) tag.

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `angular-damping` | Number | `"0"` | Angular velocity damping factor |
| `angular-factor` | Vector3 | `"1 1 1"` | Angular movement constraints as "X Y Z" values |
| `enabled` | Boolean | `"true"` | Enabled state of the component |
| `friction` | Number | `"0.5"` | Surface friction coefficient |
| `linear-damping` | Number | `"0"` | Linear velocity damping factor |
| `linear-factor` | Vector3 | `"1 1 1"` | Linear movement constraints as "X Y Z" values |
| `mass` | Number | `"1"` | Mass of the rigidbody in kilograms |
| `restitution` | Number | `"0"` | Bounce/elasticity coefficient (0-1) |
| `rolling-friction` | Number | `"0"` | Rolling resistance coefficient |
| `type` | Enum | `"static"` | Physics body type: `"static"` \| `"kinematic"` \| `"dynamic"` |

</div>

## Example

<CodePenEmbed id="XJrqjJr" title="<pc-rigidbody> example" />

## JavaScript Interface

You can programmatically create and manipulate `<pc-rigidbody>` elements using the [RigidBodyComponentElement API](https://api.playcanvas.com/web-components/classes/RigidBodyComponentElement.html).

--------------------------------------------------------------------------------

## <pc-scene>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-scene/

The `<pc-scene>` tag is used to define the scene.

:::note[Usage]

* It must be a direct child of [`<pc-app>`](../pc-app).

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `fog` | Enum | - | Fog type: `"linear"` \| `"exp"` \| `"exp2"` |
| `fog-color` | Color | - | Fog color as CSS color string or hex code |
| `fog-density` | Number | - | Fog density for exponential fog types |
| `fog-end` | Number | - | End distance for linear fog |
| `fog-start` | Number | - | Start distance for linear fog |

</div>

## Example

```html
<pc-app>
    <pc-scene>
        
    </pc-scene>
</pc-app>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-scene>` elements using the [SceneElement API](https://api.playcanvas.com/web-components/classes/SceneElement.html).

--------------------------------------------------------------------------------

## <pc-screen>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-screen/

The `<pc-screen>` tag is used to define a screen component.

:::note[Usage]

* It must be a direct child of a [`<pc-entity>`](../pc-entity).

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `blend` | Flag | - | Whether to enable alpha blending |
| `enabled` | Boolean | `"true"` | Enabled state of the component |
| `priority` | Number | `"0"` | Rendering priority (0-255) |
| `reference-resolution` | Vector2 | `"640 320"` | Reference resolution as "Width Height" values |
| `resolution` | Vector2 | `"640 320"` | Screen resolution as "Width Height" values |
| `scale-blend` | Number | `"0.5"` | Scale blending factor (0-1) |
| `screen-space` | Flag | - | Whether to render in screen space |

</div>

## Example

```html
<pc-entity>
    
    <pc-screen></pc-screen>
    
    <pc-entity>
        <pc-element type="text" asset="arial" text="Hello, World!"></pc-element>
    </pc-entity>
</pc-entity>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-screen>` elements using the [ScreenComponentElement API](https://api.playcanvas.com/web-components/classes/ScreenComponentElement.html).

--------------------------------------------------------------------------------

## <pc-script>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-script/

The `<pc-script>` tag is used to define a script.

:::note[Usage]

* It must be a direct child of a [`<pc-scripts>`](../pc-scripts) component.
* The script must have been loaded via the [`<pc-asset>`](../pc-asset) tag.

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `attributes` | String | `""` | JSON string of script attributes |
| `enabled` | Boolean | `"true"` | Enabled state of the script |
| `name` | String | - | Script name (must match the script's `scriptName` property) |

</div>

## Example

```html
<pc-entity>
    <pc-scripts>
        <pc-script name="myScript"></pc-script>
    </pc-scripts>
</pc-entity>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-script>` elements using the [ScriptElement API](https://api.playcanvas.com/web-components/classes/ScriptElement.html).

--------------------------------------------------------------------------------

## <pc-scripts>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-scripts/

The `<pc-scripts>` tag is used to define a scripts component.

:::note[Usage]

* It must be a direct child of a [`<pc-entity>`](../pc-entity).
* It can have 0..n [`<pc-script>`](../pc-script) children.

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `enabled` | Boolean | `"true"` | Enabled state of the component |

</div>

## Example

```html
<pc-entity>
    <pc-scripts>
        <pc-script name="myScript"></pc-script>
    </pc-scripts>
</pc-entity>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-scripts>` elements using the [ScriptComponentElement API](https://api.playcanvas.com/web-components/classes/ScriptComponentElement.html).

--------------------------------------------------------------------------------

## <pc-sky>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-sky/

The `<pc-sky>` tag is used to define a sky component.

:::note[Usage]

* It must be a direct child of a [`<pc-scene>`](../pc-scene).

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `asset` | String | - | Texture asset ID (must reference a `texture` type asset) |
| `center` | Vector3 | `"0 0.01 0"` | Sky center as "X Y Z" values (0-1 range) |
| `intensity` | Number | `"1"` | Sky brightness intensity |
| `level` | Number | `"0"` | Mipmap level to use for rendering |
| `rotation` | Vector3 | `"0 0 0"` | Sky rotation as "X Y Z" Euler angles |
| `scale` | Vector3 | `"100 100 100"` | Sky scale as "X Y Z" values |
| `type` | Enum | `"infinite"` | Sky type: `"box"` \| `"dome"` \| `"infinite"` \| `"none"` |

</div>

## Example

```html
<pc-asset id="skybox" src="assets/skybox.webp"></pc-asset>
<pc-scene>
    <pc-sky asset="skybox"></pc-sky>
</pc-scene>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-sky>` elements using the [SkyElement API](https://api.playcanvas.com/web-components/classes/SkyElement.html).

--------------------------------------------------------------------------------

## <pc-sound>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-sound/

The `<pc-sound>` tag is used to define a sound.

:::note[Usage]

* It must be a direct child of a [`<pc-sounds>`](../pc-sounds) component.

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `asset` | String | - | Audio asset ID (must reference an `audio` type asset) |
| `auto-play` | Flag | - | Whether the sound plays automatically |
| `duration` | Number | - | Duration of the sound in seconds |
| `loop` | Flag | - | Whether the sound loops |
| `name` | String | - | Name identifier for the sound slot |
| `overlap` | Flag | - | Whether sounds can overlap when triggered multiple times |
| `pitch` | Number | `"1"` | Pitch multiplier (1 = normal pitch) |
| `start-time` | Number | `"0"` | Start time offset in seconds |
| `volume` | Number | `"1"` | Volume level (0-1) |

</div>

## Example

```html
<pc-sounds>
    <pc-sound asset="music"></pc-sound>
</pc-sounds>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-sound>` elements using the [SoundElement API](https://api.playcanvas.com/web-components/classes/SoundElement.html).

--------------------------------------------------------------------------------

## <pc-sounds>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-sounds/

The `<pc-sounds>` tag is used to define a sound component.

:::note[Usage]

* It must be a direct child of a [`<pc-entity>`](../pc-entity).
* It can have 0..n [`<pc-sound>`](../pc-sound) children.

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `distance-model` | Enum | `"linear"` | Distance attenuation model: `"exponential"` \| `"inverse"` \| `"linear"` |
| `enabled` | Boolean | `"true"` | Enabled state of the component |
| `max-distance` | Number | `"10000"` | Maximum distance for audio falloff |
| `pitch` | Number | `"1"` | Pitch multiplier for all sounds in this component |
| `positional` | Flag | - | Whether the sound is positional (3D spatial audio) |
| `ref-distance` | Number | `"1"` | Reference distance for full volume |
| `roll-off-factor` | Number | `"1"` | Falloff rate factor for distance attenuation |
| `volume` | Number | `"1"` | Master volume for all sounds in this component |

</div>

## Example

```html
<pc-entity>
    <pc-sounds>
        <pc-sound asset="music"></pc-sound>
    </pc-sounds>
</pc-entity>
```

## JavaScript Interface

You can programmatically create and manipulate `<pc-sounds>` elements using the [SoundComponentElement API](https://api.playcanvas.com/web-components/classes/SoundComponentElement.html).

--------------------------------------------------------------------------------

## <pc-splat>

URL: https://developer.playcanvas.com/user-manual/web-components/tags/pc-splat/

The `<pc-splat>` tag is used to define a splat component for rendering 3D Gaussian Splats.

When rendering splat-based scenes, it is recommended to set `antialias` and `high-resolution` on your [`<pc-app>`](../pc-app) tag to `false` for best performance.

:::note[Usage]

* It must be a direct child of a [`<pc-entity>`](../pc-entity).

:::

## Attributes

<div className="attribute-table">

| Attribute | Type | Default | Description |
| --- | --- | --- | --- |
| `asset` | String | - | Gaussian splat asset ID (must reference a `gsplat` type asset) |
| `cast-shadows` | Flag | - | Whether the splat component casts shadows |
| `enabled` | Boolean | `"true"` | Enabled state of the component |
| `unified` | Flag | - | Enables global sorting and LOD streaming for the splat. Can only be set when the component is disabled. |

</div>

## Example

<CodePenEmbed id="MYgGZax" title="<pc-splat> example" />

## JavaScript Interface

You can programmatically create and manipulate `<pc-splat>` elements using the [SplatComponentElement API](https://api.playcanvas.com/web-components/classes/SplatComponentElement.html).

--------------------------------------------------------------------------------

## XR Support

URL: https://developer.playcanvas.com/user-manual/web-components/xr/

PlayCanvas Web Components make it easy to add Virtual Reality (VR) and Augmented Reality (AR) support to your applications.

## Basic Setup

To enable XR support, you'll need:

1. XR-specific scripts (provided by the [Engine NPM package](https://www.npmjs.com/package/playcanvas)).
2. A camera entity with the appropriate scripts attached.
3. UI for entering/exiting XR (WebXR requires a user gesture to start a session).
4. A secure context — serve your page over HTTPS (or `http://localhost` during development).

### XR Scripts

Specify the following scripts using [`<pc-asset>`](../tags/pc-asset) elements:

```html
<pc-asset src="/node_modules/playcanvas/scripts/esm/xr-controllers.mjs"></pc-asset>
<pc-asset src="/node_modules/playcanvas/scripts/esm/xr-navigation.mjs"></pc-asset>
```

Or, when using a CDN:

```html
<pc-asset src="https://cdn.jsdelivr.net/npm/playcanvas@latest/scripts/esm/xr-controllers.mjs"></pc-asset>
<pc-asset src="https://cdn.jsdelivr.net/npm/playcanvas@latest/scripts/esm/xr-navigation.mjs"></pc-asset>
```

:::note[CDN and import maps]

When loading XR scripts from a CDN, make sure your page’s import map also points the `playcanvas` module to the same CDN source and version as shown in the [Getting Started guide](getting-started.md). For production, consider pinning specific versions instead of `@latest`.

:::

* [`xr-controllers.mjs`](https://github.com/playcanvas/engine/blob/main/scripts/esm/xr-controllers.mjs) - Dynamically downloads and renders XR controller models (GLBs) for any detected XR controllers (including hands).
* [`xr-navigation.mjs`](https://github.com/playcanvas/engine/blob/main/scripts/esm/xr-navigation.mjs) - Implements basic teleportation navigation (via point and select actions).

### Camera Setup

The XR scripts require the scene's camera to be set up as follows:

```html

<pc-entity name="camera root">
    <pc-entity name="camera">
        <pc-camera></pc-camera>
    </pc-entity>
    <pc-scripts>
        <pc-script name="xrControllers"></pc-script>
        <pc-script name="xrNavigation"></pc-script>
    </pc-scripts>
</pc-entity>
```

### UI for Entering/Exiting XR

Finally, you'll need to add some UI controls to allow the user to enter and exit XR mode. This is a WebXR-specific requirement, where a user gesture is required to activate an XR session. Let's create two simple buttons to trigger either an AR or VR session.

```html
<button id="enterAR">Enter AR</button>
<button id="enterVR">Enter VR</button>
```

Let's add event listeners to the buttons to trigger an XR session when the user clicks them.

```javascript
document.addEventListener('DOMContentLoaded', async () => {
    const appElement = await document.querySelector('pc-app').ready();
    const app = appElement.app;
    const xr = app.xr;
    const camera = app.root.findComponent('camera');

    document.getElementById('enterAR').addEventListener('click', () => {
        xr.start(camera, 'immersive-ar', 'local-floor')
    });

    document.getElementById('enterVR').addEventListener('click', () => {
        xr.start(camera, 'immersive-vr', 'local-floor')
    });

    window.addEventListener('keydown', (event) => {
        if (event.key === 'Escape' && xr.active) {
            xr.end();
        }
    });
});
```

Most of the [Web Component examples](https://playcanvas.github.io/web-components/examples/) have integrated support for XR. Consult their source code to see how it's done.

## Next Steps

The PlayCanvas Engine has comprehensive XR support, with a wide range of features and options. For more information, see the [XR documentation](/user-manual/xr).

--------------------------------------------------------------------------------

## XR

URL: https://developer.playcanvas.com/user-manual/xr/

[Image: VR View]

PlayCanvas lets you create [AR](/user-manual/xr/ar/) (Augmented Reality) and [VR](/user-manual/xr/vr/) (Virtual Reality) applications for a variety of devices based on the new WebXR API, as well as through external integrations.

## Capabilities

Through extensions, WebXR is ever growing and various platforms are constantly implementing new and existing WebXR Modules. The PlayCanvas Engine provides access to these modules in the form of integrations, so they are easier to work with and work nicely with PlayCanvas' systems.

You can check a [list of currently supported modules](/user-manual/xr/capabilities/).

## Platforms

WebXR is a new API and it is being rolled out gradually to all major platforms. Up-to-date support can be checked on [caniuse.com](https://caniuse.com/webxr).

Additionally, support can be achieved with the [WebXR Polyfill](https://github.com/immersive-web/webxr-polyfill).

On **mobile**, WebXR works on Android with VR and AR session types.

On **HMDs**, such as Meta Quest, WebXR is well-supported for VR and AR session types. Apple Vision Pro currently supports VR session types when enabled in Safari settings.

On **desktop**, WebXR currently works in Chrome and Edge, and devices are linked through various native APIs, such as SteamVR, OpenXR, and others. This covers the majority of desktop-based VR devices and allows devices such as Meta Quest to be used via Steam Link.

## Testing WebXR without an XR Device

You do not need to have XR hardware to start developing with WebXR. You can install the [Immersive Web Emulator](https://chromewebstore.google.com/detail/immersive-web-emulator/cgffilbpcibhmcfbgggfhfolhkfbhmik) Chrome extension which emulates the WebXR API. It allows you to simulate various head-mounted displays and controllers via the browser's Dev Tools.

:::danger

Do not use the [WebXR API Emulator](https://chromewebstore.google.com/detail/webxr-api-emulator/mjddjgeghkdijejnciaefnkjmkafnnje) Chrome extension. It is not compatible with PlayCanvas. PlayCanvas applications will throw an exception if it is active.

:::

## Getting Started with WebXR

To start an XR session, support and availability should be checked first. Then, on a user interaction, a session can be started:

```javascript
button.element.on('click', () => {
    // check if XR is supported and VR is available
    if (app.xr.supported && app.xr.isAvailable(pc.XRTYPE_VR)) {
        // start VR session providing a camera component
        app.xr.start(entity.camera, pc.XRTYPE_VR, pc.XRSPACE_LOCALFLOOR);
    }
});
```

--------------------------------------------------------------------------------

## AR

URL: https://developer.playcanvas.com/user-manual/xr/ar/

PlayCanvas lets you create Augmented Reality (AR) applications for mobile and HMD devices through the WebXR API, as well as through integrations with popular AR frameworks.

## Platforms

AR capabilities are available using the Android Chrome Browser, Meta Quest Browser, Magic Leap Helio, Samsung Internet, Microsoft Edge and many others. Additionally, frameworks such as [8th Wall](/user-manual/xr/ar/8th-wall-integration/) and [Zappar](/user-manual/xr/ar/zappar-integration/) allow users to experience AR content in most mobile browsers, including Safari.

:::note

The aforementioned frameworks are external and have separate licensing, available on their respective websites.

:::

## Getting started with WebXR AR

When using PlayCanvas’ built-in AR support, the scene’s primary camera’s clear color must be transparent, as so:

[Image: Transparent Clear Color]

To start an AR session, device support and availability should also be checked first. Then, on user interaction, such as a button click or other input, an AR session can be started:

```javascript
button.element.on('click', () => {
    // check if XR is supported and AR is available
    if (app.xr.supported && app.xr.isAvailable(pc.XRTYPE_AR)) {
        // start AR using a camera component
        entity.camera.startXr(pc.XRTYPE_AR, pc.XRSPACE_LOCALFLOOR);
    }
});
```

Once the user is done, the AR session can be exited by calling:

```javascript
app.xr.end();
```

Augmented Reality allows blending of the real world with virtual imagery, either by composing a rendered buffer over the camera feed or with a special projection of a rendered buffer on see-through glasses. Usually, it is provided with spatial tracking relative to the real-world environment.

Suitable reference space: `pc.XRSPACE_LOCALFLOOR`.

## Starter Kits

PlayCanvas provides several ‘Starter Kit’ projects to help you and your AR experiences get up and running faster. When creating a new project, simply select from the dialog the template that suits you best.

--------------------------------------------------------------------------------

## 8th Wall Integration

URL: https://developer.playcanvas.com/user-manual/xr/ar/8th-wall-integration/

[8th Wall](https://www.8thwall.com/) is a platform that offers tools to create interactive augmented reality experiences through the web. It has a reach of over 3.5B users, and supports World tracking, Image tracking and Face tracking. In 2018, it added full integration with PlayCanvas.

## Getting started with 8th Wall

8th Wall’s documentation provides a [Getting Started with PlayCanvas](https://www.8thwall.com/docs/legacy/api/playcanvas/getting-started/) page, and they also provide a number of [Starter Projects](https://playcanvas.com/user/the8thwall) to help new users get started.

--------------------------------------------------------------------------------

## Anchors

URL: https://developer.playcanvas.com/user-manual/xr/ar/anchors/

Anchors provide the ability to create a point in 3D space that can be updated to match an ever-evolving understanding of the real world by the underlying AR system. This allows for the placement of virtual objects in relation to the real world that feel planted in the user's environment.

Each anchor is represented as a position and orientation and can be created from an arbitrary point as well as in relation to a hit test result that will make it more reliable.

### Using Anchors {#using-anchors}

To start using anchors, when a session is requested, a flag should be provided to the session:

```javascript
app.xr.start(camera, pc.XRTYPE_AR, pc.XRSPACE_LOCALFLOOR, {
    anchors: true
});
```

## Support {#support}

You can check if anchors are supported by the system:

```javascript
if (app.xr.anchors.supported) {
    // anchors are supported
}

app.xr.on('start', () => {
    if (app.xr.anchors.available) {
        // anchors are supported and available
    }
});
```

## Creating Anchors {#creating-anchors}

Then you can create an anchor, e.g. using an arbitrary position and rotation:

```javascript
app.xr.anchors.create(position, rotation, (err, anchor) => {
    if (!err) {
        // new anchor has been created
    }
});
```

Or for more reliable tracking, an anchor can be created from the [Hit Test Result](/user-manual/xr/ar/hit-testing/#anchors).

## Anchor {#anchor}

Each anchor has its position and rotation and can be updated at any point. When an anchor is updated, the application developer should update related virtual objects accordingly.

Anchors can be added and removed dynamically during the session:

```javascript
app.xr.anchors.on('add', (anchor) => {
    const entity = new pc.Entity();

    // add a cone for an anchor
    entity.addComponent('render', { type: 'cone' });
    entity.setLocalScale(0.1, 0.1, 0.1); // 10cm diameter
    app.root.addChild(entity);

    // transform
    entity.setLocalPosition(anchor.getPosition());
    entity.setLocalRotation(anchor.getRotation());
    entity.translateLocal(0, 0.05, 0); // offset cone

    // update cone when anchor changes
    anchor.on('change', () => {
        entity.setLocalPosition(anchor.getPosition());
        entity.setLocalRotation(anchor.getRotation());
        entity.translateLocal(0, 0.05, 0); // offset cone
    });

    // remove cone when anchor is destroyed
    anchor.once('destroy', () => {
        entity.destroy();
    });
});
```

## Persistence {#persistence}

Anchor persistence provides a way to remember anchors between sessions, with a limited number of anchors per origin. This allows applications to place virtual objects in relation to the real-world geometry and remain there between sessions.

You can check if persistence is supported:

```javascript
if (app.xr.anchors.persistence) {
    // application can persist anchors
}
```

Each anchor can have a UUID that allows it to be referenced and restored between sessions.

You can access a list of persistent anchors and restore them on session start:

```javascript
app.xr.on('start', () => {
    const uuids = app.xr.anchors.uuids;
    for(let i = 0; i < uuids.length; i++) {
        app.xr.anchors.restore(uuids[i]);
    }
});
```

To manage individual anchor persistence, you can use `persist` and `forget` methods:

```javascript
anchor.persist((err, uuid) => {
    if (uuid) {
        // anchor has been persisted
    }
});
```

```javascript
if (anchor.persistent) {
    anchor.forget((err) => {
        if (!err) {
            // anchor is forgotten
        }
    });
}
```

--------------------------------------------------------------------------------

## Camera Color

URL: https://developer.playcanvas.com/user-manual/xr/ar/camera-color/

In AR, the rendered image is projected over the reconstructed camera texture on the pass-through device types. This texture can be accessed by the application.

To request access to the camera color, the session should be started with an extra flag:

```javascript
app.xr.start(camera, pc.XRTYPE_AR, pc.XRSPACE_LOCALFLOOR, {
    cameraColor: true
});
```

## Support

You can check if the camera color is supported by the system:

```javascript
if (app.xr.views.supportedColor) {
    // camera color access is supported
}

app.xr.on('start', () => {
    if (app.xr.views.availableColor) {
        // camera color texture is available
    }
});
```

## Texture

WebXR can work on monoscopic as well as stereoscopic devices. This means there is a list of Views that represent either a screen (monoscopic device) or an eye (stereoscopic device).

Bear in mind that Views are not available on session start, and can be created/removed during the session's lifetime.

For a monoscopic device, we can access its view and its texture:

```javascript
app.xr.on('start', () => {
    app.xr.views.on('add', (view) => {
        if (view.eye === pc.XREYE_NONE) { // monoscopic view
            if (view.textureColor) {
                // camera color texture is available
            }
        }
    });
});
```

--------------------------------------------------------------------------------

## Depth Sensing

URL: https://developer.playcanvas.com/user-manual/xr/ar/depth-sensing/

In the MR context, immersion is achieved by visual and logical interaction of virtual objects with the real world. This is achieved by many techniques including Depth Occlusion, particles interacting with the world, 3D scanning and more.

Depth sensing provides access to depth estimation of real-world objects in real-time. Underlying systems might have different methods of estimation such as Lidar hardware or Computer Vision, which provide various levels of quality and reliability.

WebXR Depth Sensing provides access to depth information for each view and matches the color information. Various browsers might implement two paths: CPU and GPU, with various performance impacts depending on the path. PlayCanvas integrates an API abstracting away the differences as much as possible, e.g. texture is available for both CPU and GPU paths.

Platforms might implement either path: CPU or GPU, or even both.

To request access to camera depth, the session should be started as follows:

```javascript
app.xr.start(camera, pc.XRTYPE_AR, pc.XRSPACE_LOCALFLOOR, {
    depthSensing: { // request access to camera depth
        usagePreference: pc.XRDEPTHSENSINGUSAGE_GPU, // prefer GPU implementation
        dataFormatPreference: pc.XRDEPTHSENSINGFORMAT_F32 // prefer data as Float 32 array/texture
    }
});
```

## Support

You can check if camera depth is supported by the system:

```javascript
if (app.xr.views.supportedDepth) {
    // camera depth access is supported
}

app.xr.on('start', () => {
    if (app.xr.views.availableDepth) {
        // camera depth information is available

        if (app.xr.views.depthGpuOptimized) {
            // GPU path
        } else {
            // CPU path
        }
    }
});
```

## Distance Measurements

Depth estimation and availability of the data is subject to the reliability of the underlying AR system, so depth information might not be always available.

WebXR supports it only for the CPU-path. Using Depth Sensing, you can measure the distance by providing U and V, which are 0 to 1 coordinates of a screen (left-right and top-bottom).

```javascript
// get monoscopic view (mobile screens)
const view = app.xr.views.get(pc.XREYE_NONE);
if (view) {
    // get distance from the middle of a screen
    const distance = view.getDepth(0.5, 0.5);

    if (distance !== null) {
        // distance is in meters
    }
}
```

## Texture

You can access a texture of the depth. PlayCanvas augments the different CPU/GPU paths and provides one texture that can be an array texture in the case of stereoscopic screens (e.g. HMDs).

Accessing the texture:

```javascript
const view = app.xr.views.list[0];
if (view) {
    const texture = view.textureDepth;

    if (texture) {
        // get global uniform
        const scopeDepthMap = app.graphicsDevice.scope.resolve('depthMap');
        // set uniform
        scopeDepthMap.setValue(texture);
    }
}
```

### Stereo Views

When using the depth texture in the shader, depending on a monoscopic or stereoscopic scenario, a different approach should be used. This can be implemented by a `#define` in the shader:

```javascript
const view = app.xr.views.list[0];
if (view && view.eye !== pc.XREYE_NONE) {
    // add define for stereo views
    fragShader = '#define XRDEPTH_ARRAY\n' + fragShader;
}
```

### Data Format

WebXR can provide depth sensing data in two formats: F32 (array of 32-bit floats) and packed as LA8 (flat array of pairs of 8-bit values). They do provide slightly different precision: 32 vs 16 bits for depth, but even 16 bits is plenty for close-proximity use.

We can use shader branching to unpack depth values from a texture depending on the format:

```javascript
if (app.xr.views.depthPixelFormat === pc.PIXELFORMAT_R32F) {
    fragShader = '#define XRDEPTH_FLOAT\n' + fragShader;
}
```

### UV Normalization

WebXR might provide the texture rotated and flipped in any combination, so normalization should be implemented using the provided matrix. We can set this matrix like so:

```javascript
// get a global uniform scope
const scopeDepthUvMatrix = app.graphicsDevice.scope.resolve('matrix_depth_uv');
// set UV normalization matrix
scopeDepthUvMatrix.setValue(view.depthUvMatrix.data);
```

## Shader

With all the preparation we can cover mono/stereo scenarios and different texture formats in a single shader:

```glsl
uniform vec4 uScreenSize; // provided by the engine
uniform mat4 matrix_depth_uv;

#ifdef XRDEPTH_ARRAY
    uniform int view_index; // provided by the engine
    uniform highp sampler2DArray depthMap;
#else
    uniform sampler2D depthMap;
#endif

void main (void) {
    // construct UV for screen-space
    vec2 uvScreen = gl_FragCoord.xy * uScreenSize.zw;

    #ifdef XRDEPTH_ARRAY
        // stereo
        // modify screen-space based on view_index (left/right eye)
        uvScreen = uvScreen * vec2(2.0, 1.0) - vec2(view_index, 0.0);
        // normalize UV using provided matrix
        vec2 uvNormalized = (matrix_depth_uv * vec4(uvScreen.xy, 0.0, 1.0)).xy;
        // use view_index for array-texture index
        vec3 uv = vec3(uvNormalized, view_index);
    #else
        // mono
        // flip it vertically and normalize
        vec2 uv = (matrix_depth_uv * vec4(uvScreen.x, 1.0 - uvScreen.y, 0.0, 1.0)).xy;
    #endif

    #ifdef XRDEPTH_FLOAT
        // F32
        float depth = texture2D(depthMap, uv).r;
    #else
        // LA8
        vec2 packedDepth = texture2D(depthMap, uv).ra;
        // unpack from AlphaLuminance (2 floats) to a single float
        float depth = dot(packedDepth, vec2(255.0, 256.0 * 255.0));
    #endif

    // normalize to meters
    depth *= depth_raw_to_meters;

    // render as greyscale, darker - closer, lighter - further
    gl_FragColor = vec4(depth, depth, depth, 1.0);
}
```

## Example

You can check out [this example](https://playcanvas.github.io/#/xr/ar-camera-depth) that renders a quad in front of a camera with depth sensing applied with a similar shader as described above.

--------------------------------------------------------------------------------

## DOM Overlay

URL: https://developer.playcanvas.com/user-manual/xr/ar/dom-overlay/

When using mono screen displays for AR sessions, you can use regular HTML and CSS for the UI. This API provides the ability to overlay DOM elements over the AR application screen.

Before starting the AR session, you need to provide an element as a root to the DOM Overlay:

```javascript
app.xr.domOverlay.root = element;
app.xr.start(camera, pc.XRTYPE_AR, pc.XRSPACE_LOCALFLOOR);
```

## Support

You can check if the DOM Overlay is supported:

```javascript
if (app.xr.domOverlay.supported) {
    // DOM Overlay is supported
}

app.xr.on('start', () => {
    if (app.xr.domOverlay.available) {
        // DOM Overlay is available
    }
});
```

## Events

You can interact with elements just like with regular HTML. However, the input source's `select` event will still fire in an application. To prevent input source events from passing through DOM elements, you can intercept them:

```javascript
const buttons = app.xr.domOverlay.querySelectorAll('button');

for (let i = 0; i < buttons.length; i++) {
    buttons[i].addEventListener('beforexrselect', (evt) => {
        evt.preventDefault();
    });
}
```

--------------------------------------------------------------------------------

## Hit Testing

URL: https://developer.playcanvas.com/user-manual/xr/ar/hit-testing/

In an AR/MR context, the interaction of virtual world objects with the real world is achieved by many techniques. Hit Testing allows one to pick a point in space by probing real-world geometry that is estimated using the underlying AR system.

This can be used in various ways and one of the most common is placing a virtual object in space so that it feels planted in the real world.

## Reliability {#reliability}

Hit Testing is implemented by an underlying system building an understanding of ever-evolving real-world geometry. Some platforms rely on pre-captured information, while some platforms estimate geometry in real-time based on Computer Vision techniques. So the reliability of hit testing is subject to the capabilities of the underlying system.

## Support {#support}

You can check if hit testing is supported by the system:

```javascript
if (app.xr.hitTest.supported) {
    // hit testing is supported
}
```

## Hit Test Source {#hit-test-source}

When we want to start hit testing, we issue a request and are provided with a `HitTestSource` instance, which then provides results in the form of events. We can manage the lifetime of the source that way.

The most basic way is to start probing straight from the viewer forward vector:

```javascript
// start a hit test
app.xr.hitTest.start({
    spaceType: pc.XRSPACE_VIEWER, // from a viewer space
    callback: (err, hitTestSource) => {
        if (err) return;
        // subscribe to hit test results
        hitTestSource.on('result', (position, rotation) => {
            // position and rotation of hit test result
            // based on a ray facing forward from the viewer reference space
        });
    }
});
```

We can stop hit testing by removing its source:

```javascript
hitTestSource.remove();
```

## Monoscope (Touch Screen) {#monoscope-touch-screen}

When an XR session is started on a monoscopic device (such as a mobile phone with a touch screen), then it is possible to start hit tests based on user touches on the screen:

```javascript
app.xr.hitTest.start({
    profile: 'generic-touchscreen', // touch screen input sources
    callback: (err, hitTestSource) => {
        if (err) return;
        hitTestSource.on('result', (position, rotation, inputSource) => {
            // position and rotation of hit test result
            // that will be created from touch on mobile devices
        });
    }
});
```

Bear in mind that transient input sources (such as touch) do not provide hit test results straight away, as the hit test source is created as an asynchronous operation and its results are a subject of the underlying system being able to provide such information. This means that a touch might not provide any hit test results within its short-lived time.

## Input Source {#input-source}

The most common way to start hit testing is from a ray of an input source (e.g. controllers or hands):

```javascript
inputSource.hitTestStart({
    callback: (err, hitTestSource) => {
        if (err) return;
        hitTestSource.on('result', (position, rotation) => {
            // position and rotation of a hit test result
            // based on a ray of an input source
        });
    }
});
```

## Arbitrary Ray {#arbitrary-ray}

It is also possible to start a hit testing using a custom ray with the origin point and a direction:

```javascript
const ray = new pc.Ray();

ray.origin.set(0, 1, 0); // start 1 meter above the origin
ray.direction.set(0, -1, 0); // point downwards

app.xr.hitTest.start({
    spaceType: pc.XRSPACE_LOCALFLOOR,
    offsetRay: ray,
    callback: (err, hitTestSource) => {
        // hit test source that will sample real world geometry
        // from the position where AR session started
    }
});
```

## Anchors {#anchors}

Hit tests are performed against the estimation of real-world geometry and the geometry can change if the underlying system estimation process refines the planes, meshes, or points that were hit by the hit test. For that reason, an [Anchor](/user-manual/xr/ar/anchors/) can be created from these hit tests, which then can be updated. This allows a more planted and reliable placement of virtual objects:

```javascript
// subscribe to hit test results
hitTestSource.on('result', (position, rotation, inputSource, hitTestResult) => {
    // create an anchor using a hit test result
    app.xr.anchors.create(hitTestResult, (err, anchor) => {
        if (!err) {
            // a new anchor has been created
        }
    });
});
```

--------------------------------------------------------------------------------

## Image Tracking

URL: https://developer.playcanvas.com/user-manual/xr/ar/image-tracking/

Image Tracking provides the ability to track real-world images using the provided image samples and their estimated size. The underlying CV system will estimate image position and orientation and tracking status.

## Support

You can check if image tracking is supported by the system:

```javascript
if (app.xr.imageTracking.supported) {
    // image tracking is supported
}

app.xr.on('start', () => {
    if (app.xr.imageTracking.available) {
        // image tracking is supported and available
        // it can still be false if images were not provided
    }
});
```

## Images

Images are provided **before the session starts** with their real-world width (in meters). Images can be in any web-friendly format and should match real-world images as closely as possible.

**The resolution** should be at least 300x300 pixels. High resolution does **not** improve tracking performance and/or reliability.

**The color** is irrelevant, so for download size optimization, grayscale images are preferred.

**Repeating patterns** or too many geometric features will reduce tracking reliability.

## Add/Remove Tracked Images

You can modify the list of tracked images only when the XR session is not running.

Adding an image to the tracking list:

```javascript
// image that is 20cm wide (0.2m)
const trackedImage = app.xr.imageTracking.add(image, 0.2);
```

Removing a tracked image:

```javascript
app.xr.imageTracking.remove(trackedImage);
```

And you can access a list of tracked images like so:

```javascript
const trackedImages = app.xr.imageTracking.images;
for (let i = 0; i < trackedImages.length; i++) {
    const trackedImage = trackedImages[i];
}
```

## Position & Rotation

A tracked image's position and rotation (in world-space) are updated automatically and you can access the most recent information like so:

```javascript
const position = trackedImage.getPosition();
const rotation = trackedImage.getRotation();
```

## Reliability

Image Tracking is implemented using Computer Vision techniques that are running over the camera feed, which is subject to noise, unstable illumination, view angle, occlusion, motion blur, and more aspects of reality. The underlying system provides some details about its tracking state.

Check if the image is trackable in the first place:

```javascript
if (!trackedImage.trackable) {
    // it could be too small, or the underlying system is unable to parse the image
}
```

When a session starts, if the underlying system is unable to use some images, the relevant error messages will be passed:

```javascript
app.xr.imageTracking.on('error', (err) => {
    console.warn(err.message);
});
```

## Tracking State

You can check if an image is actively tracked right now:

```javascript
if (trackedImage.tracking) {
    // actively tracked
}
```

When tracking becomes unavailable, an image's position and rotation will be emulated, assuming that the real-world image has not been moved:

```javascript
if (trackedImage.emulated) {
    // position and rotation is emulated
    // based on previously known tracking information
}
```

It is possible to subscribe to events to know when an image becomes tracked or loses active tracking:

```javascript
trackedImage.on('tracked', () => {
    // image is now actively tracked
});

trackedImage.on('untracked', () => {
    // image is no longer actively tracked
});
```

--------------------------------------------------------------------------------

## Light Estimation

URL: https://developer.playcanvas.com/user-manual/xr/ar/light-estimation/

In AR, the real world can have complex illumination and various environments. For better immersion and the ability to blend between the real and the virtual world, virtual objects can be shaded and illuminated based on Light Estimation data, such as:

* **Directional light** (the most prominent), its rotation, intensity and color.
* **Ambient light** in the form of L3 spherical harmonics.
* **Reflections** in the form of a cube map ([currently not integrated](https://github.com/playcanvas/engine/issues/6070)).

## Support

You can check if light estimation is supported by the system:

```javascript
if (app.xr.lightEstimation.supportedColor) {
    // light estimation access is supported
}

app.xr.lightEstimation.on('available', () => {
    // light estimation becomes available
});
```

## Directional Light

The most basic information that light estimation provides is the most prominent directional light rotation, intensity and color:

```javascript
const lightEstimation = app.xr.lightEstimation;

// check if light estimation is available
if (lightEstimation.available) {
    // rotate entity
    entity.setRotation(lightEstimation.rotation());

    // set light parameters
    entity.light.intensity = lightEstimation.intensity;
    entity.light.color = lightEstimation.color;
}
```

## Ambient Light

As the environment is usually much more complex than a single directional light, light estimation provides ambient light information in the form of L3 SH (spherical harmonics).

To use SH, the material either has a prefiltered cube map applied (scene skybox works also), or the constant ambient shader chunk (`ambientConstantPS`) should be updated.

You can set SH data per material:

```javascript
if (app.xr.lightEstimation.available) {
    material.setParameter('ambientSH[0]', app.xr.lightEstimation.sphericalHarmonics);
}
```

If there is no prefiltered cube map or skybox on the scene, you can update the material chunk:

```javascript
material.chunks.ambientConstantPS = chunkCode;
material.update();
```

Shader chunk code:

```glsl
uniform vec3 ambientSH[9];

void addAmbient(vec3 worldNormal) {
    vec3 n = worldNormal;

    vec3 color =
        ambientSH[0] +
        ambientSH[1] * n.x +
        ambientSH[2] * n.y +
        ambientSH[3] * n.z +
        ambientSH[4] * n.x * n.z +
        ambientSH[5] * n.z * n.y +
        ambientSH[6] * n.y * n.x +
        ambientSH[7] * (3.0 * n.z * n.z - 1.0) +
        ambientSH[8] * (n.x * n.x - n.y * n.y);

    dDiffuseLight += color;
}
```

## Reflections

WebXR Light Estimation provides an estimation of the environment reflection in form of a cube map, but [at the moment](https://github.com/playcanvas/engine/issues/6070) it is not integrated into PlayCanvas Engine.

--------------------------------------------------------------------------------

## Mesh Detection

URL: https://developer.playcanvas.com/user-manual/xr/ar/mesh-detection/

Interaction between real-world and virtual objects is achieved via visual and logical interactions between the two. Mesh detection is an API that provides access to the representation of real-world geometry in the form of meshes. It can be used in a number of ways such as:

* Virtual object physics within a real-world environment
* Path finding
* Object placement
* Occlusion
* Procedural effects

This API provides a list of meshes, their geometry, transformation and semantic labeling.

The underlying system can provide pre-captured data as well as provide real-time reconstruction depending on the underlying system implementation.

## Support

```javascript
if (app.xr.meshDetection.supported) {
    // mesh detection is supported
}

app.xr.on('start', () => {
    if (app.xr.meshDetection.available) {
        // mesh detection is available
    }
});
```

## Access

A feature flag needs to be added to the session start:

```javascript
app.xr.start(camera, pc.XRTYPE_AR, pc.XRSPACE_LOCALFLOOR, {
    meshDetection: true
});
```

Meshes are added/removed asynchronously:

```javascript
app.xr.meshDetection.on('add', (xrMesh) => {
    // a new XrMesh has been added

    xrMesh.once('remove', () => {
        // an XrMesh has been removed
    });
});
```

Also, the list of XrMeshes is available:

```javascript
const xrMeshes = app.xr.meshDetection.meshes;
for (let i = 0; i < xrMeshes.length; i++) {
    const xrMesh = xrMeshes[i];
}
```

## Mesh

Each mesh is represented as an instance of XrMesh, which can be added/removed during an active session. It also has data that can be changed during its lifetime.

You can access the position and rotation (world-space) of an XrMesh:

```javascript
entity.setPosition(xrMesh.getPosition());
entity.setRotation(xrMesh.getRotation());
```

Each XrMesh has its vertices and indices (in local-space), that can be used to construct a visual mesh. An example below creates a visual mesh for each XrMesh and adds it to the root of the scene:

```javascript
app.xr.meshDetection.on('add', (xrMesh) => {
    // geometry data
    const mesh = new pc.Mesh(app.graphicsDevice);
    mesh.clear(true, true); // ensure that mesh is created with dynamic buffers
    mesh.setPositions(xrMesh.vertices); // set vertices
    mesh.setNormals(pc.calculateNormals(xrMesh.vertices, xrMesh.indices)); // calculate normals
    mesh.setIndices(xrMesh.indices); // set indices
    mesh.update(pc.PRIMITIVE_TRIANGLES); // update buffers

    const material = new pc.StandardMaterial();
    const meshInstance = new pc.MeshInstance(mesh, material);

    const entity = new pc.Entity();

    // add render component
    entity.addComponent('render', {
        meshInstances: [ meshInstance ]
    });

    // add entity to the scene root
    app.root.addChild(entity);

    // clean up after XrMesh is removed
    xrMesh.once('remove', () => {
        material.destroy();
        mesh.destroy();
        entity.destroy();
    });
});
```

## Semantic Label

XrMesh can represent various real-world objects and a label can help to identify what it represents using its property `XrMesh.label`.

These labels can be any of: `floor`, `wall`, `door`, `window`, `table`, `screen`, `global mesh`, `other`, and `more`. Here is a [list of semantic labels](https://github.com/immersive-web/semantic-labels/blob/master/labels.json), although this list is not definitive and the platform can report anything it feels fit.

## Changes

Depending on the underlying system capabilities, the XrMesh geometry can change while an XR session is active. You can subscribe to that event and update a visual mesh accordingly:

```javascript
xrMesh.on('change', () => {
    // vertices, indices and/or label has been changed
});
```

--------------------------------------------------------------------------------

## Plane Detection

URL: https://developer.playcanvas.com/user-manual/xr/ar/plane-detection/

Very similar to [Mesh Detection](/user-manual/xr/ar/mesh-detection/), Plane Detection provides access to planes that estimate real-world surfaces.

Each plane can represent a surface with optionally available labels, such as: `wall`, `floor`, `table`, etc.

The underlying system can provide pre-captured data as well as provide real-time reconstruction depending on the underlying system implementation.

## Support

```javascript
if (app.xr.planeDetection.supported) {
    // plane detection is supported
}

app.xr.on('start', () => {
    if (app.xr.planeDetection.available) {
        // plane detection is available
    }
});
```

## Access

A feature flag needs to be added to the session start:

```javascript
app.xr.start(camera, pc.XRTYPE_AR, pc.XRSPACE_LOCALFLOOR, {
    planeDetection: true
});
```

Then planes are added/removed asynchronously:

```javascript
app.xr.planeDetection.on('add', (xrPlane) => {
    // a new XrPlane has been added

    xrPlane.once('remove', () => {
        // an XrPlane has been removed
    });
});
```

Also, the list of XrPlanes is available:

```javascript
const xrPlanes = app.xr.planeDetection.meshes;
for (let i = 0; i < xrPlanes.length; i++) {
    const xrPlane = xrPlanes[i];
}
```

## Mesh

Each plane is represented as an instance of XrPlane which can be added/removed during an active session. It also has data that can be changed during its lifetime.

You can access the position and rotation (world-space) of an XrPlane:

```javascript
entity.setPosition(xrPlane.getPosition());
entity.setRotation(xrPlane.getRotation());
```

Each XrPlane has its points (in local-space) and orientation, that can be used to construct a visual mesh. The list of points represents the vertices of a polygon perimeter.

An `xrPlane.orientation` provides information as to whether a plane is vertical, horizontal or anything else.

An example below creates a visual mesh for each XrPlane and adds it to the scene root:

```javascript
app.xr.planeDetection.on('add', (xrPlane) => {
    // geometry data
    const mesh = new pc.Mesh(app.graphicsDevice);
    mesh.clear(true, true); // ensure that mesh is created with dynamic buffers

    // create a list of vertices
    const vertices = new Float32Array((xrPlane.points.length + 1) * 3);

    // first point is always in the origin
    vertices[0] = 0;
    vertices[1] = 0;
    vertices[2] = 0;

    // create a list of indices
    const indices = new Uint32Array(xrPlane.points.length * 3);

    // construct a polygon where each edge is connected to the origin of a mesh
    for (let i = 0; i < xrPlane.points.length; i++) {
        vertices[i * 3 + 3 + 0] = xrPlane.points[i].x;
        vertices[i * 3 + 3 + 1] = xrPlane.points[i].y;
        vertices[i * 3 + 3 + 2] = xrPlane.points[i].z;
        indices[i * 3 + 0] = 0;
        indices[i * 3 + 1] = i + 1;
        indices[i * 3 + 2] = ((i + 1) % xrPlane.points.length) + 1;
    }

    mesh.setPositions(vertices); // set vertices
    mesh.setNormals(pc.calculateNormals(vertices, indices)); // calculate normals
    mesh.setIndices(indices); // set indices
    mesh.update(pc.PRIMITIVE_TRIANGLES); // update buffers

    const material = new pc.StandardMaterial();
    const meshInstance = new pc.MeshInstance(mesh, material);

    const entity = new pc.Entity(xrPlane.label);

    // add render component
    entity.addComponent('render', {
        meshInstances: [ meshInstance ]
    });

    // add entity to the scene root
    app.root.addChild(entity);

    // clean up after XrPlane is removed
    xrPlane.once('remove', () => {
        material.destroy();
        mesh.destroy();
        entity.destroy();
    });
});
```

## Semantic Label

An XrPlane can represent various real-world objects and a label can help to identify what it represents using its property `XrPlane.label`.

These labels can be any of: `floor`, `wall`, `door`, `window`, `table`, `screen`, `global mesh`, `other` and more. Here is a [list of semantic labels](https://github.com/immersive-web/semantic-labels/blob/master/labels.json), although this list is not definitive and the platform can report anything it feels fit.

## Changes

Depending on the underlying system capabilities, XrPlane attributes can change while an XR session is active. You can subscribe to that event and update a visual mesh accordingly:

```javascript
xrPlane.on('change', () => {
    // points and/or label have been changed
});
```

--------------------------------------------------------------------------------

## Zappar Integration

URL: https://developer.playcanvas.com/user-manual/xr/ar/zappar-integration/

Zappar offers an [Universal AR SDK with PlayCanvas integration](https://zap.works/universal-ar/playcanvas/), including image, face and instant world tracking features. Their SDK supports Chrome for Android and Safari 11.3+ for iOS, reaching billions of users worldwide.

## Getting started with Zappar

Zappar provides a [Getting Started with PlayCanvas](https://docs.zap.works/universal-ar/playcanvas/getting-started/) page, and they also provide a number of [Starter Projects](https://playcanvas.com/user/zappar) to help new users get started, as well as template projects to get users up and running faster.

To get started with Zappar’s Universal AR SDK, first head to their [Zappar Universal AR Project](https://playcanvas.com/project/797342/overview/instant-tracking--particles) and fork it.

[Image: Zappar Universal AR]

Once there, add one of the Tracking Templates provided. Zappar provides:

- Face Tracking
- Image Tracking
- Instant Tracking

Drag any of these into your scene, ensure there are no other cameras and you are ready to go! If necessary, tweak the attributes of the script attached to the template camera:

[Image: Zappar AR Camera Settings]

After these steps, you should be able to use Zappar's SDK without issues. For further information, refer to their [Getting Started With PlayCanvas](https://docs.zap.works/universal-ar/playcanvas/getting-started/) docs.

--------------------------------------------------------------------------------

## Capabilities

URL: https://developer.playcanvas.com/user-manual/xr/capabilities/

WebXR exposes various capabilities and new APIs through Modules, which are integrated into the PlayCanvas Engine for ease of use.

Some of the capabilities can be used either in [VR](/user-manual/xr/vr/) or [AR](/user-manual/xr/ar/), and some are generic for any immersive experience.

## Supported WebXR Modules

| Feature | Description |
|-|-|
| [Anchors](/user-manual/xr/ar/anchors/) | Create anchors in space that are reliably positioned in relation to real-world geometry. |
| [Persistent Anchors](/user-manual/xr/ar/anchors/#persistence) | Allows you to persist anchors between sessions. |
| [Camera Color](/user-manual/xr/ar/camera-color/) | Provides access to a color texture of a view. |
| [Depth Sensing](/user-manual/xr/ar/depth-sensing/) | Provides access to depth texture and distance querying, that can be used for virtual object occlusion with real-world geometry and reliable object placement. |
| [DOM Overlay](/user-manual/xr/ar/dom-overlay/) | For monoscopic screens, allows you to overlay DOM elements over an AR view. |
| [Hand Tracking](/user-manual/xr/hand-tracking/) | Optical hand tracking that tracks each joint of a hand. |
| [Hit Testing](/user-manual/xr/ar/hit-testing/) | Allows you to ray cast real-world geometry using a ray to get the position and rotation of the intersection point. |
| [Image Tracking](/user-manual/xr/ar/image-tracking/) | Dynamic tracking of provided images, their position, and orientation. |
| [Input Sources](/user-manual/xr/input-sources/) | Various input source types such as controllers, hands, screen taps, gaze, and more. |
| [Light Estimation](/user-manual/xr/ar/light-estimation/) | Estimates real-world illumination by providing dominant directional light direction, color, and intensity as well as ambient light information in the form of spherical harmonics. |
| [Mesh Detection](/user-manual/xr/ar/mesh-detection/) | Access to a representation of a real-world geometry in the form of a 3D mesh, with its position, orientation, and semantic labels. This can represent furniture, screens, rooms, and other types of static geometry. |
| [Plane Detection](/user-manual/xr/ar/plane-detection/) | Similar to mesh detection, that provides geometry in the form of planes, their position, orientation, vertices, and semantic labels. This can represent large flat surfaces, such as floors, walls, ceilings, windows, doors, and more. |

## Experimental Features

The WebXR API is constantly evolving and additional APIs get released extending the XR feature set. While the engine is constantly updated with integrations for XR APIs, some of the features might come with delay. For developers willing to experiment with new features, it is possible to enable them by passing relevant `optionalFeatures` flags.

:::warning

Accessing internal, undocumented APIs is subject to engine changes that are not guaranteed to be backwards compatible.

:::

Here is an example of enabling the experimental API for [WebXR Layers](/user-manual/xr/input-sources/):

```javascript
app.xr.start(cameraComponent, pc.XRTYPE_VR, pc.XRSPACE_LOCAL, {
    optionalFeatures: [ 'layers' ],
    callback: (err) => {
        if (err) {
            console.log(err);
            return;
        }

        if (app.xr.session.renderState.layers) {
            // get access to WebXR Layers
        }
    }
});
```

--------------------------------------------------------------------------------

## Hand Tracking

URL: https://developer.playcanvas.com/user-manual/xr/hand-tracking/

If the platform supports [WebXR Hand Input](https://immersive-web.github.io/webxr-hand-input/), then an input source can have associated hand data, which is exposed as an [XrHand](https://api.playcanvas.com/engine/classes/XrHand.html), and its data in the form of [XrFinger](https://api.playcanvas.com/engine/classes/XrFinger.html)s and [XrJoint](https://api.playcanvas.com/engine/classes/XrJoint.html)s for an application developer to use, such as wrist, fingers, joints, tips and events for detecting when hands lose/restore tracking.

<img loading="lazy" src="/img/user-manual/xr/cube-hands.webp" alt="Hand tracking using cube primitives" width="512" />

## Model

Creating a basic hand model:

```javascript
const joints = [];
const hand = inputSource.hand;

if (hand) {
    for (let i = 0; i < hand.joints.length; i++) {
        const entity = new pc.Entity();
        entity.joint = hand.joints[i];
        entity.addComponent('render', { type: 'box' });
        parent.addChild(entity);
        joints.push(entity);
    }
}
```

## Updates

Every frame, joint data can change position, rotation, and other details.

```javascript
for (let i = 0; i < joints.length; i++) {
    const entity = joints[i];
    const joint = entity.joint;
    const radius = joint.radius * 2;
    entity.setLocalScale(radius, radius, radius);
    entity.setPosition(joint.getPosition());
    entity.setRotation(joint.getRotation());
}
```

## Tracking

Hand tracking is subject to the reliability and sophistication of the underlying system. There might be cases when tracking is not possible due to obstructions between cameras and hands, or when hands interlock in a complex way. While Computer Vision techniques are improving, when designing content with hands as an input source, their shortcomings should be taken into consideration.

## Skinning

A skinned mesh for a hand can be used. You can check out [this project](https://playcanvas.com/project/771952/overview/webxr-realistic-hands) as an example:

<img loading="lazy" src="/img/user-manual/xr/skinned-hands.webp" alt="Hand tracking using skinned meshes" width="512" />

--------------------------------------------------------------------------------

## Input Sources

URL: https://developer.playcanvas.com/user-manual/xr/input-sources/

An [XrInputSource](https://api.playcanvas.com/engine/classes/XrInputSource.html) represents an input mechanism that allows the user to interact with a virtual world. Those include but are not limited to handheld controllers, optically tracked hands, gaze-based input methods, and touch screens. However, an input source is not explicitly associated with traditional gamepads, mice or keyboards.

<img loading="lazy" src="/img/user-manual/xr/controllers.webp" alt="Controller models with a Ray" width="720" />

## Accessing Input Sources

A list of input sources is available on the [XrInput](https://api.playcanvas.com/engine/classes/XrInput.html) manager which is created by the [XrManager](https://api.playcanvas.com/engine/classes/XrManager.html):

```javascript
const inputSources = app.xr.input.inputSources;
for (let i = 0; i < inputSources.length; i++) {
    // iterate through available input sources
}
```

Input sources can be added and removed dynamically. This can be done by connecting physical devices or by switching input devices via the underlying platform.

Some input sources are **transient** and have a short lifespan during their primary action. Examples are:

- Touch screen tap in AR session on mobile.
- Gaze + pinch interaction used on devices with eye tracking, such as Apple Vision Pro.
- Gaze VR interaction that is common for simple VR devices.

It is best to subscribe to `add` and `remove` events and then create their visual representation if needed:

```javascript
app.xr.input.on('add', (inputSource) => {
    // input source has been added

    inputSource.once('remove', () => {
        // know when input source has been removed
    });
});
```

## Primary Action (select)

Each input source can have a primary action `select`. For controllers, it is a primary button/trigger. For the touch-screen, it is a tap. For hands, it is a pinch of thumb and index fingers. There are also `selectstart` and `selectend` events which you can subscribe to as follows:

```javascript
inputSource.on('select', () => {
    // primary action
});
```

Or through the input manager:

```javascript
app.xr.input.on('select', (inputSource) => {
    // primary action
});
```

## Ray

Each input source has a ray which has an **origin** where it points from and a **direction** in which it is pointing. A ray is transformed into world space. Some examples of input sources might be, but are not limited to:

- **Controllers** (e.g. Meta Quest Touch), will have a ray originating from the tip of the handheld device and the direction is based on the rotation of the device.
- **Hands** have a ray that originates from a point between the thumb and index tips and points forward. If the underlying system does not provide a ray for hands, the PlayCanvas engine will emulate it. So all hands should have a ray.
- **Screen**-based input. This might be available on mobile devices (mono screen) in AR session types, where the user can interact with the virtual world via a touch screen.
- **Gaze**-based input, such as a mobile phone is inserted into a Google Cardboard style device. It will have an input source with `targetRayMode` set to `pc.XRTARGETRAY_GAZE`, and will originate from the viewer's position and point straight where the user is facing.

<img loading="lazy" src="/img/user-manual/xr/controller-ray.webp" alt="A Ray from a Controller" width="480" />

You can check the type of the target ray:

```javascript
switch (inputSource.targetRayMode) {
    case pc.XRTARGETRAY_SCREEN:
        // screen-based interaction, such as touch-screen on mobile in AR mode
        break;
    case pc.XRTARGETRAY_POINTER:
        // pointer-based, such as hand-held controllers or hands
        break;
    case pc.XRTARGETRAY_GAZE:
        // gaze-based, that is based on viewer device orientation and position
        break;
}
```

Here is an example illustrating how to check whether a ray has intersected with the bounding box of a mesh:

```javascript
// set ray with input source data
ray.set(inputSource.getOrigin(), inputSource.getDirection());

// check if mesh bounding box intersects with ray
if (meshInstance.aabb.intersectsRay(ray)) {
    // input source is pointing at a mesh
}
```

## Grip

Some input sources are associated with a physical handheld device, such as a Meta Quest Touch, and can have position and rotation. Their position and rotation are provided in world space.

This can be used to render a virtual controller that matches real-world controller position and rotation.

```javascript
if (inputSource.grip) {
    // can render device model
    // position and rotate associated entity with model
    entity.setPosition(inputSource.getPosition());
    entity.setRotation(inputSource.getRotation());
}
```

## GamePad

If the platform supports the [WebXR Gamepads Module](https://www.w3.org/TR/webxr-gamepads-module-1/), then an input source might have an associated [GamePad](https://w3c.github.io/gamepad/) object with it, which provides access to its buttons, triggers, axes and other input hardware states:

```javascript
const gamepad = inputSource.gamepad;
if (gamepad) {
    if (gamepad.buttons[0] && gamepad.buttons[0].pressed) {
        // user pressed a button on a gamepad
    }
}
```

## Hands

Check out the dedicated page for [Hand Tracking](/user-manual/xr/hand-tracking/).

## Profiles

Each input source might have a list of strings describing a type of input source, which is described in a [profile registry](https://github.com/immersive-web/webxr-input-profiles/tree/master/packages/registry). Based on this, you can figure out what type of model to render for a handheld device or what capabilities it might have. Additionally, the profile registry lists gamepad mapping details, such as buttons and axes.

```javascript
if (inputSource.profiles.includes('oculus-touch-v2')) {
    // it is an Oculus Touch™ handheld device
}
```

## UI

UI elements such as 3D screens, buttons, scroll views, and other components work well with input sources. Events such as `click` will trigger regardless of input type: mouse, touch, or XR input source.

By default, all input source rays will be used to check for interaction with UI components, but you can disable this using a flag:

```javascript
inputSource.elementInput = false;
```

You can also access a UI entity with which an input source has interacted:

```javascript
const entity = inputSource.elementEntity;
if (entity) {
    // a specific entity that the input source has interacted with
}
```

It is also possible to subscribe to ButtonComponent `select` events, that are fired only by XR input sources, similar to specific mouse or touch events:

```javascript
entity.button.on('selectstart', (evt) => {
    // this button is selected by evt.inputSource
});
```

--------------------------------------------------------------------------------

## Optimizing WebXR applications

URL: https://developer.playcanvas.com/user-manual/xr/optimizing-webxr/

## Introduction

A high and consistent frame rate is critical for making an enjoyable XR experience. When creating VR/AR content, it is more important than ever to test and optimize early and maintain the target frame rate throughout development.

For AR experiences, frame rates must be managed carefully as world tracking sometimes incurs significant performance costs. This is in addition to the typically performance-constrained mobile hardware most users have access to.

For VR experiences, rendering is especially expensive due to the fact that the scene must be rendered once for each view (eye). While PlayCanvas is highly optimized to ensure VR rendering doesn't fully duplicate effort, stereo rendering remains more expensive than mono rendering.

For pass-through experiences, AR and VR requirements are combined, and the underlying system has to perform a lot of computation for positioning, image processing, and overlaying rendered images, in addition to other APIs that can be used: depth sensing, color access, image tracking, and more. And duplicate rendering for each eye.

In addition, modern HMD devices demand high frame rates, like 75Hz or 90Hz and higher, further increasing the need for highly efficient rendering.

PlayCanvas, however, includes several features specifically designed to let your application do more in less time.

### Draw Calls and Batching

Draw Calls are operations when the engine provides the necessary information to the GPU for rendering an object. The more objects you have in the scene, the more draw calls it will require to render each frame. To reduce the number of draw calls, it is recommended to minimize the number of objects in the frame by culling, [static batching](/user-manual/graphics/advanced-rendering/batching/) and [instancing](/user-manual/graphics/advanced-rendering/hardware-instancing/).

### Runtime lightmap generation

Each dynamic light has a per-frame runtime cost. The more lights you have, the higher the cost and the slower your scene will render. By baking lights into lightmaps you can hugely reduce the cost of static lights to that of simply rendering a texture. Lightmaps can be generated offline using your favorite 3D modeling tool or you can use PlayCanvas' built-in [Runtime Lightmapper](/user-manual/graphics/lighting/runtime-lightmaps/).

### Cautious use of real-time shadows

For similar reasons to dynamic lights, dynamic shadows also have a per-frame runtime cost. Omni lights, in particular, have to render the scene 6 times to generate shadow maps. You should avoid having too many lights casting dynamic shadows.

### Watch your fill rate and overdraw

Fill rate refers to the number of pixels that can be filled by the GPU over time (normally per second). If you have expensive fragment shaders (e.g. lots of lights and complex materials) and a high resolution (e.g. a mobile phone with a high device pixel ratio) then your application will spend too much time rendering the scene to maintain a high frame rate.

Overdraw refers to the rendering inefficiency that occurs when multiple layers of pixels are processed for the same screen area. This can happen for valid reasons (multiple layers of blending and/or transparency) or redundant reasons (more distant pixels being overwritten by nearer opaque pixels). For the latter case, you are wasting GPU processing trying to draw pixels that are not visible.

:::tip

Using an extension like [WebGL Insight](https://github.com/3Dparallax/insight) can help you visualize overdraw.

:::

### Garbage collection stalls

Web browsers feature automatic garbage collection of unused JavaScript objects. The PlayCanvas engine is designed to minimize runtime allocations and you should try to do the same in your code. Pre-allocate vectors and other objects and re-use them so that there are not lots of objects created and discarded every frame.

### Profiling Tools

PlayCanvas comes with a built-in [profiler tool](/user-manual/optimization/profiler/). In the Editor, enable the Profiler option in the Launch menu to run your application with profiling active.

### General optimization tips

[Many more optimization guidelines](/user-manual/optimization/guidelines/) are available.

--------------------------------------------------------------------------------

## Platforms

URL: https://developer.playcanvas.com/user-manual/xr/platforms/

WebXR has specific hardware and software requirements. It utilizes device features such as camera sensors, gyroscope, accelerometer, CPU/GPU, CV techniques, as well as OS-level and browser support.

While the market is constantly evolving with platform vendors forever improving their software, we try to ensure that PlayCanvas' WebXR support is up to date, supporting the widest range of platforms.

## Platforms and Capabilities

Below is a list of platforms that PlayCanvas has been tested on:

| Platform         | VR | AR  | Capabilities                                                                                                      |
| ---------------- | -- | --- | ----------------------------------------------------------------------------------------------------------------- |
| Apple Vision Pro | ✅ | ❌ | Hand Tracking                                                                                                     |
| Magic Leap       | ✅ | ✅ | Gamepads                                                                                                          |
| Meta Quest       | ✅ | ✅ | Anchors, Depth Sensing, Gamepads, Hand Tracking, Hit Testing, Mesh Detection, Persistent Anchors, Plane Detection |
| Mobile Android   | ✅ | ✅ | Anchors, Camera Color, Depth Sensing, DOM Overlay, Hit Testing, Image Tracking, Light Estimation, Plane Detection |
| PCVR             | ✅ | ❌ | Gamepads                                                                                                          |
| Pico             | ✅ | ✅ | Anchors, Gamepads, Hand Tracking, Hit Testing, Persistent Anchors, Plane Detection                                |

--------------------------------------------------------------------------------

## Using WebXR in PlayCanvas

URL: https://developer.playcanvas.com/user-manual/xr/using-webxr/

## Support

Browser support for WebXR is not (yet) universal. It can be checked as follows:

```javascript
if (app.xr.supported) {
    // WebXR is supported
}
```

## Starting an XR Session

To start XR session, you can use method on the Camera Component or [XrManager](https://api.playcanvas.com/engine/classes/XrManager.html) on the Application. To start an XR session, you need to provide CameraComponent and provide the type of XR session, reference space, and optional object with additional arguments:

```javascript
app.xr.start(entity.camera, pc.XRTYPE_VR, pc.XRSPACE_LOCALFLOOR);
```

It is an asynchronous operation and is only possible to start on a user interaction, such as a button click, mouse click, or touch:

```javascript
button.on('click', () => {
    // start XR session
});
```

To know when a session is started, you can subscribe to the `start` event:

```javascript
app.xr.on('start', () => {
    // XR session has started
});
```

Session type or reference space might not be available on a particular platform, so it will fail to start the session, providing an error in a callback and firing the `error` event on XrManager:

```javascript
entity.camera.startXr(pc.XRTYPE_VR, pc.XRSPACE_UNBOUNDED, {
    callback: (err) => {
        if (err) {
            // failed to start session
        }
    }
});
```

## Ending an XR Session

Exiting XR can be triggered in various ways. You can trigger an exit of XR from code:

```javascript
app.xr.end();
```

Also, the user might exit XR via some external process like the back button in the browser. [XrManager](https://api.playcanvas.com/engine/classes/XrManager.html) will fire events associated with the session `end`:

```javascript
app.xr.on('end', () => {
    // XR session has ended
});
```

## Session Types

Each platform can support different types of sessions. These are:

- **VR** (Virtual Reality) - Provides some level of viewer tracking and it provides exclusive access to XR Device. This means that the application will be rendered onto a device's frame buffer and not the HTML canvas element.
- **AR** (Augmented Reality) - This type of session provides exclusive access to the XR Device and content is meant to be blended with the real-world environment. In this mode, the camera's clear color should be transparent.

The availability of a session type can change during an application's lifetime, based on devices being plugged in or features on devices being enabled. To check if a session type is available do:

```javascript
if (app.xr.isAvailable(pc.XRTYPE_VR)) {
    // VR is available
}
```

You can subscribe to availability change events too:

```javascript
app.xr.on('available', (type, available) => {
    console.log('XR session', type, 'type is now', available ? 'available' : 'unavailable');
});

// or specific session type
app.xr.on('available:' + pc.XRTYPE_VR, (available) => {
    console.log('XR session VR type is now', available ? 'available' : 'unavailable');
});
```

## Camera Position and Orientation

When you are presenting in XR, the position and orientation of the camera are overwritten by data from the XR session. If you want to implement additional movement and rotation of the camera, you should add a parent entity to your camera and apply your manipulations to that entity.

[Image: Camera Offset]

Position, orientation and rays of different XR objects: input sources, tracked meshes, tracked planes, tracked images, and others, are provided in world space.

## Why can't I enable XR mode automatically?

Entering WebXR is required by browsers to be triggered by a *user action*. That means that it must be in response to a key press, a mouse click or a touch event. For that reason, there is no way to enter XR immediately on loading a page.

--------------------------------------------------------------------------------

## VR

URL: https://developer.playcanvas.com/user-manual/xr/vr/

[Image: VR View]

PlayCanvas also lets you create Virtual Reality (VR) applications.

## Platforms

VR capabilities are available across various platforms: desktop (Chrome, Edge), mobile (Chrome, Samsung) and HMDs (Apple Vision Pro, Meta, Magic Leap, Pico).

:::warning

Due to an issue in WebKit on Apple Vision Pro, you must currently disable `Anti-Alias` in the Scene Settings of your project.

:::

## Getting started with WebXR VR

To start a VR session, device support and availability should be checked first. Then, on a user interaction such as a button click or other input, a VR session can be started:

```javascript
button.element.on('click', () => {
    // check if XR is supported and VR is available
    if (app.xr.supported && app.xr.isAvailable(pc.XRTYPE_VR)) {
        // start AR using a camera component
        entity.camera.startXr(pc.XRTYPE_VR, pc.XRSPACE_LOCALFLOOR);
    }
});
```

Once the user is done, VR mode can be exited by calling:

```javascript
app.xr.end();
```

## Starter Kits

PlayCanvas provides a ‘VR Kit’ project to help you and your VR experience get up and running faster. When creating a new project, simply select ‘VR Kit’ from the New Project dialog.

--------------------------------------------------------------------------------

## Types of VR experiences

URL: https://developer.playcanvas.com/user-manual/xr/vr/types-of-vr/

Different types of VR experiences can have different reference spaces. Reference spaces are coordinate systems used to define the positions and orientations of objects and users within a VR environment. They allow the application to understand and manage spatial relationships in a consistent manner. Refer to the `XRSPACE` constants for more information:

* [`XRSPACE_BOUNDEDFLOOR`](https://api.playcanvas.com/engine/variables/XRSPACE_BOUNDEDFLOOR.html)
* [`XRSPACE_LOCAL`](https://api.playcanvas.com/engine/variables/XRSPACE_LOCAL.html)
* [`XRSPACE_LOCALFLOOR`](https://api.playcanvas.com/engine/variables/XRSPACE_LOCALFLOOR.html)
* [`XRSPACE_UNBOUNDED`](https://api.playcanvas.com/engine/variables/XRSPACE_UNBOUNDED.html)
* [`XRSPACE_VIEWER`](https://api.playcanvas.com/engine/variables/XRSPACE_VIEWER.html)

## Room Scale VR

Room Scale VR is supported by devices such as the Meta Quest, Apple Vision Pro, HTC Vive and many others. Room scale experiences are designed to allow a small or large amount of movement away from the origin position of the scene.

Suitable reference spaces: `pc.XRSPACE_LOCALFLOOR`, `pc.XRSPACE_BOUNDEDFLOOR`, `pc.XRSPACE_UNBOUNDED`.

## Seated VR

Seated VR or 3DoF (3 degrees of freedom) is supported by devices such as Google Cardboard, Samsung Gear VR and Oculus Rift. These experiences are based around the user remaining roughly stationary. In some cases, there is no positional data available (e.g. Google Cardboard) and only the orientation of the headset is used.

Suitable reference spaces: `pc.XRSPACE_LOCAL`, `pc.XRSPACE_LOCALFLOOR`.

--------------------------------------------------------------------------------
