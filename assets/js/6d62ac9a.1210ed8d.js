"use strict";(globalThis.webpackChunkdeveloper_playcanvas_com=globalThis.webpackChunkdeveloper_playcanvas_com||[]).push([[36395],{28453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>c});var t=s(96540);const a={},r=t.createContext(a);function i(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),t.createElement(r.Provider,{value:n},e.children)}},41459:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>h,frontMatter:()=>i,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"user-manual/xr/ar/mesh-detection","title":"Mesh Detection","description":"Interaction between real-world and virtual objects is achieved via visual and logical interactions between the two. Mesh detection is an API that provides access to the representation of real-world geometry in the form of meshes. It can be used in a number of ways such as:","source":"@site/docs/user-manual/xr/ar/mesh-detection.md","sourceDirName":"user-manual/xr/ar","slug":"/user-manual/xr/ar/mesh-detection","permalink":"/user-manual/xr/ar/mesh-detection","draft":false,"unlisted":false,"editUrl":"https://github.com/playcanvas/developer.playcanvas.com/tree/dev/docs/user-manual/xr/ar/mesh-detection.md","tags":[],"version":"current","frontMatter":{"title":"Mesh Detection"},"sidebar":"userManualSidebar","previous":{"title":"Light Estimation","permalink":"/user-manual/xr/ar/light-estimation"},"next":{"title":"Plane Detection","permalink":"/user-manual/xr/ar/plane-detection"}}');var a=s(74848),r=s(28453);const i={title:"Mesh Detection"},c=void 0,o={},l=[{value:"Support",id:"support",level:2},{value:"Access",id:"access",level:2},{value:"Mesh",id:"mesh",level:2},{value:"Semantic Label",id:"semantic-label",level:2},{value:"Changes",id:"changes",level:2}];function d(e){const n={a:"a",code:"code",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:"Interaction between real-world and virtual objects is achieved via visual and logical interactions between the two. Mesh detection is an API that provides access to the representation of real-world geometry in the form of meshes. It can be used in a number of ways such as:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Virtual object physics within a real-world environment"}),"\n",(0,a.jsx)(n.li,{children:"Path finding"}),"\n",(0,a.jsx)(n.li,{children:"Object placement"}),"\n",(0,a.jsx)(n.li,{children:"Occlusion"}),"\n",(0,a.jsx)(n.li,{children:"Procedural effects"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"This API provides a list of meshes, their geometry, transformation and semantic labeling."}),"\n",(0,a.jsx)(n.p,{children:"The underlying system can provide pre-captured data as well as provide real-time reconstruction depending on the underlying system implementation."}),"\n",(0,a.jsx)(n.h2,{id:"support",children:"Support"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:"if (app.xr.meshDetection.supported) {\n    // mesh detection is supported\n}\n\napp.xr.on('start', () => {\n    if (app.xr.meshDetection.available) {\n        // mesh detection is available\n    }\n});\n"})}),"\n",(0,a.jsx)(n.h2,{id:"access",children:"Access"}),"\n",(0,a.jsx)(n.p,{children:"A feature flag needs to be added to the session start:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:"app.xr.start(camera, pc.XRTYPE_AR, pc.XRSPACE_LOCALFLOOR, {\n    meshDetection: true\n});\n"})}),"\n",(0,a.jsx)(n.p,{children:"Meshes are added/removed asynchronously:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:"app.xr.meshDetection.on('add', (xrMesh) => {\n    // a new XrMesh has been added\n\n    xrMesh.once('remove', () => {\n        // an XrMesh has been removed\n    });\n});\n"})}),"\n",(0,a.jsx)(n.p,{children:"Also, the list of XrMeshes is available:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:"const xrMeshes = app.xr.meshDetection.meshes;\nfor (let i = 0; i < xrMeshes.length; i++) {\n    const xrMesh = xrMeshes[i];\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"mesh",children:"Mesh"}),"\n",(0,a.jsx)(n.p,{children:"Each mesh is represented as an instance of XrMesh, which can be added/removed during an active session. It also has data that can be changed during its lifetime."}),"\n",(0,a.jsx)(n.p,{children:"You can access the position and rotation (world-space) of an XrMesh:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:"entity.setPosition(xrMesh.getPosition());\nentity.setRotation(xrMesh.getRotation());\n"})}),"\n",(0,a.jsx)(n.p,{children:"Each XrMesh has its vertices and indices (in local-space), that can be used to construct a visual mesh. An example below creates a visual mesh for each XrMesh and adds it to the root of the scene:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:"app.xr.meshDetection.on('add', (xrMesh) => {\n    // geometry data\n    const mesh = new pc.Mesh(app.graphicsDevice);\n    mesh.clear(true, true); // ensure that mesh is created with dynamic buffers\n    mesh.setPositions(xrMesh.vertices); // set vertices\n    mesh.setNormals(pc.calculateNormals(xrMesh.vertices, xrMesh.indices)); // calculate normals\n    mesh.setIndices(xrMesh.indices); // set indices\n    mesh.update(pc.PRIMITIVE_TRIANGLES); // update buffers\n\n    const material = new pc.StandardMaterial();\n    const meshInstance = new pc.MeshInstance(mesh, material);\n\n    const entity = new pc.Entity();\n\n    // add render component\n    entity.addComponent('render', {\n        meshInstances: [ meshInstance ]\n    });\n\n    // add entity to the scene root\n    app.root.addChild(entity);\n\n    // clean up after XrMesh is removed\n    xrMesh.once('remove', () => {\n        material.destroy();\n        mesh.destroy();\n        entity.destroy();\n    });\n});\n"})}),"\n",(0,a.jsx)(n.h2,{id:"semantic-label",children:"Semantic Label"}),"\n",(0,a.jsxs)(n.p,{children:["XrMesh can represent various real-world objects and a label can help to identify what it represents using its property ",(0,a.jsx)(n.code,{children:"XrMesh.label"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["These labels can be any of: ",(0,a.jsx)(n.code,{children:"floor"}),", ",(0,a.jsx)(n.code,{children:"wall"}),", ",(0,a.jsx)(n.code,{children:"door"}),", ",(0,a.jsx)(n.code,{children:"window"}),", ",(0,a.jsx)(n.code,{children:"table"}),", ",(0,a.jsx)(n.code,{children:"screen"}),", ",(0,a.jsx)(n.code,{children:"global mesh"}),", ",(0,a.jsx)(n.code,{children:"other"}),", and ",(0,a.jsx)(n.code,{children:"more"}),". Here is a ",(0,a.jsx)(n.a,{href:"https://github.com/immersive-web/semantic-labels/blob/master/labels.json",children:"list of semantic labels"}),", although this list is not definitive and the platform can report anything it feels fit."]}),"\n",(0,a.jsx)(n.h2,{id:"changes",children:"Changes"}),"\n",(0,a.jsx)(n.p,{children:"Depending on the underlying system capabilities, the XrMesh geometry can change while an XR session is active. You can subscribe to that event and update a visual mesh accordingly:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:"xrMesh.on('change', () => {\n    // vertices, indices and/or label has been changed\n});\n"})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);